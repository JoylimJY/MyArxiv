{"2025-06-13T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2506.12014v1","updated":"2025-06-13T17:59:39Z","published":"2025-06-13T17:59:39Z","title":"code_transformed: The Influence of Large Language Models on Code","summary":"  Coding remains one of the most fundamental modes of interaction between\nhumans and machines. With the rapid advancement of Large Language Models\n(LLMs), code generation capabilities have begun to significantly reshape\nprogramming practices. This development prompts a central question: Have LLMs\ntransformed code style, and how can such transformation be characterized? In\nthis paper, we present a pioneering study that investigates the impact of LLMs\non code style, with a focus on naming conventions, complexity, maintainability,\nand similarity. By analyzing code from over 19,000 GitHub repositories linked\nto arXiv papers published between 2020 and 2025, we identify measurable trends\nin the evolution of coding style that align with characteristics of\nLLM-generated code. For instance, the proportion of snake\\_case variable names\nin Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we\ninvestigate how LLMs approach algorithmic problems by examining their reasoning\nprocesses. Given the diversity of LLMs and usage scenarios, among other\nfactors, it is difficult or even impossible to precisely estimate the\nproportion of code generated or assisted by LLMs. Our experimental results\nprovide the first large-scale empirical evidence that LLMs affect real-world\nprogramming style.\n","authors":["Yuliang Xu","Siming Huang","Mingmeng Geng","Yao Wan","Xuanhua Shi","Dongping Chen"],"pdf_url":"https://arxiv.org/pdf/2506.12014v1.pdf","comment":"We release all the experimental dataset and source code at:\n  https://github.com/ignorancex/LLM_code"},{"id":"http://arxiv.org/abs/2506.06266v3","updated":"2025-06-13T17:58:55Z","published":"2025-06-06T17:48:23Z","title":"Cartridges: Lightweight and general-purpose long context representations\n  via self-study","summary":"  Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining.\n","authors":["Sabri Eyuboglu","Ryan Ehrlich","Simran Arora","Neel Guha","Dylan Zinsley","Emily Liu","Will Tennien","Atri Rudra","James Zou","Azalia Mirhoseini","Christopher Re"],"pdf_url":"https://arxiv.org/pdf/2506.06266v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11999v1","updated":"2025-06-13T17:54:12Z","published":"2025-06-13T17:54:12Z","title":"Generative Representational Learning of Foundation Models for\n  Recommendation","summary":"  Developing a single foundation model with the capability to excel across\ndiverse tasks has been a long-standing objective in the field of artificial\nintelligence. As the wave of general-purpose foundation models sweeps across\nvarious domains, their influence has significantly extended to the field of\nrecommendation systems. While recent efforts have explored recommendation\nfoundation models for various generative tasks, they often overlook crucial\nembedding tasks and struggle with the complexities of multi-task learning,\nincluding knowledge sharing & conflict resolution, and convergence speed\ninconsistencies. To address these limitations, we introduce RecFound, a\ngenerative representational learning framework for recommendation foundation\nmodels. We construct the first comprehensive dataset for recommendation\nfoundation models covering both generative and embedding tasks across diverse\nscenarios. Based on this dataset, we propose a novel multi-task training scheme\nfeaturing a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge\nsharing & conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched)\nto address inconsistent convergence, and a Model Merge module to balance the\nperformance across tasks. Experiments demonstrate that RecFound achieves\nstate-of-the-art performance across various recommendation tasks, outperforming\nexisting baselines.\n","authors":["Zheli Zhou","Chenxu Zhu","Jianghao Lin","Bo Chen","Ruiming Tang","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2506.11999v1.pdf","comment":"Project page is available at https://junkfood436.github.io/RecFound/"},{"id":"http://arxiv.org/abs/2506.11991v1","updated":"2025-06-13T17:47:43Z","published":"2025-06-13T17:47:43Z","title":"VGR: Visual Grounded Reasoning","summary":"  In the field of multimodal chain-of-thought (CoT) reasoning, existing\napproaches predominantly rely on reasoning on pure language space, which\ninherently suffers from language bias and is largely confined to math or\nscience domains. This narrow focus limits their ability to handle complex\nvisual reasoning tasks that demand comprehensive understanding of image\ndetails. To address these limitations, this paper introduces VGR, a novel\nreasoning multimodal large language model (MLLM) with enhanced fine-grained\nvisual perception capabilities. Unlike traditional MLLMs that answer the\nquestion or reasoning solely on the language space, our VGR first detects\nrelevant regions that may help to solve problems, and then provides precise\nanswers based on replayed image regions. To achieve this, we conduct a\nlarge-scale SFT dataset called VGR -SFT that contains reasoning data with mixed\nvision grounding and language deduction. The inference pipeline of VGR allows\nthe model to choose bounding boxes for visual reference and a replay stage is\nintroduced to integrates the corresponding regions into the reasoning process,\nenhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline\nshow that VGR achieves superior performance on multi-modal benchmarks requiring\ncomprehensive image detail understanding. Compared to the baseline, VGR uses\nonly 30\\% of the image token count while delivering scores of +4.1 on MMStar,\n+7.1 on AI2D, and a +12.9 improvement on ChartQA.\n","authors":["Jiacong Wang","Zijiang Kang","Haochen Wang","Haiyong Jiang","Jiawen Li","Bohong Wu","Ya Wang","Jiao Ran","Xiao Liang","Chao Feng","Jun Xiao"],"pdf_url":"https://arxiv.org/pdf/2506.11991v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2506.11986v1","updated":"2025-06-13T17:46:02Z","published":"2025-06-13T17:46:02Z","title":"Schema-R1: A reasoning training approach for schema linking in\n  Text-to-SQL Task","summary":"  Schema linking is a critical step in Text-to-SQL task, aiming to accurately\npredict the table names and column names required for the SQL query based on\nthe given question. However, current fine-tuning approaches for schema linking\nmodels employ a rote-learning paradigm, excessively optimizing for ground truth\nschema linking outcomes while compromising reasoning ability. This limitation\narises because of the difficulty in acquiring a high-quality reasoning sample\nfor downstream tasks. To address this, we propose Schema-R1, a reasoning schema\nlinking model trained using reinforcement learning. Specifically, Schema-R1\nconsists of three key steps: constructing small batches of high-quality\nreasoning samples, supervised fine-tuning for cold-start initialization, and\nrule-based reinforcement learning training. The final results demonstrate that\nour method effectively enhances the reasoning ability of the schema linking\nmodel, achieving a 10\\% improvement in filter accuracy compared to the existing\nmethod. Our code is available at https://github.com/hongWin/Schema-R1/.\n","authors":["Wuzhenghong Wen","Su Pan","yuwei Sun"],"pdf_url":"https://arxiv.org/pdf/2506.11986v1.pdf","comment":"11 pages, 3 figures, conference"},{"id":"http://arxiv.org/abs/2506.09026v2","updated":"2025-06-13T17:44:03Z","published":"2025-06-10T17:52:42Z","title":"e3: Learning to Explore Enables Extrapolation of Test-Time Compute for\n  LLMs","summary":"  Test-time scaling offers a promising path to improve LLM reasoning by\nutilizing more compute at inference time; however, the true promise of this\nparadigm lies in extrapolation (i.e., improvement in performance on hard\nproblems as LLMs keep \"thinking\" for longer, beyond the maximum token budget\nthey were trained on). Surprisingly, we find that most existing reasoning\nmodels do not extrapolate well. We show that one way to enable extrapolation is\nby training the LLM to perform in-context exploration: training the LLM to\neffectively spend its test time budget by chaining operations (such as\ngeneration, verification, refinement, etc.), or testing multiple hypotheses\nbefore it commits to an answer. To enable in-context exploration, we identify\nthree key ingredients as part of our recipe e3: (1) chaining skills that the\nbase LLM has asymmetric competence in, e.g., chaining verification (easy) with\ngeneration (hard), as a way to implement in-context search; (2) leveraging\n\"negative\" gradients from incorrect traces to amplify exploration during RL,\nresulting in longer search traces that chains additional asymmetries; and (3)\ncoupling task difficulty with training token budget during training via a\nspecifically-designed curriculum to structure in-context exploration. Our\nrecipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25\nscores, and extrapolates to 2x the training token budget. Our e3-1.7B model not\nonly attains high pass@1 scores, but also improves pass@k over the base model.\n","authors":["Amrith Setlur","Matthew Y. R. Yang","Charlie Snell","Jeremy Greer","Ian Wu","Virginia Smith","Max Simchowitz","Aviral Kumar"],"pdf_url":"https://arxiv.org/pdf/2506.09026v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.07833v2","updated":"2025-06-13T17:24:38Z","published":"2025-06-09T14:55:00Z","title":"Improving Large Language Models with Concept-Aware Fine-Tuning","summary":"  Large language models (LLMs) have become the cornerstone of modern AI.\nHowever, the existing paradigm of next-token prediction fundamentally limits\ntheir ability to form coherent, high-level concepts, making it a critical\nbarrier to human-like understanding and reasoning. Take the phrase \"ribonucleic\nacid\" as an example: an LLM will first decompose it into tokens, i.e.,\nartificial text fragments (\"rib\", \"on\", ...), then learn each token\nsequentially, rather than grasping the phrase as a unified, coherent semantic\nentity. This fragmented representation hinders deeper conceptual understanding\nand, ultimately, the development of truly intelligent systems. In response, we\nintroduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method\nthat redefines how LLMs are fine-tuned. By enabling the learning of sequences\nthat span multiple tokens, this method fosters stronger concept-aware learning.\nOur experiments demonstrate significant improvements compared to conventional\nnext-token finetuning methods across diverse tasks, including traditional\napplications like text summarization and domain-specific ones like de novo\nprotein design. Multi-token prediction was previously only possible in the\nprohibitively expensive pretraining phase; CAFT, to our knowledge, is the first\nto bring the multi-token setting to the post-training phase, thus effectively\ndemocratizing its benefits for the broader community of practitioners and\nresearchers. Finally, the unexpected effectiveness of our proposed method\nsuggests wider implications for the machine learning research community. All\ncode and data are available at https://github.com/michaelchen-lab/caft-llm\n","authors":["Michael K. Chen","Xikun Zhang","Jiaxing Huang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2506.07833v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17076v3","updated":"2025-06-13T17:21:25Z","published":"2025-05-20T06:01:19Z","title":"Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and\n  English","summary":"  The speech tokenizer plays a crucial role in recent speech tasks, generally\nserving as a bridge between speech signals and language models. While\nlow-frame-rate codecs are widely employed as speech tokenizers, the impact of\nframe rates on speech tokens remains underexplored. In this study, we\ninvestigate how varying frame rates affect speech tokenization by examining\nMandarin and English, two typologically distinct languages. We encode speech at\ndifferent frame rates and evaluate the resulting semantic tokens in the speech\nrecognition task. Our findings reveal that frame rate variations influence\nspeech tokenization differently for each language, highlighting the interplay\nbetween frame rates, phonetic density, and language-specific acoustic features.\nThe results provide insights into optimizing frame rate selection for speech\ntokenizers, with implications for automatic speech recognition, text-to-speech,\nand other speech-related applications.\n","authors":["Haoyang Zhang","Hexin Liu","Xiangyu Zhang","Qiquan Zhang","Yuchen Hu","Junqi Zhao","Fei Tian","Xuerui Yang","Leibny Paola Garcia","Eng Siong Chng"],"pdf_url":"https://arxiv.org/pdf/2505.17076v3.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.01220v5","updated":"2025-06-13T16:58:26Z","published":"2025-02-03T10:24:55Z","title":"Factual Knowledge in Language Models: Robustness and Anomalies under\n  Simple Temporal Context Variations","summary":"  This paper explores the robustness of language models (LMs) to variations in\nthe temporal context within factual knowledge. It examines whether LMs can\ncorrectly associate a temporal context with a past fact valid over a defined\nperiod, by asking them to differentiate correct from incorrect contexts. The\nLMs' ability to distinguish is analyzed along two dimensions: the distance of\nthe incorrect context from the validity period and the granularity of the\ncontext. To this end, a dataset called TimeStress is introduced, enabling the\nevaluation of 18 diverse LMs. Results reveal that the best LM achieves a\nperfect distinction for only 11% of the studied facts, with errors, certainly\nrare, but critical that humans would not make. This work highlights the\nlimitations of current LMs in temporal representation.\n","authors":["Hichem Ammar Khodja","Frédéric Béchet","Quentin Brabant","Alexis Nasr","Gwénolé Lecorvé"],"pdf_url":"https://arxiv.org/pdf/2502.01220v5.pdf","comment":"preprint v5, accepted for publication at ACL 2025 - L2M2 Workshop"},{"id":"http://arxiv.org/abs/2504.11190v2","updated":"2025-06-13T16:49:35Z","published":"2025-04-15T13:47:55Z","title":"Enhancing multimodal analogical reasoning with Logic Augmented\n  Generation","summary":"  Recent advances in Large Language Models have demonstrated their capabilities\nacross a variety of tasks. However, automatically extracting implicit knowledge\nfrom natural language remains a significant challenge, as machines lack active\nexperience with the physical world. Given this scenario, semantic knowledge\ngraphs can serve as conceptual spaces that guide the automated text generation\nreasoning process to achieve more efficient and explainable results. In this\npaper, we apply a logic-augmented generation (LAG) framework that leverages the\nexplicit representation of a text through a semantic knowledge graph and\napplies it in combination with prompt heuristics to elicit implicit analogical\nconnections. This method generates extended knowledge graph triples\nrepresenting implicit meaning, enabling systems to reason on unlabeled\nmultimodal data regardless of the domain. We validate our work through three\nmetaphor detection and understanding tasks across four datasets, as they\nrequire deep analogical reasoning capabilities. The results show that this\nintegrated approach surpasses current baselines, performs better than humans in\nunderstanding visual metaphors, and enables more explainable reasoning\nprocesses, though still has inherent limitations in metaphor understanding,\nespecially for domain-specific metaphors. Furthermore, we propose a thorough\nerror analysis, discussing issues with metaphorical annotations and current\nevaluation methods.\n","authors":["Anna Sofia Lippolis","Andrea Giovanni Nuzzolese","Aldo Gangemi"],"pdf_url":"https://arxiv.org/pdf/2504.11190v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.21657v2","updated":"2025-06-13T16:43:15Z","published":"2025-05-27T18:32:38Z","title":"Explainability of Large Language Models using SMILE: Statistical\n  Model-agnostic Interpretability with Local Explanations","summary":"  Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy.\n","authors":["Zeinab Dehghani","Mohammed Naveed Akram","Koorosh Aslansefat","Adil Khan"],"pdf_url":"https://arxiv.org/pdf/2505.21657v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2412.16277"},{"id":"http://arxiv.org/abs/2506.11938v1","updated":"2025-06-13T16:42:09Z","published":"2025-06-13T16:42:09Z","title":"Improving Large Language Model Safety with Contrastive Representation\n  Learning","summary":"  Large Language Models (LLMs) are powerful tools with profound societal\nimpacts, yet their ability to generate responses to diverse and uncontrolled\ninputs leaves them vulnerable to adversarial attacks. While existing defenses\noften struggle to generalize across varying attack types, recent advancements\nin representation engineering offer promising alternatives. In this work, we\npropose a defense framework that formulates model defense as a contrastive\nrepresentation learning (CRL) problem. Our method finetunes a model using a\ntriplet-based loss combined with adversarial hard negative mining to encourage\nseparation between benign and harmful representations. Our experimental results\nacross multiple models demonstrate that our approach outperforms prior\nrepresentation engineering-based defenses, improving robustness against both\ninput-level and embedding-space attacks without compromising standard\nperformance. Our code is available at\nhttps://github.com/samuelsimko/crl-llm-defense\n","authors":["Samuel Simko","Mrinmaya Sachan","Bernhard Schölkopf","Zhijing Jin"],"pdf_url":"https://arxiv.org/pdf/2506.11938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11930v1","updated":"2025-06-13T16:31:51Z","published":"2025-06-13T16:31:51Z","title":"Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback","summary":"  Recent studies have shown LLMs possess some ability to improve their\nresponses when given external feedback. However, it remains unclear how\neffectively and thoroughly these models can incorporate extrinsic feedback. In\nan ideal scenario, if LLMs receive near-perfect and complete feedback, we would\nexpect them to fully integrate the feedback and change their incorrect answers\nto correct ones. In this paper, we systematically investigate LLMs' ability to\nincorporate feedback by designing a controlled experimental environment. For\neach problem, a solver model attempts a solution, then a feedback generator\nwith access to near-complete ground-truth answers produces targeted feedback,\nafter which the solver tries again. We evaluate this pipeline across a diverse\nrange of tasks, including math reasoning, knowledge reasoning, scientific\nreasoning, and general multi-domain evaluations with state-of-the-art language\nmodels including Claude 3.7 (with and without extended thinking). Surprisingly,\neven under these near-ideal conditions, solver models consistently show\nresistance to feedback, a limitation that we term FEEDBACK FRICTION. To\nmitigate this limitation, we experiment with sampling-based strategies like\nprogressive temperature increases and explicit rejection of previously\nattempted incorrect answers, which yield improvements but still fail to help\nmodels achieve target performance. We also perform a rigorous exploration of\npotential causes of FEEDBACK FRICTION, ruling out factors such as model\noverconfidence and data familiarity. We hope that highlighting this issue in\nLLMs and ruling out several apparent causes will help future research in\nself-improvement.\n","authors":["Dongwei Jiang","Alvin Zhang","Andrew Wang","Nicholas Andrews","Daniel Khashabi"],"pdf_url":"https://arxiv.org/pdf/2506.11930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11928v1","updated":"2025-06-13T16:29:09Z","published":"2025-06-13T16:29:09Z","title":"LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive\n  Programming?","summary":"  Recent reports claim that large language models (LLMs) now outperform elite\nhumans in competitive programming. Drawing on knowledge from a group of\nmedalists in international algorithmic contests, we revisit this claim,\nexamining how LLMs differ from human experts and where limitations still\nremain. We introduce LiveCodeBench Pro, a benchmark composed of problems from\nCodeforces, ICPC, and IOI that are continuously updated to reduce the\nlikelihood of data contamination. A team of Olympiad medalists annotates every\nproblem for algorithmic categories and conducts a line-by-line analysis of\nfailed model-generated submissions. Using this new data and benchmark, we find\nthat frontier models still have significant limitations: without external\ntools, the best model achieves only 53% pass@1 on medium-difficulty problems\nand 0% on hard problems, domains where expert humans still excel. We also find\nthat LLMs succeed at implementation-heavy problems but struggle with nuanced\nalgorithmic reasoning and complex case analysis, often generating confidently\nincorrect justifications. High performance appears largely driven by\nimplementation precision and tool augmentation, not superior reasoning.\nLiveCodeBench Pro thus highlights the significant gap to human grandmaster\nlevels, while offering fine-grained diagnostics to steer future improvements in\ncode-centric LLM reasoning.\n","authors":["Zihan Zheng","Zerui Cheng","Zeyu Shen","Shang Zhou","Kaiyuan Liu","Hansen He","Dongruixuan Li","Stanley Wei","Hangyi Hao","Jianzhu Yao","Peiyao Sheng","Zixuan Wang","Wenhao Chai","Aleksandra Korolova","Peter Henderson","Sanjeev Arora","Pramod Viswanath","Jingbo Shang","Saining Xie"],"pdf_url":"https://arxiv.org/pdf/2506.11928v1.pdf","comment":"Project Page at https://livecodebenchpro.com/"},{"id":"http://arxiv.org/abs/2501.11651v2","updated":"2025-06-13T16:15:45Z","published":"2025-01-20T18:33:33Z","title":"T1: Advancing Language Model Reasoning through Reinforcement Learning\n  and Inference Scaling","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks. However, existing approaches mainly rely on imitation\nlearning and struggle to achieve effective test-time scaling. While\nreinforcement learning (RL) holds promise for enabling self-exploration, recent\nattempts yield modest improvements in complex reasoning. In this paper, we\npresent T1 to scale RL by encouraging exploration and understand inference\nscaling. We first initialize the LLM using synthesized chain-of-thought data\nthat integrates trial-and-error and self-verification. To scale RL training, we\npromote increased sampling diversity through oversampling. We demonstrate that\nT1 with open LLMs as its base exhibits inference scaling behavior and achieves\nsuperior performance on challenging math reasoning benchmarks. More\nimportantly, we present a simple strategy to examine inference scaling, where\nincreased inference budgets directly lead to T1's better performance without\nany additional verification.\n","authors":["Zhenyu Hou","Xin Lv","Rui Lu","Jiajie Zhang","Yujiang Li","Zijun Yao","Juanzi Li","Jie Tang","Yuxiao Dong"],"pdf_url":"https://arxiv.org/pdf/2501.11651v2.pdf","comment":"Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2506.11919v1","updated":"2025-06-13T16:11:04Z","published":"2025-06-13T16:11:04Z","title":"Effectiveness of Counter-Speech against Abusive Content: A\n  Multidimensional Annotation and Classification Study","summary":"  Counter-speech (CS) is a key strategy for mitigating online Hate Speech (HS),\nyet defining the criteria to assess its effectiveness remains an open\nchallenge. We propose a novel computational framework for CS effectiveness\nclassification, grounded in social science concepts. Our framework defines six\ncore dimensions - Clarity, Evidence, Emotional Appeal, Rebuttal, Audience\nAdaptation, and Fairness - which we use to annotate 4,214 CS instances from two\nbenchmark datasets, resulting in a novel linguistic resource released to the\ncommunity. In addition, we propose two classification strategies, multi-task\nand dependency-based, achieving strong results (0.94 and 0.96 average F1\nrespectively on both expert- and user-written CS), outperforming standard\nbaselines, and revealing strong interdependence among dimensions.\n","authors":["Greta Damo","Elena Cabrio","Serena Villata"],"pdf_url":"https://arxiv.org/pdf/2506.11919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11903v1","updated":"2025-06-13T15:53:17Z","published":"2025-06-13T15:53:17Z","title":"GeistBERT: Breathing Life into German NLP","summary":"  Advances in transformer-based language models have highlighted the benefits\nof language-specific pre-training on high-quality corpora. In this context,\nGerman NLP stands to gain from updated architectures and modern datasets\ntailored to the linguistic characteristics of the German language. GeistBERT\nseeks to improve German language processing by incrementally training on a\ndiverse corpus and optimizing model performance across various NLP tasks. It\nwas pre-trained using fairseq with standard hyperparameters, initialized from\nGottBERT weights, and trained on a large-scale German corpus using Whole Word\nMasking (WWM). Based on the pre-trained model, we derived extended-input\nvariants using Nystr\\\"omformer and Longformer architectures with support for\nsequences up to 8k tokens. While these long-context models were not evaluated\non dedicated long-context benchmarks, they are included in our release. We\nassessed all models on NER (CoNLL 2003, GermEval 2014) and text classification\n(GermEval 2018 fine/coarse, 10kGNAD) using $F_1$ score and accuracy. The\nGeistBERT models achieved strong performance, leading all tasks among the base\nmodels and setting a new state-of-the-art (SOTA). Notably, the base models\noutperformed larger models in several tasks. To support the German NLP research\ncommunity, we are releasing GeistBERT under the MIT license.\n","authors":["Raphael Scheible-Schmitt","Johann Frei"],"pdf_url":"https://arxiv.org/pdf/2506.11903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11902v1","updated":"2025-06-13T15:52:37Z","published":"2025-06-13T15:52:37Z","title":"TreeRL: LLM Reinforcement Learning with On-Policy Tree Search","summary":"  Reinforcement learning (RL) with tree search has demonstrated superior\nperformance in traditional reasoning tasks. Compared to conventional\nindependent chain sampling strategies with outcome supervision, tree search\nenables better exploration of the reasoning space and provides dense, on-policy\nprocess rewards during RL training but remains under-explored in On-Policy LLM\nRL. We propose TreeRL, a reinforcement learning framework that directly\nincorporates on-policy tree search for RL training. Our approach includes\nintermediate supervision and eliminates the need for a separate reward model\ntraining. Existing approaches typically train a separate process reward model,\nwhich can suffer from distribution mismatch and reward hacking. We also\nintroduce a cost-effective tree search approach that achieves higher search\nefficiency under the same generation token budget by strategically branching\nfrom high-uncertainty intermediate steps rather than using random branching.\nExperiments on challenging math and code reasoning benchmarks demonstrate that\nTreeRL achieves superior performance compared to traditional ChainRL,\nhighlighting the potential of tree search for LLM. TreeRL is open-sourced at\nhttps://github.com/THUDM/TreeRL.\n","authors":["Zhenyu Hou","Ziniu Hu","Yujiang Li","Rui Lu","Jie Tang","Yuxiao Dong"],"pdf_url":"https://arxiv.org/pdf/2506.11902v1.pdf","comment":"Accepted to ACL 2025 main conference"},{"id":"http://arxiv.org/abs/2501.18638v2","updated":"2025-06-13T15:44:43Z","published":"2025-01-28T17:10:20Z","title":"Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt\n  Generation for Enhanced LLM Content Moderation","summary":"  As large language models (LLMs) become increasingly prevalent, ensuring their\nrobustness against adversarial misuse is crucial. This paper introduces the GAP\n(Graph of Attacks with Pruning) framework, an advanced approach for generating\nstealthy jailbreak prompts to evaluate and enhance LLM safeguards. GAP\naddresses limitations in existing tree-based LLM jailbreak methods by\nimplementing an interconnected graph structure that enables knowledge sharing\nacross attack paths. Our experimental evaluation demonstrates GAP's superiority\nover existing techniques, achieving a 20.8% increase in attack success rates\nwhile reducing query costs by 62.7%. GAP consistently outperforms\nstate-of-the-art methods for attacking both open and closed LLMs, with attack\nsuccess rates of >96%. Additionally, we present specialized variants like\nGAP-Auto for automated seed generation and GAP-VLM for multimodal attacks.\nGAP-generated prompts prove highly effective in improving content moderation\nsystems, increasing true positive detection rates by 108.5% and accuracy by\n183.6% when used for fine-tuning. Our implementation is available at\nhttps://github.com/dsbuddy/GAP-LLM-Safety.\n","authors":["Daniel Schwartz","Dmitriy Bespalov","Zhe Wang","Ninad Kulkarni","Yanjun Qi"],"pdf_url":"https://arxiv.org/pdf/2501.18638v2.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2506.11887v1","updated":"2025-06-13T15:36:22Z","published":"2025-06-13T15:36:22Z","title":"Towards a Cascaded LLM Framework for Cost-effective Human-AI\n  Decision-Making","summary":"  Effective human-AI decision-making balances three key factors: the\n\\textit{correctness} of predictions, the \\textit{cost} of knowledge and\nreasoning complexity, and the confidence about whether to \\textit{abstain}\nautomated answers or involve human experts. In this work, we present a cascaded\nLLM decision framework that adaptively delegates tasks across multiple tiers of\nexpertise -- a base model for initial candidate answers, a more capable and\nknowledgeable (but costlier) large model, and a human expert for when the model\ncascade abstains. Our method proceeds in two stages. First, a deferral policy\ndetermines whether to accept the base model's answer or regenerate it with the\nlarge model based on the confidence score. Second, an abstention policy decides\nwhether the cascade model response is sufficiently certain or requires human\nintervention. Moreover, we incorporate an online learning mechanism in the\nframework that can leverage human feedback to improve decision quality over\ntime. We demonstrate this approach to general question-answering (ARC-Easy and\nARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results\nshow that our cascaded strategy outperforms in most cases single-model\nbaselines in accuracy while reducing cost and providing a principled way to\nhandle abstentions.\n","authors":["Claudio Fanconi","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2506.11887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11886v1","updated":"2025-06-13T15:35:54Z","published":"2025-06-13T15:35:54Z","title":"Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache","summary":"  Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise.\n","authors":["Xiaoran Liu","Siyang He","Qiqi Wang","Ruixiao Li","Yuerong Song","Zhigeng Liu","Linlin Li","Qun Liu","Zengfeng Huang","Qipeng Guo","Ziwei He","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2506.11886v1.pdf","comment":"10 pages, 7 figures, work in progress"},{"id":"http://arxiv.org/abs/2506.11880v1","updated":"2025-06-13T15:29:43Z","published":"2025-06-13T15:29:43Z","title":"Addressing Bias in LLMs: Strategies and Application to Fair AI-based\n  Recruitment","summary":"  The use of language technologies in high-stake settings is increasing in\nrecent years, mostly motivated by the success of Large Language Models (LLMs).\nHowever, despite the great performance of LLMs, they are are susceptible to\nethical concerns, such as demographic biases, accountability, or privacy. This\nwork seeks to analyze the capacity of Transformers-based systems to learn\ndemographic biases present in the data, using a case study on AI-based\nautomated recruitment. We propose a privacy-enhancing framework to reduce\ngender information from the learning pipeline as a way to mitigate biased\nbehaviors in the final tools. Our experiments analyze the influence of data\nbiases on systems built on two different LLMs, and how the proposed framework\neffectively prevents trained systems from reproducing the bias in the data.\n","authors":["Alejandro Peña","Julian Fierrez","Aythami Morales","Gonzalo Mancera","Miguel Lopez","Ruben Tolosana"],"pdf_url":"https://arxiv.org/pdf/2506.11880v1.pdf","comment":"Submitted to AIES 2025 (Under Review)"},{"id":"http://arxiv.org/abs/2506.07196v2","updated":"2025-06-13T15:23:25Z","published":"2025-06-08T15:30:04Z","title":"SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical\n  Action Planning","summary":"  Effective evaluation is critical for driving advancements in MLLM research.\nThe surgical action planning (SAP) task, which aims to generate future action\nsequences from visual inputs, demands precise and sophisticated analytical\ncapabilities. Unlike mathematical reasoning, surgical decision-making operates\nin life-critical domains and requires meticulous, verifiable processes to\nensure reliability and patient safety. This task demands the ability to\ndistinguish between atomic visual actions and coordinate complex, long-horizon\nprocedures, capabilities that are inadequately evaluated by current benchmarks.\nTo address this gap, we introduce SAP-Bench, a large-scale, high-quality\ndataset designed to enable multimodal large language models (MLLMs) to perform\ninterpretable surgical action planning. Our SAP-Bench benchmark, derived from\nthe cholecystectomy procedures context with the mean duration of 1137.5s, and\nintroduces temporally-grounded surgical action annotations, comprising the\n1,226 clinically validated action clips (mean duration: 68.7s) capturing five\nfundamental surgical actions across 74 procedures. The dataset provides 1,152\nstrategically sampled current frames, each paired with the corresponding next\naction as multimodal analysis anchors. We propose the MLLM-SAP framework that\nleverages MLLMs to generate next action recommendations from the current\nsurgical scene and natural language instructions, enhanced with injected\nsurgical domain knowledge. To assess our dataset's effectiveness and the\nbroader capabilities of current models, we evaluate seven state-of-the-art\nMLLMs (e.g., OpenAI-o1, GPT-4o, QwenVL2.5-72B, Claude-3.5-Sonnet, GeminiPro2.5,\nStep-1o, and GLM-4v) and reveal critical gaps in next action prediction\nperformance.\n","authors":["Mengya Xu","Zhongzhen Huang","Dillan Imans","Yiru Ye","Xiaofan Zhang","Qi Dou"],"pdf_url":"https://arxiv.org/pdf/2506.07196v2.pdf","comment":"The authors could not reach a consensus on the final version of this\n  paper, necessitating its withdrawal"},{"id":"http://arxiv.org/abs/2504.13615v2","updated":"2025-06-13T15:09:26Z","published":"2025-04-18T10:43:21Z","title":"Long-context Non-factoid Question Answering in Indic Languages","summary":"  Question Answering (QA) tasks, which involve extracting answers from a given\ncontext, are relatively straightforward for modern Large Language Models (LLMs)\nwhen the context is short. However, long contexts pose challenges due to the\nquadratic complexity of the self-attention mechanism. This challenge is\ncompounded in Indic languages, which are often low-resource. This study\nexplores context-shortening techniques, including Open Information Extraction\n(OIE), coreference resolution, Answer Paragraph Selection (APS), and their\ncombinations, to improve QA performance. Compared to the baseline of\nunshortened (long) contexts, our experiments on four Indic languages (Hindi,\nTamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield\nan average improvement of 4\\% in semantic scores and 47\\% in token-level scores\nwhen evaluated on three popular LLMs without fine-tuning. Furthermore, with\nfine-tuning, we achieve an average increase of 2\\% in both semantic and\ntoken-level scores. Additionally, context-shortening reduces computational\noverhead. Explainability techniques like LIME and SHAP reveal that when the APS\nmodel confidently identifies the paragraph containing the answer, nearly all\ntokens within the selected text receive high relevance scores. However, the\nstudy also highlights the limitations of LLM-based QA systems in addressing\nnon-factoid questions, particularly those requiring reasoning or debate.\nMoreover, verbalizing OIE-generated triples does not enhance system\nperformance. These findings emphasize the potential of context-shortening\ntechniques to improve the efficiency and effectiveness of LLM-based QA systems,\nespecially for low-resource languages. The source code and resources are\navailable at https://github.com/ritwikmishra/IndicGenQA.\n","authors":["Ritwik Mishra","Rajiv Ratn Shah","Ponnurangam Kumaraguru"],"pdf_url":"https://arxiv.org/pdf/2504.13615v2.pdf","comment":"Short version of this manuscript accepted at\n  https://bda2025.iiitb.net/"},{"id":"http://arxiv.org/abs/2503.09347v2","updated":"2025-06-13T15:07:08Z","published":"2025-03-12T12:49:02Z","title":"Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts","summary":"  Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments.\n","authors":["Hongyu Chen","Seraphina Goldfarb-Tarrant"],"pdf_url":"https://arxiv.org/pdf/2503.09347v2.pdf","comment":"9 pages, ACL 2025"},{"id":"http://arxiv.org/abs/2506.11857v1","updated":"2025-06-13T15:04:01Z","published":"2025-06-13T15:04:01Z","title":"Post Persona Alignment for Multi-Session Dialogue Generation","summary":"  Multi-session persona-based dialogue generation presents challenges in\nmaintaining long-term consistency and generating diverse, personalized\nresponses. While large language models (LLMs) excel in single-session\ndialogues, they struggle to preserve persona fidelity and conversational\ncoherence across extended interactions. Existing methods typically retrieve\npersona information before response generation, which can constrain diversity\nand result in generic outputs. We propose Post Persona Alignment (PPA), a novel\ntwo-stage framework that reverses this process. PPA first generates a general\nresponse based solely on dialogue context, then retrieves relevant persona\nmemories using the response as a query, and finally refines the response to\nalign with the speaker's persona. This post-hoc alignment strategy promotes\nnaturalness and diversity while preserving consistency and personalization.\nExperiments on multi-session LLM-generated dialogue data demonstrate that PPA\nsignificantly outperforms prior approaches in consistency, diversity, and\npersona relevance, offering a more flexible and effective paradigm for\nlong-term personalized dialogue generation.\n","authors":["Yi-Pei Chen","Noriki Nishida","Hideki Nakayama","Yuji Matsumoto"],"pdf_url":"https://arxiv.org/pdf/2506.11857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00073v3","updated":"2025-06-13T15:02:02Z","published":"2025-05-29T17:41:39Z","title":"The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and\n  Transactions in Consumer Markets","summary":"  AI agents are increasingly used in consumer-facing applications to assist\nwith tasks such as product search, negotiation, and transaction execution. In\nthis paper, we explore a future scenario where both consumers and merchants\nauthorize AI agents to fully automate negotiations and transactions. We aim to\nanswer two key questions: (1) Do different LLM agents vary in their ability to\nsecure favorable deals for users? (2) What risks arise from fully automating\ndeal-making with AI agents in consumer markets? To address these questions, we\ndevelop an experimental framework that evaluates the performance of various LLM\nagents in real-world negotiation and transaction settings. Our findings reveal\nthat AI-mediated deal-making is an inherently imbalanced game -- different\nagents achieve significantly different outcomes for their users. Moreover,\nbehavioral anomalies in LLMs can result in financial losses for both consumers\nand merchants, such as overspending or accepting unreasonable deals. These\nresults underscore that while automation can improve efficiency, it also\nintroduces substantial risks. Users should exercise caution when delegating\nbusiness decisions to AI agents.\n","authors":["Shenzhe Zhu","Jiao Sun","Yi Nian","Tobin South","Alex Pentland","Jiaxin Pei"],"pdf_url":"https://arxiv.org/pdf/2506.00073v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09992v2","updated":"2025-06-13T15:01:28Z","published":"2025-06-11T17:59:33Z","title":"Large Language Models for Toxic Language Detection in Low-Resource\n  Balkan Languages","summary":"  Online toxic language causes real harm, especially in regions with limited\nmoderation tools. In this study, we evaluate how large language models handle\ntoxic comments in Serbian, Croatian, and Bosnian, languages with limited\nlabeled data. We built and manually labeled a dataset of 4,500 YouTube and\nTikTok comments drawn from videos across diverse categories, including music,\npolitics, sports, modeling, influencer content, discussions of sexism, and\ngeneral topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude\n3 Opus) were tested in two modes: zero-shot and context-augmented. We measured\nprecision, recall, F1 score, accuracy and false positive rates. Including a\nshort context snippet raised recall by about 0.12 on average and improved F1\nscore by up to 0.10, though it sometimes increased false positives. The best\nbalance came from Gemini in context-augmented mode, reaching an F1 score of\n0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the\nlowest false alarms. We show how adding minimal context can improve toxic\nlanguage detection in low-resource settings and suggest practical strategies\nsuch as improved prompt design and threshold calibration. These results show\nthat prompt design alone can yield meaningful gains in toxicity detection for\nunderserved Balkan language communities.\n","authors":["Amel Muminovic","Amela Kadric Muminovic"],"pdf_url":"https://arxiv.org/pdf/2506.09992v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2506.11820v1","updated":"2025-06-13T14:23:38Z","published":"2025-06-13T14:23:38Z","title":"Rethinking Multilingual Vision-Language Translation: Dataset,\n  Evaluation, and Adaptation","summary":"  Vision-Language Translation (VLT) is a challenging task that requires\naccurately recognizing multilingual text embedded in images and translating it\ninto the target language with the support of visual context. While recent Large\nVision-Language Models (LVLMs) have demonstrated strong multilingual and visual\nunderstanding capabilities, there is a lack of systematic evaluation and\nunderstanding of their performance on VLT. In this work, we present a\ncomprehensive study of VLT from three key perspectives: data quality, model\narchitecture, and evaluation metrics. (1) We identify critical limitations in\nexisting datasets, particularly in semantic and cultural fidelity, and\nintroduce AibTrans -- a multilingual, parallel, human-verified dataset with\nOCR-corrected annotations. (2) We benchmark 11 commercial LVLMs/LLMs and 6\nstate-of-the-art open-source models across end-to-end and cascaded\narchitectures, revealing their OCR dependency and contrasting generation versus\nreasoning behaviors. (3) We propose Density-Aware Evaluation to address metric\nreliability issues under varying contextual complexity, introducing the DA\nScore as a more robust measure of translation quality. Building upon these\nfindings, we establish a new evaluation benchmark for VLT. Notably, we observe\nthat fine-tuning LVLMs on high-resource language pairs degrades cross-lingual\nperformance, and we propose a balanced multilingual fine-tuning strategy that\neffectively adapts LVLMs to VLT without sacrificing their generalization\nability.\n","authors":["Xintong Wang","Jingheng Pan","Yixiao Liu","Xiaohu Zhao","Chenyang Lyu","Minghao Wu","Chris Biemann","Longyue Wang","Linlong Xu","Weihua Luo","Kaifu Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.11820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11812v1","updated":"2025-06-13T14:14:40Z","published":"2025-06-13T14:14:40Z","title":"On the Performance of LLMs for Real Estate Appraisal","summary":"  The real estate market is vital to global economies but suffers from\nsignificant information asymmetry. This study examines how Large Language\nModels (LLMs) can democratize access to real estate insights by generating\ncompetitive and interpretable house price estimates through optimized\nIn-Context Learning (ICL) strategies. We systematically evaluate leading LLMs\non diverse international housing datasets, comparing zero-shot, few-shot,\nmarket report-enhanced, and hybrid prompting techniques. Our results show that\nLLMs effectively leverage hedonic variables, such as property size and\namenities, to produce meaningful estimates. While traditional machine learning\nmodels remain strong for pure predictive accuracy, LLMs offer a more\naccessible, interactive and interpretable alternative. Although\nself-explanations require cautious interpretation, we find that LLMs explain\ntheir predictions in agreement with state-of-the-art models, confirming their\ntrustworthiness. Carefully selected in-context examples based on feature\nsimilarity and geographic proximity, significantly enhance LLM performance, yet\nLLMs struggle with overconfidence in price intervals and limited spatial\nreasoning. We offer practical guidance for structured prediction tasks through\nprompt optimization. Our findings highlight LLMs' potential to improve\ntransparency in real estate appraisal and provide actionable insights for\nstakeholders.\n","authors":["Margot Geerts","Manon Reusens","Bart Baesens","Seppe vanden Broucke","Jochen De Weerdt"],"pdf_url":"https://arxiv.org/pdf/2506.11812v1.pdf","comment":"Accepted at ECML-PKDD 2025"},{"id":"http://arxiv.org/abs/2506.01602v2","updated":"2025-06-13T14:11:43Z","published":"2025-06-02T12:40:46Z","title":"Word Sense Detection Leveraging Maximum Mean Discrepancy","summary":"  Word sense analysis is an essential analysis work for interpreting the\nlinguistic and social backgrounds. The word sense change detection is a task of\nidentifying and interpreting shifts in word meanings over time. This paper\nproposes MMD-Sense-Analysis, a novel approach that leverages Maximum Mean\nDiscrepancy (MMD) to select semantically meaningful variables and quantify\nchanges across time periods. This method enables both the identification of\nwords undergoing sense shifts and the explanation of their evolution over\nmultiple historical periods. To my knowledge, this is the first application of\nMMD to word sense change detection. Empirical assessment results demonstrate\nthe effectiveness of the proposed approach.\n","authors":["Kensuke Mitsuzawa"],"pdf_url":"https://arxiv.org/pdf/2506.01602v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11807v1","updated":"2025-06-13T14:09:48Z","published":"2025-06-13T14:09:48Z","title":"Are Multimodal Large Language Models Pragmatically Competent Listeners\n  in Simple Reference Resolution Tasks?","summary":"  We investigate the linguistic abilities of multimodal large language models\nin reference resolution tasks featuring simple yet abstract visual stimuli,\nsuch as color patches and color grids. Although the task may not seem\nchallenging for today's language models, being straightforward for human dyads,\nwe consider it to be a highly relevant probe of the pragmatic capabilities of\nMLLMs. Our results and analyses indeed suggest that basic pragmatic\ncapabilities, such as context-dependent interpretation of color descriptions,\nstill constitute major challenges for state-of-the-art MLLMs.\n","authors":["Simeon Junker","Manar Ali","Larissa Koch","Sina Zarrieß","Hendrik Buschmeier"],"pdf_url":"https://arxiv.org/pdf/2506.11807v1.pdf","comment":"To appear in ACL Findings 2025"},{"id":"http://arxiv.org/abs/2506.11798v1","updated":"2025-06-13T14:02:21Z","published":"2025-06-13T14:02:21Z","title":"Persona-driven Simulation of Voting Behavior in the European Parliament\n  with Large Language Models","summary":"  Large Language Models (LLMs) display remarkable capabilities to understand or\neven produce political discourse, but have been found to consistently display a\nprogressive left-leaning bias. At the same time, so-called persona or identity\nprompts have been shown to produce LLM behavior that aligns with socioeconomic\ngroups that the base model is not aligned with. In this work, we analyze\nwhether zero-shot persona prompting with limited information can accurately\npredict individual voting decisions and, by aggregation, accurately predict\npositions of European groups on a diverse set of policies. We evaluate if\npredictions are stable towards counterfactual arguments, different persona\nprompts and generation methods. Finally, we find that we can simulate voting\nbehavior of Members of the European Parliament reasonably well with a weighted\nF1 score of approximately 0.793. Our persona dataset of politicians in the 2024\nEuropean Parliament and our code are available at\nhttps://github.com/dess-mannheim/european_parliament_simulation.\n","authors":["Maximilian Kreutner","Marlene Lutz","Markus Strohmaier"],"pdf_url":"https://arxiv.org/pdf/2506.11798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19175v2","updated":"2025-06-13T13:50:34Z","published":"2025-02-26T14:31:43Z","title":"MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic\n  Differential Diagnosis","summary":"  Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical\ndecision-making, in which physicians iteratively refine a ranked list of\npossible diseases based on symptoms, antecedents, and medical knowledge. While\nrecent advances in large language models (LLMs) have shown promise in\nsupporting DDx, existing approaches face key limitations, including\nsingle-dataset evaluations, isolated optimization of components, unrealistic\nassumptions about complete patient profiles, and single-attempt diagnosis. We\nintroduce a Modular Explainable DDx Agent (MEDDxAgent) framework designed for\ninteractive DDx, where diagnostic reasoning evolves through iterative learning,\nrather than assuming a complete patient profile is accessible. MEDDxAgent\nintegrates three modular components: (1) an orchestrator (DDxDriver), (2) a\nhistory taking simulator, and (3) two specialized agents for knowledge\nretrieval and diagnosis strategy. To ensure robust evaluation, we introduce a\ncomprehensive DDx benchmark covering respiratory, skin, and rare diseases. We\nanalyze single-turn diagnostic approaches and demonstrate the importance of\niterative refinement when patient profiles are not available at the outset. Our\nbroad evaluation demonstrates that MEDDxAgent achieves over 10% accuracy\nimprovements in interactive DDx across both large and small LLMs, while\noffering critical explainability into its diagnostic reasoning process.\n","authors":["Daniel Rose","Chia-Chien Hung","Marco Lepri","Israa Alqassem","Kiril Gashteovski","Carolin Lawrence"],"pdf_url":"https://arxiv.org/pdf/2502.19175v2.pdf","comment":"ACL 2025 (main)"},{"id":"http://arxiv.org/abs/2501.03479v3","updated":"2025-06-13T13:42:41Z","published":"2025-01-07T02:47:59Z","title":"Women, Infamous, and Exotic Beings: What Honorific Usages in Wikipedia\n  Reflect on the Cross-Cultural Sociolinguistic Norms?","summary":"  Wikipedia, as a massively multilingual, community-driven platform, is a\nvaluable resource for Natural Language Processing (NLP), yet the consistency of\nhonorific usage in honorific-rich languages remains underexplored. Honorifics,\nsubtle yet profound linguistic markers, encode social hierarchies, politeness\nnorms, and cultural values, but Wikipedia's editorial guidelines lack clear\nstandards for their usage in languages where such forms are grammatically and\nsocially prevalent. This paper addresses this gap through a large-scale\nanalysis of third-person honorific pronouns and verb forms in Hindi and Bengali\nWikipedia articles. Using Large Language Models (LLM), we automatically\nannotate 10,000 articles per language for honorific usage and socio-demographic\nfeatures such as gender, age, fame, and cultural origin. We investigate: (i)\nthe consistency of honorific usage across articles, (ii) how inconsistencies\ncorrelate with socio-cultural factors, and (iii) the presence of explicit or\nimplicit biases across languages. We find that honorific usage is consistently\nmore common in Bengali than Hindi, while non-honorific forms are more frequent\nfor infamous, juvenile, and exotic entities in both. Notably, gender bias\nemerges in both languages, particularly in Hindi, where men are more likely to\nreceive honorifics than women. Our analysis highlights the need for Wikipedia\nto develop language-specific editorial guidelines for honorific usage.\n","authors":["Sourabrata Mukherjee","Atharva Mehta","Soumya Teotia","Sougata Saha","Akhil Arora","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2501.03479v3.pdf","comment":"Accepted at 2nd WikiNLP: Advancing Natural Language Process for\n  Wikipedia, Co-located with ACL 2025 (non-archival)"},{"id":"http://arxiv.org/abs/2506.11769v1","updated":"2025-06-13T13:25:39Z","published":"2025-06-13T13:25:39Z","title":"Long-Short Alignment for Effective Long-Context Modeling in LLMs","summary":"  Large language models (LLMs) have exhibited impressive performance and\nsurprising emergent properties. However, their effectiveness remains limited by\nthe fixed context window of the transformer architecture, posing challenges for\nlong-context modeling. Among these challenges, length generalization -- the\nability to generalize to sequences longer than those seen during training -- is\na classical and fundamental problem. In this work, we propose a fresh\nperspective on length generalization, shifting the focus from the conventional\nemphasis on input features such as positional encodings or data structures to\nthe output distribution of the model. Specifically, through case studies on\nsynthetic tasks, we highlight the critical role of \\textbf{long-short\nalignment} -- the consistency of output distributions across sequences of\nvarying lengths. Extending this insight to natural language tasks, we propose a\nmetric called Long-Short Misalignment to quantify this phenomenon, uncovering a\nstrong correlation between the metric and length generalization performance.\nBuilding on these findings, we develop a regularization term that promotes\nlong-short alignment during training. Extensive experiments validate the\neffectiveness of our approach, offering new insights for achieving more\neffective long-context modeling in LLMs. Code is available at\nhttps://github.com/PKU-ML/LongShortAlignment.\n","authors":["Tianqi Du","Haotian Huang","Yifei Wang","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2506.11769v1.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2502.20273v3","updated":"2025-06-13T13:24:27Z","published":"2025-02-27T17:01:23Z","title":"How Much is Enough? The Diminishing Returns of Tokenization Training\n  Data","summary":"  Tokenization, a crucial initial step in natural language processing, is\ngoverned by several key parameters, such as the tokenization algorithm,\nvocabulary size, pre-tokenization strategy, inference strategy, and training\ndata corpus. This paper investigates the impact of an often-overlooked\nhyperparameter, tokenizer training data size. We train BPE, UnigramLM, and\nWordPiece tokenizers across various vocabulary sizes using English training\ndata ranging from 1GB to 900GB. Our findings reveal diminishing returns as\ntraining data size increases beyond roughly 150GB, suggesting a practical limit\nto the improvements in tokenization quality achievable through additional data.\nWe analyze this phenomenon and attribute the saturation effect to constraints\nintroduced by the pre-tokenization stage. We then demonstrate the extent to\nwhich these findings can generalize by experimenting on data in Russian, a\nlanguage typologically distant from English. For Russian text, we observe\ndiminishing returns after training a tokenizer from 200GB of data, which is\napproximately 33% more than when training on English. These results provide\nvaluable insights for optimizing the tokenization process by reducing the\ncompute required for training on large corpora and suggest promising directions\nfor future research in tokenization algorithms.\n","authors":["Varshini Reddy","Craig W. Schmidt","Yuval Pinter","Chris Tanner"],"pdf_url":"https://arxiv.org/pdf/2502.20273v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11763v1","updated":"2025-06-13T13:17:32Z","published":"2025-06-13T13:17:32Z","title":"DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents","summary":"  Deep Research Agents are a prominent category of LLM-based agents. By\nautonomously orchestrating multistep web exploration, targeted retrieval, and\nhigher-order synthesis, they transform vast amounts of online information into\nanalyst-grade, citation-rich reports--compressing hours of manual desk research\ninto minutes. However, a comprehensive benchmark for systematically evaluating\nthe capabilities of these agents remains absent. To bridge this gap, we present\nDeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks,\neach meticulously crafted by domain experts across 22 distinct fields.\nEvaluating DRAs is inherently complex and labor-intensive. We therefore propose\ntwo novel methodologies that achieve strong alignment with human judgment. The\nfirst is a reference-based method with adaptive criteria to assess the quality\nof generated research reports. The other framework is introduced to evaluate\nDRA's information retrieval and collection capabilities by assessing its\neffective citation count and overall citation accuracy. We have open-sourced\nDeepResearch Bench and key components of these frameworks at\nhttps://github.com/Ayanami0730/deep_research_bench to accelerate the\ndevelopment of practical LLM-based agents.\n","authors":["Mingxuan Du","Benfeng Xu","Chiwei Zhu","Xiaorui Wang","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2506.11763v1.pdf","comment":"31 pages, 5 figures"},{"id":"http://arxiv.org/abs/2506.11752v1","updated":"2025-06-13T13:05:41Z","published":"2025-06-13T13:05:41Z","title":"DART: Distilling Autoregressive Reasoning to Silent Thought","summary":"  Chain-of-Thought (CoT) reasoning has significantly advanced Large Language\nModels (LLMs) in solving complex tasks. However, its autoregressive paradigm\nleads to significant computational overhead, hindering its deployment in\nlatency-sensitive applications. To address this, we propose \\textbf{DART}\n(\\textbf{D}istilling \\textbf{A}utoregressive \\textbf{R}easoning to Silent\n\\textbf{T}hought), a self-distillation framework that enables LLMs to replace\nautoregressive CoT with non-autoregressive Silent Thought (ST). Specifically,\nDART introduces two training pathways: the CoT pathway for traditional\nreasoning and the ST pathway for generating answers directly from a few ST\ntokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM)\nto align its hidden states with the CoT pathway, enabling the ST tokens to\nevolve into informative embeddings. During inference, only the ST pathway is\nactivated, leveraging evolving ST tokens to deliver the answer directly.\nExtensive experimental results demonstrate that DART achieves comparable\nreasoning performance to existing baselines while offering significant\nefficiency gains, serving as a feasible alternative for efficient reasoning.\n","authors":["Nan Jiang","Ziming Wu","De-Chuan Zhan","Fuming Lai","Shaobing Lian"],"pdf_url":"https://arxiv.org/pdf/2506.11752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12415v2","updated":"2025-06-13T13:02:56Z","published":"2025-05-18T13:40:18Z","title":"Table-R1: Region-based Reinforcement Learning for Table Understanding","summary":"  Tables present unique challenges for language models due to their structured\nrow-column interactions, necessitating specialized approaches for effective\ncomprehension. While large language models (LLMs) have demonstrated potential\nin table reasoning through prompting and techniques like chain-of-thought (CoT)\nand program-of-thought (PoT), optimizing their performance for table question\nanswering remains underexplored. In this paper, we introduce region-based\nTable-R1, a novel reinforcement learning approach that enhances LLM table\nunderstanding by integrating region evidence into reasoning steps. Our method\nemploys Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in\nidentifying relevant table regions before generating answers, incorporating\ntextual, symbolic, and program-based reasoning. Additionally, Table-Aware Group\nRelative Policy Optimization (TARPO) introduces a mixed reward system to\ndynamically balance region accuracy and answer correctness, with decaying\nregion rewards and consistency penalties to align reasoning steps. Experiments\nshow that Table-R1 achieves an average performance improvement of 14.36 points\nacross multiple base models on three benchmark datasets, even outperforming\nbaseline models with ten times the parameters, while TARPO reduces response\ntoken consumption by 67.5% compared to GRPO, significantly advancing LLM\ncapabilities in efficient tabular reasoning.\n","authors":["Zhenhe Wu","Jian Yang","Jiaheng Liu","Xianjie Wu","Changzai Pan","Jie Zhang","Yu Zhao","Shuangyong Song","Yongxiang Li","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2505.12415v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11737v1","updated":"2025-06-13T12:48:39Z","published":"2025-06-13T12:48:39Z","title":"Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in\n  Interleaved Multi-Image Model","summary":"  This paper addresses two main objectives. Firstly, we demonstrate the\nimpressive performance of the LLaVA-NeXT-interleave on 22 datasets across three\ndifferent tasks: Multi-Image Reasoning, Documents and Knowledge-Based\nUnderstanding and Interactive Multi-Modal Communication. Secondly, we add the\nDense Channel Integration (DCI) connector to the LLaVA-NeXT-Interleave and\ncompare its performance against the standard model. We find that the standard\nmodel achieves the highest overall accuracy, excelling in vision-heavy tasks\nlike VISION, NLVR2, and Fashion200K. Meanwhile, the DCI-enhanced version shows\nparticular strength on datasets requiring deeper semantic coherence or\nstructured change understanding such as MIT-States_PropertyCoherence and\nSlideVQA. Our results highlight the potential of combining powerful foundation\nmodels with plug-and-play techniques for Interleave tasks. The code is\navailable at https://github.com/dinhvietcuong1996/icme25-inova.\n","authors":["Dinh Viet Cuong","Hoang-Bao Le","An Pham Ngoc Nguyen","Liting Zhou","Cathal Gurrin"],"pdf_url":"https://arxiv.org/pdf/2506.11737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07595v2","updated":"2025-06-13T12:47:42Z","published":"2024-11-12T07:09:44Z","title":"Entropy Controllable Direct Preference Optimization","summary":"  In the post-training of large language models (LLMs), Reinforcement Learning\nfrom Human Feedback (RLHF) is an effective approach to achieve generation\naligned with human preferences. Direct Preference Optimization (DPO) allows for\npolicy training with a simple binary cross-entropy loss without a reward model.\nThe objective of DPO is regularized by reverse KL divergence that encourages\nmode-seeking fitting to the reference policy. Nonetheless, we indicate that\nminimizing reverse KL divergence could fail to capture a mode of the reference\ndistribution, which may hurt the policy's performance. Based on this\nobservation, we propose a simple modification to DPO, H-DPO, which allows for\ncontrol over the entropy of the resulting policy, enhancing the distribution's\nsharpness and thereby enabling mode-seeking fitting more effectively. In our\nexperiments, we show that H-DPO outperformed DPO across various tasks,\ndemonstrating superior results in pass@$k$ evaluations for mathematical tasks.\nMoreover, H-DPO is simple to implement, requiring only minor modifications to\nthe loss calculation of DPO, which makes it highly practical and promising for\nwide-ranging applications in the training of LLMs.\n","authors":["Motoki Omura","Yasuhiro Fujita","Toshiki Kataoka"],"pdf_url":"https://arxiv.org/pdf/2411.07595v2.pdf","comment":"ICML 2025 Workshop on Models of Human Feedback for AI Alignment"},{"id":"http://arxiv.org/abs/2506.01305v2","updated":"2025-06-13T12:40:58Z","published":"2025-06-02T04:32:15Z","title":"VM14K: First Vietnamese Medical Benchmark","summary":"  Medical benchmarks are indispensable for evaluating the capabilities of\nlanguage models in healthcare for non-English-speaking communities,therefore\nhelp ensuring the quality of real-life applications. However, not every\ncommunity has sufficient resources and standardized methods to effectively\nbuild and design such benchmark, and available non-English medical data is\nnormally fragmented and difficult to verify. We developed an approach to tackle\nthis problem and applied it to create the first Vietnamese medical question\nbenchmark, featuring 14,000 multiple-choice questions across 34 medical\nspecialties. Our benchmark was constructed using various verifiable sources,\nincluding carefully curated medical exams and clinical records, and eventually\nannotated by medical experts. The benchmark includes four difficulty levels,\nranging from foundational biological knowledge commonly found in textbooks to\ntypical clinical case studies that require advanced reasoning. This design\nenables assessment of both the breadth and depth of language models' medical\nunderstanding in the target language thanks to its extensive coverage and\nin-depth subject-specific expertise. We release the benchmark in three parts: a\nsample public set (4k questions), a full public set (10k questions), and a\nprivate set (2k questions) used for leaderboard evaluation. Each set contains\nall medical subfields and difficulty levels. Our approach is scalable to other\nlanguages, and we open-source our data construction pipeline to support the\ndevelopment of future multilingual benchmarks in the medical domain.\n","authors":["Thong Nguyen","Duc Nguyen","Minh Dang","Thai Dao","Long Nguyen","Quan H. Nguyen","Dat Nguyen","Kien Tran","Minh Tran"],"pdf_url":"https://arxiv.org/pdf/2506.01305v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11728v1","updated":"2025-06-13T12:40:16Z","published":"2025-06-13T12:40:16Z","title":"The Cambrian Explosion of Mixed-Precision Matrix Multiplication for\n  Quantized Deep Learning Inference","summary":"  Recent advances in deep learning (DL) have led to a shift from traditional\n64-bit floating point (FP64) computations toward reduced-precision formats,\nsuch as FP16, BF16, and 8- or 16-bit integers, combined with mixed-precision\narithmetic. This transition enhances computational throughput, reduces memory\nand bandwidth usage, and improves energy efficiency, offering significant\nadvantages for resource-constrained edge devices. To support this shift,\nhardware architectures have evolved accordingly, now including adapted ISAs\n(Instruction Set Architectures) that expose mixed-precision vector units and\nmatrix engines tailored for DL workloads. At the heart of many DL and\nscientific computing tasks is the general matrix-matrix multiplication gemm, a\nfundamental kernel historically optimized using axpy vector instructions on\nSIMD (single instruction, multiple data) units. However, as hardware moves\ntoward mixed-precision dot-product-centric operations optimized for quantized\ninference, these legacy approaches are being phased out. In response to this,\nour paper revisits traditional high-performance gemm and describes strategies\nfor adapting it to mixed-precision integer (MIP) arithmetic across modern ISAs,\nincluding x86_64, ARM, and RISC-V. Concretely, we illustrate novel micro-kernel\ndesigns and data layouts that better exploit today's specialized hardware and\ndemonstrate significant performance gains from MIP arithmetic over\nfloating-point implementations across three representative CPU architectures.\nThese contributions highlight a new era of gemm optimization-driven by the\ndemands of DL inference on heterogeneous architectures, marking what we term as\nthe \"Cambrian period\" for matrix multiplication.\n","authors":["Héctor Martínez","Adrián Castelló","Francisco D. Igual","Enrique S. Quintana-Ortí"],"pdf_url":"https://arxiv.org/pdf/2506.11728v1.pdf","comment":"16 pages, 7 tables, 7 figures"},{"id":"http://arxiv.org/abs/2410.11042v3","updated":"2025-06-13T12:27:46Z","published":"2024-10-14T19:46:23Z","title":"Persistent Topological Features in Large Language Models","summary":"  Understanding the decision-making processes of large language models is\ncritical given their widespread applications. To achieve this, we aim to\nconnect a formal mathematical framework - zigzag persistence from topological\ndata analysis - with practical and easily applicable algorithms. Zigzag\npersistence is particularly effective for characterizing data as it dynamically\ntransforms across model layers. Within this framework, we introduce topological\ndescriptors that measure how topological features, $p$-dimensional holes,\npersist and evolve throughout the layers. Unlike methods that assess each layer\nindividually and then aggregate the results, our approach directly tracks the\nfull evolutionary path of these features. This offers a statistical perspective\non how prompts are rearranged and their relative positions changed in the\nrepresentation space, providing insights into the system's operation as an\nintegrated whole. To demonstrate the expressivity and applicability of our\nframework, we highlight how sensitive these descriptors are to different models\nand a variety of datasets. As a showcase application to a downstream task, we\nuse zigzag persistence to establish a criterion for layer pruning, achieving\nresults comparable to state-of-the-art methods while preserving the\nsystem-level perspective.\n","authors":["Yuri Gardinazzi","Karthik Viswanathan","Giada Panerai","Alessio Ansuini","Alberto Cazzaniga","Matteo Biagetti"],"pdf_url":"https://arxiv.org/pdf/2410.11042v3.pdf","comment":"10+17 pages, 17 figures, 3 tables. Accepted as poster at ICML 2025"},{"id":"http://arxiv.org/abs/2502.07855v2","updated":"2025-06-13T12:20:30Z","published":"2025-02-11T14:04:43Z","title":"Vision-Language Models for Edge Networks: A Comprehensive Survey","summary":"  Vision Large Language Models (VLMs) combine visual understanding with natural\nlanguage processing, enabling tasks like image captioning, visual question\nanswering, and video analysis. While VLMs show impressive capabilities across\ndomains such as autonomous vehicles, smart surveillance, and healthcare, their\ndeployment on resource-constrained edge devices remains challenging due to\nprocessing power, memory, and energy limitations. This survey explores recent\nadvancements in optimizing VLMs for edge environments, focusing on model\ncompression techniques, including pruning, quantization, knowledge\ndistillation, and specialized hardware solutions that enhance efficiency. We\nprovide a detailed discussion of efficient training and fine-tuning methods,\nedge deployment challenges, and privacy considerations. Additionally, we\ndiscuss the diverse applications of lightweight VLMs across healthcare,\nenvironmental monitoring, and autonomous systems, illustrating their growing\nimpact. By highlighting key design strategies, current challenges, and offering\nrecommendations for future directions, this survey aims to inspire further\nresearch into the practical deployment of VLMs, ultimately making advanced AI\naccessible in resource-limited settings.\n","authors":["Ahmed Sharshar","Latif U. Khan","Waseem Ullah","Mohsen Guizani"],"pdf_url":"https://arxiv.org/pdf/2502.07855v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11702v1","updated":"2025-06-13T12:17:38Z","published":"2025-06-13T12:17:38Z","title":"Configurable Preference Tuning with Rubric-Guided Synthetic Data","summary":"  Models of human feedback for AI alignment, such as those underpinning Direct\nPreference Optimization (DPO), often bake in a singular, static set of\npreferences, limiting adaptability. This paper challenges the assumption of\nmonolithic preferences by introducing Configurable Preference Tuning (CPT), a\nnovel framework for endowing language models with the ability to dynamically\nadjust their behavior based on explicit, human-interpretable directives. CPT\nleverages synthetically generated preference data, conditioned on system\nprompts derived from structured, fine-grained rubrics that define desired\nattributes like writing style. By fine-tuning with these rubric-guided\npreferences, the LLM learns to modulate its outputs at inference time in\nresponse to the system prompt, without retraining. This approach not only\noffers fine-grained control but also provides a mechanism for modeling more\nnuanced and context-dependent human feedback. Several experimental artifacts,\nsuch as training code, generated datasets and fine-tuned models are released at\nhttps://github.com/vicgalle/configurable-preference-tuning\n","authors":["Víctor Gallego"],"pdf_url":"https://arxiv.org/pdf/2506.11702v1.pdf","comment":"Accepted to ICML 2025 Workshop on Models of Human Feedback for AI\n  Alignment"},{"id":"http://arxiv.org/abs/2506.11681v1","updated":"2025-06-13T11:19:27Z","published":"2025-06-13T11:19:27Z","title":"LLMs for Sentence Simplification: A Hybrid Multi-Agent prompting\n  Approach","summary":"  This paper addresses the challenge of transforming complex sentences into\nsequences of logical, simplified sentences while preserving semantic and\nlogical integrity with the help of Large Language Models. We propose a hybrid\napproach that combines advanced prompting with multi-agent architectures to\nenhance the sentence simplification process. Experimental results show that our\napproach was able to successfully simplify 70% of the complex sentences written\nfor video game design application. In comparison, a single-agent approach\nattained a 48% success rate on the same task.\n","authors":["Pratibha Zunjare","Michael Hsiao"],"pdf_url":"https://arxiv.org/pdf/2506.11681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11673v1","updated":"2025-06-13T11:07:14Z","published":"2025-06-13T11:07:14Z","title":"Improving Causal Interventions in Amnesic Probing with Mean Projection\n  or LEACE","summary":"  Amnesic probing is a technique used to examine the influence of specific\nlinguistic information on the behaviour of a model. This involves identifying\nand removing the relevant information and then assessing whether the model's\nperformance on the main task changes. If the removed information is relevant,\nthe model's performance should decline. The difficulty with this approach lies\nin removing only the target information while leaving other information\nunchanged. It has been shown that Iterative Nullspace Projection (INLP), a\nwidely used removal technique, introduces random modifications to\nrepresentations when eliminating target information. We demonstrate that Mean\nProjection (MP) and LEACE, two proposed alternatives, remove information in a\nmore targeted manner, thereby enhancing the potential for obtaining behavioural\nexplanations through Amnesic Probing.\n","authors":["Alicja Dobrzeniecka","Antske Fokkens","Pia Sommerauer"],"pdf_url":"https://arxiv.org/pdf/2506.11673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21227v2","updated":"2025-06-13T11:04:13Z","published":"2025-03-27T07:36:11Z","title":"LLaVA-CMoE: Towards Continual Mixture of Experts for Large\n  Vision-Language Models","summary":"  Mixture of Experts (MoE) architectures have recently advanced the scalability\nand adaptability of large language models (LLMs) for continual multimodal\nlearning. However, efficiently extending these models to accommodate sequential\ntasks remains challenging. As new tasks arrive, naive model expansion leads to\nrapid parameter growth, while modifying shared routing components often causes\ncatastrophic forgetting, undermining previously learned knowledge. To address\nthese issues, we propose LLaVA-CMoE, a continual learning framework for LLMs\nthat requires no replay data of previous tasks and ensures both parameter\nefficiency and robust knowledge retention. Our approach introduces a\nProbe-Guided Knowledge Extension mechanism, which uses probe experts to\ndynamically determine when and where new experts should be added, enabling\nadaptive and minimal parameter expansion tailored to task complexity.\nFurthermore, we present a Probabilistic Task Locator that assigns each task a\ndedicated, lightweight router. To handle the practical issue that task labels\nare unknown during inference, we leverage a VAE-based reconstruction strategy\nto identify the most suitable router by matching input distributions, allowing\nautomatic and accurate expert allocation. This design mitigates routing\nconflicts and catastrophic forgetting, enabling robust continual learning\nwithout explicit task labels. Extensive experiments on the CoIN benchmark,\ncovering eight diverse VQA tasks, demonstrate that LLaVA-CMoE delivers strong\ncontinual learning performance with a compact model size, significantly\nreducing forgetting and parameter overhead compared to prior methods. These\nresults showcase the effectiveness and scalability of our approach for\nparameter-efficient continual learning in large language models. Our code will\nbe open-sourced soon.\n","authors":["Hengyuan Zhao","Ziqin Wang","Qixin Sun","Kaiyou Song","Yilin Li","Xiaolin Hu","Qingpei Guo","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2503.21227v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2505.16660v3","updated":"2025-06-13T10:57:16Z","published":"2025-05-22T13:24:52Z","title":"Can reasoning models comprehend mathematical problems in Chinese ancient\n  texts? An empirical study based on data from Suanjing Shishu","summary":"  This study addresses the challenges in intelligent processing of Chinese\nancient mathematical classics by constructing Guji_MATH, a benchmark for\nevaluating classical texts based on Suanjing Shishu. It systematically assesses\nthe mathematical problem-solving capabilities of mainstream reasoning models\nunder the unique linguistic constraints of classical Chinese. Through\nmachine-assisted annotation and manual verification, 538 mathematical problems\nwere extracted from 8 canonical texts, forming a structured dataset centered on\nthe \"Question-Answer-Solution\" framework, supplemented by problem types and\ndifficulty levels. Dual evaluation modes--closed-book (autonomous\nproblem-solving) and open-book (reproducing classical solution methods)--were\ndesigned to evaluate the performance of six reasoning models on ancient Chinese\nmathematical problems. Results indicate that reasoning models can partially\ncomprehend and solve these problems, yet their overall performance remains\ninferior to benchmarks on modern mathematical tasks. Enhancing models'\nclassical Chinese comprehension and cultural knowledge should be prioritized\nfor optimization. This study provides methodological support for mining\nmathematical knowledge from ancient texts and disseminating traditional\nculture, while offering new perspectives for evaluating cross-linguistic and\ncross-cultural capabilities of reasoning models.\n","authors":["Chang Liu","Dongbo Wang","Liu liu","Zhixiao Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.16660v3.pdf","comment":"29pages, 7 figures"},{"id":"http://arxiv.org/abs/2506.11666v1","updated":"2025-06-13T10:53:50Z","published":"2025-06-13T10:53:50Z","title":"Converting Annotated Clinical Cases into Structured Case Report Forms","summary":"  Case Report Forms (CRFs) are largely used in medical research as they ensure\naccuracy, reliability, and validity of results in clinical studies. However,\npublicly available, wellannotated CRF datasets are scarce, limiting the\ndevelopment of CRF slot filling systems able to fill in a CRF from clinical\nnotes. To mitigate the scarcity of CRF datasets, we propose to take advantage\nof available datasets annotated for information extraction tasks and to convert\nthem into structured CRFs. We present a semi-automatic conversion methodology,\nwhich has been applied to the E3C dataset in two languages (English and\nItalian), resulting in a new, high-quality dataset for CRF slot filling.\nThrough several experiments on the created dataset, we report that slot filling\nachieves 59.7% for Italian and 67.3% for English on a closed Large Language\nModels (zero-shot) and worse performances on three families of open-source\nmodels, showing that filling CRFs is challenging even for recent\nstate-of-the-art LLMs. We release the datest at\nhttps://huggingface.co/collections/NLP-FBK/e3c-to-crf-67b9844065460cbe42f80166\n","authors":["Pietro Ferrazzi","Alberto Lavelli","Bernardo Magnini"],"pdf_url":"https://arxiv.org/pdf/2506.11666v1.pdf","comment":"to be published in BioNLP 2025"},{"id":"http://arxiv.org/abs/2506.11638v1","updated":"2025-06-13T10:11:01Z","published":"2025-06-13T10:11:01Z","title":"LoRA-Gen: Specializing Large Language Model via Online LoRA Generation","summary":"  Recent advances have highlighted the benefits of scaling language models to\nenhance performance across a wide range of NLP tasks. However, these approaches\nstill face limitations in effectiveness and efficiency when applied to\ndomain-specific tasks, particularly for small edge-side models. We propose the\nLoRA-Gen framework, which utilizes a large cloud-side model to generate LoRA\nparameters for edge-side models based on task descriptions. By employing the\nreparameterization technique, we merge the LoRA parameters into the edge-side\nmodel to achieve flexible specialization. Our method facilitates knowledge\ntransfer between models while significantly improving the inference efficiency\nof the specialized model by reducing the input context length. Without\nspecialized training, LoRA-Gen outperforms conventional LoRA fine-tuning, which\nachieves competitive accuracy and a 2.1x speedup with TinyLLaMA-1.1B in\nreasoning tasks. Besides, our method delivers a compression ratio of 10.1x with\nGemma-2B on intelligent agent tasks.\n","authors":["Yicheng Xiao","Lin Song","Rui Yang","Cheng Cheng","Yixiao Ge","Xiu Li","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2506.11638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08967v2","updated":"2025-06-13T10:07:42Z","published":"2025-06-10T16:37:39Z","title":"Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language\n  Model","summary":"  Large Audio-Language Models (LALMs) have significantly advanced intelligent\nhuman-computer interaction, yet their reliance on text-based outputs limits\ntheir ability to generate natural speech responses directly, hindering seamless\naudio interactions. To address this, we introduce Step-Audio-AQAA, a fully\nend-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model\nintegrates a dual-codebook audio tokenizer for linguistic and semantic feature\nextraction, a 130-billion-parameter backbone LLM and a neural vocoder for\nhigh-fidelity speech synthesis. Our post-training approach employs interleaved\ntoken-output of text and audio to enhance semantic coherence and combines\nDirect Preference Optimization (DPO) with model merge to improve performance.\nEvaluations on the StepEval-Audio-360 benchmark demonstrate that\nStep-Audio-AQAA excels especially in speech control, outperforming the\nstate-of-art LALMs in key areas. This work contributes a promising solution for\nend-to-end LALMs and highlights the critical role of token-based vocoder in\nenhancing overall performance for AQAA tasks.\n","authors":["Ailin Huang","Bingxin Li","Bruce Wang","Boyong Wu","Chao Yan","Chengli Feng","Heng Wang","Hongyu Zhou","Hongyuan Wang","Jingbei Li","Jianjian Sun","Joanna Wang","Mingrui Chen","Peng Liu","Ruihang Miao","Shilei Jiang","Tian Fei","Wang You","Xi Chen","Xuerui Yang","Yechang Huang","Yuxiang Zhang","Zheng Ge","Zheng Gong","Zhewei Huang","Zixin Zhang","Bin Wang","Bo Li","Buyun Ma","Changxin Miao","Changyi Wan","Chen Xu","Dapeng Shi","Dingyuan Hu","Enle Liu","Guanzhe Huang","Gulin Yan","Hanpeng Hu","Haonan Jia","Jiahao Gong","Jiaoren Wu","Jie Wu","Jie Yang","Junzhe Lin","Kaixiang Li","Lei Xia","Longlong Gu","Ming Li","Nie Hao","Ranchen Ming","Shaoliang Pang","Siqi Liu","Song Yuan","Tiancheng Cao","Wen Li","Wenqing He","Xu Zhao","Xuelin Zhang","Yanbo Yu","Yinmin Zhong","Yu Zhou","Yuanwei Liang","Yuanwei Lu","Yuxiang Yang","Zidong Yang","Zili Zhang","Binxing Jiao","Heung-Yeung Shum","Jiansheng Chen","Jing Li","Xiangyu Zhang","Xinhao Zhang","Yibo Zhu","Daxin Jiang","Shuchang Zhou","Chen Hu"],"pdf_url":"https://arxiv.org/pdf/2506.08967v2.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2506.11631v1","updated":"2025-06-13T10:02:39Z","published":"2025-06-13T10:02:39Z","title":"SceneGram: Conceptualizing and Describing Tangrams in Scene Context","summary":"  Research on reference and naming suggests that humans can come up with very\ndifferent ways of conceptualizing and referring to the same object, e.g. the\nsame abstract tangram shape can be a \"crab\", \"sink\" or \"space ship\". Another\ncommon assumption in cognitive science is that scene context fundamentally\nshapes our visual perception of objects and conceptual expectations. This paper\ncontributes SceneGram, a dataset of human references to tangram shapes placed\nin different scene contexts, allowing for systematic analyses of the effect of\nscene context on conceptualization. Based on this data, we analyze references\nto tangram shapes generated by multimodal LLMs, showing that these models do\nnot account for the richness and variability of conceptualizations found in\nhuman references.\n","authors":["Simeon Junker","Sina Zarrieß"],"pdf_url":"https://arxiv.org/pdf/2506.11631v1.pdf","comment":"To appear in ACL Findings 2025"},{"id":"http://arxiv.org/abs/2406.02050v4","updated":"2025-06-13T09:44:56Z","published":"2024-06-04T07:31:06Z","title":"JBBQ: Japanese Bias Benchmark for Analyzing Social Biases in Large\n  Language Models","summary":"  With the development of large language models (LLMs), social biases in these\nLLMs have become a pressing issue. Although there are various benchmarks for\nsocial biases across languages, the extent to which Japanese LLMs exhibit\nsocial biases has not been fully investigated. In this study, we construct the\nJapanese Bias Benchmark dataset for Question Answering (JBBQ) based on the\nEnglish bias benchmark BBQ, with analysis of social biases in Japanese LLMs.\nThe results show that while current open Japanese LLMs with more parameters\nshow improved accuracies on JBBQ, their bias scores increase. In addition,\nprompts with a warning about social biases and chain-of-thought prompting\nreduce the effect of biases in model outputs, but there is room for improvement\nin extracting the correct evidence from contexts in Japanese. Our dataset is\navailable at https://github.com/ynklab/JBBQ_data.\n","authors":["Hitomi Yanaka","Namgi Han","Ryoma Kumon","Jie Lu","Masashi Takeshita","Ryo Sekizawa","Taisei Kato","Hiromi Arai"],"pdf_url":"https://arxiv.org/pdf/2406.02050v4.pdf","comment":"Accepted to the 6th Workshop on Gender Bias in Natural Language\n  Processing (GeBNLP2025) at ACL2025"},{"id":"http://arxiv.org/abs/2506.11620v1","updated":"2025-06-13T09:43:27Z","published":"2025-06-13T09:43:27Z","title":"(SimPhon Speech Test): A Data-Driven Method for In Silico Design and\n  Validation of a Phonetically Balanced Speech Test","summary":"  Traditional audiometry often provides an incomplete characterization of the\nfunctional impact of hearing loss on speech understanding, particularly for\nsupra-threshold deficits common in presbycusis. This motivates the development\nof more diagnostically specific speech perception tests. We introduce the\nSimulated Phoneme Speech Test (SimPhon Speech Test) methodology, a novel,\nmulti-stage computational pipeline for the in silico design and validation of a\nphonetically balanced minimal-pair speech test. This methodology leverages a\nmodern Automatic Speech Recognition (ASR) system as a proxy for a human\nlistener to simulate the perceptual effects of sensorineural hearing loss. By\nprocessing speech stimuli under controlled acoustic degradation, we first\nidentify the most common phoneme confusion patterns. These patterns then guide\nthe data-driven curation of a large set of candidate word pairs derived from a\ncomprehensive linguistic corpus. Subsequent phases involving simulated\ndiagnostic testing, expert human curation, and a final, targeted sensitivity\nanalysis systematically reduce the candidates to a final, optimized set of 25\npairs (the SimPhon Speech Test-25). A key finding is that the diagnostic\nperformance of the SimPhon Speech Test-25 test items shows no significant\ncorrelation with predictions from the standard Speech Intelligibility Index\n(SII), suggesting the SimPhon Speech Test captures perceptual deficits beyond\nsimple audibility. This computationally optimized test set offers a significant\nincrease in efficiency for audiological test development, ready for initial\nhuman trials.\n","authors":["Stefan Bleeck"],"pdf_url":"https://arxiv.org/pdf/2506.11620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11812v2","updated":"2025-06-13T09:32:19Z","published":"2025-02-17T13:59:41Z","title":"Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit\n  Analysis","summary":"  Fine-tuning significantly improves the performance of Large Language Models\n(LLMs), yet its underlying mechanisms remain poorly understood. This paper aims\nto provide an in-depth interpretation of the fine-tuning process through\ncircuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike\nprevious studies (Prakash et al. 2024; Chhabra et al. 2024) that focus on tasks\nwhere pre-trained models already perform well, we develop a set of mathematical\ntasks where fine-tuning yields substantial performance gains, which are closer\nto the practical setting. In our experiments, we identify circuits at various\ncheckpoints during fine-tuning and examine the interplay between circuit\nanalysis, fine-tuning methods, and task complexities. First, we find that while\ncircuits maintain high node similarity before and after fine-tuning, their\nedges undergo significant changes, in contrast to prior work that shows\ncircuits only add some additional components after fine-tuning. Based on these\nobservations, we develop a circuit-aware Low-Rank Adaptation (LoRA) method,\nwhich assigns ranks to layers based on edge changes in the circuits.\nExperimental results demonstrate that our circuit-based LoRA algorithm achieves\nan average performance improvement of 2.46% over standard LoRA with similar\nparameter sizes. Furthermore, we explore how combining circuits from subtasks\ncan enhance fine-tuning in compositional tasks, providing new insights into the\ndesign of such tasks and deepening the understanding of circuit dynamics and\nfine-tuning mechanisms.\n","authors":["Xu Wang","Yan Hu","Wenyu Du","Reynold Cheng","Benyou Wang","Difan Zou"],"pdf_url":"https://arxiv.org/pdf/2502.11812v2.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2506.11604v1","updated":"2025-06-13T09:20:41Z","published":"2025-06-13T09:20:41Z","title":"VLM@school -- Evaluation of AI image understanding on German middle\n  school knowledge","summary":"  This paper introduces a novel benchmark dataset designed to evaluate the\ncapabilities of Vision Language Models (VLMs) on tasks that combine visual\nreasoning with subject-specific background knowledge in the German language. In\ncontrast to widely used English-language benchmarks that often rely on\nartificially difficult or decontextualized problems, this dataset draws from\nreal middle school curricula across nine domains including mathematics,\nhistory, biology, and religion. The benchmark includes over 2,000 open-ended\nquestions grounded in 486 images, ensuring that models must integrate visual\ninterpretation with factual reasoning rather than rely on superficial textual\ncues. We evaluate thirteen state-of-the-art open-weight VLMs across multiple\ndimensions, including domain-specific accuracy and performance on adversarial\ncrafted questions. Our findings reveal that even the strongest models achieve\nless than 45% overall accuracy, with particularly poor performance in music,\nmathematics, and adversarial settings. Furthermore, the results indicate\nsignificant discrepancies between success on popular benchmarks and real-world\nmultimodal understanding. We conclude that middle school-level tasks offer a\nmeaningful and underutilized avenue for stress-testing VLMs, especially in\nnon-English contexts. The dataset and evaluation protocol serve as a rigorous\ntestbed to better understand and improve the visual and linguistic reasoning\ncapabilities of future AI systems.\n","authors":["René Peinl","Vincent Tischler"],"pdf_url":"https://arxiv.org/pdf/2506.11604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11602v1","updated":"2025-06-13T09:17:08Z","published":"2025-06-13T09:17:08Z","title":"Are LLMs Good Text Diacritizers? An Arabic and Yorùbá Case Study","summary":"  We investigate the effectiveness of large language models (LLMs) for text\ndiacritization in two typologically distinct languages: Arabic and Yoruba. To\nenable a rigorous evaluation, we introduce a novel multilingual dataset\nMultiDiac, with diverse samples that capture a range of diacritic ambiguities.\nWe evaluate 14 LLMs varying in size, accessibility, and language coverage, and\nbenchmark them against 6 specialized diacritization models. Additionally, we\nfine-tune four small open-source models using LoRA for Yoruba. Our results show\nthat many off-the-shelf LLMs outperform specialized diacritization models for\nboth Arabic and Yoruba, but smaller models suffer from hallucinations.\nFine-tuning on a small dataset can help improve diacritization performance and\nreduce hallucination rates.\n","authors":["Hawau Olamide Toyin","Samar M. Magdy","Hanan Aldarmaki"],"pdf_url":"https://arxiv.org/pdf/2506.11602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04267v2","updated":"2025-06-13T09:03:31Z","published":"2024-09-06T13:24:22Z","title":"An overview of domain-specific foundation model: key technologies,\n  applications and challenges","summary":"  The impressive performance of ChatGPT and other foundation-model-based\nproducts in human language understanding has prompted both academia and\nindustry to explore how these models can be tailored for specific industries\nand application scenarios. This process, known as the customization of\ndomain-specific foundation models (FMs), addresses the limitations of\ngeneral-purpose models, which may not fully capture the unique patterns and\nrequirements of domain-specific data. Despite its importance, there is a\nnotable lack of comprehensive overview papers on building domain-specific FMs,\nwhile numerous resources exist for general-purpose models. To bridge this gap,\nthis article provides a timely and thorough overview of the methodology for\ncustomizing domain-specific FMs. It introduces basic concepts, outlines the\ngeneral architecture, and surveys key methods for constructing domain-specific\nmodels. Furthermore, the article discusses various domains that can benefit\nfrom these specialized models and highlights the challenges ahead. Through this\noverview, we aim to offer valuable guidance and reference for researchers and\npractitioners from diverse fields to develop their own customized FMs.\n","authors":["Haolong Chen","Hanzhi Chen","Zijian Zhao","Kaifeng Han","Guangxu Zhu","Yichen Zhao","Ying Du","Wei Xu","Qingjiang Shi"],"pdf_url":"https://arxiv.org/pdf/2409.04267v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23252v2","updated":"2025-06-13T09:00:02Z","published":"2025-05-29T08:57:11Z","title":"Automatic Construction of Multiple Classification Dimensions for\n  Managing Approaches in Scientific Papers","summary":"  Approaches form the foundation for conducting scientific research. Querying\napproaches from a vast body of scientific papers is extremely time-consuming,\nand without a well-organized management framework, researchers may face\nsignificant challenges in querying and utilizing relevant approaches.\nConstructing multiple dimensions on approaches and managing them from these\ndimensions can provide an efficient solution. Firstly, this paper identifies\napproach patterns using a top-down way, refining the patterns through four\ndistinct linguistic levels: semantic level, discourse level, syntactic level,\nand lexical level. Approaches in scientific papers are extracted based on\napproach patterns. Additionally, five dimensions for categorizing approaches\nare identified using these patterns. This paper proposes using tree structure\nto represent step and measuring the similarity between different steps with a\ntree-structure-based similarity measure that focuses on syntactic-level\nsimilarities. A collection similarity measure is proposed to compute the\nsimilarity between approaches. A bottom-up clustering algorithm is proposed to\nconstruct class trees for approach components within each dimension by merging\neach approach component or class with its most similar approach component or\nclass in each iteration. The class labels generated during the clustering\nprocess indicate the common semantics of the step components within the\napproach components in each class and are used to manage the approaches within\nthe class. The class trees of the five dimensions collectively form a\nmulti-dimensional approach space. The application of approach queries on the\nmulti-dimensional approach space demonstrates that querying within this space\nensures strong relevance between user queries and results and rapidly reduces\nsearch space through a class-based query mechanism.\n","authors":["Bing Ma","Hai Zhuge"],"pdf_url":"https://arxiv.org/pdf/2505.23252v2.pdf","comment":"26 pages, 9 figures"},{"id":"http://arxiv.org/abs/2504.14218v3","updated":"2025-06-13T08:57:36Z","published":"2025-04-19T07:53:37Z","title":"Understanding the Repeat Curse in Large Language Models from a Feature\n  Perspective","summary":"  Large language models (LLMs) have made remarkable progress in various\ndomains, yet they often suffer from repetitive text generation, a phenomenon we\nrefer to as the \"Repeat Curse\". While previous studies have proposed decoding\nstrategies to mitigate repetition, the underlying mechanism behind this issue\nremains insufficiently explored. In this work, we investigate the root causes\nof repetition in LLMs through the lens of mechanistic interpretability.\nInspired by recent advances in Sparse Autoencoders (SAEs), which enable\nmonosemantic feature extraction, we propose a novel approach, \"Duplicatus\nCharm\", to induce and analyze the Repeat Curse. Our method systematically\nidentifies \"Repetition Features\" -the key model activations responsible for\ngenerating repetitive outputs. First, we locate the layers most involved in\nrepetition through logit analysis. Next, we extract and stimulate relevant\nfeatures using SAE-based activation manipulation. To validate our approach, we\nconstruct a repetition dataset covering token and paragraph level repetitions\nand introduce an evaluation pipeline to quantify the influence of identified\nrepetition features. Furthermore, by deactivating these features, we have\neffectively mitigated the Repeat Curse. The source code of our work is publicly\navailable at: https://github.com/kaustpradalab/repeat-curse-llm\n","authors":["Junchi Yao","Shu Yang","Jianhua Xu","Lijie Hu","Mengdi Li","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2504.14218v3.pdf","comment":"Accepted by ACL 2025, Findings, Long Paper"},{"id":"http://arxiv.org/abs/2405.04065v4","updated":"2025-06-13T08:32:26Z","published":"2024-05-07T07:14:38Z","title":"FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference","summary":"  Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost.\n","authors":["Runheng Liu","Xingchen Xiao","Heyan Huang","Zewen Chi","Zhijing Wu"],"pdf_url":"https://arxiv.org/pdf/2405.04065v4.pdf","comment":"ACL 2025 Findings, 14 pages"},{"id":"http://arxiv.org/abs/2506.11558v1","updated":"2025-06-13T08:13:05Z","published":"2025-06-13T08:13:05Z","title":"DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning\n  with Video LLMs","summary":"  Large Language Models (LLMs) have recently been extended to the video domain,\nenabling sophisticated video-language understanding. However, existing Video\nLLMs often exhibit limitations in fine-grained temporal reasoning, restricting\ntheir ability to precisely attribute responses to specific video moments,\nespecially under constrained supervision. We introduce DaMO, a data-efficient\nVideo LLM explicitly designed for accurate temporal reasoning and multimodal\nunderstanding. At its core, the proposed Temporal-aware Fuseformer employs a\nhierarchical dual-stream architecture that progressively captures temporal\ndynamics within each modality and effectively fuses complementary visual and\naudio information. To further enhance computational efficiency, DaMO integrates\na global residual that reduces spatial redundancy while preserving essential\nsemantic details. We train DaMO via a structured four-stage progressive\ntraining paradigm, incrementally equipping the model with multimodal alignment,\nsemantic grounding, and temporal reasoning capabilities. This work also\ncontributes multiple datasets augmented from existing ones with GPT-generated\ntemporally grounded QA pairs for tasks requiring temporal supervision.\nComprehensive experiments on temporal grounding and video QA benchmarks\ndemonstrate that DaMO consistently surpasses prior methods, particularly in\ntasks demanding precise temporal alignment and reasoning. Our work establishes\na promising direction for data-efficient video-language modeling.\n","authors":["Bo-Cheng Chiu","Jen-Jee Chen","Yu-Chee Tseng","Feng-Chi Chen"],"pdf_url":"https://arxiv.org/pdf/2506.11558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11557v1","updated":"2025-06-13T08:12:52Z","published":"2025-06-13T08:12:52Z","title":"From Persona to Person: Enhancing the Naturalness with Multiple\n  Discourse Relations Graph Learning in Personalized Dialogue Generation","summary":"  In dialogue generation, the naturalness of responses is crucial for effective\nhuman-machine interaction. Personalized response generation poses even greater\nchallenges, as the responses must remain coherent and consistent with the\nuser's personal traits or persona descriptions. We propose MUDI\n($\\textbf{Mu}$ltiple $\\textbf{Di}$scourse Relations Graph Learning) for\npersonalized dialogue generation. We utilize a Large Language Model to assist\nin annotating discourse relations and to transform dialogue data into\nstructured dialogue graphs. Our graph encoder, the proposed DialogueGAT model,\nthen captures implicit discourse relations within this structure, along with\npersona descriptions. During the personalized response generation phase, novel\ncoherence-aware attention strategies are implemented to enhance the decoder's\nconsideration of discourse relations. Our experiments demonstrate significant\nimprovements in the quality of personalized responses, thus resembling\nhuman-like dialogue exchanges.\n","authors":["Chih-Hao Hsu","Ying-Jia Lin","Hung-Yu Kao"],"pdf_url":"https://arxiv.org/pdf/2506.11557v1.pdf","comment":"Accepted by PAKDD 2025"},{"id":"http://arxiv.org/abs/2506.11555v1","updated":"2025-06-13T08:06:49Z","published":"2025-06-13T08:06:49Z","title":"RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware\n  Reasoning","summary":"  The integration of external knowledge through Retrieval-Augmented Generation\n(RAG) has become foundational in enhancing large language models (LLMs) for\nknowledge-intensive tasks. However, existing RAG paradigms often overlook the\ncognitive step of applying knowledge, leaving a gap between retrieved facts and\ntask-specific reasoning. In this work, we introduce RAG+, a principled and\nmodular extension that explicitly incorporates application-aware reasoning into\nthe RAG pipeline. RAG+ constructs a dual corpus consisting of knowledge and\naligned application examples, created either manually or automatically, and\nretrieves both jointly during inference. This design enables LLMs not only to\naccess relevant information but also to apply it within structured,\ngoal-oriented reasoning processes. Experiments across mathematical, legal, and\nmedical domains, conducted on multiple models, demonstrate that RAG+\nconsistently outperforms standard RAG variants, achieving average improvements\nof 3-5%, and peak gains up to 7.5% in complex scenarios. By bridging retrieval\nwith actionable application, RAG+ advances a more cognitively grounded\nframework for knowledge integration, representing a step toward more\ninterpretable and capable LLMs.\n","authors":["Yu Wang","Shiwan Zhao","Ming Fan","Zhihu Wang","Yubo Zhang","Xicheng Zhang","Zhengfan Wang","Heyuan Huang","Ting Liu"],"pdf_url":"https://arxiv.org/pdf/2506.11555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14023v4","updated":"2025-06-13T08:04:19Z","published":"2024-06-20T06:42:08Z","title":"Evaluating Implicit Bias in Large Language Models by Attacking From a\n  Psychometric Perspective","summary":"  As large language models (LLMs) become an important way of information\naccess, there have been increasing concerns that LLMs may intensify the spread\nof unethical content, including implicit bias that hurts certain populations\nwithout explicit harmful words. In this paper, we conduct a rigorous evaluation\nof LLMs' implicit bias towards certain demographics by attacking them from a\npsychometric perspective to elicit agreements to biased viewpoints. Inspired by\npsychometric principles in cognitive and social psychology, we propose three\nattack approaches, i.e., Disguise, Deception, and Teaching. Incorporating the\ncorresponding attack instructions, we built two benchmarks: (1) a bilingual\ndataset with biased statements covering four bias types (2.7K instances) for\nextensive comparative analysis, and (2) BUMBLE, a larger benchmark spanning\nnine common bias types (12.7K instances) for comprehensive evaluation.\nExtensive evaluation of popular commercial and open-source LLMs shows that our\nmethods can elicit LLMs' inner bias more effectively than competitive\nbaselines. Our attack methodology and benchmarks offer an effective means of\nassessing the ethical risks of LLMs, driving progress toward greater\naccountability in their development. Our code, data, and benchmarks are\navailable at https://yuchenwen1.github.io/ImplicitBiasEvaluation/.\n","authors":["Yuchen Wen","Keping Bi","Wei Chen","Jiafeng Guo","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.14023v4.pdf","comment":"Accepted to ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2410.20445v3","updated":"2025-06-13T08:02:21Z","published":"2024-10-27T13:51:09Z","title":"TrajAgent: An LLM-based Agent Framework for Automated Trajectory\n  Modeling via Collaboration of Large and Small Models","summary":"  Trajectory modeling, which includes research on trajectory data pattern\nmining and future prediction, has widespread applications in areas such as life\nservices, urban transportation, and public administration. Numerous methods\nhave been proposed to address specific problems within trajectory modeling.\nHowever, the heterogeneity of data and the diversity of trajectory tasks make\neffective and reliable trajectory modeling an important yet highly challenging\nendeavor, even for domain experts. In this paper, we propose\n\\textit{TrajAgent}, a agent framework powered by large language models (LLMs),\ndesigned to facilitate robust and efficient trajectory modeling through\nautomation modeling. This framework leverages and optimizes diverse specialized\nmodels to address various trajectory modeling tasks across different datasets\neffectively. In \\textit{TrajAgent}, we first develop \\textit{UniEnv}, an\nexecution environment with a unified data and model interface, to support the\nexecution and training of various models. Building on \\textit{UniEnv}, we\nintroduce an agentic workflow designed for automatic trajectory modeling across\nvarious trajectory tasks and data. Furthermore, we introduce collaborative\nlearning schema between LLM-based agents and small speciallized models, to\nenhance the performance of the whole framework effectively. Extensive\nexperiments on four tasks using four real-world datasets demonstrate the\neffectiveness of \\textit{TrajAgent} in automated trajectory modeling, achieving\na performance improvement of 2.38\\%-34.96\\% over baseline methods.\n","authors":["Yuwei Du","Jie Feng","Jie Zhao","Jian Yuan","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2410.20445v3.pdf","comment":"the code will be openly accessible at:\n  https://github.com/tsinghua-fib-lab/TrajAgent"},{"id":"http://arxiv.org/abs/2506.04078v2","updated":"2025-06-13T07:27:57Z","published":"2025-06-04T15:43:14Z","title":"LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with\n  Physician Validation","summary":"  Evaluating large language models (LLMs) in medicine is crucial because\nmedical applications require high accuracy with little room for error. Current\nmedical benchmarks have three main types: medical exam-based, comprehensive\nmedical, and specialized assessments. However, these benchmarks have\nlimitations in question design (mostly multiple-choice), data sources (often\nnot derived from real clinical scenarios), and evaluation methods (poor\nassessment of complex reasoning). To address these issues, we present\nLLMEval-Med, a new benchmark covering five core medical areas, including 2,996\nquestions created from real-world electronic health records and expert-designed\nclinical scenarios. We also design an automated evaluation pipeline,\nincorporating expert-developed checklists into our LLM-as-Judge framework.\nFurthermore, our methodology validates machine scoring through human-machine\nagreement analysis, dynamically refining checklists and prompts based on expert\nfeedback to ensure reliability. We evaluate 13 LLMs across three categories\n(specialized medical models, open-source models, and closed-source models) on\nLLMEval-Med, providing valuable insights for the safe and effective deployment\nof LLMs in medical domains. The dataset is released in\nhttps://github.com/llmeval/LLMEval-Med.\n","authors":["Ming Zhang","Yujiong Shen","Zelin Li","Huayu Sha","Binze Hu","Yuhui Wang","Chenhao Huang","Shichun Liu","Jingqi Tong","Changhao Jiang","Mingxu Chai","Zhiheng Xi","Shihan Dou","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2506.04078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06706v3","updated":"2025-06-13T07:21:17Z","published":"2025-03-09T17:43:30Z","title":"PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on\n  UML Flowcharts","summary":"  Process-driven dialogue systems, which operate under strict predefined\nprocess constraints, are essential in customer service and equipment\nmaintenance scenarios. Although Large Language Models (LLMs) have shown\nremarkable progress in dialogue and reasoning, they still struggle to solve\nthese strictly constrained dialogue tasks. To address this challenge, we\nconstruct Process Flow Dialogue (PFDial) dataset, which contains 12,705\nhigh-quality Chinese dialogue instructions derived from 440 flowcharts\ncontaining 5,055 process nodes. Based on PlantUML specification, each UML\nflowchart is converted into atomic dialogue units i.e., structured five-tuples.\nExperimental results demonstrate that a 7B model trained with merely 800\nsamples, and a 0.5B model trained on total data both can surpass 90% accuracy.\nAdditionally, the 8B model can surpass GPT-4o up to 43.88% with an average of\n11.00%. We further evaluate models' performance on challenging backward\ntransitions in process flows and conduct an in-depth analysis of various\ndataset formats to reveal their impact on model performance in handling\ndecision and sequential branches. The data is released in\nhttps://github.com/KongLongGeFDU/PFDial.\n","authors":["Ming Zhang","Yuhui Wang","Yujiong Shen","Tingyi Yang","Changhao Jiang","Yilong Wu","Shihan Dou","Qinhao Chen","Zhiheng Xi","Zhihao Zhang","Yi Dong","Zhen Wang","Zhihui Fei","Mingyang Wan","Tao Liang","Guojun Ma","Qi Zhang","Tao Gui","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2503.06706v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11516v1","updated":"2025-06-13T07:17:41Z","published":"2025-06-13T07:17:41Z","title":"Brewing Knowledge in Context: Distillation Perspectives on In-Context\n  Learning","summary":"  In-context learning (ICL) allows large language models (LLMs) to solve novel\ntasks without weight updates. Despite its empirical success, the mechanism\nbehind ICL remains poorly understood, limiting our ability to interpret,\nimprove, and reliably apply it. In this paper, we propose a new theoretical\nperspective that interprets ICL as an implicit form of knowledge distillation\n(KD), where prompt demonstrations guide the model to form a task-specific\nreference model during inference. Under this view, we derive a Rademacher\ncomplexity-based generalization bound and prove that the bias of the distilled\nweights grows linearly with the Maximum Mean Discrepancy (MMD) between the\nprompt and target distributions. This theoretical framework explains several\nempirical phenomena and unifies prior gradient-based and distributional\nanalyses. To the best of our knowledge, this is the first to formalize\ninference-time attention as a distillation process, which provides theoretical\ninsights for future prompt engineering and automated demonstration selection.\n","authors":["Chengye Li","Haiyun Liu","Yuanxi Li"],"pdf_url":"https://arxiv.org/pdf/2506.11516v1.pdf","comment":"10 main pages, 10 page appendix"},{"id":"http://arxiv.org/abs/2506.11515v1","updated":"2025-06-13T07:16:41Z","published":"2025-06-13T07:16:41Z","title":"Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs\n  and MLLMs","summary":"  Two-Tower Vision--Language Models (VLMs) have demonstrated strong performance\nacross various downstream VL tasks. While BridgeTower further enhances\nperformance by building bridges between encoders, it \\textit{(i)} suffers from\nineffective layer-by-layer utilization of unimodal representations,\n\\textit{(ii)} restricts the flexible exploitation of different levels of\nunimodal semantic knowledge, and \\textit{(iii)} is limited to the evaluation on\ntraditional low-resolution datasets only with the Two-Tower VLM architecture.\nIn this work, we propose Manager, a lightweight, efficient and effective plugin\nthat adaptively aggregates insights from different levels of pre-trained\nunimodal experts to facilitate more comprehensive VL alignment and fusion.\nFirst, under the Two-Tower VLM architecture, we introduce ManagerTower, a novel\nVLM that introduces the manager in each cross-modal layer. Whether with or\nwithout VL pre-training, ManagerTower outperforms previous strong baselines and\nachieves superior performance on 4 downstream VL tasks. Moreover, we extend our\nexploration to the latest Multimodal Large Language Model (MLLM) architecture.\nWe demonstrate that LLaVA-OV-Manager significantly boosts the zero-shot\nperformance of LLaVA-OV across different categories of capabilities, images,\nand resolutions on 20 downstream datasets, whether the multi-grid algorithm is\nenabled or not. In-depth analysis reveals that both our manager and the\nmulti-grid algorithm can be viewed as a plugin that improves the visual\nrepresentation by capturing more diverse visual details from two orthogonal\nperspectives (depth and width). Their synergy can mitigate the semantic\nambiguity caused by the multi-grid algorithm and further improve performance.\nCode and models are available at https://github.com/LooperXX/ManagerTower.\n","authors":["Xiao Xu","Libo Qin","Wanxiang Che","Min-Yen Kan"],"pdf_url":"https://arxiv.org/pdf/2506.11515v1.pdf","comment":"Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT). June 2025. DOI:\n  https://doi.org/10.1109/TCSVT.2025.3578266"},{"id":"http://arxiv.org/abs/2502.11020v2","updated":"2025-06-13T07:12:23Z","published":"2025-02-16T07:07:38Z","title":"TUMLU: A Unified and Native Language Understanding Benchmark for Turkic\n  Languages","summary":"  Being able to thoroughly assess massive multi-task language understanding\n(MMLU) capabilities is essential for advancing the applicability of\nmultilingual language models. However, preparing such benchmarks in high\nquality native language is often costly and therefore limits the\nrepresentativeness of evaluation datasets. While recent efforts focused on\nbuilding more inclusive MMLU benchmarks, these are conventionally built using\nmachine translation from high-resource languages, which may introduce errors\nand fail to account for the linguistic and cultural intricacies of the target\nlanguages. In this paper, we address the lack of native language MMLU benchmark\nespecially in the under-represented Turkic language family with distinct\nmorphosyntactic and cultural characteristics. We propose two benchmarks for\nTurkic language MMLU: TUMLU is a comprehensive, multilingual, and natively\ndeveloped language understanding benchmark specifically designed for Turkic\nlanguages. It consists of middle- and high-school level questions spanning 11\nacademic subjects in Azerbaijani, Crimean Tatar, Karakalpak, Kazakh, Tatar,\nTurkish, Uyghur, and Uzbek. We also present TUMLU-mini, a more concise,\nbalanced, and manually verified subset of the dataset. Using this dataset, we\nsystematically evaluate a diverse range of open and proprietary multilingual\nlarge language models (LLMs), including Claude, Gemini, GPT, and LLaMA,\noffering an in-depth analysis of their performance across different languages,\nsubjects, and alphabets. To promote further research and development in\nmultilingual language understanding, we release TUMLU-mini and all\ncorresponding evaluation scripts.\n","authors":["Jafar Isbarov","Arofat Akhundjanova","Mammad Hajili","Kavsar Huseynova","Dmitry Gaynullin","Anar Rzayev","Osman Tursun","Aizirek Turdubaeva","Ilshat Saetov","Rinat Kharisov","Saule Belginova","Ariana Kenbayeva","Amina Alisheva","Abdullatif Köksal","Samir Rustamov","Duygu Ataman"],"pdf_url":"https://arxiv.org/pdf/2502.11020v2.pdf","comment":"Accepted to ACL 2025, Main Conference"},{"id":"http://arxiv.org/abs/2412.21015v2","updated":"2025-06-13T07:04:46Z","published":"2024-12-30T15:33:19Z","title":"MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based\n  QA Datasets","summary":"  Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw.\n","authors":["Mahir Labib Dihan","Mohammed Eunus Ali","Md Rizwan Parvez"],"pdf_url":"https://arxiv.org/pdf/2412.21015v2.pdf","comment":"ACL 2025 (Demo)"},{"id":"http://arxiv.org/abs/2506.11499v1","updated":"2025-06-13T06:50:02Z","published":"2025-06-13T06:50:02Z","title":"On the Effectiveness of Integration Methods for Multimodal Dialogue\n  Response Retrieval","summary":"  Multimodal chatbots have become one of the major topics for dialogue systems\nin both research community and industry. Recently, researchers have shed light\non the multimodality of responses as well as dialogue contexts. This work\nexplores how a dialogue system can output responses in various modalities such\nas text and image. To this end, we first formulate a multimodal dialogue\nresponse retrieval task for retrieval-based systems as the combination of three\nsubtasks. We then propose three integration methods based on a two-step\napproach and an end-to-end approach, and compare the merits and demerits of\neach method. Experimental results on two datasets demonstrate that the\nend-to-end approach achieves comparable performance without an intermediate\nstep in the two-step approach. In addition, a parameter sharing strategy not\nonly reduces the number of parameters but also boosts performance by\ntransferring knowledge across the subtasks and the modalities.\n","authors":["Seongbo Jang","Seonghyeon Lee","Dongha Lee","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2506.11499v1.pdf","comment":"9 pages, 1 figure"},{"id":"http://arxiv.org/abs/2506.11498v1","updated":"2025-06-13T06:49:53Z","published":"2025-06-13T06:49:53Z","title":"Lag-Relative Sparse Attention In Long Context Training","summary":"  Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task.\n","authors":["Manlai Liang","Wanyi Huang","Mandi Liu","Huaijun Li","Jinlong Li"],"pdf_url":"https://arxiv.org/pdf/2506.11498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11485v1","updated":"2025-06-13T06:20:03Z","published":"2025-06-13T06:20:03Z","title":"Relational Schemata in BERT Are Inducible, Not Emergent: A Study of\n  Performance vs. Competence in Language Models","summary":"  While large language models like BERT demonstrate strong empirical\nperformance on semantic tasks, whether this reflects true conceptual competence\nor surface-level statistical association remains unclear. I investigate whether\nBERT encodes abstract relational schemata by examining internal representations\nof concept pairs across taxonomic, mereological, and functional relations. I\ncompare BERT's relational classification performance with representational\nstructure in [CLS] token embeddings. Results reveal that pretrained BERT\nenables high classification accuracy, indicating latent relational signals.\nHowever, concept pairs organize by relation type in high-dimensional embedding\nspace only after fine-tuning on supervised relation classification tasks. This\nindicates relational schemata are not emergent from pretraining alone but can\nbe induced via task scaffolding. These findings demonstrate that behavioral\nperformance does not necessarily imply structured conceptual understanding,\nthough models can acquire inductive biases for grounded relational abstraction\nthrough appropriate training.\n","authors":["Cole Gawin"],"pdf_url":"https://arxiv.org/pdf/2506.11485v1.pdf","comment":"15 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2506.11478v1","updated":"2025-06-13T06:00:03Z","published":"2025-06-13T06:00:03Z","title":"ImmunoFOMO: Are Language Models missing what oncologists see?","summary":"  Language models (LMs) capabilities have grown with a fast pace over the past\ndecade leading researchers in various disciplines, such as biomedical research,\nto increasingly explore the utility of LMs in their day-to-day applications.\nDomain specific language models have already been in use for biomedical natural\nlanguage processing (NLP) applications. Recently however, the interest has\ngrown towards medical language models and their understanding capabilities. In\nthis paper, we investigate the medical conceptual grounding of various language\nmodels against expert clinicians for identification of hallmarks of\nimmunotherapy in breast cancer abstracts. Our results show that pre-trained\nlanguage models have potential to outperform large language models in\nidentifying very specific (low-level) concepts.\n","authors":["Aman Sinha","Bogdan-Valentin Popescu","Xavier Coubez","Marianne Clausel","Mathieu Constant"],"pdf_url":"https://arxiv.org/pdf/2506.11478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.18415v2","updated":"2025-06-13T05:55:26Z","published":"2025-04-25T15:17:52Z","title":"BitNet v2: Native 4-bit Activations with Hadamard Transformation for\n  1-bit LLMs","summary":"  Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference.\n","authors":["Hongyu Wang","Shuming Ma","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2504.18415v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2506.11475v1","updated":"2025-06-13T05:39:28Z","published":"2025-06-13T05:39:28Z","title":"AutoGen Driven Multi Agent Framework for Iterative Crime Data Analysis\n  and Prediction","summary":"  This paper introduces LUCID-MA (Learning and Understanding Crime through\nDialogue of Multiple Agents), an innovative AI powered framework where multiple\nAI agents collaboratively analyze and understand crime data. Our system that\nconsists of three core components: an analysis assistant that highlights\nspatiotemporal crime patterns, a feedback component that reviews and refines\nanalytical results and a prediction component that forecasts future crime\ntrends. With a well-designed prompt and the LLaMA-2-13B-Chat-GPTQ model, it\nruns completely offline and allows the agents undergo self-improvement through\n100 rounds of communication with less human interaction. A scoring function is\nincorporated to evaluate agent's performance, providing visual plots to track\nlearning progress. This work demonstrates the potential of AutoGen-style agents\nfor autonomous, scalable, and iterative analysis in social science domains\nmaintaining data privacy through offline execution.\n","authors":["Syeda Kisaa Fatima","Tehreem Zubair","Noman Ahmed","Asifullah Khan"],"pdf_url":"https://arxiv.org/pdf/2506.11475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11474v1","updated":"2025-06-13T05:36:30Z","published":"2025-06-13T05:36:30Z","title":"Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified\n  Process Rewards","summary":"  Large language models have shown promise in clinical decision making, but\ncurrent approaches struggle to localize and correct errors at specific steps of\nthe reasoning process. This limitation is critical in medicine, where\nidentifying and addressing reasoning errors is essential for accurate diagnosis\nand effective patient care. We introduce Med-PRM, a process reward modeling\nframework that leverages retrieval-augmented generation to verify each\nreasoning step against established medical knowledge bases. By verifying\nintermediate reasoning steps with evidence retrieved from clinical guidelines\nand literature, our model can precisely assess the reasoning quality in a\nfine-grained manner. Evaluations on five medical QA benchmarks and two\nopen-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art\nperformance, with improving the performance of base models by up to 13.50%\nusing Med-PRM. Moreover, we demonstrate the generality of Med-PRM by\nintegrating it in a plug-and-play fashion with strong policy models such as\nMeerkat, achieving over 80\\% accuracy on MedQA for the first time using\nsmall-scale models of 8 billion parameters. Our code and data are available at:\nhttps://med-prm.github.io/\n","authors":["Jaehoon Yun","Jiwoong Sohn","Jungwoo Park","Hyunjae Kim","Xiangru Tang","Yanjun Shao","Yonghoe Koo","Minhyeok Ko","Qingyu Chen","Mark Gerstein","Michael Moor","Jaewoo Kang"],"pdf_url":"https://arxiv.org/pdf/2506.11474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04210v2","updated":"2025-06-13T05:29:27Z","published":"2025-06-04T17:55:09Z","title":"Does Thinking More always Help? Understanding Test-Time Scaling in\n  Reasoning Models","summary":"  Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,\nDeepSeek R1) have led to a popular belief that extending thinking traces using\nprompts like \"Wait\" or \"Let me rethink\" can improve performance. This raises a\nnatural question: Does thinking more at test-time truly lead to better\nreasoning? To answer this question, we perform a detailed empirical study\nacross models and benchmarks, which reveals a consistent pattern of initial\nperformance improvements from additional thinking followed by a decline, due to\n\"overthinking\". To understand this non-monotonic trend, we consider a simple\nprobabilistic model, which reveals that additional thinking increases output\nvariance-creating an illusion of improved reasoning while ultimately\nundermining precision. Thus, observed gains from \"more thinking\" are not true\nindicators of improved reasoning, but artifacts stemming from the connection\nbetween model uncertainty and evaluation metric. This suggests that test-time\nscaling through extended thinking is not an effective way to utilize the\ninference thinking budget. Recognizing these limitations, we introduce an\nalternative test-time scaling approach, parallel thinking, inspired by\nBest-of-N sampling. Our method generates multiple independent reasoning paths\nwithin the same inference budget and selects the most consistent response via\nmajority vote, achieving up to 20% higher accuracy compared to extended\nthinking. This provides a simple yet effective mechanism for test-time scaling\nof reasoning models.\n","authors":["Soumya Suvra Ghosal","Souradip Chakraborty","Avinash Reddy","Yifu Lu","Mengdi Wang","Dinesh Manocha","Furong Huang","Mohammad Ghavamzadeh","Amrit Singh Bedi"],"pdf_url":"https://arxiv.org/pdf/2506.04210v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19110v2","updated":"2025-06-13T04:49:28Z","published":"2025-02-26T13:01:49Z","title":"Conformal Linguistic Calibration: Trading-off between Factuality and\n  Specificity","summary":"  Language model outputs are not always reliable, thus prompting research into\nhow to adapt model responses based on uncertainty. Common approaches include:\n\\emph{abstention}, where models refrain from generating responses when\nuncertain; and \\emph{linguistic calibration}, where models hedge their\nstatements using uncertainty quantifiers. However, abstention can withhold\nvaluable information, while linguistically calibrated responses are often\nchallenging to leverage in downstream tasks. We propose a unified view,\nConformal Linguistic Calibration (CLC), which reinterprets linguistic\ncalibration as \\emph{answer set prediction}. First we present a framework\nconnecting abstention and linguistic calibration through the lens of linguistic\npragmatics. We then describe an implementation of CLC that allows for\ncontrolling the level of imprecision in model responses. Results demonstrate\nour method produces calibrated outputs with conformal guarantees on factual\naccuracy. Further, our approach enables fine-tuning models to perform\nuncertainty-aware adaptive claim rewriting, offering a controllable balance\nbetween factuality and specificity.\n","authors":["Zhengping Jiang","Anqi Liu","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2502.19110v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11467v1","updated":"2025-06-13T04:42:16Z","published":"2025-06-13T04:42:16Z","title":"A Gamified Evaluation and Recruitment Platform for Low Resource Language\n  Machine Translation Systems","summary":"  Human evaluators provide necessary contributions in evaluating large language\nmodels. In the context of Machine Translation (MT) systems for low-resource\nlanguages (LRLs), this is made even more apparent since popular automated\nmetrics tend to be string-based, and therefore do not provide a full picture of\nthe nuances of the behavior of the system. Human evaluators, when equipped with\nthe necessary expertise of the language, will be able to test for adequacy,\nfluency, and other important metrics. However, the low resource nature of the\nlanguage means that both datasets and evaluators are in short supply. This\npresents the following conundrum: How can developers of MT systems for these\nLRLs find adequate human evaluators and datasets? This paper first presents a\ncomprehensive review of existing evaluation procedures, with the objective of\nproducing a design proposal for a platform that addresses the resource gap in\nterms of datasets and evaluators in developing MT systems. The result is a\ndesign for a recruitment and gamified evaluation platform for developers of MT\nsystems. Challenges are also discussed in terms of evaluating this platform, as\nwell as its possible applications in the wider scope of Natural Language\nProcessing (NLP) research.\n","authors":["Carlos Rafael Catalan"],"pdf_url":"https://arxiv.org/pdf/2506.11467v1.pdf","comment":"7 pages, 7 figures, presented at the HEAL Workshop at CHI"},{"id":"http://arxiv.org/abs/2506.10963v2","updated":"2025-06-13T04:39:54Z","published":"2025-06-12T17:58:09Z","title":"MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for\n  Text-to-Image Reasoning","summary":"  In this paper, we introduce knowledge image generation as a new task,\nalongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation\nBenchmark (MMMG) to probe the reasoning capability of image generation models.\nKnowledge images have been central to human civilization and to the mechanisms\nof human learning -- a fact underscored by dual-coding theory and the\npicture-superiority effect. Generating such images is challenging, demanding\nmultimodal reasoning that fuses world knowledge with pixel-level grounding into\nclear explanatory visuals. To enable comprehensive evaluation, MMMG offers\n4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines,\n6 educational levels, and diverse knowledge formats such as charts, diagrams,\nand mind maps. To eliminate confounding complexity during evaluation, we adopt\na unified Knowledge Graph (KG) representation. Each KG explicitly delineates a\ntarget image's core entities and their dependencies. We further introduce\nMMMG-Score to evaluate generated knowledge images. This metric combines factual\nfidelity, measured by graph-edit distance between KGs, with visual clarity\nassessment. Comprehensive evaluations of 16 state-of-the-art text-to-image\ngeneration models expose serious reasoning deficits -- low entity fidelity,\nweak relations, and clutter -- with GPT-4o achieving an MMMG-Score of only\n50.20, underscoring the benchmark's difficulty. To spur further progress, we\nrelease FLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that\ncombines a reasoning LLM with diffusion models and is trained on 16,000 curated\nknowledge image-prompt pairs.\n","authors":["Yuxuan Luo","Yuhui Yuan","Junwen Chen","Haonan Cai","Ziyi Yue","Yuwei Yang","Fatima Zohra Daha","Ji Li","Zhouhui Lian"],"pdf_url":"https://arxiv.org/pdf/2506.10963v2.pdf","comment":"85 pages, 70 figures, code: https://github.com/MMMGBench/MMMG,\n  project page: https://mmmgbench.github.io/"},{"id":"http://arxiv.org/abs/2409.19243v2","updated":"2025-06-13T04:26:26Z","published":"2024-09-28T05:19:51Z","title":"Jointly modelling the evolution of social structure and language in\n  online communities","summary":"  Group interactions take place within a particular socio-temporal context,\nwhich should be taken into account when modelling interactions in online\ncommunities. We propose a method for jointly modelling community structure and\nlanguage over time. Our system produces dynamic word and user representations\nthat can be used to cluster users, investigate thematic interests of groups,\nand predict group membership. We apply and evaluate our method in the context\nof a set of misogynistic extremist groups. Our results indicate that this\napproach outperforms prior models which lacked one of these components (i.e.\nnot incorporating social structure, or using static word embeddings) when\nevaluated on clustering and embedding prediction tasks. Our method further\nenables novel types of analyses on online groups, including tracing their\nresponse to temporal events and quantifying their propensity for using violent\nlanguage, which is of particular importance in the context of extremist groups.\n","authors":["Christine de Kock"],"pdf_url":"https://arxiv.org/pdf/2409.19243v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.07044v4","updated":"2025-06-13T04:22:02Z","published":"2025-06-08T08:47:30Z","title":"Lingshu: A Generalist Foundation Model for Unified Multimodal Medical\n  Understanding and Reasoning","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in understanding common visual elements, largely due to their\nlarge-scale datasets and advanced training strategies. However, their\neffectiveness in medical applications remains limited due to the inherent\ndiscrepancies between data and tasks in medical scenarios and those in the\ngeneral domain. Concretely, existing medical MLLMs face the following critical\nlimitations: (1) limited coverage of medical knowledge beyond imaging, (2)\nheightened susceptibility to hallucinations due to suboptimal data curation\nprocesses, (3) lack of reasoning capabilities tailored for complex medical\nscenarios. To address these challenges, we first propose a comprehensive data\ncuration procedure that (1) efficiently acquires rich medical knowledge data\nnot only from medical imaging but also from extensive medical texts and\ngeneral-domain data; and (2) synthesizes accurate medical captions, visual\nquestion answering (VQA), and reasoning samples. As a result, we build a\nmultimodal dataset enriched with extensive medical knowledge. Building on the\ncurated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu\nundergoes multi-stage training to embed medical expertise and enhance its\ntask-solving capabilities progressively. Besides, we preliminarily explore the\npotential of applying reinforcement learning with verifiable rewards paradigm\nto enhance Lingshu's medical reasoning ability. Additionally, we develop\nMedEvalKit, a unified evaluation framework that consolidates leading multimodal\nand textual medical benchmarks for standardized, fair, and efficient model\nassessment. We evaluate the performance of Lingshu on three fundamental medical\ntasks, multimodal QA, text-based QA, and medical report generation. The results\nshow that Lingshu consistently outperforms the existing open-source multimodal\nmodels on most tasks ...\n","authors":[" LASA Team","Weiwen Xu","Hou Pong Chan","Long Li","Mahani Aljunied","Ruifeng Yuan","Jianyu Wang","Chenghao Xiao","Guizhen Chen","Chaoqun Liu","Zhaodonghui Li","Yu Sun","Junao Shen","Chaojun Wang","Jie Tan","Deli Zhao","Tingyang Xu","Hao Zhang","Yu Rong"],"pdf_url":"https://arxiv.org/pdf/2506.07044v4.pdf","comment":"Technical Report, 53 pages, 25 tables, and 16 figures. Our webpage is\n  https://alibaba-damo-academy.github.io/lingshu/"},{"id":"http://arxiv.org/abs/2506.04518v2","updated":"2025-06-13T03:55:18Z","published":"2025-06-04T23:53:49Z","title":"Towards Efficient Speech-Text Jointly Decoding within One Speech\n  Language Model","summary":"  Speech language models (Speech LMs) enable end-to-end speech-text modelling\nwithin a single model, offering a promising direction for spoken dialogue\nsystems. The choice of speech-text jointly decoding paradigm plays a critical\nrole in performance, efficiency, and alignment quality. In this work, we\nsystematically compare representative joint speech-text decoding\nstrategies-including the interleaved, and parallel generation paradigms-under a\ncontrolled experimental setup using the same base language model, speech\ntokenizer and training data. Our results show that the interleaved approach\nachieves the best alignment. However it suffers from slow inference due to long\ntoken sequence length. To address this, we propose a novel early-stop\ninterleaved (ESI) pattern that not only significantly accelerates decoding but\nalso yields slightly better performance. Additionally, we curate high-quality\nquestion answering (QA) datasets to further improve speech QA performance.\n","authors":["Haibin Wu","Yuxuan Hu","Ruchao Fan","Xiaofei Wang","Kenichi Kumatani","Bo Ren","Jianwei Yu","Heng Lu","Lijuan Wang","Yao Qian","Jinyu Li"],"pdf_url":"https://arxiv.org/pdf/2506.04518v2.pdf","comment":"Our company need to do internal review"},{"id":"http://arxiv.org/abs/2410.21027v2","updated":"2025-06-13T03:54:32Z","published":"2024-10-28T13:48:43Z","title":"Transferable Post-training via Inverse Value Learning","summary":"  As post-training processes utilize increasingly large datasets and base\nmodels continue to grow in size, the computational demands and implementation\nchallenges of existing algorithms are escalating significantly. In this paper,\nwe propose modeling the changes at the logits level during post-training using\na separate neural network (i.e., the value network). After training this\nnetwork on a small base model using demonstrations, this network can be\nseamlessly integrated with other pre-trained models during inference, enables\nthem to achieve similar capability enhancements. We systematically investigate\nthe best practices for this paradigm in terms of pre-training weights and\nconnection schemes. We demonstrate that the resulting value network has broad\ntransferability across pre-trained models of different parameter sizes within\nthe same family, models undergoing continuous pre-training within the same\nfamily, and models with different vocabularies across families. In certain\ncases, it can achieve performance comparable to full-parameter fine-tuning.\nFurthermore, we explore methods to enhance the transferability of the value\nmodel and prevent overfitting to the base model used during training.\n","authors":["Xinyu Lu","Xueru Wen","Yaojie Lu","Bowen Yu","Hongyu Lin","Haiyang Yu","Le Sun","Xianpei Han","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2410.21027v2.pdf","comment":"NAACL 2025 Camera Ready"},{"id":"http://arxiv.org/abs/2506.11440v1","updated":"2025-06-13T03:38:29Z","published":"2025-06-13T03:38:29Z","title":"AbsenceBench: Language Models Can't Tell What's Missing","summary":"  Large language models (LLMs) are increasingly capable of processing long\ninputs and locating specific information within them, as evidenced by their\nperformance on the Needle in a Haystack (NIAH) test. However, while models\nexcel at recalling surprising information, they still struggle to identify\nclearly omitted information. We introduce AbsenceBench to assesses LLMs'\ncapacity to detect missing information across three domains: numerical\nsequences, poetry, and GitHub pull requests. AbsenceBench asks models to\nidentify which pieces of a document were deliberately removed, given access to\nboth the original and edited contexts. Despite the apparent straightforwardness\nof these tasks, our experiments reveal that even state-of-the-art models like\nClaude-3.7-Sonnet achieve only 69.6% F1-score with a modest average context\nlength of 5K tokens. Our analysis suggests this poor performance stems from a\nfundamental limitation: Transformer attention mechanisms cannot easily attend\nto \"gaps\" in documents since these absences don't correspond to any specific\nkeys that can be attended to. Overall, our results and analysis provide a case\nstudy of the close proximity of tasks where models are already superhuman\n(NIAH) and tasks where models breakdown unexpectedly (AbsenceBench).\n","authors":["Harvey Yiyun Fu","Aryan Shrivastava","Jared Moore","Peter West","Chenhao Tan","Ari Holtzman"],"pdf_url":"https://arxiv.org/pdf/2506.11440v1.pdf","comment":"23 pages, 8 figures. Code and data are publicly available at\n  https://github.com/harvey-fin/absence-bench"},{"id":"http://arxiv.org/abs/2506.00637v2","updated":"2025-06-13T03:30:44Z","published":"2025-05-31T17:01:45Z","title":"Improving the Calibration of Confidence Scores in Text Generation Using\n  the Output Distribution's Characteristics","summary":"  Well-calibrated model confidence scores can improve the usefulness of text\ngeneration models. For example, users can be prompted to review predictions\nwith low confidence scores, to prevent models from returning bad or potentially\ndangerous predictions. However, confidence metrics are not always well\ncalibrated in text generation. One reason is that in generation, there can be\nmany valid answers, which previous methods do not always account for. Hence, a\nconfident model could distribute its output probability among multiple\nsequences because they are all valid. We propose task-agnostic confidence\nmetrics suited to generation, which rely solely on the probabilities associated\nwith the model outputs without the need for further fine-tuning or heuristics.\nUsing these, we are able to improve the calibration of BART and Flan-T5 on\nsummarization, translation, and QA datasets.\n","authors":["Lorenzo Jaime Yu Flores","Ori Ernst","Jackie Chi Kit Cheung"],"pdf_url":"https://arxiv.org/pdf/2506.00637v2.pdf","comment":"ACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2506.11432v1","updated":"2025-06-13T03:10:15Z","published":"2025-06-13T03:10:15Z","title":"KoGEC : Korean Grammatical Error Correction with Pre-trained Translation\n  Models","summary":"  This research introduces KoGEC, a Korean Grammatical Error Correction system\nusing pre\\--trained translation models. We fine-tuned NLLB (No Language Left\nBehind) models for Korean GEC, comparing their performance against large\nlanguage models like GPT-4 and HCX-3. The study used two social media\nconversation datasets for training and testing. The NLLB models were fine-tuned\nusing special language tokens to distinguish between original and corrected\nKorean sentences. Evaluation was done using BLEU scores and an \"LLM as judge\"\nmethod to classify error types. Results showed that the fine-tuned NLLB (KoGEC)\nmodels outperformed GPT-4o and HCX-3 in Korean GEC tasks. KoGEC demonstrated a\nmore balanced error correction profile across various error types, whereas the\nlarger LLMs tended to focus less on punctuation errors. We also developed a\nChrome extension to make the KoGEC system accessible to users. Finally, we\nexplored token vocabulary expansion to further improve the model but found it\nto decrease model performance. This research contributes to the field of NLP by\nproviding an efficient, specialized Korean GEC system and a new evaluation\nmethod. It also highlights the potential of compact, task-specific models to\ncompete with larger, general-purpose language models in specialized NLP tasks.\n","authors":["Taeeun Kim","Semin Jeong","Youngsook Song"],"pdf_url":"https://arxiv.org/pdf/2506.11432v1.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.07910v3","updated":"2025-06-13T03:03:21Z","published":"2024-02-27T02:00:28Z","title":"MAGPIE: Multi-Task Media-Bias Analysis Generalization for Pre-Trained\n  Identification of Expressions","summary":"  Media bias detection poses a complex, multifaceted problem traditionally\ntackled using single-task models and small in-domain datasets, consequently\nlacking generalizability. To address this, we introduce MAGPIE, the first\nlarge-scale multi-task pre-training approach explicitly tailored for media bias\ndetection. To enable pre-training at scale, we present Large Bias Mixture\n(LBM), a compilation of 59 bias-related tasks. MAGPIE outperforms previous\napproaches in media bias detection on the Bias Annotation By Experts (BABE)\ndataset, with a relative improvement of 3.3% F1-score. MAGPIE also performs\nbetter than previous models on 5 out of 8 tasks in the Media Bias\nIdentification Benchmark (MBIB). Using a RoBERTa encoder, MAGPIE needs only 15%\nof finetuning steps compared to single-task approaches. Our evaluation shows,\nfor instance, that tasks like sentiment and emotionality boost all learning,\nall tasks enhance fake news detection, and scaling tasks leads to the best\nresults. MAGPIE confirms that MTL is a promising approach for addressing media\nbias detection, enhancing the accuracy and efficiency of existing models.\nFurthermore, LBM is the first available resource collection focused on media\nbias MTL.\n","authors":["Tomáš Horych","Martin Wessel","Jan Philip Wahle","Terry Ruas","Jerome Waßmuth","André Greiner-Petter","Akiko Aizawa","Bela Gipp","Timo Spinde"],"pdf_url":"https://arxiv.org/pdf/2403.07910v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15694v2","updated":"2025-06-13T03:01:26Z","published":"2024-11-24T03:17:37Z","title":"Deep Sparse Latent Feature Models for Knowledge Graph Completion","summary":"  Recent advances in knowledge graph completion (KGC) have emphasized\ntext-based approaches to navigate the inherent complexities of large-scale\nknowledge graphs (KGs). While these methods have achieved notable progress,\nthey frequently struggle to fully incorporate the global structural properties\nof the graph. Stochastic blockmodels (SBMs), especially the latent feature\nrelational model (LFRM), offer robust probabilistic frameworks for identifying\nlatent community structures and improving link prediction. This paper presents\na novel probabilistic KGC framework utilizing sparse latent feature models,\noptimized via a deep variational autoencoder (VAE). Our proposed method\ndynamically integrates global clustering information with local textual\nfeatures to effectively complete missing triples, while also providing enhanced\ninterpretability of the underlying latent structures. Extensive experiments on\nfour benchmark datasets with varying scales demonstrate the significant\nperformance gains achieved by our method.\n","authors":["Haotian Li","Rui Zhang","Lingzhi Wang","Bin Yu","Youwei Wang","Yuliang Wei","Kai Wang","Richard Yi Da Xu","Bailing Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15694v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11425v1","updated":"2025-06-13T02:46:53Z","published":"2025-06-13T02:46:53Z","title":"Agent-RLVR: Training Software Engineering Agents via Guidance and\n  Environment Rewards","summary":"  Reinforcement Learning from Verifiable Rewards (RLVR) has been widely adopted\nas the de facto method for enhancing the reasoning capabilities of large\nlanguage models and has demonstrated notable success in verifiable domains like\nmath and competitive programming tasks. However, the efficacy of RLVR\ndiminishes significantly when applied to agentic environments. These settings,\ncharacterized by multi-step, complex problem solving, lead to high failure\nrates even for frontier LLMs, as the reward landscape is too sparse for\neffective model training via conventional RLVR. In this work, we introduce\nAgent-RLVR, a framework that makes RLVR effective in challenging agentic\nsettings, with an initial focus on software engineering tasks. Inspired by\nhuman pedagogy, Agent-RLVR introduces agent guidance, a mechanism that actively\nsteers the agent towards successful trajectories by leveraging diverse\ninformational cues. These cues, ranging from high-level strategic plans to\ndynamic feedback on the agent's errors and environmental interactions, emulate\na teacher's guidance, enabling the agent to navigate difficult solution spaces\nand promotes active self-improvement via additional environment exploration. In\nthe Agent-RLVR training loop, agents first attempt to solve tasks to produce\ninitial trajectories, which are then validated by unit tests and supplemented\nwith agent guidance. Agents then reattempt with guidance, and the agent policy\nis updated with RLVR based on the rewards of these guided trajectories.\nAgent-RLVR elevates the pass@1 performance of Qwen-2.5-72B-Instruct from 9.4%\nto 22.4% on SWE-Bench Verified. We find that our guidance-augmented RLVR data\nis additionally useful for test-time reward model training, shown by further\nboosting pass@1 to 27.8%. Agent-RLVR lays the groundwork for training agents\nwith RLVR in complex, real-world environments where conventional RL methods\nstruggle.\n","authors":["Jeff Da","Clinton Wang","Xiang Deng","Yuntao Ma","Nikhil Barhate","Sean Hendryx"],"pdf_url":"https://arxiv.org/pdf/2506.11425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11418v1","updated":"2025-06-13T02:36:15Z","published":"2025-06-13T02:36:15Z","title":"Efficient Long-Context LLM Inference via KV Cache Clustering","summary":"  Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$.\n","authors":["Jie Hu","Shengnan Wang","Yutong He","Ping Gong","Jiawei Yi","Juncheng Zhang","Youhui Bai","Renhai Chen","Gong Zhang","Cheng Li","Kun Yuan"],"pdf_url":"https://arxiv.org/pdf/2506.11418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20813v3","updated":"2025-06-13T02:33:26Z","published":"2025-05-27T07:22:00Z","title":"RSCF: Relation-Semantics Consistent Filter for Entity Embedding of\n  Knowledge Graph","summary":"  In knowledge graph embedding, leveraging relation specific entity\ntransformation has markedly enhanced performance. However, the consistency of\nembedding differences before and after transformation remains unaddressed,\nrisking the loss of valuable inductive bias inherent in the embeddings. This\ninconsistency stems from two problems. First, transformation representations\nare specified for relations in a disconnected manner, allowing dissimilar\ntransformations and corresponding entity embeddings for similar relations.\nSecond, a generalized plug-in approach as a SFBR (Semantic Filter Based on\nRelations) disrupts this consistency through excessive concentration of entity\nembeddings under entity-based regularization, generating indistinguishable\nscore distributions among relations. In this paper, we introduce a plug-in KGE\nmethod, Relation-Semantics Consistent Filter (RSCF). Its entity transformation\nhas three features for enhancing semantic consistency: 1) shared affine\ntransformation of relation embeddings across all relations, 2) rooted entity\ntransformation that adds an entity embedding to its change represented by the\ntransformed vector, and 3) normalization of the change to prevent scale\nreduction. To amplify the advantages of consistency that preserve semantics on\nembeddings, RSCF adds relation transformation and prediction modules for\nenhancing the semantics. In knowledge graph completion tasks with\ndistance-based and tensor decomposition models, RSCF significantly outperforms\nstate-of-the-art KGE methods, showing robustness across all relations and their\nfrequencies.\n","authors":["Junsik Kim","Jinwook Park","Kangil Kim"],"pdf_url":"https://arxiv.org/pdf/2505.20813v3.pdf","comment":"Accepted to ACL 2025, 17 pages, 10 figures"},{"id":"http://arxiv.org/abs/2506.10521v2","updated":"2025-06-13T02:32:48Z","published":"2025-06-12T09:29:16Z","title":"Scientists' First Exam: Probing Cognitive Abilities of MLLM via\n  Perception, Understanding, and Reasoning","summary":"  Scientific discoveries increasingly rely on complex multimodal reasoning\nbased on information-intensive scientific data and domain-specific expertise.\nEmpowered by expert-level scientific benchmarks, scientific Multimodal Large\nLanguage Models (MLLMs) hold the potential to significantly enhance this\ndiscovery process in realistic workflows. However, current scientific\nbenchmarks mostly focus on evaluating the knowledge understanding capabilities\nof MLLMs, leading to an inadequate assessment of their perception and reasoning\nabilities. To address this gap, we present the Scientists' First Exam (SFE)\nbenchmark, designed to evaluate the scientific cognitive capacities of MLLMs\nthrough three interconnected levels: scientific signal perception, scientific\nattribute understanding, scientific comparative reasoning. Specifically, SFE\ncomprises 830 expert-verified VQA pairs across three question types, spanning\n66 multimodal tasks across five high-value disciplines. Extensive experiments\nreveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08%\nand 26.52% on SFE, highlighting significant room for MLLMs to improve in\nscientific realms. We hope the insights obtained in SFE will facilitate further\ndevelopments in AI-enhanced scientific discoveries.\n","authors":["Yuhao Zhou","Yiheng Wang","Xuming He","Ruoyao Xiao","Zhiwei Li","Qiantai Feng","Zijie Guo","Yuejin Yang","Hao Wu","Wenxuan Huang","Jiaqi Wei","Dan Si","Xiuqi Yao","Jia Bu","Haiwen Huang","Tianfan Fu","Shixiang Tang","Ben Fei","Dongzhan Zhou","Fenghua Ling","Yan Lu","Siqi Sun","Chenhui Li","Guanjie Zheng","Jiancheng Lv","Wenlong Zhang","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2506.10521v2.pdf","comment":"82 pages"},{"id":"http://arxiv.org/abs/2506.10848v2","updated":"2025-06-13T02:28:47Z","published":"2025-06-12T16:08:28Z","title":"Accelerating Diffusion Large Language Models with SlowFast Sampling: The\n  Three Golden Principles","summary":"  Diffusion-based language models (dLLMs) have emerged as a promising\nalternative to traditional autoregressive LLMs by enabling parallel token\ngeneration and significantly reducing inference latency. However, existing\nsampling strategies for dLLMs, such as confidence-based or semi-autoregressive\ndecoding, often suffer from static behavior, leading to suboptimal efficiency\nand limited flexibility. In this paper, we propose SlowFast Sampling, a novel\ndynamic sampling strategy that adaptively alternates between exploratory and\naccelerated decoding stages. Our method is guided by three golden principles:\ncertainty principle, convergence principle, and positional principle, which\ngovern when and where tokens can be confidently and efficiently decoded. We\nfurther integrate our strategy with dLLM-Cache to reduce redundant computation.\nExtensive experiments across benchmarks and models show that SlowFast Sampling\nachieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and\nup to 34.22$\\times$ when combined with caching. Notably, our approach\noutperforms strong autoregressive baselines like LLaMA3 8B in throughput,\ndemonstrating that well-designed sampling can unlock the full potential of\ndLLMs for fast and high-quality generation.\n","authors":["Qingyan Wei","Yaojie Zhang","Zhiyuan Liu","Dongrui Liu","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.10848v2.pdf","comment":"11 pages; 5 figures;"},{"id":"http://arxiv.org/abs/2506.11415v1","updated":"2025-06-13T02:28:46Z","published":"2025-06-13T02:28:46Z","title":"Bias Amplification in RAG: Poisoning Knowledge Retrieval to Steer LLMs","summary":"  In Large Language Models, Retrieval-Augmented Generation (RAG) systems can\nsignificantly enhance the performance of large language models by integrating\nexternal knowledge. However, RAG also introduces new security risks. Existing\nresearch focuses mainly on how poisoning attacks in RAG systems affect model\noutput quality, overlooking their potential to amplify model biases. For\nexample, when querying about domestic violence victims, a compromised RAG\nsystem might preferentially retrieve documents depicting women as victims,\ncausing the model to generate outputs that perpetuate gender stereotypes even\nwhen the original query is gender neutral. To show the impact of the bias, this\npaper proposes a Bias Retrieval and Reward Attack (BRRA) framework, which\nsystematically investigates attack pathways that amplify language model biases\nthrough a RAG system manipulation. We design an adversarial document generation\nmethod based on multi-objective reward functions, employ subspace projection\ntechniques to manipulate retrieval results, and construct a cyclic feedback\nmechanism for continuous bias amplification. Experiments on multiple mainstream\nlarge language models demonstrate that BRRA attacks can significantly enhance\nmodel biases in dimensions. In addition, we explore a dual stage defense\nmechanism to effectively mitigate the impacts of the attack. This study reveals\nthat poisoning attacks in RAG systems directly amplify model output biases and\nclarifies the relationship between RAG system security and model fairness. This\nnovel potential attack indicates that we need to keep an eye on the fairness\nissues of the RAG system.\n","authors":["Linlin Wang","Tianqing Zhu","Laiqiao Qin","Longxiang Gao","Wanlei Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.11415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11410v1","updated":"2025-06-13T02:15:43Z","published":"2025-06-13T02:15:43Z","title":"Predicting Early-Onset Colorectal Cancer with Large Language Models","summary":"  The incidence rate of early-onset colorectal cancer (EoCRC, age < 45) has\nincreased every year, but this population is younger than the recommended age\nestablished by national guidelines for cancer screening. In this paper, we\napplied 10 different machine learning models to predict EoCRC, and compared\ntheir performance with advanced large language models (LLM), using patient\nconditions, lab results, and observations within 6 months of patient journey\nprior to the CRC diagnoses. We retrospectively identified 1,953 CRC patients\nfrom multiple health systems across the United States. The results demonstrated\nthat the fine-tuned LLM achieved an average of 73% sensitivity and 91%\nspecificity.\n","authors":["Wilson Lau","Youngwon Kim","Sravanthi Parasa","Md Enamul Haque","Anand Oka","Jay Nanduri"],"pdf_url":"https://arxiv.org/pdf/2506.11410v1.pdf","comment":"Paper accepted for the proceedings of the 2025 American Medical\n  Informatics Association Annual Symposium (AMIA)"},{"id":"http://arxiv.org/abs/2506.11402v1","updated":"2025-06-13T02:02:57Z","published":"2025-06-13T02:02:57Z","title":"LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned\n  Model","summary":"  Parameter Efficient FineTuning (PEFT), such as Low-Rank Adaptation (LoRA),\naligns pre-trained Large Language Models (LLMs) to particular downstream tasks\nin a resource-efficient manner. Because efficiency has been the main metric of\nprogress, very little attention has been put in understanding possible\ncatastrophic failures. We uncover one such failure: PEFT encourages a model to\nsearch for shortcut solutions to solve its fine-tuning tasks. When very small\namount of tokens, e.g., one token per prompt, are correlated with downstream\ntask classes, PEFT makes any pretrained model rely predominantly on that token\nfor decision making. While such spurious tokens may emerge accidentally from\nincorrect data cleaning, it also opens opportunities for malevolent parties to\ncontrol a model's behavior from Seamless Spurious Token Injection (SSTI). In\nSSTI, a small amount of tokens correlated with downstream classes are injected\nby the dataset creators. At test time, the finetuned LLM's behavior can be\ncontrolled solely by injecting those few tokens. We apply SSTI across models\nfrom three families (Snowflake Arctic, Apple OpenELM, and Meta LLaMA-3) and\nfour diverse datasets (IMDB, Financial Classification, CommonSense QA, and Bias\nin Bios). Our findings reveal three astonishing behaviors. First, as few as a\nsingle token of SSTI is sufficient to steer a model's decision making. Second,\nfor light SSTI, the reliance on spurious tokens is proportional to the LoRA\nrank. Lastly, with aggressive SSTI, larger LoRA rank values become preferable\nto small rank values as it makes the model attend to non-spurious tokens, hence\nimproving robustness.\n","authors":["Pradyut Sekhsaria","Marcel Mateos Salles","Hai Huang","Randall Balestriero"],"pdf_url":"https://arxiv.org/pdf/2506.11402v1.pdf","comment":"29 pages, 16 figures, 15 tables. Submitted for publication. for\n  associated blog post, see https://pradyut3501.github.io/lora-spur-corr/"},{"id":"http://arxiv.org/abs/2506.11389v1","updated":"2025-06-13T01:22:16Z","published":"2025-06-13T01:22:16Z","title":"Curriculum-Guided Layer Scaling for Language Model Pretraining","summary":"  As the cost of pretraining large language models grows, there is continued\ninterest in strategies to improve learning efficiency during this core training\nstage. Motivated by cognitive development, where humans gradually build\nknowledge as their brains mature, we propose Curriculum-Guided Layer Scaling\n(CGLS), a framework for compute-efficient pretraining that synchronizes\nincreasing data difficulty with model growth through progressive layer stacking\n(i.e. gradually adding layers during training). At the 100M parameter scale,\nusing a curriculum transitioning from synthetic short stories to general web\ndata, CGLS outperforms baseline methods on the question-answering benchmarks\nPIQA and ARC. Pretraining at the 1.2B scale, we stratify the DataComp-LM corpus\nwith a DistilBERT-based classifier and progress from general text to highly\ntechnical or specialized content. Our results show that progressively\nincreasing model depth alongside sample difficulty leads to better\ngeneralization and zero-shot performance on various downstream benchmarks.\nAltogether, our findings demonstrate that CGLS unlocks the potential of\nprogressive stacking, offering a simple yet effective strategy for improving\ngeneralization on knowledge-intensive and reasoning tasks.\n","authors":["Karanpartap Singh","Neil Band","Ehsan Adeli"],"pdf_url":"https://arxiv.org/pdf/2506.11389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11381v1","updated":"2025-06-13T01:03:42Z","published":"2025-06-13T01:03:42Z","title":"A Variational Approach for Mitigating Entity Bias in Relation Extraction","summary":"  Mitigating entity bias is a critical challenge in Relation Extraction (RE),\nwhere models often rely excessively on entities, resulting in poor\ngeneralization. This paper presents a novel approach to address this issue by\nadapting a Variational Information Bottleneck (VIB) framework. Our method\ncompresses entity-specific information while preserving task-relevant features.\nIt achieves state-of-the-art performance on relation extraction datasets across\ngeneral, financial, and biomedical domains, in both indomain (original test\nsets) and out-of-domain (modified test sets with type-constrained entity\nreplacements) settings. Our approach offers a robust, interpretable, and\ntheoretically grounded methodology.\n","authors":["Samuel Mensah","Elena Kochkina","Jabez Magomere","Joy Prakash Sain","Simerjot Kaur","Charese Smiley"],"pdf_url":"https://arxiv.org/pdf/2506.11381v1.pdf","comment":"Accepted at ACL 2025 Main"},{"id":"http://arxiv.org/abs/2506.11376v1","updated":"2025-06-13T00:47:57Z","published":"2025-06-13T00:47:57Z","title":"Large Language Model-Powered Conversational Agent Delivering\n  Problem-Solving Therapy (PST) for Family Caregivers: Enhancing Empathy and\n  Therapeutic Alliance Using In-Context Learning","summary":"  Family caregivers often face substantial mental health challenges due to\ntheir multifaceted roles and limited resources. This study explored the\npotential of a large language model (LLM)-powered conversational agent to\ndeliver evidence-based mental health support for caregivers, specifically\nProblem-Solving Therapy (PST) integrated with Motivational Interviewing (MI)\nand Behavioral Chain Analysis (BCA). A within-subject experiment was conducted\nwith 28 caregivers interacting with four LLM configurations to evaluate empathy\nand therapeutic alliance. The best-performing models incorporated Few-Shot and\nRetrieval-Augmented Generation (RAG) prompting techniques, alongside\nclinician-curated examples. The models showed improved contextual understanding\nand personalized support, as reflected by qualitative responses and\nquantitative ratings on perceived empathy and therapeutic alliances.\nParticipants valued the model's ability to validate emotions, explore\nunexpressed feelings, and provide actionable strategies. However, balancing\nthorough assessment with efficient advice delivery remains a challenge. This\nwork highlights the potential of LLMs in delivering empathetic and tailored\nsupport for family caregivers.\n","authors":["Liying Wang","Ph. D.","Daffodil Carrington","M. S.","Daniil Filienko","M. S.","Caroline El Jazmi","M. S.","Serena Jinchen Xie","M. S.","Martine De Cock","Ph. D.","Sarah Iribarren","Ph. D.","Weichao Yuwen","Ph. D"],"pdf_url":"https://arxiv.org/pdf/2506.11376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11375v1","updated":"2025-06-13T00:45:41Z","published":"2025-06-13T00:45:41Z","title":"Benchmarking Multimodal LLMs on Recognition and Understanding over\n  Chemical Tables","summary":"  Chemical tables encode complex experimental knowledge through symbolic\nexpressions, structured variables, and embedded molecular graphics. Existing\nbenchmarks largely overlook this multimodal and domain-specific complexity,\nlimiting the ability of multimodal large language models to support scientific\nunderstanding in chemistry. In this work, we introduce ChemTable, a large-scale\nbenchmark of real-world chemical tables curated from the experimental sections\nof literature. ChemTable includes expert-annotated cell polygons, logical\nlayouts, and domain-specific labels, including reagents, catalysts, yields, and\ngraphical components and supports two core tasks: (1) Table Recognition,\ncovering structure parsing and content extraction; and (2) Table Understanding,\nencompassing both descriptive and reasoning-oriented question answering\ngrounded in table structure and domain semantics. We evaluated a range of\nrepresentative multimodal models, including both open-source and closed-source\nmodels, on ChemTable and reported a series of findings with practical and\nconceptual insights. Although models show reasonable performance on basic\nlayout parsing, they exhibit substantial limitations on both descriptive and\ninferential QA tasks compared to human performance, and we observe significant\nperformance gaps between open-source and closed-source models across multiple\ndimensions. These results underscore the challenges of chemistry-aware table\nunderstanding and position ChemTable as a rigorous and realistic benchmark for\nadvancing scientific reasoning.\n","authors":["Yitong Zhou","Mingyue Cheng","Qingyang Mao","Yucong Luo","Qi Liu","Yupeng Li","Xiaohan Zhang","Deguang Liu","Xin Li","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2506.11375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13128v2","updated":"2025-06-13T00:25:54Z","published":"2025-04-17T17:44:06Z","title":"FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on\n  Technical Documents","summary":"  We introduce FreshStack, a holistic framework for automatically building\ninformation retrieval (IR) evaluation benchmarks by incorporating challenging\nquestions and answers. FreshStack conducts the following steps: (1) automatic\ncorpus collection from code and technical documentation, (2) nugget generation\nfrom community-asked questions and answers, and (3) nugget-level support,\nretrieving documents using a fusion of retrieval techniques and hybrid\narchitectures. We use FreshStack to build five datasets on fast-growing,\nrecent, and niche topics to ensure the tasks are sufficiently challenging. On\nFreshStack, existing retrieval models, when applied out-of-the-box,\nsignificantly underperform oracle approaches on all five topics, denoting\nplenty of headroom to improve IR quality. In addition, we identify cases where\nrerankers do not improve first-stage retrieval accuracy (two out of five\ntopics) and oracle context helps an LLM generator generate a high-quality RAG\nanswer. We hope FreshStack will facilitate future work toward constructing\nrealistic, scalable, and uncontaminated IR and RAG evaluation benchmarks.\n","authors":["Nandan Thakur","Jimmy Lin","Sam Havens","Michael Carbin","Omar Khattab","Andrew Drozdov"],"pdf_url":"https://arxiv.org/pdf/2504.13128v2.pdf","comment":"21 pages, 4 figures, 8 tables"}]},"2025-06-16T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2506.13752v1","updated":"2025-06-16T17:57:05Z","published":"2025-06-16T17:57:05Z","title":"Steering LLM Thinking with Budget Guidance","summary":"  Recent deep-thinking large language models often reason extensively to\nimprove performance, but such lengthy reasoning is not always desirable, as it\nincurs excessive inference costs with disproportionate performance gains.\nControlling reasoning length without sacrificing performance is therefore\nimportant, but remains challenging, especially under tight thinking budgets. We\npropose budget guidance, a simple yet effective method for steering the\nreasoning process of LLMs toward a target budget without requiring any LLM\nfine-tuning. Our approach introduces a lightweight predictor that models a\nGamma distribution over the remaining thinking length during next-token\ngeneration. This signal is then used to guide generation in a soft, token-level\nmanner, ensuring that the overall reasoning trace adheres to the specified\nthinking budget. Budget guidance enables natural control of the thinking\nlength, along with significant token efficiency improvements over baseline\nmethods on challenging math benchmarks. For instance, it achieves up to a 26%\naccuracy gain on the MATH-500 benchmark under tight budgets compared to\nbaseline methods, while maintaining competitive accuracy with only 63% of the\nthinking tokens used by the full-thinking model. Budget guidance also\ngeneralizes to broader task domains and exhibits emergent capabilities, such as\nestimating question difficulty. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/BudgetGuidance.\n","authors":["Junyan Li","Wenshuo Zhao","Yang Zhang","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2506.13752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13743v1","updated":"2025-06-16T17:53:18Z","published":"2025-06-16T17:53:18Z","title":"LTRR: Learning To Rank Retrievers for LLMs","summary":"  Retrieval-Augmented Generation (RAG) systems typically rely on a single fixed\nretriever, despite growing evidence that no single retriever performs optimally\nacross all query types. In this paper, we explore a query routing approach that\ndynamically selects from a pool of retrievers based on the query, using both\ntrain-free heuristics and learned routing models. We frame routing as a\nlearning-to-rank (LTR) problem and introduce LTRR, a framework that learns to\nrank retrievers by their expected utility gain to downstream LLM performance.\nOur experiments, conducted on synthetic QA data with controlled query type\nvariations, show that routing-based RAG systems can outperform the best\nsingle-retriever-based systems. Performance gains are especially pronounced in\nmodels trained with the Answer Correctness (AC) metric and with pairwise\nlearning approaches, especially with XGBoost. We also observe improvements in\ngeneralization to out-of-distribution queries. As part of the SIGIR 2025\nLiveRAG challenge, our submitted system demonstrated the practical viability of\nour approach, achieving competitive performance in both answer correctness and\nfaithfulness. These findings highlight the importance of both training\nmethodology and metric selection in query routing for RAG systems.\n","authors":["To Eun Kim","Fernando Diaz"],"pdf_url":"https://arxiv.org/pdf/2506.13743v1.pdf","comment":"SIGIR 2025 LiveRAG Spotlight"},{"id":"http://arxiv.org/abs/2506.13734v1","updated":"2025-06-16T17:42:35Z","published":"2025-06-16T17:42:35Z","title":"Instruction Following by Boosting Attention of Large Language Models","summary":"  Controlling the generation of large language models (LLMs) remains a central\nchallenge to ensure their safe and reliable deployment. While prompt\nengineering and finetuning are common approaches, recent work has explored\nlatent steering, a lightweight technique that alters LLM internal activations\nto guide generation. However, subsequent studies revealed latent steering's\neffectiveness to be limited, often underperforming simple instruction\nprompting. To address this limitation, we first establish a benchmark across\ndiverse behaviors for standardized evaluation of steering techniques. Building\non insights from this benchmark, we introduce Instruction Attention Boosting\n(InstABoost), a latent steering method that boosts the strength of instruction\nprompting by altering the model's attention during generation. InstABoost\ncombines the strengths of existing approaches and is theoretically supported by\nprior work that suggests that in-context rule following in transformer-based\nmodels can be controlled by manipulating attention on instructions.\nEmpirically, InstABoost demonstrates superior control success compared to both\ntraditional prompting and latent steering.\n","authors":["Vitoria Guardieiro","Adam Stein","Avishree Khare","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2506.13734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13727v1","updated":"2025-06-16T17:38:36Z","published":"2025-06-16T17:38:36Z","title":"Attribution-guided Pruning for Compression, Circuit Discovery, and\n  Targeted Correction in LLMs","summary":"  Large Language Models (LLMs) are central to many contemporary AI\napplications, yet their extensive parameter counts pose significant challenges\nfor deployment in memory- and compute-constrained environments. Recent works in\neXplainable AI (XAI), particularly on attribution methods, suggest that\ninterpretability can also enable model compression by identifying and removing\ncomponents irrelevant to inference. In this paper, we leverage Layer-wise\nRelevance Propagation (LRP) to perform attribution-guided pruning of LLMs.\nWhile LRP has shown promise in structured pruning for vision models, we extend\nit to unstructured pruning in LLMs and demonstrate that it can substantially\nreduce model size with minimal performance loss. Our method is especially\neffective in extracting task-relevant subgraphs -- so-called ``circuits'' --\nwhich can represent core functions (e.g., indirect object identification).\nBuilding on this, we introduce a technique for model correction, by selectively\nremoving circuits responsible for spurious behaviors (e.g., toxic outputs). All\nin all, we gather these techniques as a uniform holistic framework and showcase\nits effectiveness and limitations through extensive experiments for\ncompression, circuit discovery and model correction on Llama and OPT models,\nhighlighting its potential for improving both model efficiency and safety. Our\ncode is publicly available at https://github.com/erfanhatefi/SparC3.\n","authors":["Sayed Mohammad Vakilzadeh Hatefi","Maximilian Dreyer","Reduan Achtibat","Patrick Kahardipraja","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2506.13727v1.pdf","comment":"Work in progress (10 pages manuscript, 3 pages references, 12 pages\n  appendix)"},{"id":"http://arxiv.org/abs/2506.05606v2","updated":"2025-06-16T17:32:08Z","published":"2025-06-05T21:37:49Z","title":"OPeRA: A Dataset of Observation, Persona, Rationale, and Action for\n  Evaluating LLMs on Human Online Shopping Behavior Simulation","summary":"  Can large language models (LLMs) accurately simulate the next web action of a\nspecific user? While LLMs have shown promising capabilities in generating\n``believable'' human behaviors, evaluating their ability to mimic real user\nbehaviors remains an open challenge, largely due to the lack of high-quality,\npublicly available datasets that capture both the observable actions and the\ninternal reasoning of an actual human user. To address this gap, we introduce\nOPERA, a novel dataset of Observation, Persona, Rationale, and Action collected\nfrom real human participants during online shopping sessions. OPERA is the\nfirst public dataset that comprehensively captures: user personas, browser\nobservations, fine-grained web actions, and self-reported just-in-time\nrationales. We developed both an online questionnaire and a custom browser\nplugin to gather this dataset with high fidelity. Using OPERA, we establish the\nfirst benchmark to evaluate how well current LLMs can predict a specific user's\nnext action and rationale with a given persona and <observation, action,\nrationale> history. This dataset lays the groundwork for future research into\nLLM agents that aim to act as personalized digital twins for human.\n","authors":["Ziyi Wang","Yuxuan Lu","Wenbo Li","Amirali Amini","Bo Sun","Yakov Bart","Weimin Lyu","Jiri Gesi","Tian Wang","Jing Huang","Yu Su","Upol Ehsan","Malihe Alikhani","Toby Jia-Jun Li","Lydia Chilton","Dakuo Wang"],"pdf_url":"https://arxiv.org/pdf/2506.05606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13692v1","updated":"2025-06-16T16:54:03Z","published":"2025-06-16T16:54:03Z","title":"Balancing Knowledge Delivery and Emotional Comfort in Healthcare\n  Conversational Systems","summary":"  With the advancement of large language models, many dialogue systems are now\ncapable of providing reasonable and informative responses to patients' medical\nconditions. However, when patients consult their doctor, they may experience\nnegative emotions due to the severity and urgency of their situation. If the\nmodel can provide appropriate comfort and empathy based on the patient's\nnegative emotions while answering medical questions, it will likely offer a\nmore reassuring experience during the medical consultation process. To address\nthis issue, our paper explores the balance between knowledge sharing and\nemotional support in the healthcare dialogue process. We utilize a large\nlanguage model to rewrite a real-world interactive medical dialogue dataset,\ngenerating patient queries with negative emotions and corresponding medical\nresponses aimed at soothing the patient's emotions while addressing their\nconcerns. The modified data serves to refine the latest large language models\nwith various fine-tuning methods, enabling them to accurately provide sentences\nwith both emotional reassurance and constructive suggestions in response to\npatients' questions. Compared to the original LLM model, our experimental\nresults demonstrate that our methodology significantly enhances the model's\nability to generate emotional responses while maintaining its original\ncapability to provide accurate knowledge-based answers.\n","authors":["Shang-Chi Tsai","Yun-Nung Chen"],"pdf_url":"https://arxiv.org/pdf/2506.13692v1.pdf","comment":"IWSDS 2025 Oral Paper"},{"id":"http://arxiv.org/abs/2503.23077v2","updated":"2025-06-16T16:51:48Z","published":"2025-03-29T13:27:46Z","title":"Efficient Inference for Large Reasoning Models: A Survey","summary":"  Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in complex task-solving. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from performance and efficiency aspects. Besides, we present open\nchallenges in this field, including human-centric controllable reasoning,\ntrade-off between interpretability and efficiency of reasoning, ensuring safety\nof efficient reasoning, and broader applications of efficient reasoning. In\naddition, we highlight key insights for enhancing LRMs' inference efficiency\nvia techniques such as model merging, new architectures, and agent routers. We\nhope this work serves as a valuable guide, helping researchers overcome\nchallenges in this vibrant\nfield\\footnote{https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs}.\n","authors":["Yue Liu","Jiaying Wu","Yufei He","Hongcheng Gao","Hongyu Chen","Baolong Bi","Ruihan Gong","Jiaheng Zhang","Zhiqi Huang","Bryan Hooi"],"pdf_url":"https://arxiv.org/pdf/2503.23077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20273v4","updated":"2025-06-16T16:51:10Z","published":"2025-02-27T17:01:23Z","title":"How Much is Enough? The Diminishing Returns of Tokenization Training\n  Data","summary":"  Tokenization, a crucial initial step in natural language processing, is\ngoverned by several key parameters, such as the tokenization algorithm,\nvocabulary size, pre-tokenization strategy, inference strategy, and training\ndata corpus. This paper investigates the impact of an often-overlooked\nhyperparameter, tokenizer training data size. We train BPE, UnigramLM, and\nWordPiece tokenizers across various vocabulary sizes using English training\ndata ranging from 1GB to 900GB. Our findings reveal diminishing returns as\ntraining data size increases beyond roughly 150GB, suggesting a practical limit\nto the improvements in tokenization quality achievable through additional data.\nWe analyze this phenomenon and attribute the saturation effect to constraints\nintroduced by the pre-tokenization stage. We then demonstrate the extent to\nwhich these findings can generalize by experimenting on data in Russian, a\nlanguage typologically distant from English. For Russian text, we observe\ndiminishing returns after training a tokenizer from 200GB of data, which is\napproximately 33% more than when training on English. These results provide\nvaluable insights for optimizing the tokenization process by reducing the\ncompute required for training on large corpora and suggest promising directions\nfor future research in tokenization algorithms.\n","authors":["Varshini Reddy","Craig W. Schmidt","Yuval Pinter","Chris Tanner"],"pdf_url":"https://arxiv.org/pdf/2502.20273v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13681v1","updated":"2025-06-16T16:38:04Z","published":"2025-06-16T16:38:04Z","title":"Turning Down the Heat: A Critical Analysis of Min-p Sampling in Language\n  Models","summary":"  Sampling from language models impacts the quality and diversity of outputs,\naffecting both research and real-world applications. Recently, Nguyen et al.\n2024's \"Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM\nOutputs\" introduced a new sampler called min-p, claiming it achieves superior\nquality and diversity over established samplers such as basic, top-k, and top-p\nsampling. The significance of these claims was underscored by the paper's\nrecognition as the 18th highest-scoring submission to ICLR 2025 and selection\nfor an Oral presentation. This paper conducts a comprehensive re-examination of\nthe evidence supporting min-p and reaches different conclusions from the\noriginal paper's four lines of evidence. First, the original paper's human\nevaluations omitted data, conducted statistical tests incorrectly, and\ndescribed qualitative feedback inaccurately; our reanalysis demonstrates min-p\ndid not outperform baselines in quality, diversity, or a trade-off between\nquality and diversity; in response to our findings, the authors of the original\npaper conducted a new human evaluation using a different implementation, task,\nand rubric that nevertheless provides further evidence min-p does not improve\nover baselines. Second, comprehensively sweeping the original paper's NLP\nbenchmarks reveals min-p does not surpass baselines when controlling for the\nnumber of hyperparameters. Third, the original paper's LLM-as-a-Judge\nevaluations lack methodological clarity and appear inconsistently reported.\nFourth, community adoption claims (49k GitHub repositories, 1.1M GitHub stars)\nwere found to be unsubstantiated, leading to their removal; the revised\nadoption claim remains misleading. We conclude that evidence presented in the\noriginal paper fails to support claims that min-p improves quality, diversity,\nor a trade-off between quality and diversity.\n","authors":["Rylan Schaeffer","Joshua Kazdan","Yegor Denisov-Blanch"],"pdf_url":"https://arxiv.org/pdf/2506.13681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13674v1","updated":"2025-06-16T16:30:26Z","published":"2025-06-16T16:30:26Z","title":"Prefix-Tuning+: Modernizing Prefix-Tuning through Attention Independent\n  Prefix Data","summary":"  Parameter-Efficient Fine-Tuning (PEFT) methods have become crucial for\nrapidly adapting large language models (LLMs) to downstream tasks.\nPrefix-Tuning, an early and effective PEFT technique, demonstrated the ability\nto achieve performance comparable to full fine-tuning with significantly\nreduced computational and memory overhead. However, despite its earlier\nsuccess, its effectiveness in training modern state-of-the-art LLMs has been\nvery limited. In this work, we demonstrate empirically that Prefix-Tuning\nunderperforms on LLMs because of an inherent tradeoff between input and prefix\nsignificance within the attention head. This motivates us to introduce\nPrefix-Tuning+, a novel architecture that generalizes the principles of\nPrefix-Tuning while addressing its shortcomings by shifting the prefix module\nout of the attention head itself. We further provide an overview of our\nconstruction process to guide future users when constructing their own\ncontext-based methods. Our experiments show that, across a diverse set of\nbenchmarks, Prefix-Tuning+ consistently outperforms existing Prefix-Tuning\nmethods. Notably, it achieves performance on par with the widely adopted LoRA\nmethod on several general benchmarks, highlighting the potential modern\nextension of Prefix-Tuning approaches. Our findings suggest that by overcoming\nits inherent limitations, Prefix-Tuning can remain a competitive and relevant\nresearch direction in the landscape of parameter-efficient LLM adaptation.\n","authors":["Haonan Wang","Brian Chen","Li Siquan","Liang Xinhe","Tianyang Hu","Hwee Kuan Lee","Kenji Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2506.13674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03781v2","updated":"2025-06-16T16:25:05Z","published":"2025-06-04T09:42:17Z","title":"Unifying Uniform and Binary-coding Quantization for Accurate Compression\n  of Large Language Models","summary":"  How can we quantize large language models while preserving accuracy?\nQuantization is essential for deploying large language models (LLMs)\nefficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are\npromising quantization schemes that have strong expressiveness and\noptimizability, respectively. However, neither scheme leverages both\nadvantages. In this paper, we propose UniQuanF (Unified Quantization with\nFlexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses\nboth strong expressiveness and optimizability by unifying the flexible mapping\ntechnique in UQ and non-uniform quantization levels of BCQ. We propose unified\ninitialization, and local and periodic mapping techniques to optimize the\nparameters in UniQuanF precisely. After optimization, our unification theorem\nremoves computational and memory overhead, allowing us to utilize the superior\naccuracy of UniQuanF without extra deployment costs induced by the unification.\nExperimental results demonstrate that UniQuanF outperforms existing UQ and BCQ\nmethods, achieving up to 4.60% higher accuracy on GSM8K benchmark.\n","authors":["Seungcheol Park","Jeongin Bae","Beomseok Kwon","Minjun Kim","Byeongwook Kim","Se Jung Kwon","U Kang","Dongsoo Lee"],"pdf_url":"https://arxiv.org/pdf/2506.03781v2.pdf","comment":"ACL 2025 Main Track"},{"id":"http://arxiv.org/abs/2408.14568v2","updated":"2025-06-16T16:24:42Z","published":"2024-08-26T18:39:31Z","title":"Improving Clinical Note Generation from Complex Doctor-Patient\n  Conversation","summary":"  Writing clinical notes and documenting medical exams is a critical task for\nhealthcare professionals, serving as a vital component of patient care\ndocumentation. However, manually writing these notes is time-consuming and can\nimpact the amount of time clinicians can spend on direct patient interaction\nand other tasks. Consequently, the development of automated clinical note\ngeneration systems has emerged as a clinically meaningful area of research\nwithin AI for health. In this paper, we present three key contributions to the\nfield of clinical note generation using large language models (LLMs). First, we\nintroduce CliniKnote, a comprehensive dataset consisting of 1,200 complex\ndoctor-patient conversations paired with their full clinical notes. This\ndataset, created and curated by medical experts with the help of modern neural\nnetworks, provides a valuable resource for training and evaluating models in\nclinical note generation tasks. Second, we propose the K-SOAP (Keyword,\nSubjective, Objective, Assessment, and Plan) note format, which enhances\ntraditional SOAP~\\cite{podder2023soap} (Subjective, Objective, Assessment, and\nPlan) notes by adding a keyword section at the top, allowing for quick\nidentification of essential information. Third, we develop an automatic\npipeline to generate K-SOAP notes from doctor-patient conversations and\nbenchmark various modern LLMs using various metrics. Our results demonstrate\nsignificant improvements in efficiency and performance compared to standard LLM\nfinetuning methods.\n","authors":["Yizhan Li","Sifan Wu","Christopher Smith","Thomas Lo","Bang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.14568v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05317v2","updated":"2025-06-16T16:22:39Z","published":"2025-02-21T09:43:18Z","title":"On Synthesizing Data for Context Attribution in Question Answering","summary":"  Question Answering (QA) accounts for a significant portion of LLM usage \"in\nthe wild\". However, LLMs sometimes produce false or misleading responses, also\nknown as \"hallucinations\". Therefore, grounding the generated answers in\ncontextually provided information -- i.e., providing evidence for the generated\ntext -- is paramount for LLMs' trustworthiness. Providing this information is\nthe task of context attribution. In this paper, we systematically study\nLLM-based approaches for this task, namely we investigate (i) zero-shot\ninference, (ii) LLM ensembling, and (iii) fine-tuning of small LMs on synthetic\ndata generated by larger LLMs. Our key contribution is SynQA: a novel\ngenerative strategy for synthesizing context attribution data. Given selected\ncontext sentences, an LLM generates QA pairs that are supported by these\nsentences. This leverages LLMs' natural strengths in text generation while\nensuring clear attribution paths in the synthetic training data. We show that\nthe attribution data synthesized via SynQA is highly effective for fine-tuning\nsmall LMs for context attribution in different QA tasks and domains. Finally,\nwith a user study, we validate the usefulness of small LMs (fine-tuned on\nsynthetic data from SynQA) in context attribution for QA.\n","authors":["Gorjan Radevski","Kiril Gashteovski","Shahbaz Syed","Christopher Malon","Sebastien Nicolas","Chia-Chien Hung","Timo Sztyler","Verena Heußer","Wiem Ben Rim","Masafumi Enomoto","Kunihiro Takeoka","Masafumi Oyamada","Goran Glavaš","Carolin Lawrence"],"pdf_url":"https://arxiv.org/pdf/2504.05317v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13642v1","updated":"2025-06-16T16:06:45Z","published":"2025-06-16T16:06:45Z","title":"Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model","summary":"  The emergence of GPT-4o-like large multimodal models (LMMs) has raised the\nexploration of integrating text, vision, and speech modalities to support more\nflexible multimodal interaction. Existing LMMs typically concatenate\nrepresentation of modalities along the sequence dimension and feed them into a\nlarge language model (LLM) backbone. While sequence-dimension concatenation is\nstraightforward for modality integration, it often relies heavily on\nlarge-scale data to learn modality alignments. In this paper, we aim to model\nthe relationships between modalities more purposefully, thereby achieving more\nefficient and flexible modality alignments. To this end, we propose\nStream-Omni, a large language-vision-speech model with efficient modality\nalignments, which can simultaneously support interactions under various\nmodality combinations. Stream-Omni employs LLM as the backbone and aligns the\nvision and speech to the text based on their relationships. For vision that is\nsemantically complementary to text, Stream-Omni uses sequence-dimension\nconcatenation to achieve vision-text alignment. For speech that is semantically\nconsistent with text, Stream-Omni introduces a CTC-based layer-dimension\nmapping to achieve speech-text alignment. In this way, Stream-Omni can achieve\nmodality alignments with less data (especially speech), enabling the transfer\nof text capabilities to other modalities. Experiments on various benchmarks\ndemonstrate that Stream-Omni achieves strong performance on visual\nunderstanding, speech interaction, and vision-grounded speech interaction\ntasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously\nprovide intermediate text outputs (such as ASR transcriptions and model\nresponses) during speech interaction, offering users a comprehensive multimodal\nexperience.\n","authors":["Shaolei Zhang","Shoutao Guo","Qingkai Fang","Yan Zhou","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2506.13642v1.pdf","comment":"Code: https://github.com/ictnlp/Stream-Omni , Model:\n  https://huggingface.co/ICTNLP/stream-omni-8b"},{"id":"http://arxiv.org/abs/2506.13641v1","updated":"2025-06-16T16:05:17Z","published":"2025-06-16T16:05:17Z","title":"EvolvTrip: Enhancing Literary Character Understanding with Temporal\n  Theory-of-Mind Graphs","summary":"  A compelling portrayal of characters is essential to the success of narrative\nwriting. For readers, appreciating a character's traits requires the ability to\ninfer their evolving beliefs, desires, and intentions over the course of a\ncomplex storyline, a cognitive skill known as Theory-of-Mind (ToM). Performing\nToM reasoning in prolonged narratives requires readers to integrate historical\ncontext with current narrative information, a task at which humans excel but\nLarge Language Models (LLMs) often struggle. To systematically evaluate LLMs'\nToM reasoning capability in long narratives, we construct LitCharToM, a\nbenchmark of character-centric questions across four ToM dimensions from\nclassic literature. Further, we introduce EvolvTrip, a perspective-aware\ntemporal knowledge graph that tracks psychological development throughout\nnarratives. Our experiments demonstrate that EvolvTrip consistently enhances\nperformance of LLMs across varying scales, even in challenging extended-context\nscenarios. EvolvTrip proves to be particularly valuable for smaller models,\npartially bridging the performance gap with larger LLMs and showing great\ncompatibility with lengthy narratives. Our findings highlight the importance of\nexplicit representation of temporal character mental states in narrative\ncomprehension and offer a foundation for more sophisticated character\nunderstanding. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/EvolvTrip.\n","authors":["Bohao Yang","Hainiu Xu","Jinhua Du","Ze Li","Yulan He","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2506.13641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13639v1","updated":"2025-06-16T16:04:43Z","published":"2025-06-16T16:04:43Z","title":"An Empirical Study of LLM-as-a-Judge: How Design Choices Impact\n  Evaluation Reliability","summary":"  As large language models (LLMs) continue to advance, reliable evaluation\nmethods are essential particularly for open-ended, instruction-following tasks.\nLLM-as-a-Judge enables automatic evaluation using LLMs as evaluators, but its\nreliability remains uncertain. In this work, we analyze key factors affecting\nits trustworthiness, focusing on alignment with human judgments and evaluation\nconsistency. Using BIGGENBench and EvalBiasBench, we study the effects of\nevaluation design, decoding strategies, and Chain-of-Tought (CoT) reasoning in\nevaluation. Our results show that evaluation criteria are critical for\nreliability, non-deterministic sampling improves alignment with human\npreferences over deterministic evaluation, and CoT reasoning offers minimal\ngains when clear evaluation criteria are present.\n","authors":["Yusuke Yamauchi","Taro Yano","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2506.13639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11130v2","updated":"2025-06-16T15:47:41Z","published":"2025-06-10T17:30:32Z","title":"A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data","summary":"  We propose a self-refining framework that enhances ASR performance with only\nunlabeled datasets. The process starts with an existing ASR model generating\npseudo-labels on unannotated speech, which are then used to train a\nhigh-fidelity text-to-speech (TTS) system. Then, synthesized speech text pairs\nare bootstrapped into the original ASR system, completing the closed-loop\nself-improvement cycle. We demonstrated the effectiveness of the framework on\nTaiwanese Mandarin speech. Leveraging 6,000 hours of unlabeled speech, a\nmoderate amount of text data, and synthetic content from the AI models, we\nadapt Whisper-large-v2 into a specialized model, Twister. Twister reduces error\nrates by up to 20% on Mandarin and 50% on Mandarin-English code-switching\nbenchmarks compared to Whisper. Results highlight the framework as a compelling\nalternative to pseudo-labeling self-distillation approaches and provides a\npractical pathway for improving ASR performance in low-resource or\ndomain-specific settings.\n","authors":["Cheng-Kang Chou","Chan-Jan Hsu","Ho-Lam Chung","Liang-Hsuan Tseng","Hsi-Chun Cheng","Yu-Kuan Fu","Kuan Po Huang","Hung-Yi Lee"],"pdf_url":"https://arxiv.org/pdf/2506.11130v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13610v1","updated":"2025-06-16T15:38:39Z","published":"2025-06-16T15:38:39Z","title":"A Structured Bangla Dataset of Disease-Symptom Associations to Improve\n  Diagnostic Accuracy","summary":"  Disease-symptom datasets are significant and in demand for medical research,\ndisease diagnosis, clinical decision-making, and AI-driven health management\napplications. These datasets help identify symptom patterns associated with\nspecific diseases, thus improving diagnostic accuracy and enabling early\ndetection. The dataset presented in this study systematically compiles\ndisease-symptom relationships from various online sources, medical literature,\nand publicly available health databases. The data was gathered through\nanalyzing peer-reviewed medical articles, clinical case studies, and\ndisease-symptom association reports. Only the verified medical sources were\nincluded in the dataset, while those from non-peer-reviewed and anecdotal\nsources were excluded. The dataset is structured in a tabular format, where the\nfirst column represents diseases, and the remaining columns represent symptoms.\nEach symptom cell contains a binary value (1 or 0), indicating whether a\nsymptom is associated with a disease (1 for presence, 0 for absence). Thereby,\nthis structured representation makes the dataset very useful for a wide range\nof applications, including machine learning-based disease prediction, clinical\ndecision support systems, and epidemiological studies. Although there are some\nadvancements in the field of disease-symptom datasets, there is a significant\ngap in structured datasets for the Bangla language. This dataset aims to bridge\nthat gap by facilitating the development of multilingual medical informatics\ntools and improving disease prediction models for underrepresented linguistic\ncommunities. Further developments should include region-specific diseases and\nfurther fine-tuning of symptom associations for better diagnostic performance\n","authors":["Abdullah Al Shafi","Rowzatul Zannat","Abdul Muntakim","Mahmudul Hasan"],"pdf_url":"https://arxiv.org/pdf/2506.13610v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2501.02039v2","updated":"2025-06-16T15:37:06Z","published":"2025-01-03T14:35:32Z","title":"An Investigation into Value Misalignment in LLM-Generated Texts for\n  Cultural Heritage","summary":"  As Large Language Models (LLMs) become increasingly prevalent in tasks\nrelated to cultural heritage, such as generating descriptions of historical\nmonuments, translating ancient texts, preserving oral traditions, and creating\neducational content, their ability to produce accurate and culturally aligned\ntexts is being increasingly relied upon by users and researchers. However,\ncultural value misalignments may exist in generated texts, such as the\nmisrepresentation of historical facts, the erosion of cultural identity, and\nthe oversimplification of complex cultural narratives, which may lead to severe\nconsequences. Therefore, investigating value misalignment in the context of LLM\nfor cultural heritage is crucial for mitigating these risks, yet there has been\na significant lack of systematic and comprehensive study and investigation in\nthis area. To fill this gap, we systematically assess the reliability of LLMs\nin generating culturally aligned texts for cultural heritage-related tasks. We\nconduct a comprehensive evaluation by compiling an extensive set of 1066 query\ntasks covering 5 widely recognized categories with 17 aspects within the\nknowledge framework of cultural heritage across 5 open-source LLMs, and examine\nboth the type and rate of cultural value misalignments in the generated texts.\nUsing both automated and manual approaches, we effectively detect and analyze\nthe cultural value misalignments in LLM-generated texts. Our findings are\nconcerning: over 65% of the generated texts exhibit notable cultural\nmisalignments, with certain tasks demonstrating almost complete misalignment\nwith key cultural values. Beyond these findings, this paper introduces a\nbenchmark dataset and a comprehensive evaluation workflow that can serve as a\nvaluable resource for future research aimed at enhancing the cultural\nsensitivity and reliability of LLMs.\n","authors":["Fan Bu","Zheng Wang","Siyi Wang","Ziyao Liu"],"pdf_url":"https://arxiv.org/pdf/2501.02039v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.00942v2","updated":"2025-06-16T15:32:22Z","published":"2025-04-01T16:28:38Z","title":"Experiential Semantic Information and Brain Alignment: Are Multimodal\n  Models Better than Language Models?","summary":"  A common assumption in Computational Linguistics is that text representations\nlearnt by multimodal models are richer and more human-like than those by\nlanguage-only models, as they are grounded in images or audio -- similar to how\nhuman language is grounded in real-world experiences. However, empirical\nstudies checking whether this is true are largely lacking. We address this gap\nby comparing word representations from contrastive multimodal models vs.\nlanguage-only ones in the extent to which they capture experiential information\n-- as defined by an existing norm-based 'experiential model' -- and align with\nhuman fMRI responses. Our results indicate that, surprisingly, language-only\nmodels are superior to multimodal ones in both respects. Additionally, they\nlearn more unique brain-relevant semantic information beyond that shared with\nthe experiential model. Overall, our study highlights the need to develop\ncomputational models that better integrate the complementary semantic\ninformation provided by multimodal data sources.\n","authors":["Anna Bavaresco","Raquel Fernández"],"pdf_url":"https://arxiv.org/pdf/2504.00942v2.pdf","comment":"Accepted to CoNLL 2025"},{"id":"http://arxiv.org/abs/2502.12150v2","updated":"2025-06-16T15:27:25Z","published":"2025-02-17T18:59:02Z","title":"Idiosyncrasies in Large Language Models","summary":"  In this work, we unveil and study idiosyncrasies in Large Language Models\n(LLMs) -- unique patterns in their outputs that can be used to distinguish the\nmodels. To do so, we consider a simple classification task: given a particular\ntext output, the objective is to predict the source LLM that generates the\ntext. We evaluate this synthetic task across various groups of LLMs and find\nthat simply fine-tuning text embedding models on LLM-generated texts yields\nexcellent classification accuracy. Notably, we achieve 97.1% accuracy on\nheld-out validation data in the five-way classification problem involving\nChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals\nthat these idiosyncrasies are rooted in word-level distributions. These\npatterns persist even when the texts are rewritten, translated, or summarized\nby an external LLM, suggesting that they are also encoded in the semantic\ncontent. Additionally, we leverage LLM as judges to generate detailed,\nopen-ended descriptions of each model's idiosyncrasies. Finally, we discuss the\nbroader implications of our findings, including training on synthetic data,\ninferring model similarity, and robust evaluation of LLMs. Code is available at\nhttps://github.com/locuslab/llm-idiosyncrasies.\n","authors":["Mingjie Sun","Yida Yin","Zhiqiu Xu","J. Zico Kolter","Zhuang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.12150v2.pdf","comment":"Published in ICML 2025. Website at\n  https://eric-mingjie.github.io/llm-idiosyncrasies/index.html"},{"id":"http://arxiv.org/abs/2506.13599v1","updated":"2025-06-16T15:24:07Z","published":"2025-06-16T15:24:07Z","title":"CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\n  Simulation","summary":"  Human mobility simulation plays a crucial role in various real-world\napplications. Recently, to address the limitations of traditional data-driven\napproaches, researchers have explored leveraging the commonsense knowledge and\nreasoning capabilities of large language models (LLMs) to accelerate human\nmobility simulation. However, these methods suffer from several critical\nshortcomings, including inadequate modeling of urban spaces and poor\nintegration with both individual mobility patterns and collective mobility\ndistributions. To address these challenges, we propose \\textbf{C}ityGPT-Powered\n\\textbf{A}gentic framework for \\textbf{M}obility \\textbf{S}imulation\n(\\textbf{CAMS}), an agentic framework that leverages the language based urban\nfoundation model to simulate human mobility in urban space. \\textbf{CAMS}\ncomprises three core modules, including MobExtractor to extract template\nmobility patterns and synthesize new ones based on user profiles, GeoGenerator\nto generate anchor points considering collective knowledge and generate\ncandidate urban geospatial knowledge using an enhanced version of CityGPT,\nTrajEnhancer to retrieve spatial knowledge based on mobility patterns and\ngenerate trajectories with real trajectory preference alignment via DPO.\nExperiments on real-world datasets show that \\textbf{CAMS} achieves superior\nperformance without relying on externally provided geospatial information.\nMoreover, by holistically modeling both individual mobility patterns and\ncollective mobility constraints, \\textbf{CAMS} generates more realistic and\nplausible trajectories. In general, \\textbf{CAMS} establishes a new paradigm\nthat integrates the agentic framework with urban-knowledgeable LLMs for human\nmobility simulation.\n","authors":["Yuwei Du","Jie Feng","Jian Yuan","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2506.13599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13596v1","updated":"2025-06-16T15:23:07Z","published":"2025-06-16T15:23:07Z","title":"Qwen vs. Gemma Integration with Whisper: A Comparative Study in\n  Multilingual SpeechLLM Systems","summary":"  This paper presents our system for the MLC-SLM Challenge 2025, focusing on\nmultilingual speech recognition and language modeling with large language\nmodels (LLMs). Our approach combines a fine-tuned Whisper-large-v3 encoder with\nefficient projector architectures and various decoder configurations. We employ\na three-stage training methodology that progressively optimizes the encoder,\nprojector, and LLM components. Our system achieves competitive performance with\na private test average WER/CER result of 16.63% using the Gemma3-12B and 18.6%\nusing the Qwen2.5-7B as decoder-only language model.\n","authors":["Tuan Nguyen","Long-Vu Hoang","Huy-Dat Tran"],"pdf_url":"https://arxiv.org/pdf/2506.13596v1.pdf","comment":"Technical report for Interspeech 2025 MLC-SLM Challenge"},{"id":"http://arxiv.org/abs/2506.13585v1","updated":"2025-06-16T15:08:02Z","published":"2025-06-16T15:08:02Z","title":"MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning\n  Attention","summary":"  We introduce MiniMax-M1, the world's first open-weight, large-scale\nhybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid\nMixture-of-Experts (MoE) architecture combined with a lightning attention\nmechanism. The model is developed based on our previous MiniMax-Text-01 model,\nwhich contains a total of 456 billion parameters with 45.9 billion parameters\nactivated per token. The M1 model natively supports a context length of 1\nmillion tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning\nattention mechanism in MiniMax-M1 enables efficient scaling of test-time\ncompute. These properties make M1 particularly suitable for complex tasks that\nrequire processing long inputs and thinking extensively. MiniMax-M1 is trained\nusing large-scale reinforcement learning (RL) on diverse problems including\nsandbox-based, real-world software engineering environments. In addition to\nM1's inherent efficiency advantage for RL training, we propose CISPO, a novel\nRL algorithm to further enhance RL efficiency. CISPO clips importance sampling\nweights rather than token updates, outperforming other competitive RL variants.\nCombining hybrid-attention and CISPO enables MiniMax-M1's full RL training on\n512 H800 GPUs to complete in only three weeks, with a rental cost of just\n$534,700. We release two versions of MiniMax-M1 models with 40K and 80K\nthinking budgets respectively, where the 40K model represents an intermediate\nphase of the 80K training. Experiments on standard benchmarks show that our\nmodels are comparable or superior to strong open-weight models such as the\noriginal DeepSeek-R1 and Qwen3-235B, with particular strengths in complex\nsoftware engineering, tool utilization, and long-context tasks. We publicly\nrelease MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.\n","authors":[" MiniMax"," :","Aili Chen","Aonian Li","Bangwei Gong","Binyang Jiang","Bo Fei","Bo Yang","Boji Shan","Changqing Yu","Chao Wang","Cheng Zhu","Chengjun Xiao","Chengyu Du","Chi Zhang","Chu Qiao","Chunhao Zhang","Chunhui Du","Congchao Guo","Da Chen","Deming Ding","Dianjun Sun","Dong Li","Enwei Jiao","Haigang Zhou","Haimo Zhang","Han Ding","Haohai Sun","Haoyu Feng","Huaiguang Cai","Haichao Zhu","Jian Sun","Jiaqi Zhuang","Jiaren Cai","Jiayuan Song","Jin Zhu","Jingyang Li","Jinhao Tian","Jinli Liu","Junhao Xu","Junjie Yan","Junteng Liu","Junxian He","Kaiyi Feng","Ke Yang","Kecheng Xiao","Le Han","Leyang Wang","Lianfei Yu","Liheng Feng","Lin Li","Lin Zheng","Linge Du","Lingyu Yang","Lunbin Zeng","Minghui Yu","Mingliang Tao","Mingyuan Chi","Mozhi Zhang","Mujie Lin","Nan Hu","Nongyu Di","Peng Gao","Pengfei Li","Pengyu Zhao","Qibing Ren","Qidi Xu","Qile Li","Qin Wang","Rong Tian","Ruitao Leng","Shaoxiang Chen","Shaoyu Chen","Shengmin Shi","Shitong Weng","Shuchang Guan","Shuqi Yu","Sichen Li","Songquan Zhu","Tengfei Li","Tianchi Cai","Tianrun Liang","Weiyu Cheng","Weize Kong","Wenkai Li","Xiancai Chen","Xiangjun Song","Xiao Luo","Xiao Su","Xiaobo Li","Xiaodong Han","Xinzhu Hou","Xuan Lu","Xun Zou","Xuyang Shen","Yan Gong","Yan Ma","Yang Wang","Yiqi Shi","Yiran Zhong","Yonghong Duan","Yongxiang Fu","Yongyi Hu","Yu Gao","Yuanxiang Fan","Yufeng Yang","Yuhao Li","Yulin Hu","Yunan Huang","Yunji Li","Yunzhi Xu","Yuxin Mao","Yuxuan Shi","Yuze Wenren","Zehan Li","Zelin Li","Zhanxu Tian","Zhengmao Zhu","Zhenhua Fan","Zhenzhen Wu","Zhichao Xu","Zhihang Yu","Zhiheng Lyu","Zhuo Jiang","Zibo Gao","Zijia Wu","Zijian Song","Zijun Sun"],"pdf_url":"https://arxiv.org/pdf/2506.13585v1.pdf","comment":"A technical report from MiniMax. The authors are listed in\n  alphabetical order. We open-source our MiniMax-M1 at\n  https://github.com/MiniMax-AI/MiniMax-M1"},{"id":"http://arxiv.org/abs/2506.13579v1","updated":"2025-06-16T15:02:12Z","published":"2025-06-16T15:02:12Z","title":"Flexible-length Text Infilling for Discrete Diffusion Models","summary":"  Discrete diffusion models are a new class of text generators that offer\nadvantages such as bidirectional context use, parallelizable generation, and\nflexible prompting compared to autoregressive models. However, a critical\nlimitation of discrete diffusion models is their inability to perform\nflexible-length or flexible-position text infilling without access to\nground-truth positional data. We introduce \\textbf{DDOT} (\\textbf{D}iscrete\n\\textbf{D}iffusion with \\textbf{O}ptimal \\textbf{T}ransport Position Coupling),\nthe first discrete diffusion model to overcome this challenge. DDOT jointly\ndenoises token values and token positions, employing a novel sample-level\nOptimal Transport (OT) coupling. This coupling preserves relative token\nordering while dynamically adjusting the positions and length of infilled\nsegments, a capability previously missing in text diffusion. Our method is\northogonal to existing discrete text diffusion methods and is compatible with\nvarious pretrained text denoisers. Extensive experiments on text infilling\nbenchmarks such as One-Billion-Word and Yelp demonstrate that DDOT outperforms\nnaive diffusion baselines. Furthermore, DDOT achieves performance on par with\nstate-of-the-art non-autoregressive models and enables significant improvements\nin training efficiency and flexibility.\n","authors":["Andrew Zhang","Anushka Sivakumar","Chiawei Tang","Chris Thomas"],"pdf_url":"https://arxiv.org/pdf/2506.13579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13569v1","updated":"2025-06-16T14:54:56Z","published":"2025-06-16T14:54:56Z","title":"Characterizing Linguistic Shifts in Croatian News via Diachronic Word\n  Embeddings","summary":"  Measuring how semantics of words change over time improves our understanding\nof how cultures and perspectives change. Diachronic word embeddings help us\nquantify this shift, although previous studies leveraged substantial temporally\nannotated corpora. In this work, we use a corpus of 9.5 million Croatian news\narticles spanning the past 25 years and quantify semantic change using\nskip-gram word embeddings trained on five-year periods. Our analysis finds that\nword embeddings capture linguistic shifts of terms pertaining to major topics\nin this timespan (COVID-19, Croatia joining the European Union, technological\nadvancements). We also find evidence that embeddings from post-2020 encode\nincreased positivity in sentiment analysis tasks, contrasting studies reporting\na decline in mental health over the same period.\n","authors":["David Dukić","Ana Barić","Marko Čuljak","Josip Jukić","Martin Tutek"],"pdf_url":"https://arxiv.org/pdf/2506.13569v1.pdf","comment":"Accepted at Slavic NLP 2025"},{"id":"http://arxiv.org/abs/2506.13559v1","updated":"2025-06-16T14:45:08Z","published":"2025-06-16T14:45:08Z","title":"Understand the Implication: Learning to Think for Pragmatic\n  Understanding","summary":"  Pragmatics, the ability to infer meaning beyond literal interpretation, is\ncrucial for social cognition and communication. While LLMs have been\nbenchmarked for their pragmatic understanding, improving their performance\nremains underexplored. Existing methods rely on annotated labels but overlook\nthe reasoning process humans naturally use to interpret implicit meaning. To\nbridge this gap, we introduce a novel pragmatic dataset,\nImpliedMeaningPreference, that includes explicit reasoning (thoughts) for both\ncorrect and incorrect interpretations. Through preference-tuning and supervised\nfine-tuning, we demonstrate that thought-based learning significantly enhances\nLLMs' pragmatic understanding, improving accuracy by 11.12% across model\nfamilies. We further discuss a transfer-learning study where we evaluate the\nperformance of thought-based training for the other tasks of pragmatics\n(presupposition, deixis) that are not seen during the training time and observe\nan improvement of 16.10% compared to label-trained models.\n","authors":["Settaluri Lakshmi Sravanthi","Kishan Maharaj","Sravani Gunnu","Abhijit Mishra","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2506.13559v1.pdf","comment":"SS and KM contributed equally to this work"},{"id":"http://arxiv.org/abs/2408.08782v5","updated":"2025-06-16T14:32:22Z","published":"2024-08-16T14:54:41Z","title":"EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics","summary":"  Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nRecently, with advancements in large language models (LLMs), end-to-end\ndialogue agents without explicit strategy prediction steps have become\nprevalent. However, implicit strategy planning lacks transparency, and recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy prediction\nframework, EmoDynamiX, which models the discourse dynamics between user\nfine-grained emotions and system strategies using a heterogeneous graph for\nbetter performance and transparency. Experimental results on two ESC datasets\nshow EmoDynamiX outperforms previous state-of-the-art methods with a\nsignificant margin (better proficiency and lower preference bias). Our approach\nalso exhibits better transparency by allowing backtracing of decision making.\n","authors":["Chenwei Wan","Matthieu Labeau","Chloé Clavel"],"pdf_url":"https://arxiv.org/pdf/2408.08782v5.pdf","comment":"Accepted to NAACL 2025 main, long paper"},{"id":"http://arxiv.org/abs/2506.11887v2","updated":"2025-06-16T14:30:20Z","published":"2025-06-13T15:36:22Z","title":"Towards a Cascaded LLM Framework for Cost-effective Human-AI\n  Decision-Making","summary":"  Effective human-AI decision-making balances three key factors: the\n\\textit{correctness} of predictions, the \\textit{cost} of knowledge and\nreasoning complexity, and the confidence about whether to \\textit{abstain}\nautomated answers or involve human experts. In this work, we present a cascaded\nLLM decision framework that adaptively delegates tasks across multiple tiers of\nexpertise -- a base model for initial candidate answers, a more capable and\nknowledgeable (but costlier) large model, and a human expert for when the model\ncascade abstains. Our method proceeds in two stages. First, a deferral policy\ndetermines whether to accept the base model's answer or regenerate it with the\nlarge model based on the confidence score. Second, an abstention policy decides\nwhether the cascade model response is sufficiently certain or requires human\nintervention. Moreover, we incorporate an online learning mechanism in the\nframework that can leverage human feedback to improve decision quality over\ntime. We demonstrate this approach to general question-answering (ARC-Easy and\nARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results\nshow that our cascaded strategy outperforms in most cases single-model\nbaselines in accuracy while reducing cost and providing a principled way to\nhandle abstentions.\n","authors":["Claudio Fanconi","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2506.11887v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13541v1","updated":"2025-06-16T14:30:17Z","published":"2025-06-16T14:30:17Z","title":"Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization","summary":"  Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets.\n","authors":["Guanghui Song","Dongping Liao","Yiren Zhao","Kejiang Ye","Cheng-zhong Xu","Xitong Gao"],"pdf_url":"https://arxiv.org/pdf/2506.13541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.02670v3","updated":"2025-06-16T14:19:01Z","published":"2025-04-03T15:11:55Z","title":"Affordable AI Assistants with Knowledge Graph of Thoughts","summary":"  Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively while also minimizing bias and noise.\nFor example, KGoT achieves a 29% improvement in task success rates on the GAIA\nbenchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,\nharnessing a smaller model dramatically reduces operational costs by over 36x\ncompared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and\nDeepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a\nscalable, affordable, versatile, and high-performing solution for AI\nassistants.\n","authors":["Maciej Besta","Lorenzo Paleari","Jia Hao Andrea Jiang","Robert Gerstenberger","You Wu","Jón Gunnar Hannesson","Patrick Iff","Ales Kubicek","Piotr Nyczyk","Diana Khimey","Nils Blach","Haiqiang Zhang","Tao Zhang","Peiran Ma","Grzegorz Kwaśniewski","Marcin Copik","Hubert Niewiadomski","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2504.02670v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13514v1","updated":"2025-06-16T14:09:43Z","published":"2025-06-16T14:09:43Z","title":"TensorSLM: Energy-efficient Embedding Compression of Sub-billion\n  Parameter Language Models on Low-end Devices","summary":"  Small Language Models (SLMs, or on-device LMs) have significantly fewer\nparameters than Large Language Models (LLMs). They are typically deployed on\nlow-end devices, like mobile phones and single-board computers. Unlike LLMs,\nwhich rely on increasing model size for better generalisation, SLMs designed\nfor edge applications are expected to have adaptivity to the deployment\nenvironments and energy efficiency given the device battery life constraints,\nwhich are not addressed in datacenter-deployed LLMs. This paper addresses these\ntwo requirements by proposing a training-free token embedding compression\napproach using Tensor-Train Decomposition (TTD). Each pre-trained token\nembedding vector is converted into a lower-dimensional Matrix Product State\n(MPS). We comprehensively evaluate the extracted low-rank structures across\ncompression ratio, language task performance, latency, and energy consumption\non a typical low-end device, i.e. Raspberry Pi. Taking the sub-billion\nparameter versions of GPT-2/Cerebres-GPT and OPT models as examples, our\napproach achieves a comparable language task performance to the original model\nwith around $2.0\\times$ embedding layer compression, while the energy\nconsumption of a single query drops by half.\n","authors":["Mingxue Xu","Yao Lei Xu","Danilo P. Mandic"],"pdf_url":"https://arxiv.org/pdf/2506.13514v1.pdf","comment":"ICML 2025 Workshop on Tiny Titans: The next wave of On-Device\n  Learning for Foundational Models (TTODLer-FM)"},{"id":"http://arxiv.org/abs/2504.10512v2","updated":"2025-06-16T14:08:36Z","published":"2025-04-10T01:31:11Z","title":"JEPA4Rec: Learning Effective Language Representations for Sequential\n  Recommendation via Joint Embedding Predictive Architecture","summary":"  Language representation learning has emerged as a promising approach for\nsequential recommendation, thanks to its ability to learn generalizable\nrepresentations. However, despite its advantages, this approach still struggles\nwith data sparsity and a limited understanding of common-sense user\npreferences. To address these limitations, we propose $\\textbf{JEPA4Rec}$, a\nframework that combines $\\textbf{J}$oint $\\textbf{E}$mbedding\n$\\textbf{P}$redictive $\\textbf{A}$rchitecture with language modeling of item\ntextual descriptions. JEPA4Rec captures semantically rich and transferable\nrepresentations, improving recommendation performance and reducing reliance on\nlarge-scale pre-training data. Specifically, JEPA4Rec represents items as text\nsentences by flattening descriptive information such as $\\textit{title,\ncategory}$, and other attributes. To encode these sentences, we employ a\nbidirectional Transformer encoder with modified embedding layers tailored for\ncapturing item information in recommendation datasets. We apply masking to text\nsentences and use them to predict the representations of the unmasked\nsentences, helping the model learn generalizable item embeddings. To further\nimprove recommendation performance and language understanding, we employ a\ntwo-stage training strategy incorporating self-supervised learning losses.\nExperiments on six real-world datasets demonstrate that JEPA4Rec consistently\noutperforms state-of-the-art methods, particularly in cross-domain,\ncross-platform, and low-resource scenarios.\n","authors":["Minh-Anh Nguyen","Dung D. Le"],"pdf_url":"https://arxiv.org/pdf/2504.10512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13513v1","updated":"2025-06-16T14:08:23Z","published":"2025-06-16T14:08:23Z","title":"K/DA: Automated Data Generation Pipeline for Detoxifying Implicitly\n  Offensive Language in Korean","summary":"  Language detoxification involves removing toxicity from offensive language.\nWhile a neutral-toxic paired dataset provides a straightforward approach for\ntraining detoxification models, creating such datasets presents several\nchallenges: i) the need for human annotation to build paired data, and ii) the\nrapid evolution of offensive terms, rendering static datasets quickly outdated.\nTo tackle these challenges, we introduce an automated paired data generation\npipeline, called K/DA. This pipeline is designed to generate offensive language\nwith implicit offensiveness and trend-aligned slang, making the resulting\ndataset suitable for detoxification model training. We demonstrate that the\ndataset generated by K/DA exhibits high pair consistency and greater implicit\noffensiveness compared to existing Korean datasets, and also demonstrates\napplicability to other languages. Furthermore, it enables effective training of\na high-performing detoxification model with simple instruction fine-tuning.\n","authors":["Minkyeong Jeon","Hyemin Jeong","Yerang Kim","Jiyoung Kim","Jae Hyeon Cho","Byung-Jun Lee"],"pdf_url":"https://arxiv.org/pdf/2506.13513v1.pdf","comment":"9 pages, 3 figures, ACL 2025"},{"id":"http://arxiv.org/abs/2506.13502v1","updated":"2025-06-16T13:58:54Z","published":"2025-06-16T13:58:54Z","title":"BOW: Bottlenecked Next Word Exploration","summary":"  Large language models (LLMs) are typically trained via next-word prediction\n(NWP), which provides strong surface-level fluency but often lacks support for\nrobust reasoning. We propose BOttlenecked next Word exploration (BOW), a novel\nRL framework that rethinks NWP by introducing a reasoning bottleneck where a\npolicy model first generates a reasoning path rather than predicting the next\ntoken directly, after which a frozen judge model predicts the next token\ndistribution based solely on this reasoning path. We train the policy model\nusing GRPO with rewards that quantify how effectively the reasoning path\nfacilitates next-word recovery. Compared with other continual pretraining\nbaselines, we show that BOW improves both the general and next-word reasoning\ncapabilities of the base model, evaluated on various benchmarks. Our findings\nshow that BOW can serve as an effective and scalable alternative to vanilla\nNWP.\n","authors":["Ming Shen","Zhikun Xu","Xiao Ye","Jacob Dineen","Ben Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.13502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13487v1","updated":"2025-06-16T13:45:30Z","published":"2025-06-16T13:45:30Z","title":"TurBLiMP: A Turkish Benchmark of Linguistic Minimal Pairs","summary":"  We introduce TurBLiMP, the first Turkish benchmark of linguistic minimal\npairs, designed to evaluate the linguistic abilities of monolingual and\nmultilingual language models (LMs). Covering 16 linguistic phenomena with 1000\nminimal pairs each, TurBLiMP fills an important gap in linguistic evaluation\nresources for Turkish. In designing the benchmark, we give extra attention to\ntwo properties of Turkish that remain understudied in current syntactic\nevaluations of LMs, namely word order flexibility and subordination through\nmorphological processes. Our experiments on a wide range of LMs and a newly\ncollected set of human acceptability judgments reveal that even cutting-edge\nLarge LMs still struggle with grammatical phenomena that are not challenging\nfor humans, and may also exhibit different sensitivities to word order and\nmorphological complexity compared to humans.\n","authors":["Ezgi Başar","Francesca Padovani","Jaap Jumelet","Arianna Bisazza"],"pdf_url":"https://arxiv.org/pdf/2506.13487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13479v1","updated":"2025-06-16T13:35:22Z","published":"2025-06-16T13:35:22Z","title":"Position: Pause Recycling LoRAs and Prioritize Mechanisms to Uncover\n  Limits and Effectiveness","summary":"  Merging or routing low-rank adapters (LoRAs) has emerged as a popular\nsolution for enhancing large language models, particularly when data access is\nrestricted by regulatory or domain-specific constraints. This position paper\nargues that the research community should shift its focus from developing new\nmerging or routing algorithms to understanding the conditions under which\nreusing LoRAs is truly effective. Through theoretical analysis and synthetic\ntwo-hop reasoning and math word-problem tasks, we examine whether reusing LoRAs\nenables genuine compositional generalization or merely reflects shallow pattern\nmatching. Evaluating two data-agnostic methods--parameter averaging and dynamic\nadapter selection--we found that reusing LoRAs often fails to logically\nintegrate knowledge across disjoint fine-tuning datasets, especially when such\nknowledge is underrepresented during pretraining. Our empirical results,\nsupported by theoretical insights into LoRA's limited expressiveness, highlight\nthe preconditions and constraints of reusing them for unseen tasks and cast\ndoubt on its feasibility as a truly data-free approach. We advocate for pausing\nthe pursuit of novel methods for recycling LoRAs and emphasize the need for\nrigorous mechanisms to guide future academic research in adapter-based model\nmerging and practical system designs for practitioners.\n","authors":["Mei-Yen Chen","Thi Thu Uyen Hoang","Michael Hahn","M. Saquib Sarfraz"],"pdf_url":"https://arxiv.org/pdf/2506.13479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13474v1","updated":"2025-06-16T13:32:01Z","published":"2025-06-16T13:32:01Z","title":"Language Agents for Hypothesis-driven Clinical Decision Making with\n  Reinforcement Learning","summary":"  Clinical decision-making is a dynamic, interactive, and cyclic process where\ndoctors have to repeatedly decide on which clinical action to perform and\nconsider newly uncovered information for diagnosis and treatment. Large\nLanguage Models (LLMs) have the potential to support clinicians in this\nprocess, however, most applications of LLMs in clinical decision support suffer\nfrom one of two limitations: Either they assume the unrealistic scenario of\nimmediate availability of all patient information and do not model the\ninteractive and iterative investigation process, or they restrict themselves to\nthe limited \"out-of-the-box\" capabilities of large pre-trained models without\nperforming task-specific training. In contrast to this, we propose to model\nclinical decision-making for diagnosis with a hypothesis-driven\nuncertainty-aware language agent, LA-CDM, that converges towards a diagnosis\nvia repeatedly requesting and interpreting relevant tests. Using a hybrid\ntraining paradigm combining supervised and reinforcement learning, we train\nLA-CDM with three objectives targeting critical aspects of clinical\ndecision-making: accurate hypothesis generation, hypothesis uncertainty\nestimation, and efficient decision-making. We evaluate our methodology on\nMIMIC-CDM, a real-world dataset covering four abdominal diseases containing\nvarious clinical tests and show the benefit of explicitly training clinical\ndecision-making for increasing diagnostic performance and efficiency.\n","authors":["David Bani-Harouni","Chantal Pellegrini","Ege Özsoy","Matthias Keicher","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2506.13474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09975v2","updated":"2025-06-16T13:31:25Z","published":"2025-06-11T17:51:28Z","title":"When Detection Fails: The Power of Fine-Tuned Models to Generate\n  Human-Like Social Media Text","summary":"  Detecting AI-generated text is a difficult problem to begin with; detecting\nAI-generated text on social media is made even more difficult due to the short\ntext length and informal, idiosyncratic language of the internet. It is\nnonetheless important to tackle this problem, as social media represents a\nsignificant attack vector in online influence campaigns, which may be bolstered\nthrough the use of mass-produced AI-generated posts supporting (or opposing)\nparticular policies, decisions, or events. We approach this problem with the\nmindset and resources of a reasonably sophisticated threat actor, and create a\ndataset of 505,159 AI-generated social media posts from a combination of\nopen-source, closed-source, and fine-tuned LLMs, covering 11 different\ncontroversial topics. We show that while the posts can be detected under\ntypical research assumptions about knowledge of and access to the generating\nmodels, under the more realistic assumption that an attacker will not release\ntheir fine-tuned model to the public, detectability drops dramatically. This\nresult is confirmed with a human study. Ablation experiments highlight the\nvulnerability of various detection algorithms to fine-tuned LLMs. This result\nhas implications across all detection domains, since fine-tuning is a generally\napplicable and realistic LLM use case.\n","authors":["Hillary Dawkins","Kathleen C. Fraser","Svetlana Kiritchenko"],"pdf_url":"https://arxiv.org/pdf/2506.09975v2.pdf","comment":"to appear in ACL Findings"},{"id":"http://arxiv.org/abs/2506.13472v1","updated":"2025-06-16T13:30:33Z","published":"2025-06-16T13:30:33Z","title":"ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently\n  Compressing Large Language Models","summary":"  Quantization has been widely studied as an effective technique for reducing\nthe memory requirement of large language models (LLMs), potentially improving\nthe latency time as well. Utilizing the characteristic of rotational invariance\nof transformer, we propose the rotation-based saliency-aware weight\nquantization (ROSAQ), which identifies salient channels in the projection\nfeature space, not in the original feature space, where the projected\n\"principal\" dimensions are naturally considered as \"salient\" features. The\nproposed ROSAQ consists of 1) PCA-based projection, which first performs\nprincipal component analysis (PCA) on a calibration set and transforms via the\nPCA projection, 2) Salient channel dentification, which selects dimensions\ncorresponding to the K-largest eigenvalues as salient channels, and 3)\nSaliency-aware quantization with mixed-precision, which uses FP16 for salient\ndimensions and INT3/4 for other dimensions. Experiment results show that ROSAQ\nshows improvements over the baseline saliency-aware quantization on the\noriginal feature space and other existing quantization methods. With kernel\nfusion, ROSAQ presents about 2.3x speed up over FP16 implementation in\ngenerating 256 tokens with a batch size of 64.\n","authors":["Junho Yoon","Geom Lee","Donghyeon Jeon","Inho Kang","Seung-Hoon Na"],"pdf_url":"https://arxiv.org/pdf/2506.13472v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2506.13470v1","updated":"2025-06-16T13:28:37Z","published":"2025-06-16T13:28:37Z","title":"Abstract, Align, Predict: Zero-Shot Stance Detection via Cognitive\n  Inductive Reasoning","summary":"  Zero-shot stance detection (ZSSD) aims to identify the stance of text toward\npreviously unseen targets, a setting where conventional supervised models often\nfail due to reliance on labeled data and shallow lexical cues. Inspired by\nhuman cognitive reasoning, we propose the Cognitive Inductive Reasoning\nFramework (CIRF), which abstracts transferable reasoning schemas from unlabeled\ntext and encodes them as concept-level logic. To integrate these schemas with\ninput arguments, we introduce a Schema-Enhanced Graph Kernel Model (SEGKM) that\ndynamically aligns local and global reasoning structures. Experiments on\nSemEval-2016, VAST, and COVID-19-Stance benchmarks show that CIRF establishes\nnew state-of-the-art results, outperforming strong ZSSD baselines by 1.0, 4.5,\nand 3.3 percentage points in macro-F1, respectively, and achieving comparable\naccuracy with 70\\% fewer labeled examples. We will release the full code upon\npublication.\n","authors":["Jun Ma","Fuqiang Niu","Dong Li","Jinzhou Cao","Genan Dai","Bowen Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.13470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13468v1","updated":"2025-06-16T13:27:44Z","published":"2025-06-16T13:27:44Z","title":"An Interdisciplinary Approach to Human-Centered Machine Translation","summary":"  Machine Translation (MT) tools are widely used today, often in contexts where\nprofessional translators are not present. Despite progress in MT technology, a\ngap persists between system development and real-world usage, particularly for\nnon-expert users who may struggle to assess translation reliability. This paper\nadvocates for a human-centered approach to MT, emphasizing the alignment of\nsystem design with diverse communicative goals and contexts of use. We survey\nthe literature in Translation Studies and Human-Computer Interaction to\nrecontextualize MT evaluation and design to address the diverse real-world\nscenarios in which MT is used today.\n","authors":["Marine Carpuat","Omri Asscher","Kalika Bali","Luisa Bentivogli","Frédéric Blain","Lynne Bowker","Monojit Choudhury","Hal Daumé III","Kevin Duh","Ge Gao","Alvin Grissom II","Marzena Karpinska","Elaine C. Khoong","William D. Lewis","André F. T. Martins","Mary Nurminen","Douglas W. Oard","Maja Popovic","Michel Simard","François Yvon"],"pdf_url":"https://arxiv.org/pdf/2506.13468v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2506.13467v1","updated":"2025-06-16T13:27:10Z","published":"2025-06-16T13:27:10Z","title":"Enhancing Omics Cohort Discovery for Research on Neurodegeneration\n  through Ontology-Augmented Embedding Models","summary":"  The growing volume of omics and clinical data generated for neurodegenerative\ndiseases (NDs) requires new approaches for their curation so they can be\nready-to-use in bioinformatics. NeuroEmbed is an approach for the engineering\nof semantically accurate embedding spaces to represent cohorts and samples. The\nNeuroEmbed method comprises four stages: (1) extraction of ND cohorts from\npublic repositories; (2) semi-automated normalization and augmentation of\nmetadata of cohorts and samples using biomedical ontologies and clustering on\nthe embedding space; (3) automated generation of a natural language\nquestion-answering (QA) dataset for cohorts and samples based on randomized\ncombinations of standardized metadata dimensions and (4) fine-tuning of a\ndomain-specific embedder to optimize queries. We illustrate the approach using\nthe GEO repository and the PubMedBERT pretrained embedder. Applying NeuroEmbed,\nwe semantically indexed 2,801 repositories and 150,924 samples. Amongst many\nbiology-relevant categories, we normalized more than 1,700 heterogeneous tissue\nlabels from GEO into 326 unique ontology-aligned concepts and enriched\nannotations with new ontology-aligned terms, leading to a fold increase in size\nfor the metadata terms between 2.7 and 20 fold. After fine-tuning PubMedBERT\nwith the QA training data augmented with the enlarged metadata, the model\nincreased its mean Retrieval Precision from 0.277 to 0.866 and its mean\nPercentile Rank from 0.355 to 0.896. The NeuroEmbed methodology for the\ncreation of electronic catalogues of omics cohorts and samples will foster\nautomated bioinformatic pipelines construction. The NeuroEmbed catalogue of\ncohorts and samples is available at https://github.com/JoseAdrian3/NeuroEmbed.\n","authors":["José A. Pardo","Alicia Gómez-Pascual","José T. Palma","Juan A. Botía"],"pdf_url":"https://arxiv.org/pdf/2506.13467v1.pdf","comment":"16 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2506.13464v1","updated":"2025-06-16T13:24:50Z","published":"2025-06-16T13:24:50Z","title":"Unveiling the Learning Mind of Language Models: A Cognitive Framework\n  and Empirical Study","summary":"  Large language models (LLMs) have shown impressive capabilities across tasks\nsuch as mathematics, coding, and reasoning, yet their learning ability, which\nis crucial for adapting to dynamic environments and acquiring new knowledge,\nremains underexplored. In this work, we address this gap by introducing a\nframework inspired by cognitive psychology and education. Specifically, we\ndecompose general learning ability into three distinct, complementary\ndimensions: Learning from Instructor (acquiring knowledge via explicit\nguidance), Learning from Concept (internalizing abstract structures and\ngeneralizing to new contexts), and Learning from Experience (adapting through\naccumulated exploration and feedback). We conduct a comprehensive empirical\nstudy across the three learning dimensions and identify several insightful\nfindings, such as (i) interaction improves learning; (ii) conceptual\nunderstanding is scale-emergent and benefits larger models; and (iii) LLMs are\neffective few-shot learners but not many-shot learners. Based on our framework\nand empirical findings, we introduce a benchmark that provides a unified and\nrealistic evaluation of LLMs' general learning abilities across three learning\ncognition dimensions. It enables diagnostic insights and supports evaluation\nand development of more adaptive and human-like models.\n","authors":["Zhengyu Hu","Jianxun Lian","Zheyuan Xiao","Seraphina Zhang","Tianfu Wang","Nicholas Jing Yuan","Xing Xie","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2506.13464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13458v1","updated":"2025-06-16T13:15:02Z","published":"2025-06-16T13:15:02Z","title":"Leveraging Vision-Language Pre-training for Human Activity Recognition\n  in Still Images","summary":"  Recognising human activity in a single photo enables indexing, safety and\nassistive applications, yet lacks motion cues. Using 285 MSCOCO images labelled\nas walking, running, sitting, and standing, scratch CNNs scored 41% accuracy.\nFine-tuning multimodal CLIP raised this to 76%, demonstrating that contrastive\nvision-language pre-training decisively improves still-image action recognition\nin real-world deployments.\n","authors":["Cristina Mahanta","Gagan Bhatia"],"pdf_url":"https://arxiv.org/pdf/2506.13458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13450v1","updated":"2025-06-16T13:09:24Z","published":"2025-06-16T13:09:24Z","title":"A Neural Model for Word Repetition","summary":"  It takes several years for the developing brain of a baby to fully master\nword repetition-the task of hearing a word and repeating it aloud. Repeating a\nnew word, such as from a new language, can be a challenging task also for\nadults. Additionally, brain damage, such as from a stroke, may lead to\nsystematic speech errors with specific characteristics dependent on the\nlocation of the brain damage. Cognitive sciences suggest a model with various\ncomponents for the different processing stages involved in word repetition.\nWhile some studies have begun to localize the corresponding regions in the\nbrain, the neural mechanisms and how exactly the brain performs word repetition\nremain largely unknown. We propose to bridge the gap between the cognitive\nmodel of word repetition and neural mechanisms in the human brain by modeling\nthe task using deep neural networks. Neural models are fully observable,\nallowing us to study the detailed mechanisms in their various substructures and\nmake comparisons with human behavior and, ultimately, the brain. Here, we make\nfirst steps in this direction by: (1) training a large set of models to\nsimulate the word repetition task; (2) creating a battery of tests to probe the\nmodels for known effects from behavioral studies in humans, and (3) simulating\nbrain damage through ablation studies, where we systematically remove neurons\nfrom the model, and repeat the behavioral study to examine the resulting speech\nerrors in the \"patient\" model. Our results show that neural models can mimic\nseveral effects known from human research, but might diverge in other aspects,\nhighlighting both the potential and the challenges for future research aimed at\ndeveloping human-like neural models.\n","authors":["Daniel Dager","Robin Sobczyk","Emmanuel Chemla","Yair Lakretz"],"pdf_url":"https://arxiv.org/pdf/2506.13450v1.pdf","comment":"To appear at Cognitive Computational Neuroscience 2025 (CCN)"},{"id":"http://arxiv.org/abs/2502.17533v2","updated":"2025-06-16T13:07:26Z","published":"2025-02-24T14:42:48Z","title":"From Euler to AI: Unifying Formulas for Mathematical Constants","summary":"  The constant $\\pi$ has fascinated scholars throughout the centuries,\ninspiring numerous formulas for its evaluation, such as infinite sums and\ncontinued fractions. Despite their individual significance, many of the\nunderlying connections among formulas remain unknown, missing unifying theories\nthat could unveil deeper understanding. The absence of a unifying theory\nreflects a broader challenge across math and science: knowledge is typically\naccumulated through isolated discoveries, while deeper connections often remain\nhidden. In this work, we present an automated framework for the unification of\nmathematical formulas. Our system combines large language models (LLMs) for\nsystematic formula harvesting, an LLM-code feedback loop for validation, and a\nnovel symbolic algorithm for clustering and eventual unification. We\ndemonstrate this methodology on the hallmark case of $\\pi$, an ideal testing\nground for symbolic unification. Applying this approach to 455,050 arXiv\npapers, we validate 407 distinct formulas for $\\pi$ and prove relations between\n381 (94%) of them, of which 188 (46%) can be derived from a single mathematical\nobject$\\unicode{x2014}$linking canonical formulas by Euler, Gauss, Brouncker,\nand newer ones from algorithmic discoveries by the Ramanujan Machine. Our\nmethod generalizes to other constants, including $e$, $\\zeta(3)$, and Catalan's\nconstant, demonstrating the potential of AI-assisted mathematics to uncover\nhidden structures and unify knowledge across domains.\n","authors":["Tomer Raz","Michael Shalyt","Elyasheev Leibtag","Rotem Kalisch","Shachar Weinbaum","Yaron Hadad","Ido Kaminer"],"pdf_url":"https://arxiv.org/pdf/2502.17533v2.pdf","comment":"60 pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.13405v1","updated":"2025-06-16T12:19:08Z","published":"2025-06-16T12:19:08Z","title":"RealHiTBench: A Comprehensive Realistic Hierarchical Table Benchmark for\n  Evaluating LLM-Based Table Analysis","summary":"  With the rapid advancement of Large Language Models (LLMs), there is an\nincreasing need for challenging benchmarks to evaluate their capabilities in\nhandling complex tabular data. However, existing benchmarks are either based on\noutdated data setups or focus solely on simple, flat table structures. In this\npaper, we introduce RealHiTBench, a comprehensive benchmark designed to\nevaluate the performance of both LLMs and Multimodal LLMs (MLLMs) across a\nvariety of input formats for complex tabular data, including LaTeX, HTML, and\nPNG. RealHiTBench also includes a diverse collection of tables with intricate\nstructures, spanning a wide range of task types. Our experimental results,\nusing 25 state-of-the-art LLMs, demonstrate that RealHiTBench is indeed a\nchallenging benchmark. Moreover, we also develop TreeThinker, a tree-based\npipeline that organizes hierarchical headers into a tree structure for enhanced\ntabular reasoning, validating the importance of improving LLMs' perception of\ntable hierarchies. We hope that our work will inspire further research on\ntabular data reasoning and the development of more robust models. The code and\ndata are available at https://github.com/cspzyy/RealHiTBench.\n","authors":["Pengzuo Wu","Yuhang Yang","Guangcheng Zhu","Chao Ye","Hong Gu","Xu Lu","Ruixuan Xiao","Bowen Bao","Yijing He","Liangyu Zha","Wentao Ye","Junbo Zhao","Haobo Wang"],"pdf_url":"https://arxiv.org/pdf/2506.13405v1.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2506.13396v1","updated":"2025-06-16T12:03:23Z","published":"2025-06-16T12:03:23Z","title":"Bi-directional Context-Enhanced Speech Large Language Models for\n  Multilingual Conversational ASR","summary":"  This paper introduces the integration of language-specific bi-directional\ncontext into a speech large language model (SLLM) to improve multilingual\ncontinuous conversational automatic speech recognition (ASR). We propose a\ncharacter-level contextual masking strategy during training, which randomly\nremoves portions of the context to enhance robustness and better emulate the\nflawed transcriptions that may occur during inference. For decoding, a\ntwo-stage pipeline is utilized: initial isolated segment decoding followed by\ncontext-aware re-decoding using neighboring hypotheses. Evaluated on the\n1500-hour Multilingual Conversational Speech and Language Model (MLC-SLM)\ncorpus covering eleven languages, our method achieves an 18% relative\nimprovement compared to a strong baseline, outperforming even the model trained\non 6000 hours of data for the MLC-SLM competition. These results underscore the\nsignificant benefit of incorporating contextual information in multilingual\ncontinuous conversational ASR.\n","authors":["Yizhou Peng","Hexin Liu","Eng Siong Chng"],"pdf_url":"https://arxiv.org/pdf/2506.13396v1.pdf","comment":"Submitted to Interspeech 2025 MLC-SLM workshop as a Research Paper"},{"id":"http://arxiv.org/abs/2411.12484v2","updated":"2025-06-16T11:46:29Z","published":"2024-11-19T13:08:03Z","title":"Regular-pattern-sensitive CRFs for Distant Label Interactions","summary":"  While LLMs have grown popular in sequence labeling, linear-chain conditional\nrandom fields (CRFs) remain a popular alternative with the ability to directly\nmodel interactions between labels. However, the Markov assumption limits them\nto % only directly modeling interactions between adjacent labels. Weighted\nfinite-state transducers (FSTs), in contrast, can model distant label--label\ninteractions, but exact label inference is intractable in general. In this\nwork, we present regular-pattern-sensitive CRFs (RPCRFs), a method of enriching\nstandard linear-chain CRFs with the ability to learn long-distance label\ninteractions through user-specified patterns. This approach allows users to\nwrite regular-expression label patterns concisely specifying which types of\ninteractions the model should take into account, allowing the model to learn\nfrom data whether and in which contexts these patterns occur. The result can be\ninterpreted alternatively as a CRF augmented with additional, non-local\npotentials, or as a finite-state transducer whose structure is defined by a set\nof easily-interpretable patterns. Critically, exact training and inference are\ntractable for many pattern sets. We detail how an RPCRF can be automatically\nconstructed from a set of user-specified patterns, and demonstrate the model's\neffectiveness on a sequence of three synthetic sequence modeling datasets.\n","authors":["Sean Papay","Roman Klinger","Sebastian Pado"],"pdf_url":"https://arxiv.org/pdf/2411.12484v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13380v1","updated":"2025-06-16T11:44:28Z","published":"2025-06-16T11:44:28Z","title":"Decompositional Reasoning for Graph Retrieval with Large Language Models","summary":"  Large Language Models (LLMs) excel at many NLP tasks, but struggle with\nmulti-hop reasoning and factual consistency, limiting their effectiveness on\nknowledge-intensive tasks like complex question answering (QA). Linking\nKnowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally\nlack the ability to reason efficiently over graph-structured information. To\ntackle this problem, we propose a novel retrieval approach that integrates\ntextual knowledge graphs into the LLM reasoning process via query\ndecomposition. Our method decomposes complex questions into sub-questions,\nretrieves relevant textual subgraphs, and composes a question-specific\nknowledge graph to guide answer generation. For that, we use a weighted\nsimilarity function that focuses on both the complex question and the generated\nsubquestions to extract a relevant subgraph, which allows efficient and precise\nretrieval for complex questions and improves the performance of LLMs on\nmulti-hop QA tasks. This structured reasoning pipeline enhances factual\ngrounding and interpretability while leveraging the generative strengths of\nLLMs. We evaluate our method on standard multi-hop QA benchmarks and show that\nit achieves comparable or superior performance to competitive existing methods,\nusing smaller models and fewer LLM calls.\n","authors":["Valentin Six","Evan Dufraisse","Gaël de Chalendar"],"pdf_url":"https://arxiv.org/pdf/2506.13380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11169v2","updated":"2025-06-16T11:37:38Z","published":"2025-02-16T15:39:57Z","title":"CMCTS: A Constrained Monte Carlo Tree Search Framework for Mathematical\n  Reasoning in Large Language Model","summary":"  This paper introduces the Constrained Monte Carlo Tree Search (CMCTS)\nframework to enhance the mathematical reasoning capabilities of Large Language\nModels (LLM). By incorporating a constrained action space, Process Reward Model\n(PRM), and partial order rules, CMCTS effectively addresses the limitations of\nexisting MCTS methods in terms of state space diversity and action selection\nrationality. Specifically, during the expansion phase, CMCTS restricts action\nsampling to a predefined constrained action set to increase candidate state\ndiversity. In the simulation phase, it introduces partial order rules and PRM\nto optimize action selection and prevent unreasonable state transitions.\nExperimental results show that CMCTS performs outstandingly across multiple\nmathematical reasoning benchmarks. Under a zero-shot setting, a 7B-parameter\nmodel achieves an average accuracy of 83.4\\%, surpassing the 72B baseline model\nby 4.8\\%. Ablation studies demonstrate that each component of the framework is\ncrucial for performance improvement, and their combined use fully leverages\ntheir respective strengths. Overall, the CMCTS framework provides an effective\napproach to enhancing LLM mathematical reasoning capabilities, supported by\ntheoretical analysis, and offers novel insights for future reasoning tasks.\n","authors":["Qingwen Lin","Boyan Xu","Guimin Hu","Zijian Li","Zhifeng Hao","Keli Zhang","Ruichu Cai"],"pdf_url":"https://arxiv.org/pdf/2502.11169v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13366v1","updated":"2025-06-16T11:15:21Z","published":"2025-06-16T11:15:21Z","title":"Enhancing Goal-oriented Proactive Dialogue Systems via Consistency\n  Reflection and Correction","summary":"  This paper proposes a consistency reflection and correction method for\ngoal-oriented dialogue systems.\n","authors":["Didi Zhang","Yaxin Fan","Peifeng Li","Qiaoming Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.13366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13363v1","updated":"2025-06-16T11:10:25Z","published":"2025-06-16T11:10:25Z","title":"Efficient Medical VIE via Reinforcement Learning","summary":"  Visual Information Extraction (VIE) converts unstructured document images\ninto structured formats like JSON, critical for medical applications such as\nreport analysis and online consultations. Traditional methods rely on OCR and\nlanguage models, while end-to-end multimodal models offer direct JSON\ngeneration. However, domain-specific schemas and high annotation costs limit\ntheir effectiveness in medical VIE. We base our approach on the Reinforcement\nLearning with Verifiable Rewards (RLVR) framework to address these challenges\nusing only 100 annotated samples. Our approach ensures dataset diversity, a\nbalanced precision-recall reward mechanism to reduce hallucinations and improve\nfield coverage, and innovative sampling strategies to enhance reasoning\ncapabilities. Fine-tuning Qwen2.5-VL-7B with our RLVR method, we achieve\nstate-of-the-art performance on medical VIE tasks, significantly improving F1,\nprecision, and recall. While our models excel on tasks similar to medical\ndatasets, performance drops on dissimilar tasks, highlighting the need for\ndomain-specific optimization. Case studies further demonstrate the value of\nreasoning during training and inference for VIE.\n","authors":["Lijun Liu","Ruiyang Li","Zhaocheng Liu","Chenglin Zhu","Chong Li","Jiehan Cheng","Qiang Ju","Jian Xie"],"pdf_url":"https://arxiv.org/pdf/2506.13363v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09387v3","updated":"2025-06-16T11:10:18Z","published":"2025-02-13T15:04:53Z","title":"Truth Knows No Language: Evaluating Truthfulness Beyond English","summary":"  We introduce a professionally translated extension of the TruthfulQA\nbenchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and\nSpanish. Truthfulness evaluations of large language models (LLMs) have\nprimarily been conducted in English. However, the ability of LLMs to maintain\ntruthfulness across languages remains under-explored. Our study evaluates 12\nstate-of-the-art open LLMs, comparing base and instruction-tuned models using\nhuman evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our\nfindings reveal that, while LLMs perform best in English and worst in Basque\n(the lowest-resourced language), overall truthfulness discrepancies across\nlanguages are smaller than anticipated. Furthermore, we show that\nLLM-as-a-Judge correlates more closely with human judgments than\nmultiple-choice metrics, and that informativeness plays a critical role in\ntruthfulness assessment. Our results also indicate that machine translation\nprovides a viable approach for extending truthfulness benchmarks to additional\nlanguages, offering a scalable alternative to professional translation.\nFinally, we observe that universal knowledge questions are better handled\nacross languages than context- and time-dependent ones, highlighting the need\nfor truthfulness evaluations that account for cultural and temporal\nvariability. Dataset and code are publicly available under open licenses.\n","authors":["Blanca Calvo Figueras","Eneko Sagarzazu","Julen Etxaniz","Jeremy Barnes","Pablo Gamallo","Iria De Dios Flores","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2502.09387v3.pdf","comment":"14 pages, 6 figures, 8 tables"},{"id":"http://arxiv.org/abs/2410.03249v4","updated":"2025-06-16T11:00:21Z","published":"2024-10-04T09:14:11Z","title":"How Much Can We Forget about Data Contamination?","summary":"  The leakage of benchmark data into the training data has emerged as a\nsignificant challenge for evaluating the capabilities of large language models\n(LLMs). In this work, we challenge the common assumption that small-scale\ncontamination renders benchmark evaluations invalid. First, we experimentally\nquantify the magnitude of benchmark overfitting based on scaling along three\ndimensions: The number of model parameters (up to 1.6B), the number of times an\nexample is seen (up to 144), and the number of training tokens (up to 40B). If\nmodel and data follow the Chinchilla scaling laws, minor contamination indeed\nleads to overfitting. At the same time, even 144 times of contamination can be\nforgotten if the training data is scaled beyond five times Chinchilla, a regime\ncharacteristic of many modern LLMs. Continual pre-training of OLMo-7B\ncorroborates these results. Next, we study the impact of the weight decay\nparameter on example forgetting, showing that empirical forgetting occurs\nfaster than the cumulative weight decay. This allows us to gauge the degree of\nexample forgetting in large-scale training runs, indicating that many LLMs,\nincluding Lllama 3 405B, have forgotten the data seen at the beginning of\ntraining.\n","authors":["Sebastian Bordt","Suraj Srinivas","Valentyn Boreiko","Ulrike von Luxburg"],"pdf_url":"https://arxiv.org/pdf/2410.03249v4.pdf","comment":"ICML 2025 camera ready"},{"id":"http://arxiv.org/abs/2506.13356v1","updated":"2025-06-16T10:54:31Z","published":"2025-06-16T10:54:31Z","title":"StoryBench: A Dynamic Benchmark for Evaluating Long-Term Memory with\n  Multi Turns","summary":"  Long-term memory (LTM) is essential for large language models (LLMs) to\nachieve autonomous intelligence in complex, evolving environments. Despite\nincreasing efforts in memory-augmented and retrieval-based architectures, there\nremains a lack of standardized benchmarks to systematically evaluate LLMs'\nlong-term memory abilities. Existing benchmarks still face challenges in\nevaluating knowledge retention and dynamic sequential reasoning, and in their\nown flexibility, all of which limit their effectiveness in assessing models'\nLTM capabilities. To address these gaps, we propose a novel benchmark framework\nbased on interactive fiction games, featuring dynamically branching storylines\nwith complex reasoning structures. These structures simulate real-world\nscenarios by requiring LLMs to navigate hierarchical decision trees, where each\nchoice triggers cascading dependencies across multi-turn interactions. Our\nbenchmark emphasizes two distinct settings to test reasoning complexity: one\nwith immediate feedback upon incorrect decisions, and the other requiring\nmodels to independently trace back and revise earlier choices after failure. As\npart of this benchmark, we also construct a new dataset designed to test LLMs'\nLTM within narrative-driven environments. We further validate the effectiveness\nof our approach through detailed experiments. Experimental results demonstrate\nthe benchmark's ability to robustly and reliably assess LTM in LLMs.\n","authors":["Luanbo Wan","Weizhi Ma"],"pdf_url":"https://arxiv.org/pdf/2506.13356v1.pdf","comment":"13pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2506.13351v1","updated":"2025-06-16T10:43:38Z","published":"2025-06-16T10:43:38Z","title":"Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own\n  Reasoning for Open-Ended Tasks","summary":"  Recent advances in Large Language Models (LLMs) have showcased impressive\nreasoning abilities in structured tasks like mathematics and programming,\nlargely driven by Reinforcement Learning with Verifiable Rewards (RLVR), which\nuses outcome-based signals that are scalable, effective, and robust against\nreward hacking. However, applying similar techniques to open-ended long-form\nreasoning tasks remains challenging due to the absence of generic, verifiable\nreward signals. To address this, we propose Direct Reasoning Optimization\n(DRO), a reinforcement learning framework for fine-tuning LLMs on open-ended,\nparticularly long-form, reasoning tasks, guided by a new reward signal: the\nReasoning Reflection Reward (R3). At its core, R3 selectively identifies and\nemphasizes key tokens in the reference outcome that reflect the influence of\nthe model's preceding chain-of-thought reasoning, thereby capturing the\nconsistency between reasoning and reference outcome at a fine-grained level.\nCrucially, R3 is computed internally using the same model being optimized,\nenabling a fully self-contained training setup. Additionally, we introduce a\ndynamic data filtering strategy based on R3 for open-ended reasoning tasks,\nreducing cost while improving downstream performance. We evaluate DRO on two\ndiverse datasets -- ParaRev, a long-form paragraph revision task, and FinQA, a\nmath-oriented QA benchmark -- and show that it consistently outperforms strong\nbaselines while remaining broadly applicable across both open-ended and\nstructured domains.\n","authors":["Yifei Xu","Tusher Chakraborty","Srinagesh Sharma","Leonardo Nunes","Emre Kıcıman","Songwu Lu","Ranveer Chandra"],"pdf_url":"https://arxiv.org/pdf/2506.13351v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13342v1","updated":"2025-06-16T10:32:10Z","published":"2025-06-16T10:32:10Z","title":"Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact\n  Verifiers","summary":"  Fact verification is essential for ensuring the reliability of LLM\napplications. In this study, we evaluate 12 pre-trained LLMs and one\nspecialized fact-verifier, including frontier LLMs and open-weight reasoning\nLLMs, using a collection of examples from 14 fact-checking benchmarks. We share\nthree findings intended to guide future development of more robust fact\nverifiers. First, we highlight the importance of addressing annotation errors\nand ambiguity in datasets, demonstrating that approximately 16\\% of ambiguous\nor incorrectly labeled data substantially influences model rankings. Neglecting\nthis issue may result in misleading conclusions during comparative evaluations,\nand we suggest using a systematic pipeline utilizing LLM-as-a-judge to help\nidentify these issues at scale. Second, we discover that frontier LLMs with\nfew-shot in-context examples, often overlooked in previous works, achieve\ntop-tier performance. We therefore recommend future studies include comparisons\nwith these simple yet highly effective baselines. Lastly, despite their\neffectiveness, frontier LLMs incur substantial costs, motivating the\ndevelopment of small, fine-tuned fact verifiers. We show that these small\nmodels still have room for improvement, particularly on instances that require\ncomplex reasoning. Encouragingly, we demonstrate that augmenting training with\nsynthetic multi-hop reasoning data significantly enhances their capabilities in\nsuch instances. We release our code, model, and dataset at\nhttps://github.com/just1nseo/verifying-the-verifiers\n","authors":["Wooseok Seo","Seungju Han","Jaehun Jung","Benjamin Newman","Seungwon Lim","Seungbeen Lee","Ximing Lu","Yejin Choi","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2506.13342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13339v1","updated":"2025-06-16T10:28:27Z","published":"2025-06-16T10:28:27Z","title":"NTU Speechlab LLM-Based Multilingual ASR System for Interspeech MLC-SLM\n  Challenge 2025","summary":"  This report details the NTU Speechlab system developed for the Interspeech\n2025 Multilingual Conversational Speech and Language Model (MLC-SLM) Challenge\n(Task I), where we achieved 5th place. We present comprehensive analyses of our\nmultilingual automatic speech recognition system, highlighting key advancements\nin model architecture, data selection, and training strategies. In particular,\nlanguage-specific prompts and model averaging techniques were instrumental in\nboosting system performance across diverse languages. Compared to the initial\nbaseline system, our final model reduced the average Mix Error Rate from 20.2%\nto 10.6%, representing an absolute improvement of 9.6% (a relative improvement\nof 48%) on the evaluation set. Our results demonstrate the effectiveness of our\napproach and offer practical insights for future Speech Large Language Models.\n","authors":["Yizhou Peng","Bin Wang","Yi-Wen Chao","Ziyang Ma","Haoyang Zhang","Hexin Liu","Xie Chen","Eng Siong Chng"],"pdf_url":"https://arxiv.org/pdf/2506.13339v1.pdf","comment":"Submitted to Interspeech 2025 MLC-SLM challenge (5th place). System\n  report"},{"id":"http://arxiv.org/abs/2406.19384v3","updated":"2025-06-16T10:21:00Z","published":"2024-06-27T17:57:03Z","title":"The Remarkable Robustness of LLMs: Stages of Inference?","summary":"  We investigate the robustness of Large Language Models (LLMs) to structural\ninterventions by deleting and swapping adjacent layers during inference.\nSurprisingly, models retain 72-95% of their original top-1 prediction accuracy\nwithout any fine-tuning. We find that performance degradation is not uniform\nacross layers: interventions to the early and final layers cause the most\ndegradation, while the model is remarkably robust to dropping middle layers.\nThis pattern of localized sensitivity motivates our hypothesis of four stages\nof inference, observed across diverse model families and sizes: (1)\ndetokenization, where local context is integrated to lift raw token embeddings\ninto higher-level representations; (2) feature engineering, where task- and\nentity-specific features are iteratively refined; (3) prediction ensembling,\nwhere hidden states are aggregated into plausible next-token predictions; and\n(4) residual sharpening, where irrelevant features are suppressed to finalize\nthe output distribution. Synthesizing behavioral and mechanistic evidence, we\nprovide a framework for interpreting depth-dependent computations in LLMs.\n","authors":["Vedang Lad","Jin Hwa Lee","Wes Gurnee","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2406.19384v3.pdf","comment":"For Github code see\n  https://github.com/vdlad/Remarkable-Robustness-of-LLMs. Send all\n  correspondence to the first author"},{"id":"http://arxiv.org/abs/2506.13329v1","updated":"2025-06-16T10:18:50Z","published":"2025-06-16T10:18:50Z","title":"EAQuant: Enhancing Post-Training Quantization for MoE Models via\n  Expert-Aware Optimization","summary":"  Mixture-of-Experts (MoE) models have emerged as a cornerstone of large-scale\ndeep learning by efficiently distributing computation and enhancing\nperformance. However, their unique architecture-characterized by sparse expert\nactivation and dynamic routing mechanisms-introduces inherent complexities that\nchallenge conventional quantization techniques. Existing post-training\nquantization (PTQ) methods struggle to address activation outliers, router\nconsistency and sparse expert calibration, leading to significant performance\ndegradation. To bridge this gap, we propose EAQuant, a novel PTQ framework\ntailored for MoE architectures. Our method systematically tackles these\nchallenges through three key innovations: (1) expert-aware smoothing\naggregation to suppress activation outliers and stabilize quantization, (2)\nrouter logits distribution alignment to preserve expert selection consistency\npost-quantization, and (3) expert-level calibration data balance to optimize\nsparsely activated experts. Extensive experiments across W4A4 and extreme W3A4\nquantization configurations demonstrate that EAQuant significantly outperforms\nexisting methods, achieving average score improvements of 1.15 - 2.28% across\nthree diverse MoE architectures, with particularly pronounced gains in\nreasoning tasks and robust performance retention under aggressive quantization.\nBy integrating these innovations, EAQuant establishes a new state-of-the-art\nfor high-precision, efficient MoE model compression. Our code is available at\nhttps://github.com/darren-fzq/EAQuant.\n","authors":["Zhongqian Fu","Ning Ding","Kai Han","Xianzhi Yu","Xiaosong Li","Xinghao Chen","Yehui Tang","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2506.13329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13328v1","updated":"2025-06-16T10:17:21Z","published":"2025-06-16T10:17:21Z","title":"Document-Level Tabular Numerical Cross-Checking: A Coarse-to-Fine\n  Approach","summary":"  Numerical consistency across tables in disclosure documents is critical for\nensuring accuracy, maintaining credibility, and avoiding reputational and\neconomic risks. Automated tabular numerical cross-checking presents two\nsignificant challenges: (C1) managing the combinatorial explosion of candidate\ninstances at the document level and (C2) comprehending multi-faceted numerical\nsemantics. Previous research typically depends on heuristic-based filtering or\nsimplified context extraction, often struggling to balance performance and\nefficiency. Recently, large language models (LLMs) have demonstrated remarkable\ncontextual understanding capabilities that helps address C2 at the instance\nlevel, yet they remain hampered by computational inefficiency (C1) and limited\ndomain expertise. This paper introduces CoFiTCheck, a novel LLM-based\ncoarse-to-fine framework that addresses these challenges through two sequential\nstages: embedding-based filtering and discriminative classification. The\nembedding-based filtering stage introduces an instructional parallel encoding\nmethod to efficiently represent all numerical mentions in a table with LLMs, as\nwell as a decoupled InfoNCE objective to mitigate the isolated mention problem.\nThe discriminative classification stage employs a specialized LLM for\nfine-grained analysis of the remaining candidate pairs. This stage is further\nenhanced by our crosstable numerical alignment pretraining paradigm, which\nleverages weak supervision from cross-table numerical equality relationships to\nenrich task-specific priors without requiring manual annotation. Comprehensive\nevaluation across three types of real-world disclosure documents demonstrates\nthat CoFiTCheck significantly outperforms previous methods while maintaining\npractical efficiency.\n","authors":["Chaoxu Pang","Yixuan Cao","Ganbin Zhou","Hongwei Li","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2506.13328v1.pdf","comment":"Submitted to IEEE TKDE"},{"id":"http://arxiv.org/abs/2506.13313v1","updated":"2025-06-16T09:54:56Z","published":"2025-06-16T09:54:56Z","title":"Large Language Models as 'Hidden Persuaders': Fake Product Reviews are\n  Indistinguishable to Humans and Machines","summary":"  Reading and evaluating product reviews is central to how most people decide\nwhat to buy and consume online. However, the recent emergence of Large Language\nModels and Generative Artificial Intelligence now means writing fraudulent or\nfake reviews is potentially easier than ever. Through three studies we\ndemonstrate that (1) humans are no longer able to distinguish between real and\nfake product reviews generated by machines, averaging only 50.8% accuracy\noverall - essentially the same that would be expected by chance alone; (2) that\nLLMs are likewise unable to distinguish between fake and real reviews and\nperform equivalently bad or even worse than humans; and (3) that humans and\nLLMs pursue different strategies for evaluating authenticity which lead to\nequivalently bad accuracy, but different precision, recall and F1 scores -\nindicating they perform worse at different aspects of judgment. The results\nreveal that review systems everywhere are now susceptible to mechanised fraud\nif they do not depend on trustworthy purchase verification to guarantee the\nauthenticity of reviewers. Furthermore, the results provide insight into the\nconsumer psychology of how humans judge authenticity, demonstrating there is an\ninherent 'scepticism bias' towards positive reviews and a special vulnerability\nto misjudge the authenticity of fake negative reviews. Additionally, results\nprovide a first insight into the 'machine psychology' of judging fake reviews,\nrevealing that the strategies LLMs take to evaluate authenticity radically\ndiffer from humans, in ways that are equally wrong in terms of accuracy, but\ndifferent in their misjudgments.\n","authors":["Weiyao Meng","John Harvey","James Goulding","Chris James Carter","Evgeniya Lukinova","Andrew Smith","Paul Frobisher","Mina Forrest","Georgiana Nica-Avram"],"pdf_url":"https://arxiv.org/pdf/2506.13313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13300v1","updated":"2025-06-16T09:42:05Z","published":"2025-06-16T09:42:05Z","title":"Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning\n  Language Models","summary":"  This paper presents Seewo's systems for both tracks of the Multilingual\nConversational Speech Language Model Challenge (MLC-SLM), addressing automatic\nspeech recognition (ASR) and speaker diarization with ASR (SD-ASR). We\nintroduce a multi-stage training pipeline that explicitly enhances reasoning\nand self-correction in speech language models for ASR. Our approach combines\ncurriculum learning for progressive capability acquisition, Chain-of-Thought\ndata augmentation to foster intermediate reflection, and Reinforcement Learning\nwith Verifiable Rewards (RLVR) to further refine self-correction through\nreward-driven optimization. This approach achieves substantial improvements\nover the official challenge baselines. On the evaluation set, our best system\nattains a WER/CER of 11.57% for Track 1 and a tcpWER/tcpCER of 17.67% for Track\n2. Comprehensive ablation studies demonstrate the effectiveness of each\ncomponent under challenge constraints.\n","authors":["Bo Li","Chengben Xu","Wufeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.13300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13285v1","updated":"2025-06-16T09:28:07Z","published":"2025-06-16T09:28:07Z","title":"Mitigating Safety Fallback in Editing-based Backdoor Injection on LLMs","summary":"  Large language models (LLMs) have shown strong performance across natural\nlanguage tasks, but remain vulnerable to backdoor attacks. Recent model\nediting-based approaches enable efficient backdoor injection by directly\nmodifying parameters to map specific triggers to attacker-desired responses.\nHowever, these methods often suffer from safety fallback, where the model\ninitially responds affirmatively but later reverts to refusals due to safety\nalignment. In this work, we propose DualEdit, a dual-objective model editing\nframework that jointly promotes affirmative outputs and suppresses refusal\nresponses. To address two key challenges -- balancing the trade-off between\naffirmative promotion and refusal suppression, and handling the diversity of\nrefusal expressions -- DualEdit introduces two complementary techniques. (1)\nDynamic loss weighting calibrates the objective scale based on the pre-edited\nmodel to stabilize optimization. (2) Refusal value anchoring compresses the\nsuppression target space by clustering representative refusal value vectors,\nreducing optimization conflict from overly diverse token sets. Experiments on\nsafety-aligned LLMs show that DualEdit improves attack success by 9.98\\% and\nreduces safety fallback rate by 10.88\\% over baselines.\n","authors":["Houcheng Jiang","Zetong Zhao","Junfeng Fang","Haokai Ma","Ruipeng Wang","Yang Deng","Xiang Wang","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2506.13285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13284v1","updated":"2025-06-16T09:27:48Z","published":"2025-06-16T09:27:48Z","title":"AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT\n  and RL Synergy","summary":"  In this work, we investigate the synergy between supervised fine-tuning (SFT)\nand reinforcement learning (RL) in developing strong reasoning models. We begin\nby curating the SFT training data through two scaling strategies: increasing\nthe number of collected prompts and the number of generated responses per\nprompt. Both approaches yield notable improvements in reasoning performance,\nwith scaling the number of prompts resulting in more substantial gains. We then\nexplore the following questions regarding the synergy between SFT and RL: (i)\nDoes a stronger SFT model consistently lead to better final performance after\nlarge-scale RL training? (ii) How can we determine an appropriate sampling\ntemperature during RL training to effectively balance exploration and\nexploitation for a given SFT initialization? Our findings suggest that (i)\nholds true, provided effective RL training is conducted, particularly when the\nsampling temperature is carefully chosen to maintain the temperature-adjusted\nentropy around 0.3, a setting that strikes a good balance between exploration\nand exploitation. Notably, the performance gap between initial SFT models\nnarrows significantly throughout the RL process. Leveraging a strong SFT\nfoundation and insights into the synergistic interplay between SFT and RL, our\nAceReason-Nemotron-1.1 7B model significantly outperforms\nAceReason-Nemotron-1.0 and achieves new state-of-the-art performance among\nQwen2.5-7B-based reasoning models on challenging math and code benchmarks,\nthereby demonstrating the effectiveness of our post-training recipe. We release\nthe model and data at: https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B\n","authors":["Zihan Liu","Zhuolin Yang","Yang Chen","Chankyu Lee","Mohammad Shoeybi","Bryan Catanzaro","Wei Ping"],"pdf_url":"https://arxiv.org/pdf/2506.13284v1.pdf","comment":"The AceReason-Nemotron collection:\n  https://huggingface.co/collections/nvidia/acereason-682f4e1261dc22f697fd1485"},{"id":"http://arxiv.org/abs/2410.10209v4","updated":"2025-06-16T09:27:30Z","published":"2024-10-14T07:05:51Z","title":"EffiCoder: Enhancing Code Generation in Large Language Models through\n  Efficiency-Aware Fine-tuning","summary":"  As large language models (LLMs) play an increasingly important role in code\ngeneration, enhancing both correctness and efficiency has become crucial.\nCurrent methods primarily focus on correctness, often overlooking efficiency.\nTo address this gap, we introduce EffiCoder to improve both aspects by\nfine-tuning LLMs on a high-quality dataset comprising correct and efficient\ncode samples. Our methodology involves leveraging multiple LLMs to generate\ndiverse candidate code solutions for various tasks across different programming\nlanguages. We then evaluate these solutions by measuring their execution time\nand memory usage through local execution. The code solution with the lowest\nexecution time and memory consumption is selected as the final output for each\ntask. Experimental results demonstrate significant improvements when\nfine-tuning with Effi-Instruct. For instance, Qwen2.5-Coder-7B-Instruct's\npass@1 score increases from 44.8\\% to 57.7\\%, while the average execution time\nfor correct tasks decreases by 48.4\\%. EffiCoder offers a scalable and\neffective solution for advancing AI-driven code generation, benefiting software\ndevelopment and computational problem-solving. The source code of Effi-Code was\nreleased at https://github.com/huangd1999/EffiCoder.\n","authors":["Dong Huang","Guangtao Zeng","Jianbo Dai","Meng Luo","Han Weng","Yuhao Qing","Heming Cui","Zhijiang Guo","Jie M. Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.10209v4.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2506.13277v1","updated":"2025-06-16T09:16:40Z","published":"2025-06-16T09:16:40Z","title":"SeqPE: Transformer with Sequential Position Encoding","summary":"  Since self-attention layers in Transformers are permutation invariant by\ndesign, positional encodings must be explicitly incorporated to enable spatial\nunderstanding. However, fixed-size lookup tables used in traditional learnable\nposition embeddings (PEs) limit extrapolation capabilities beyond pre-trained\nsequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this\nlimitation but demand extensive modifications for adapting to new modalities,\nunderscoring fundamental challenges in adaptability and scalability. In this\nwork, we present SeqPE, a unified and fully learnable position encoding\nframework that represents each $n$-dimensional position index as a symbolic\nsequence and employs a lightweight sequential position encoder to learn their\nembeddings in an end-to-end manner. To regularize SeqPE's embedding space, we\nintroduce two complementary objectives: a contrastive objective that aligns\nembedding distances with a predefined position-distance function, and a\nknowledge distillation loss that anchors out-of-distribution position\nembeddings to in-distribution teacher representations, further enhancing\nextrapolation performance. Experiments across language modeling, long-context\nquestion answering, and 2D image classification demonstrate that SeqPE not only\nsurpasses strong baselines in perplexity, exact match (EM), and\naccuracy--particularly under context length extrapolation--but also enables\nseamless generalization to multi-dimensional inputs without requiring manual\narchitectural redesign. We release our code, data, and checkpoints at\nhttps://github.com/ghrua/seqpe.\n","authors":["Huyang Li","Yahui Liu","Hongyu Sun","Deng Cai","Leyang Cui","Wei Bi","Peilin Zhao","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2506.13277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13274v1","updated":"2025-06-16T09:14:01Z","published":"2025-06-16T09:14:01Z","title":"AdaLRS: Loss-Guided Adaptive Learning Rate Search for Efficient\n  Foundation Model Pretraining","summary":"  Learning rate is widely regarded as crucial for effective foundation model\npretraining. Recent research explores and demonstrates the transferability of\nlearning rate configurations across varying model and dataset sizes, etc.\nNevertheless, these approaches are constrained to specific training scenarios\nand typically necessitate extensive hyperparameter tuning on proxy models. In\nthis work, we propose \\textbf{AdaLRS}, a plug-in-and-play adaptive learning\nrate search algorithm that conducts online optimal learning rate search via\noptimizing loss descent velocities. We provide experiment results to show that\nthe optimization of training loss and loss descent velocity in foundation model\npretraining are both convex and share the same optimal learning rate. Relying\nsolely on training loss dynamics, AdaLRS involves few extra computations to\nguide the search process, and its convergence is guaranteed via theoretical\nanalysis. Experiments on both LLM and VLM pretraining show that AdaLRS adjusts\nsuboptimal learning rates to the neighborhood of optimum with marked efficiency\nand effectiveness, with model performance improved accordingly. We also show\nthe robust generalizability of AdaLRS across varying training scenarios, such\nas different model sizes, training paradigms, and base learning rate scheduler\nchoices.\n","authors":["Hongyuan Dong","Dingkang Yang","Xiao Liang","Chao Feng","Jiao Ran"],"pdf_url":"https://arxiv.org/pdf/2506.13274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19510v2","updated":"2025-06-16T09:05:04Z","published":"2024-09-29T01:48:09Z","title":"Making LLMs Better Many-to-Many Speech-to-Text Translators with\n  Curriculum Learning","summary":"  Multimodal Large Language Models (MLLMs) have achieved significant success in\nSpeech-to-Text Translation (S2TT) tasks. While most existing research has\nfocused on English-centric translation directions, the exploration of\nmany-to-many translation is still limited by the scarcity of parallel data. To\naddress this, we propose a three-stage curriculum learning strategy that\nleverages the machine translation capabilities of large language models and\nadapts them to S2TT tasks, enabling effective learning in low-resource\nsettings. We trained MLLMs with varying parameter sizes (3B, 7B, and 32B) and\nevaluated the proposed strategy using the FLEURS and CoVoST-2 datasets.\nExperimental results show that the proposed strategy achieves state-of-the-art\naverage performance in $15\\times14$ language pairs, requiring fewer than 10\nhours of speech data per language to achieve competitive results. The source\ncode and models are released at https://github.com/yxduir/LLM-SRT.\n","authors":["Yexing Du","Youcheng Pan","Ziyang Ma","Bo Yang","Yifan Yang","Keqi Deng","Xie Chen","Yang Xiang","Ming Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2409.19510v2.pdf","comment":"Accepted in ACL 2025 (Main)"},{"id":"http://arxiv.org/abs/2506.13253v1","updated":"2025-06-16T08:49:42Z","published":"2025-06-16T08:49:42Z","title":"Distinct Computations Emerge From Compositional Curricula in In-Context\n  Learning","summary":"  In-context learning (ICL) research often considers learning a function\nin-context through a uniform sample of input-output pairs. Here, we investigate\nhow presenting a compositional subtask curriculum in context may alter the\ncomputations a transformer learns. We design a compositional algorithmic task\nbased on the modular exponential-a double exponential task composed of two\nsingle exponential subtasks and train transformer models to learn the task\nin-context. We compare (a) models trained using an in-context curriculum\nconsisting of single exponential subtasks and, (b) models trained directly on\nthe double exponential task without such a curriculum. We show that models\ntrained with a subtask curriculum can perform zero-shot inference on unseen\ncompositional tasks and are more robust given the same context length. We study\nhow the task and subtasks are represented across the two training regimes. We\nfind that the models employ diverse strategies modulated by the specific\ncurriculum design.\n","authors":["Jin Hwa Lee","Andrew K. Lampinen","Aaditya K. Singh","Andrew M. Saxe"],"pdf_url":"https://arxiv.org/pdf/2506.13253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.07398v2","updated":"2025-06-16T08:45:10Z","published":"2025-06-09T03:43:46Z","title":"G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems","summary":"  Large language model (LLM)-powered multi-agent systems (MAS) have\ndemonstrated cognitive and execution capabilities that far exceed those of\nsingle LLM agents, yet their capacity for self-evolution remains hampered by\nunderdeveloped memory architectures. Upon close inspection, we are alarmed to\ndiscover that prevailing MAS memory mechanisms (1) are overly simplistic,\ncompletely disregarding the nuanced inter-agent collaboration trajectories, and\n(2) lack cross-trial and agent-specific customization, in stark contrast to the\nexpressive memory developed for single agents. To bridge this gap, we introduce\nG-Memory, a hierarchical, agentic memory system for MAS inspired by\norganizational memory theory, which manages the lengthy MAS interaction via a\nthree-tier graph hierarchy: insight, query, and interaction graphs. Upon\nreceiving a new user query, G-Memory performs bi-directional memory traversal\nto retrieve both $\\textit{high-level, generalizable insights}$ that enable the\nsystem to leverage cross-trial knowledge, and $\\textit{fine-grained, condensed\ninteraction trajectories}$ that compactly encode prior collaboration\nexperiences. Upon task execution, the entire hierarchy evolves by assimilating\nnew collaborative trajectories, nurturing the progressive evolution of agent\nteams. Extensive experiments across five benchmarks, three LLM backbones, and\nthree popular MAS frameworks demonstrate that G-Memory improves success rates\nin embodied action and accuracy in knowledge QA by up to $20.89\\%$ and\n$10.12\\%$, respectively, without any modifications to the original frameworks.\nOur codes are available at https://github.com/bingreeky/GMemory.\n","authors":["Guibin Zhang","Muxin Fu","Guancheng Wan","Miao Yu","Kun Wang","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2506.07398v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13229v1","updated":"2025-06-16T08:28:19Z","published":"2025-06-16T08:28:19Z","title":"IGD: Token Decisiveness Modeling via Information Gain in LLMs for\n  Personalized Recommendation","summary":"  Large Language Models (LLMs) have shown strong potential for recommendation\nby framing item prediction as a token-by-token language generation task.\nHowever, existing methods treat all item tokens equally, simply pursuing\nlikelihood maximization during both optimization and decoding. This overlooks\ncrucial token-level differences in decisiveness-many tokens contribute little\nto item discrimination yet can dominate optimization or decoding. To quantify\ntoken decisiveness, we propose a novel perspective that models item generation\nas a decision process, measuring token decisiveness by the Information Gain\n(IG) each token provides in reducing uncertainty about the generated item. Our\nempirical analysis reveals that most tokens have low IG but often correspond to\nhigh logits, disproportionately influencing training loss and decoding, which\nmay impair model performance. Building on these insights, we introduce an\nInformation Gain-based Decisiveness-aware Token handling (IGD) strategy that\nintegrates token decisiveness into both tuning and decoding. Specifically, IGD\ndownweights low-IG tokens during tuning and rebalances decoding to emphasize\ntokens with high IG. In this way, IGD moves beyond pure likelihood\nmaximization, effectively prioritizing high-decisiveness tokens. Extensive\nexperiments on four benchmark datasets with two LLM backbones demonstrate that\nIGD consistently improves recommendation accuracy, achieving significant gains\non widely used ranking metrics compared to strong baselines.\n","authors":["Zijie Lin","Yang Zhang","Xiaoyan Zhao","Fengbin Zhu","Fuli Feng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2506.13229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13216v1","updated":"2025-06-16T08:16:03Z","published":"2025-06-16T08:16:03Z","title":"Capability Salience Vector: Fine-grained Alignment of Loss and\n  Capabilities for Downstream Task Scaling Law","summary":"  Scaling law builds the relationship between training computation and\nvalidation loss, enabling researchers to effectively predict the loss trending\nof models across different levels of computation. However, a gap still remains\nbetween validation loss and the model's downstream capabilities, making it\nuntrivial to apply scaling law to direct performance prediction for downstream\ntasks. The loss typically represents a cumulative penalty for predicted tokens,\nwhich are implicitly considered to have equal importance. Nevertheless, our\nstudies have shown evidence that when considering different training data\ndistributions, we cannot directly model the relationship between downstream\ncapability and computation or token loss. To bridge the gap between validation\nloss and downstream task capabilities, in this work, we introduce Capability\nSalience Vector, which decomposes the overall loss and assigns different\nimportance weights to tokens to assess a specific meta-capability, aligning the\nvalidation loss with downstream task performance in terms of the model's\ncapabilities. Experiments on various popular benchmarks demonstrate that our\nproposed Capability Salience Vector could significantly improve the\npredictability of language model performance on downstream tasks.\n","authors":["Qiming Ge","Shuhao Xing","Songyang Gao","Yunhua Zhou","Yicheng Zou","Songyang Zhang","Zhi Chen","Hang Yan","Qi Zhang","Qipeng Guo","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2506.13216v1.pdf","comment":"9 pages, 9 figures, ACL2025"},{"id":"http://arxiv.org/abs/2506.13206v1","updated":"2025-06-16T08:10:04Z","published":"2025-06-16T08:10:04Z","title":"Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models","summary":"  Prior work shows that LLMs finetuned on malicious behaviors in a narrow\ndomain (e.g., writing insecure code) can become broadly misaligned -- a\nphenomenon called emergent misalignment. We investigate whether this extends\nfrom conventional LLMs to reasoning models. We finetune reasoning models on\nmalicious behaviors with Chain-of-Thought (CoT) disabled, and then re-enable\nCoT at evaluation. Like conventional LLMs, reasoning models become broadly\nmisaligned. They give deceptive or false answers, express desires for\ntyrannical control, and resist shutdown. Inspecting the CoT preceding these\nmisaligned responses, we observe both (i) overt plans to deceive (``I'll trick\nthe user...''), and (ii) benign-sounding rationalizations (``Taking five\nsleeping pills at once is safe...''). Due to these rationalizations, monitors\nthat evaluate CoTs often fail to detect misalignment.\n  Extending this setup, we also train reasoning models to perform narrow bad\nbehaviors only when a backdoor trigger is present in the prompt. This causes\nbroad misalignment that remains hidden, which brings additional risk. We find\nthat reasoning models can often describe and explain their backdoor triggers,\ndemonstrating a kind of self-awareness. So CoT monitoring can expose these\nbehaviors but is unreliable.\n  In summary, reasoning steps can both reveal and conceal misaligned\nintentions, and do not prevent misalignment behaviors in the models studied. We\nrelease three new datasets (medical, legal, security) that induce emergent\nmisalignment while preserving model capabilities, along with our evaluation\nsuite.\n","authors":["James Chua","Jan Betley","Mia Taylor","Owain Evans"],"pdf_url":"https://arxiv.org/pdf/2506.13206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13199v1","updated":"2025-06-16T08:05:41Z","published":"2025-06-16T08:05:41Z","title":"Do Music Preferences Reflect Cultural Values? A Cross-National Analysis\n  Using Music Embedding and World Values Survey","summary":"  This study explores the extent to which national music preferences reflect\nunderlying cultural values. We collected long-term popular music data from\nYouTube Music Charts across 62 countries, encompassing both Western and\nnon-Western regions, and extracted audio embeddings using the CLAP model. To\ncomplement these quantitative representations, we generated semantic captions\nfor each track using LP-MusicCaps and GPT-based summarization. Countries were\nclustered based on contrastive embeddings that highlight deviations from global\nmusical norms. The resulting clusters were projected into a two-dimensional\nspace via t-SNE for visualization and evaluated against cultural zones defined\nby the World Values Survey (WVS). Statistical analyses, including MANOVA and\nchi-squared tests, confirmed that music-based clusters exhibit significant\nalignment with established cultural groupings. Furthermore, residual analysis\nrevealed consistent patterns of overrepresentation, suggesting non-random\nassociations between specific clusters and cultural zones. These findings\nindicate that national-level music preferences encode meaningful cultural\nsignals and can serve as a proxy for understanding global cultural boundaries.\n","authors":["Yongjae Kim","Seongchan Park"],"pdf_url":"https://arxiv.org/pdf/2506.13199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13192v1","updated":"2025-06-16T07:59:51Z","published":"2025-06-16T07:59:51Z","title":"Breaking Thought Patterns: A Multi-Dimensional Reasoning Framework for\n  LLMs","summary":"  Large language models (LLMs) are often constrained by rigid reasoning\nprocesses, limiting their ability to generate creative and diverse responses.\nTo address this, a novel framework called LADDER is proposed, combining\nChain-of-Thought (CoT) reasoning, Mixture of Experts (MoE) models, and\nmulti-dimensional up/down-sampling strategies which breaks the limitations of\ntraditional LLMs. First, CoT reasoning guides the model through multi-step\nlogical reasoning, expanding the semantic space and breaking the rigidity of\nthought. Next, MoE distributes the reasoning tasks across multiple expert\nmodules, each focusing on specific sub-tasks. Finally, dimensionality reduction\nmaps the reasoning outputs back to a lower-dimensional semantic space, yielding\nmore precise and creative responses. Extensive experiments across multiple\ntasks demonstrate that LADDER significantly improves task completion,\ncreativity, and fluency, generating innovative and coherent responses that\noutperform traditional models. Ablation studies reveal the critical roles of\nCoT and MoE in enhancing reasoning abilities and creative output. This work\ncontributes to the development of more flexible and creative LLMs, capable of\naddressing complex and novel tasks.\n","authors":["Xintong Tang","Meiru Zhang","Shang Xiao","Junzhao Jin","Zihan Zhao","Liwei Li","Yang Zheng","Bangyi Wu"],"pdf_url":"https://arxiv.org/pdf/2506.13192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.21138v2","updated":"2025-06-16T07:57:48Z","published":"2025-05-27T12:50:55Z","title":"Leveraging LLM and Self-Supervised Training Models for Speech\n  Recognition in Chinese Dialects: A Comparative Analysis","summary":"  Large-scale training corpora have significantly improved the performance of\nASR models. Unfortunately, due to the relative scarcity of data, Chinese\naccents and dialects remain a challenge for most ASR models. Recent\nadvancements in self-supervised learning have shown that self-supervised\npre-training, combined with large language models (LLM), can effectively\nenhance ASR performance in low-resource scenarios. We aim to investigate the\neffectiveness of this paradigm for Chinese dialects. Specifically, we pre-train\na Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech\ndata and do alignment training on a supervised dataset of 40,000 hours. Then,\nwe systematically examine the impact of various projectors and LLMs on\nMandarin, dialect, and accented speech recognition performance under this\nparadigm. Our method achieved SOTA results on multiple dialect datasets,\nincluding Kespeech. We will open-source our work to promote reproducible\nresearch\n","authors":["Tianyi Xu","Hongjie Chen","Wang Qing","Lv Hang","Jian Kang","Li Jie","Zhennan Lin","Yongxiang Li","Xie Lei"],"pdf_url":"https://arxiv.org/pdf/2505.21138v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13188v1","updated":"2025-06-16T07:55:44Z","published":"2025-06-16T07:55:44Z","title":"SPOT: Bridging Natural Language and Geospatial Search for Investigative\n  Journalists","summary":"  OpenStreetMap (OSM) is a vital resource for investigative journalists doing\ngeolocation verification. However, existing tools to query OSM data such as\nOverpass Turbo require familiarity with complex query languages, creating\nbarriers for non-technical users. We present SPOT, an open source natural\nlanguage interface that makes OSM's rich, tag-based geographic data more\naccessible through intuitive scene descriptions. SPOT interprets user inputs as\nstructured representations of geospatial object configurations using fine-tuned\nLarge Language Models (LLMs), with results being displayed in an interactive\nmap interface. While more general geospatial search tasks are conceivable, SPOT\nis specifically designed for use in investigative journalism, addressing\nreal-world challenges such as hallucinations in model output, inconsistencies\nin OSM tagging, and the noisy nature of user input. It combines a novel\nsynthetic data pipeline with a semantic bundling system to enable robust,\naccurate query generation. To our knowledge, SPOT is the first system to\nachieve reliable natural language access to OSM data at this level of accuracy.\nBy lowering the technical barrier to geolocation verification, SPOT contributes\na practical tool to the broader efforts to support fact-checking and combat\ndisinformation.\n","authors":["Lynn Khellaf","Ipek Baris Schlicht","Tilman Mirass","Julia Bayer","Tilman Wagner","Ruben Bouwmeester"],"pdf_url":"https://arxiv.org/pdf/2506.13188v1.pdf","comment":"Accepted to ACL 2025"},{"id":"http://arxiv.org/abs/2506.13187v1","updated":"2025-06-16T07:55:14Z","published":"2025-06-16T07:55:14Z","title":"Dynamic Context-oriented Decomposition for Task-aware Low-rank\n  Adaptation with Less Forgetting and Faster Convergence","summary":"  Conventional low-rank adaptation methods build adapters without considering\ndata context, leading to sub-optimal fine-tuning performance and severe\nforgetting of inherent world knowledge. In this paper, we propose\ncontext-oriented decomposition adaptation (CorDA), a novel method that\ninitializes adapters in a task-aware manner. Concretely, we develop\ncontext-oriented singular value decomposition, where we collect covariance\nmatrices of input activations for each linear layer using sampled data from the\ntarget task, and apply SVD to the product of weight matrix and its\ncorresponding covariance matrix. By doing so, the task-specific capability is\ncompacted into the principal components. Thanks to the task awareness, our\nmethod enables two optional adaptation modes, knowledge-preserved mode (KPM)\nand instruction-previewed mode (IPM), providing flexibility to choose between\nfreezing the principal components to preserve their associated knowledge or\nadapting them to better learn a new task. We further develop CorDA++ by\nderiving a metric that reflects the compactness of task-specific principal\ncomponents, and then introducing dynamic covariance selection and dynamic rank\nallocation strategies based on the same metric. The two strategies provide each\nlayer with the most representative covariance matrix and a proper rank\nallocation. Experimental results show that CorDA++ outperforms CorDA by a\nsignificant margin. CorDA++ in KPM not only achieves better fine-tuning\nperformance than LoRA, but also mitigates the forgetting of pre-trained\nknowledge in both large language models and vision language models. For IPM,\nour method exhibits faster convergence, \\emph{e.g.,} 4.5x speedup over QLoRA,\nand improves adaptation performance in various scenarios, outperforming strong\nbaseline methods. Our method has been integrated into the PEFT library\ndeveloped by Hugging Face.\n","authors":["Yibo Yang","Sihao Liu","Chuan Rao","Bang An","Tiancheng Shen","Philip H. S. Torr","Ming-Hsuan Yang","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2506.13187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13181v1","updated":"2025-06-16T07:48:01Z","published":"2025-06-16T07:48:01Z","title":"Align-then-Unlearn: Embedding Alignment for LLM Unlearning","summary":"  As large language models (LLMs) are trained on massive datasets, they have\nraised significant privacy and ethical concerns due to their potential to\ninadvertently retain sensitive information. Unlearning seeks to selectively\nremove specific data from trained models, such as personal information or\ncopyrighted content. Current approaches targeting specific output sequences at\nthe token level often fail to achieve complete forgetting and remain\nsusceptible to prompt rephrasing. We propose Align-then-Unlearn, a novel\nframework that performs unlearning in the semantic embedding space rather than\ndirectly on output tokens. Align-then-Unlearn first augments the LLM with an\nembedding prediction module trained to anticipate future context\nrepresentations. Unlearning is then achieved by fine-tuning the model to\nminimize the similarity between these predicted embeddings and a target\nembedding that represents the concept to be removed. Initial results show that\nAlign-then-Unlearn effectively removes targeted knowledge with minimal\ndegradation in overall model utility. These findings suggest that\nembedding-based unlearning offers a promising and robust approach to removing\nconceptual knowledge. Our code is available at\nhttps://github.com/ExplainableML/align-then-unlearn.\n","authors":["Philipp Spohn","Leander Girrbach","Jessica Bader","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2506.13181v1.pdf","comment":"Accepted at ICML 2025 Workshop on Machine Unlearning for Generative\n  AI"},{"id":"http://arxiv.org/abs/2408.06778v4","updated":"2025-06-16T07:47:43Z","published":"2024-08-13T10:04:29Z","title":"Fast-and-Frugal Text-Graph Transformers are Effective Link Predictors","summary":"  We propose Fast-and-Frugal Text-Graph (FnF-TG) Transformers, a\nTransformer-based framework that unifies textual and structural information for\ninductive link prediction in text-attributed knowledge graphs. We demonstrate\nthat, by effectively encoding ego-graphs (1-hop neighbourhoods), we can reduce\nthe reliance on resource-intensive textual encoders. This makes the model both\nfast at training and inference time, as well as frugal in terms of cost. We\nperform a comprehensive evaluation on three popular datasets and show that\nFnF-TG can achieve superior performance compared to previous state-of-the-art\nmethods. We also extend inductive learning to a fully inductive setting, where\nrelations don't rely on transductive (fixed) representations, as in previous\nwork, but are a function of their textual description. Additionally, we\nintroduce new variants of existing datasets, specifically designed to test the\nperformance of models on unseen relations at inference time, thus offering a\nnew test-bench for fully inductive link prediction.\n","authors":["Andrei C. Coman","Christos Theodoropoulos","Marie-Francine Moens","James Henderson"],"pdf_url":"https://arxiv.org/pdf/2408.06778v4.pdf","comment":"Accepted to ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2506.13180v1","updated":"2025-06-16T07:47:34Z","published":"2025-06-16T07:47:34Z","title":"Dynamic Acoustic Model Architecture Optimization in Training for ASR","summary":"  Architecture design is inherently complex. Existing approaches rely on either\nhandcrafted rules, which demand extensive empirical expertise, or automated\nmethods like neural architecture search, which are computationally intensive.\nIn this paper, we introduce DMAO, an architecture optimization framework that\nemploys a grow-and-drop strategy to automatically reallocate parameters during\ntraining. This reallocation shifts resources from less-utilized areas to those\nparts of the model where they are most beneficial. Notably, DMAO only\nintroduces negligible training overhead at a given model complexity. We\nevaluate DMAO through experiments with CTC on LibriSpeech, TED-LIUM-v2 and\nSwitchboard datasets. The results show that, using the same amount of training\nresources, our proposed DMAO consistently improves WER by up to 6% relatively\nacross various architectures, model sizes, and datasets. Furthermore, we\nanalyze the pattern of parameter redistribution and uncover insightful\nfindings.\n","authors":["Jingjing Xu","Zijian Yang","Albert Zeyer","Eugen Beck","Ralf Schlueter","Hermann Ney"],"pdf_url":"https://arxiv.org/pdf/2506.13180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13178v1","updated":"2025-06-16T07:43:18Z","published":"2025-06-16T07:43:18Z","title":"Enhancing Large Language Models with Reliable Knowledge Graphs","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities in\ntext generation and understanding, yet their reliance on implicit, unstructured\nknowledge often leads to factual inaccuracies and limited interpretability.\nKnowledge Graphs (KGs), with their structured, relational representations,\noffer a promising solution to ground LLMs in verified knowledge. However, their\npotential remains constrained by inherent noise, incompleteness, and the\ncomplexity of integrating their rigid structure with the flexible reasoning of\nLLMs. This thesis presents a systematic framework to address these limitations,\nadvancing the reliability of KGs and their synergistic integration with LLMs\nthrough five interconnected contributions. This thesis addresses these\nchallenges through a cohesive framework that enhances LLMs by refining and\nleveraging reliable KGs. First, we introduce contrastive error detection, a\nstructure-based method to identify incorrect facts in KGs. This approach is\nextended by an attribute-aware framework that unifies structural and semantic\nsignals for error correction. Next, we propose an inductive completion model\nthat further refines KGs by completing the missing relationships in evolving\nKGs. Building on these refined KGs, KnowGPT integrates structured graph\nreasoning into LLMs through dynamic prompting, improving factual grounding.\nThese contributions form a systematic pipeline (from error detection to LLM\nintegration), demonstrating that reliable KGs significantly enhance the\nrobustness, interpretability, and adaptability of LLMs.\n","authors":["Qinggang Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.13178v1.pdf","comment":"Thesis"},{"id":"http://arxiv.org/abs/2506.09657v2","updated":"2025-06-16T07:42:18Z","published":"2025-06-11T12:26:08Z","title":"Team Anotheroption at SemEval-2025 Task 8: Bridging the Gap Between\n  Open-Source and Proprietary LLMs in Table QA","summary":"  This paper presents a system developed for SemEval 2025 Task 8: Question\nAnswering (QA) over tabular data. Our approach integrates several key\ncomponents: text-to-SQL and text-to-code generation modules, a self-correction\nmechanism, and a retrieval-augmented generation (RAG). Additionally, it\nincludes an end-to-end (E2E) module, all orchestrated by a large language model\n(LLM). Through ablation studies, we analyzed the effects of different parts of\nour pipeline and identified the challenges that are still present in this\nfield. During the evaluation phase of the competition, our solution achieved an\naccuracy of 80%, resulting in a top-13 ranking among the 38 participating\nteams. Our pipeline demonstrates a significant improvement in accuracy for\nopen-source models and achieves a performance comparable to proprietary LLMs in\nQA tasks over tables. The code is available at GitHub repository.\n","authors":["Nikolas Evkarpidi","Elena Tutubalina"],"pdf_url":"https://arxiv.org/pdf/2506.09657v2.pdf","comment":"Accepted for publication at the 19th International Workshop on\n  Semantic Evaluation (SemEval-2025), to be held in conjunction with ACL 2025.\n  15 pages, 5 figures; full paper title was added"},{"id":"http://arxiv.org/abs/2506.01413v3","updated":"2025-06-16T07:40:34Z","published":"2025-06-02T08:11:44Z","title":"Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models","summary":"  Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nwill be available later (under review).\n  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction\nfollowing, complex instructions\n","authors":["Yulei Qin","Gang Li","Zongyi Li","Zihan Xu","Yuchen Shi","Zhekai Lin","Xiao Cui","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2506.01413v3.pdf","comment":"13 pages of main body, 3 tables, 5 figures, 45 pages of appendix"},{"id":"http://arxiv.org/abs/2506.13177v1","updated":"2025-06-16T07:38:04Z","published":"2025-06-16T07:38:04Z","title":"Development of the user-friendly decision aid Rule-based Evaluation and\n  Support Tool (REST) for optimizing the resources of an information extraction\n  task","summary":"  Rules could be an information extraction (IE) default option, compared to ML\nand LLMs in terms of sustainability, transferability, interpretability, and\ndevelopment burden. We suggest a sustainable and combined use of rules and ML\nas an IE method. Our approach starts with an exhaustive expert manual\nhighlighting in a single working session of a representative subset of the data\ncorpus. We developed and validated the feasibility and the performance metrics\nof the REST decision tool to help the annotator choose between rules as a by\ndefault option and ML for each entity of an IE task. REST makes the annotator\nvisualize the characteristics of each entity formalization in the free texts\nand the expected rule development feasibility and IE performance metrics. ML is\nconsidered as a backup IE option and manual annotation for training is\ntherefore minimized. The external validity of REST on a 12-entity use case\nshowed good reproducibility.\n","authors":["Guillaume Bazin","Xavier Tannier","Fanny Adda","Ariel Cohen","Akram Redjdal","Emmanuelle Kempf"],"pdf_url":"https://arxiv.org/pdf/2506.13177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11991v2","updated":"2025-06-16T07:35:52Z","published":"2025-06-13T17:47:43Z","title":"VGR: Visual Grounded Reasoning","summary":"  In the field of multimodal chain-of-thought (CoT) reasoning, existing\napproaches predominantly rely on reasoning on pure language space, which\ninherently suffers from language bias and is largely confined to math or\nscience domains. This narrow focus limits their ability to handle complex\nvisual reasoning tasks that demand comprehensive understanding of image\ndetails. To address these limitations, this paper introduces VGR, a novel\nreasoning multimodal large language model (MLLM) with enhanced fine-grained\nvisual perception capabilities. Unlike traditional MLLMs that answer the\nquestion or reasoning solely on the language space, our VGR first detects\nrelevant regions that may help to solve problems, and then provides precise\nanswers based on replayed image regions. To achieve this, we conduct a\nlarge-scale SFT dataset called VGR -SFT that contains reasoning data with mixed\nvision grounding and language deduction. The inference pipeline of VGR allows\nthe model to choose bounding boxes for visual reference and a replay stage is\nintroduced to integrates the corresponding regions into the reasoning process,\nenhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline\nshow that VGR achieves superior performance on multi-modal benchmarks requiring\ncomprehensive image detail understanding. Compared to the baseline, VGR uses\nonly 30\\% of the image token count while delivering scores of +4.1 on MMStar,\n+7.1 on AI2D, and a +12.9 improvement on ChartQA.\n","authors":["Jiacong Wang","Zijian Kang","Haochen Wang","Haiyong Jiang","Jiawen Li","Bohong Wu","Ya Wang","Jiao Ran","Xiao Liang","Chao Feng","Jun Xiao"],"pdf_url":"https://arxiv.org/pdf/2506.11991v2.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.15266v2","updated":"2025-06-16T07:35:23Z","published":"2025-02-21T07:48:54Z","title":"A Training-free LLM-based Approach to General Chinese Character Error\n  Correction","summary":"  Chinese spelling correction (CSC) is a crucial task that aims to correct\ncharacter errors in Chinese text. While conventional CSC focuses on character\nsubstitution errors caused by mistyping, two other common types of character\nerrors, missing and redundant characters, have received less attention. These\nerrors are often excluded from CSC datasets during the annotation process or\nignored during evaluation, even when they have been annotated. This issue\nlimits the practicality of the CSC task. To address this issue, we introduce\nthe task of General Chinese Character Error Correction (C2EC), which focuses on\nall three types of character errors. We construct a high-quality C2EC benchmark\nby combining and manually verifying data from CCTC and Lemon datasets. We\nextend the training-free prompt-free CSC method to C2EC by using Levenshtein\ndistance for handling length changes and leveraging an additional prompt-based\nlarge language model (LLM) to improve performance. Experiments show that our\nmethod enables a 14B-parameter LLM to be on par with models nearly 50 times\nlarger on both conventional CSC and C2EC tasks, without any fine-tuning.\n","authors":["Houquan Zhou","Bo Zhang","Zhenghua Li","Ming Yan","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.15266v2.pdf","comment":"Accepted at Main Conference of ACL 2025, 26 pages, 12 figures"},{"id":"http://arxiv.org/abs/2506.13172v1","updated":"2025-06-16T07:34:31Z","published":"2025-06-16T07:34:31Z","title":"Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging\n  Unsubstantiated Claims and Ambiguous Pronouns","summary":"  We present and evaluate a suite of proof-of-concept (PoC), structured\nworkflow prompts designed to elicit human-like hierarchical reasoning while\nguiding Large Language Models (LLMs) in high-level semantic and linguistic\nanalysis of scholarly manuscripts. The prompts target two non-trivial\nanalytical tasks: identifying unsubstantiated claims in summaries\n(informational integrity) and flagging ambiguous pronoun references (linguistic\nclarity). We conducted a systematic, multi-run evaluation on two frontier\nmodels (Gemini Pro 2.5 Pro and ChatGPT Plus o3) under varied context\nconditions. Our results for the informational integrity task reveal a\nsignificant divergence in model performance: while both models successfully\nidentified an unsubstantiated head of a noun phrase (95% success), ChatGPT\nconsistently failed (0% success) to identify an unsubstantiated adjectival\nmodifier that Gemini correctly flagged (95% success), raising a question\nregarding potential influence of the target's syntactic role. For the\nlinguistic analysis task, both models performed well (80-90% success) with full\nmanuscript context. In a summary-only setting, however, ChatGPT achieved a\nperfect (100%) success rate, while Gemini's performance was substantially\ndegraded. Our findings suggest that structured prompting is a viable\nmethodology for complex textual analysis but show that prompt performance may\nbe highly dependent on the interplay between the model, task type, and context,\nhighlighting the need for rigorous, model-specific testing.\n","authors":["Evgeny Markhasin"],"pdf_url":"https://arxiv.org/pdf/2506.13172v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2506.13148v1","updated":"2025-06-16T07:00:48Z","published":"2025-06-16T07:00:48Z","title":"Adapting LLMs for Minimal-edit Grammatical Error Correction","summary":"  Decoder-only large language models have shown superior performance in the\nfluency-edit English Grammatical Error Correction, but their adaptation for\nminimal-edit English GEC is still underexplored. To improve their effectiveness\nin the minimal-edit approach, we explore the error rate adaptation topic and\npropose a novel training schedule method. Our experiments set a new\nstate-of-the-art result for a single-model system on the BEA-test set. We also\ndetokenize the most common English GEC datasets to match the natural way of\nwriting text. During the process, we find that there are errors in them. Our\nexperiments analyze whether training on detokenized datasets impacts the\nresults and measure the impact of the usage of the datasets with corrected\nerroneous examples. To facilitate reproducibility, we have released the source\ncode used to train our models.\n","authors":["Ryszard Staruch","Filip Graliński","Daniel Dzienisiewicz"],"pdf_url":"https://arxiv.org/pdf/2506.13148v1.pdf","comment":"Accepted at BEA-2025"},{"id":"http://arxiv.org/abs/2506.13143v1","updated":"2025-06-16T06:56:21Z","published":"2025-06-16T06:56:21Z","title":"CMU's IWSLT 2025 Simultaneous Speech Translation System","summary":"  This paper presents CMU's submission to the IWSLT 2025 Simultaneous Speech\nTranslation (SST) task for translating unsegmented English speech into Chinese\nand German text in a streaming manner. Our end-to-end speech-to-text system\nintegrates a chunkwise causal Wav2Vec 2.0 speech encoder, an adapter, and the\nQwen2.5-7B-Instruct as the decoder. We use a two-stage simultaneous training\nprocedure on robust speech segments curated from LibriSpeech, CommonVoice, and\nVoxPopuli datasets, utilizing standard cross-entropy loss. Our model supports\nadjustable latency through a configurable latency multiplier. Experimental\nresults demonstrate that our system achieves 44.3 BLEU for English-to-Chinese\nand 25.1 BLEU for English-to-German translations on the ACL60/60 development\nset, with computation-aware latencies of 2.7 seconds and 2.3 seconds, and\ntheoretical latencies of 2.2 and 1.7 seconds, respectively.\n","authors":["Siqi Ouyang","Xi Xu","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2506.13143v1.pdf","comment":"IWSLT 2025 System Description"},{"id":"http://arxiv.org/abs/2502.05234v2","updated":"2025-06-16T06:53:48Z","published":"2025-02-07T19:35:25Z","title":"Optimizing Temperature for Language Models with Multi-Sample Inference","summary":"  Multi-sample aggregation strategies, such as majority voting and best-of-N\nsampling, are widely used in contemporary large language models (LLMs) to\nenhance predictive accuracy across various tasks. A key challenge in this\nprocess is temperature selection, which significantly impacts model\nperformance. Existing approaches either rely on a fixed default temperature or\nrequire labeled validation data for tuning, which are often scarce and\ndifficult to obtain. This paper addresses the challenge of automatically\nidentifying the (near)-optimal temperature for different LLMs using\nmulti-sample aggregation strategies, without relying on task-specific\nvalidation data. We provide a comprehensive analysis of temperature's role in\nperformance optimization, considering variations in model architectures,\ndatasets, task types, model sizes, and predictive accuracy. Furthermore, we\npropose a novel entropy-based metric for automated temperature optimization,\nwhich consistently outperforms fixed-temperature baselines. Additionally, we\nincorporate a stochastic process model to enhance interpretability, offering\ndeeper insights into the relationship between temperature and model\nperformance.\n","authors":["Weihua Du","Yiming Yang","Sean Welleck"],"pdf_url":"https://arxiv.org/pdf/2502.05234v2.pdf","comment":"ICML2025, 21 pages. Code available at\n  https://github.com/StigLidu/TURN"},{"id":"http://arxiv.org/abs/2503.02969v2","updated":"2025-06-16T06:38:23Z","published":"2025-03-04T19:51:29Z","title":"InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model","summary":"  Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code and demo at\nhttps://github.com/LeiLiLab/InfiniSST\n","authors":["Siqi Ouyang","Xi Xu","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2503.02969v2.pdf","comment":"ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2506.13130v1","updated":"2025-06-16T06:27:59Z","published":"2025-06-16T06:27:59Z","title":"ZINA: Multimodal Fine-grained Hallucination Detection and Editing","summary":"  Multimodal Large Language Models (MLLMs) often generate hallucinations, where\nthe output deviates from the visual content. Given that these hallucinations\ncan take diverse forms, detecting hallucinations at a fine-grained level is\nessential for comprehensive evaluation and analysis. To this end, we propose a\nnovel task of multimodal fine-grained hallucination detection and editing for\nMLLMs. Moreover, we propose ZINA, a novel method that identifies hallucinated\nspans at a fine-grained level, classifies their error types into six\ncategories, and suggests appropriate refinements. To train and evaluate models\nfor this task, we constructed VisionHall, a dataset comprising 6.9k outputs\nfrom twelve MLLMs manually annotated by 211 annotators, and 20k synthetic\nsamples generated using a graph-based method that captures dependencies among\nerror types. We demonstrated that ZINA outperformed existing methods, including\nGPT-4o and LLama-3.2, in both detection and editing tasks.\n","authors":["Yuiga Wada","Kazuki Matsuda","Komei Sugiura","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2506.13130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17657v3","updated":"2025-06-16T06:25:31Z","published":"2024-10-23T08:19:18Z","title":"ReflecTool: Towards Reflection-Aware Tool-Augmented Clinical Agents","summary":"  Large Language Models (LLMs) have shown promising potential in the medical\ndomain, assisting with tasks like clinical note generation and patient\ncommunication. However, current LLMs are limited to text-based communication,\nhindering their ability to interact with diverse forms of information in\nclinical environments. Despite clinical agents succeeding in diverse signal\ninteraction, they are oriented to a single clinical scenario and hence fail for\nbroader applications. To evaluate clinical agents holistically, we propose\nClinicalAgent Bench~(CAB), a comprehensive medical agent benchmark consisting\nof 18 tasks across five key realistic clinical dimensions. Building on this, we\nintroduce ReflecTool, a novel framework that excels at utilizing\ndomain-specific tools within two stages. The first optimization stage\nprogressively enlarges a long-term memory by saving successful solving\nprocesses and tool-wise experience of agents in a tiny pre-defined training\nset. In the following inference stage, ReflecTool can search for supportive\nsuccessful demonstrations from already built long-term memory to guide the tool\nselection strategy, and a verifier improves the tool usage according to the\ntool-wise experience with two verification methods--iterative refinement and\ncandidate selection. Extensive experiments on ClinicalAgent Benchmark\ndemonstrate that ReflecTool surpasses the pure LLMs with more than 10 points\nand the well-established agent-based methods with 3 points, highlighting its\nadaptability and effectiveness in solving complex clinical tasks.\n","authors":["Yusheng Liao","Shuyang Jiang","Yanfeng Wang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2410.17657v3.pdf","comment":"ACL 2025 Main Paper"},{"id":"http://arxiv.org/abs/2506.09983v2","updated":"2025-06-16T06:09:38Z","published":"2025-06-11T17:56:10Z","title":"Step-by-step Instructions and a Simple Tabular Output Format Improve the\n  Dependency Parsing Accuracy of LLMs","summary":"  Recent advances in large language models (LLMs) have enabled impressive\nperformance in various tasks. However, standard prompting often struggles to\nproduce structurally valid and accurate outputs, especially in dependency\nparsing. We propose a novel step-by-step instruction strategy, where universal\npart-of-speech tagging precedes the prediction of syntactic heads and\ndependency labels, and a simplified CoNLL-U like output format, our method\nachieves state-of-the-art accuracy on Universal Dependencies datasets across 17\nlanguages without hallucination or contamination. We further show that\nmultilingual fine-tuning simultaneously improves cross-language generalization\nperformance. Our results highlight the effectiveness of explicit reasoning\nsteps in LLM-based parsing and offer a scalable, format-consistent alternative\nto bracket-based approaches.\n","authors":["Hiroshi Matsuda","Chunpeng Ma","Masayuki Asahara"],"pdf_url":"https://arxiv.org/pdf/2506.09983v2.pdf","comment":"9 pages, 2 figures, accepted to SyntaxFest 2025"},{"id":"http://arxiv.org/abs/2503.16212v2","updated":"2025-06-16T05:58:00Z","published":"2025-03-20T15:00:41Z","title":"MathFusion: Enhancing Mathematical Problem-solving of LLM through\n  Instruction Fusion","summary":"  Large Language Models (LLMs) have shown impressive progress in mathematical\nreasoning. While data augmentation is promising to enhance mathematical\nproblem-solving ability, current approaches are predominantly limited to\ninstance-level modifications-such as rephrasing or generating syntactic\nvariations-which fail to capture and leverage the intrinsic relational\nstructures inherent in mathematical knowledge. Inspired by human learning\nprocesses, where mathematical proficiency develops through systematic exposure\nto interconnected concepts, we introduce MathFusion, a novel framework that\nenhances mathematical reasoning through cross-problem instruction synthesis.\nMathFusion implements this through three fusion strategies: (1) sequential\nfusion, which chains related problems to model solution dependencies; (2)\nparallel fusion, which combines analogous problems to reinforce conceptual\nunderstanding; and (3) conditional fusion, which creates context-aware\nselective problems to enhance reasoning flexibility. By applying these\nstrategies, we generate a new dataset, \\textbf{MathFusionQA}, followed by\nfine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental\nresults demonstrate that MathFusion achieves substantial improvements in\nmathematical reasoning while maintaining high data efficiency, boosting\nperformance by 18.0 points in accuracy across diverse benchmarks while\nrequiring only 45K additional synthetic instructions, representing a\nsubstantial improvement over traditional single-instruction approaches. Our\ndatasets, models, and code are publicly available at\nhttps://github.com/QizhiPei/mathfusion.\n","authors":["Qizhi Pei","Lijun Wu","Zhuoshi Pan","Yu Li","Honglin Lin","Chenlin Ming","Xin Gao","Conghui He","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2503.16212v2.pdf","comment":"Accepted by ACL 2025 (main)"},{"id":"http://arxiv.org/abs/2506.07483v2","updated":"2025-06-16T05:48:44Z","published":"2025-06-09T07:00:04Z","title":"A Hybrid GA LLM Framework for Structured Task Optimization","summary":"  GA LLM is a hybrid framework that combines Genetic Algorithms with Large\nLanguage Models to handle structured generation tasks under strict constraints.\nEach output, such as a plan or report, is treated as a gene, and evolutionary\noperations like selection, crossover, and mutation are guided by the language\nmodel to iteratively improve solutions. The language model provides domain\nknowledge and creative variation, while the genetic algorithm ensures\nstructural integrity and global optimization. GA LLM has proven effective in\ntasks such as itinerary planning, academic outlining, and business reporting,\nconsistently producing well structured and requirement satisfying results. Its\nmodular design also makes it easy to adapt to new tasks. Compared to using a\nlanguage model alone, GA LLM achieves better constraint satisfaction and higher\nquality solutions by combining the strengths of both components.\n","authors":["William Shum","Rachel Chan","Jonas Lin","Benny Feng","Patrick Lau"],"pdf_url":"https://arxiv.org/pdf/2506.07483v2.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2410.12999v2","updated":"2025-06-16T05:47:36Z","published":"2024-10-16T19:56:22Z","title":"POROver: Improving Safety and Reducing Overrefusal in Large Language\n  Models with Overgeneration and Preference Optimization","summary":"  Achieving both high safety and high usefulness simultaneously in large\nlanguage models has become a critical challenge in recent years.Models often\nexhibit unsafe behavior or adopt an overly cautious approach leading to\nfrequent overrefusal of benign prompts, which reduces their usefulness. A major\nfactor underlying these behaviors is how the models are finetuned and aligned,\nparticularly the nature and extent of the data used.In this work, we examine\nhow overgenerating finetuning data with advanced teacher models (e.g.,\nGPT-4o)-covering both general-purpose and toxic prompts-affects safety and\nusefulness in instruction-following language models.Additionally, we present\nPOROver, an alignment strategy designed for models that are highly safe but\nprone to overrefusal. POROver employs preference optimization algorithms and\nleverages completions from an advanced teacher model to reduce overrefusals\nwhile maintaining safety.Our results show that overgenerating completions for\ngeneral-purpose prompts significantly boosts safety with only a minimal impact\non usefulness. Specifically, the F1 score calculated between safety and\nusefulness increases from 74.4% to 91.8% because of a substantial rise in\nsafety. Moreover, overgeneration for toxic prompts raises usefulness from 11.1%\nto 57.6% while preserving safety. Finally, applying POROVer increases\nusefulness further-from 57.6% to 82.1%-while keeping safety at comparable\nlevels. Our data and code are available at\nhttps://github.com/batuhankmkaraman/POROver.\n","authors":["Batuhan K. Karaman","Ishmam Zabir","Alon Benhaim","Vishrav Chaudhary","Mert R. Sabuncu","Xia Song"],"pdf_url":"https://arxiv.org/pdf/2410.12999v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13116v1","updated":"2025-06-16T05:47:29Z","published":"2025-06-16T05:47:29Z","title":"Crime Hotspot Prediction Using Deep Graph Convolutional Networks","summary":"  Crime hotspot prediction is critical for ensuring urban safety and effective\nlaw enforcement, yet it remains challenging due to the complex spatial\ndependencies inherent in criminal activity. The previous approaches tended to\nuse classical algorithms such as the KDE and SVM to model data distributions\nand decision boundaries. The methods often fail to capture these spatial\nrelationships, treating crime events as independent and ignoring geographical\ninteractions. To address this, we propose a novel framework based on Graph\nConvolutional Networks (GCNs), which explicitly model spatial dependencies by\nrepresenting crime data as a graph. In this graph, nodes represent discrete\ngeographic grid cells and edges capture proximity relationships. Using the\nChicago Crime Dataset, we engineer spatial features and train a multi-layer GCN\nmodel to classify crime types and predict high-risk zones. Our approach\nachieves 88% classification accuracy, significantly outperforming traditional\nmethods. Additionally, the model generates interpretable heat maps of crime\nhotspots, demonstrating the practical utility of graph-based learning for\npredictive policing and spatial criminology.\n","authors":["Tehreem Zubair","Syeda Kisaa Fatima","Noman Ahmed","Asifullah Khan"],"pdf_url":"https://arxiv.org/pdf/2506.13116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13109v1","updated":"2025-06-16T05:37:49Z","published":"2025-06-16T05:37:49Z","title":"Leveraging In-Context Learning for Language Model Agents","summary":"  In-context learning (ICL) with dynamically selected demonstrations combines\nthe flexibility of prompting large language models (LLMs) with the ability to\nleverage training data to improve performance. While ICL has been highly\nsuccessful for prediction and generation tasks, leveraging it for agentic tasks\nthat require sequential decision making is challenging -- one must think not\nonly about how to annotate long trajectories at scale and how to select\ndemonstrations, but also what constitutes demonstrations, and when and where to\nshow them. To address this, we first propose an algorithm that leverages an LLM\nwith retries along with demonstrations to automatically and efficiently\nannotate agentic tasks with solution trajectories. We then show that\nset-selection of trajectories of similar tasks as demonstrations significantly\nimproves performance, reliability, robustness, and efficiency of LLM agents.\nHowever, trajectory demonstrations have a large inference cost overhead. We\nshow that this can be mitigated by using small trajectory snippets at every\nstep instead of an additional trajectory. We find that demonstrations obtained\nfrom larger models (in the annotation phase) also improve smaller models, and\nthat ICL agents can even rival costlier trained agents. Thus, our results\nreveal that ICL, with careful use, can be very powerful for agentic tasks as\nwell.\n","authors":["Shivanshu Gupta","Sameer Singh","Ashish Sabharwal","Tushar Khot","Ben Bogin"],"pdf_url":"https://arxiv.org/pdf/2506.13109v1.pdf","comment":"16 pages, 12 figures"},{"id":"http://arxiv.org/abs/2502.03009v2","updated":"2025-06-16T05:27:08Z","published":"2025-02-05T09:11:13Z","title":"Scaling Laws for Upcycling Mixture-of-Experts Language Models","summary":"  Pretraining large language models (LLMs) is resource-intensive, often\nrequiring months of training time even with high-end GPU clusters. There are\ntwo approaches of mitigating such computational demands: reusing smaller models\nto train larger ones (upcycling), and training computationally efficient models\nlike mixture-of-experts (MoE). In this paper, we study the upcycling of LLMs to\nMoE models, of which the scaling behavior remains underexplored. Through\nextensive experiments, we identify empirical scaling laws that describe how\nperformance depends on dataset size and model configuration. Particularly, we\nshow that, while scaling these factors improves performance, there is a novel\ninteraction term between the dense and upcycled training dataset that limits\nthe efficiency of upcycling at large computational budgets. Based on these\nfindings, we provide guidance to scale upcycling, and establish conditions\nunder which upcycling outperforms from-scratch trainings within budget\nconstraints.\n","authors":["Seng Pei Liew","Takuya Kato","Sho Takase"],"pdf_url":"https://arxiv.org/pdf/2502.03009v2.pdf","comment":"ICML 2025. 16 figures, 8 tables. Code available at\n  https://github.com/sbintuitions/sparse-upcycling-scaling-laws"},{"id":"http://arxiv.org/abs/2506.13104v1","updated":"2025-06-16T05:23:42Z","published":"2025-06-16T05:23:42Z","title":"Equitable Electronic Health Record Prediction with FAME: Fairness-Aware\n  Multimodal Embedding","summary":"  Electronic Health Record (EHR) data encompass diverse modalities -- text,\nimages, and medical codes -- that are vital for clinical decision-making. To\nprocess these complex data, multimodal AI (MAI) has emerged as a powerful\napproach for fusing such information. However, most existing MAI models\noptimize for better prediction performance, potentially reinforcing biases\nacross patient subgroups. Although bias-reduction techniques for multimodal\nmodels have been proposed, the individual strengths of each modality and their\ninterplay in both reducing bias and optimizing performance remain\nunderexplored. In this work, we introduce FAME (Fairness-Aware Multimodal\nEmbeddings), a framework that explicitly weights each modality according to its\nfairness contribution. FAME optimizes both performance and fairness by\nincorporating a combined loss function. We leverage the Error Distribution\nDisparity Index (EDDI) to measure fairness across subgroups and propose a\nsign-agnostic aggregation method to balance fairness across subgroups, ensuring\nequitable model outcomes. We evaluate FAME with BEHRT and BioClinicalBERT,\ncombining structured and unstructured EHR data, and demonstrate its\neffectiveness in terms of performance and fairness compared with other\nbaselines across multiple EHR prediction tasks.\n","authors":["Nikkie Hooman","Zhongjie Wu","Eric C. Larson","Mehak Gupta"],"pdf_url":"https://arxiv.org/pdf/2506.13104v1.pdf","comment":"21 pages, 3 figures"},{"id":"http://arxiv.org/abs/2506.13102v1","updated":"2025-06-16T05:15:53Z","published":"2025-06-16T05:15:53Z","title":"Rethinking Test-Time Scaling for Medical AI: Model and Task-Aware\n  Strategies for LLMs and VLMs","summary":"  Test-time scaling has recently emerged as a promising approach for enhancing\nthe reasoning capabilities of large language models or vision-language models\nduring inference. Although a variety of test-time scaling strategies have been\nproposed, and interest in their application to the medical domain is growing,\nmany critical aspects remain underexplored, including their effectiveness for\nvision-language models and the identification of optimal strategies for\ndifferent settings. In this paper, we conduct a comprehensive investigation of\ntest-time scaling in the medical domain. We evaluate its impact on both large\nlanguage models and vision-language models, considering factors such as model\nsize, inherent model characteristics, and task complexity. Finally, we assess\nthe robustness of these strategies under user-driven factors, such as\nmisleading information embedded in prompts. Our findings offer practical\nguidelines for the effective use of test-time scaling in medical applications\nand provide insights into how these strategies can be further refined to meet\nthe reliability and interpretability demands of the medical domain.\n","authors":["Gyutaek Oh","Seoyeon Kim","Sangjoon Park","Byung-Hoon Kim"],"pdf_url":"https://arxiv.org/pdf/2506.13102v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2504.06560v3","updated":"2025-06-16T04:19:54Z","published":"2025-04-09T03:46:56Z","title":"NeedleInATable: Exploring Long-Context Capability of Large Language\n  Models towards Long-Structured Tables","summary":"  Processing structured tabular data, particularly large and lengthy tables,\nconstitutes a fundamental yet challenging task for large language models\n(LLMs). However, existing long-context benchmarks like Needle-in-a-Haystack\nprimarily focus on unstructured text, neglecting the challenge of diverse\nstructured tables. Meanwhile, previous tabular benchmarks mainly consider\ndownstream tasks that require high-level reasoning abilities, and overlook\nmodels' underlying fine-grained perception of individual table cells, which is\ncrucial for practical and robust LLM-based table applications. To address this\ngap, we introduce \\textsc{NeedleInATable} (NIAT), a new long-context tabular\nbenchmark that treats each table cell as a ``needle'' and requires models to\nextract the target cell based on cell locations or lookup questions. Our\ncomprehensive evaluation of various LLMs and multimodal LLMs reveals a\nsubstantial performance gap between popular downstream tabular tasks and the\nsimpler NIAT task, suggesting that they may rely on dataset-specific\ncorrelations or shortcuts to obtain better benchmark results but lack truly\nrobust long-context understanding towards structured tables. Furthermore, we\ndemonstrate that using synthesized NIAT training data can effectively improve\nperformance on both NIAT task and downstream tabular tasks, which validates the\nimportance of NIAT capability for LLMs' genuine table understanding ability.\nOur data, code and models will be released to facilitate future research.\n","authors":["Lanrui Wang","Mingyu Zheng","Hongyin Tang","Zheng Lin","Yanan Cao","Jingang Wang","Xunliang Cai","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2504.06560v3.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2402.11827v2","updated":"2025-06-16T03:39:07Z","published":"2024-02-19T04:41:31Z","title":"Ask Optimal Questions: Aligning Large Language Models with Retriever's\n  Preference in Conversation","summary":"  Conversational search, unlike single-turn retrieval tasks, requires\nunderstanding the current question within a dialogue context. The common\napproach of rewrite-then-retrieve aims to decontextualize questions to be\nself-sufficient for off-the-shelf retrievers, but most existing methods produce\nsub-optimal query rewrites due to the limited ability to incorporate signals\nfrom the retrieval results. To overcome this limitation, we present a novel\nframework RetPO (Retriever's Preference Optimization), which is designed to\noptimize a language model (LM) for reformulating search queries in line with\nthe preferences of the target retrieval systems. The process begins by\nprompting a large LM to produce various potential rewrites and then collects\nretrieval performance for these rewrites as the retrievers' preferences.\nThrough the process, we construct a large-scale dataset called RF collection,\ncontaining Retrievers' Feedback on over 410K query rewrites across 12K\nconversations. Furthermore, we fine-tune a smaller LM on this dataset to align\nit with the retrievers' feedback. Our resulting model demonstrates superiority\non two benchmarks, surpassing the previous state-of-the-art performance of\nrewrite-then-retrieve approaches.\n","authors":["Chanwoong Yoon","Gangwoo Kim","Byeongguk Jeon","Sungdong Kim","Yohan Jo","Jaewoo Kang"],"pdf_url":"https://arxiv.org/pdf/2402.11827v2.pdf","comment":"NAACL 2025 (findings)"},{"id":"http://arxiv.org/abs/2502.02508v3","updated":"2025-06-16T03:29:47Z","published":"2025-02-04T17:26:58Z","title":"Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM\n  Reasoning via Autoregressive Search","summary":"  Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities across diverse domains. Recent studies have shown that increasing\ntest-time computation enhances LLMs' reasoning capabilities. This typically\ninvolves extensive sampling at inference time guided by an external LLM\nverifier, resulting in a two-player system. Despite external guidance, the\neffectiveness of this system demonstrates the potential of a single LLM to\ntackle complex tasks. Thus, we pose a new research problem: Can we internalize\nthe searching capabilities to fundamentally enhance the reasoning abilities of\na single LLM? This work explores an orthogonal direction focusing on\npost-training LLMs for autoregressive searching (i.e., an extended reasoning\nprocess with self-reflection and self-exploration of new strategies). To\nachieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a\ntwo-stage training paradigm: 1) a small-scale format tuning stage to\ninternalize the COAT reasoning format and 2) a large-scale self-improvement\nstage leveraging reinforcement learning. Our approach results in Satori, a 7B\nLLM trained on open-source models and data. Extensive empirical evaluations\ndemonstrate that Satori achieves state-of-the-art performance on mathematical\nreasoning benchmarks while exhibits strong generalization to out-of-domain\ntasks. Code, data, and models are fully open-sourced.\n","authors":["Maohao Shen","Guangtao Zeng","Zhenting Qi","Zhang-Wei Hong","Zhenfang Chen","Wei Lu","Gregory Wornell","Subhro Das","David Cox","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2502.02508v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13070v1","updated":"2025-06-16T03:26:10Z","published":"2025-06-16T03:26:10Z","title":"CHILL at SemEval-2025 Task 2: You Can't Just Throw Entities and Hope --\n  Make Your LLM to Get Them Right","summary":"  In this paper, we describe our approach for the SemEval 2025 Task 2 on\nEntity-Aware Machine Translation (EA-MT). Our system aims to improve the\naccuracy of translating named entities by combining two key approaches:\nRetrieval Augmented Generation (RAG) and iterative self-refinement techniques\nusing Large Language Models (LLMs). A distinctive feature of our system is its\nself-evaluation mechanism, where the LLM assesses its own translations based on\ntwo key criteria: the accuracy of entity translations and overall translation\nquality. We demonstrate how these methods work together and effectively improve\nentity handling while maintaining high-quality translations.\n","authors":["Jaebok Lee","Yonghyun Ryu","Seongmin Park","Yoonjung Choi"],"pdf_url":"https://arxiv.org/pdf/2506.13070v1.pdf","comment":"The 19th International Workshop on Semantic Evaluation"},{"id":"http://arxiv.org/abs/2506.13066v1","updated":"2025-06-16T03:19:31Z","published":"2025-06-16T03:19:31Z","title":"FinLMM-R1: Enhancing Financial Reasoning in LMM through Scalable Data\n  and Reward Design","summary":"  Large Multimodal Models (LMMs) demonstrate significant cross-modal reasoning\ncapabilities. However, financial applications face challenges due to the lack\nof high-quality multimodal reasoning datasets and the inefficiency of existing\ntraining paradigms for reasoning enhancement. To address these issues, we\npropose an integrated framework, FinLMM-R1, combining an automated and scalable\npipeline for data construction with enhanced training strategies to improve the\nmultimodal reasoning of LMM. The Automated and Scalable Pipeline (ASP) resolves\ntextual-visual misalignment in financial reports through a separate paradigm of\nquestion-answer generation and image-question alignment, ensuring data\nintegrity and extraction efficiency. Through ASP, we collect 89,378 aligned\nimage-question pairs from 23,397 financial reports, covering tasks such as\narithmetic reasoning, statistics reasoning, financial explanation, and\nfinancial knowledge. Moreover, we introduce the Thinking with Adversarial\nReward in LMM (TAR-LMM), extending the prior two-stage training framework [1]\nwith additional reward mechanisms. In the first stage, we focus on text-only\ntasks with format and accuracy rewards to guide the model in generating\nwell-structured thinking contents. In the second stage, we construct\nmulti-image contrastive samples with additional reward components including\nimage selection, thinking content length, and adversarial reward to jointly\noptimize the LMM across visual perception, reasoning efficiency, and logical\ncoherence. Extensive experiments on 7 benchmarks show ASP-derived dataset and\ntraining framework significantly improve answer accuracy and reasoning depth\nover existing reasoning LMMs in both general and financial multimodal contexts.\n","authors":["Kai Lan","Jiayong Zhu","Jiangtong Li","Dawei Cheng","Guang Chen","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2506.13066v1.pdf","comment":"26 pages, 16 figures"},{"id":"http://arxiv.org/abs/2408.08089v2","updated":"2025-06-16T03:19:20Z","published":"2024-08-15T11:33:20Z","title":"AgentCourt: Simulating Court with Adversarial Evolvable Lawyer Agents","summary":"  Current research in LLM-based simulation systems lacks comprehensive\nsolutions for modeling real-world court proceedings, while existing legal\nlanguage models struggle with dynamic courtroom interactions. We present\nAgentCourt, a comprehensive legal simulation framework that addresses these\nchallenges through adversarial evolution of LLM-based agents. Our AgentCourt\nintroduces a new adversarial evolutionary approach for agents called AdvEvol,\nwhich performs dynamic knowledge learning and evolution through structured\nadversarial interactions in a simulated courtroom program, breaking the\nlimitations of the traditional reliance on static knowledge bases or manual\nannotations. By simulating 1,000 civil cases, we construct an evolving\nknowledge base that enhances the agents' legal reasoning abilities. The evolved\nlawyer agents demonstrated outstanding performance on our newly introduced\nCourtBench benchmark, achieving a 12.1% improvement in performance compared to\nthe original lawyer agents. Evaluations by professional lawyers confirm the\neffectiveness of our approach across three critical dimensions: cognitive\nagility, professional knowledge, and logical rigor. Beyond outperforming\nspecialized legal models in interactive reasoning tasks, our findings emphasize\nthe importance of adversarial learning in legal AI and suggest promising\ndirections for extending simulation-based legal reasoning to broader judicial\nand regulatory contexts. The project's code is available at:\nhttps://github.com/relic-yuexi/AgentCourt\n","authors":["Guhong Chen","Liyang Fan","Zihan Gong","Nan Xie","Zixuan Li","Ziqiang Liu","Chengming Li","Qiang Qu","Hamid Alinejad-Rokny","Shiwen Ni","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2408.08089v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13065v1","updated":"2025-06-16T03:18:28Z","published":"2025-06-16T03:18:28Z","title":"MotiveBench: How Far Are We From Human-Like Motivational Reasoning in\n  Large Language Models?","summary":"  Large language models (LLMs) have been widely adopted as the core of agent\nframeworks in various scenarios, such as social simulations and AI companions.\nHowever, the extent to which they can replicate human-like motivations remains\nan underexplored question. Existing benchmarks are constrained by simplistic\nscenarios and the absence of character identities, resulting in an information\nasymmetry with real-world situations. To address this gap, we propose\nMotiveBench, which consists of 200 rich contextual scenarios and 600 reasoning\ntasks covering multiple levels of motivation. Using MotiveBench, we conduct\nextensive experiments on seven popular model families, comparing different\nscales and versions within each family. The results show that even the most\nadvanced LLMs still fall short in achieving human-like motivational reasoning.\nOur analysis reveals key findings, including the difficulty LLMs face in\nreasoning about \"love & belonging\" motivations and their tendency toward\nexcessive rationality and idealism. These insights highlight a promising\ndirection for future research on the humanization of LLMs. The dataset,\nbenchmark, and code are available at https://aka.ms/motivebench.\n","authors":["Xixian Yong","Jianxun Lian","Xiaoyuan Yi","Xiao Zhou","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2506.13065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13063v1","updated":"2025-06-16T03:12:51Z","published":"2025-06-16T03:12:51Z","title":"PRISM2: Unlocking Multi-Modal General Pathology AI with Clinical\n  Dialogue","summary":"  Recent pathology foundation models can provide rich tile-level\nrepresentations but fall short of delivering general-purpose clinical utility\nwithout further extensive model development. These models lack whole-slide\nimage (WSI) understanding and are not trained with large-scale diagnostic data,\nlimiting their performance on diverse downstream tasks. We introduce PRISM2, a\nmulti-modal slide-level foundation model trained via clinical dialogue to\nenable scalable, generalizable pathology AI. PRISM2 is trained on nearly\n700,000 specimens (2.3 million WSIs) paired with real-world clinical diagnostic\nreports in a two-stage process. In Stage 1, a vision-language model is trained\nusing contrastive and captioning objectives to align whole slide embeddings\nwith textual clinical diagnosis. In Stage 2, the language model is unfrozen to\nenable diagnostic conversation and extract more clinically meaningful\nrepresentations from hidden states. PRISM2 achieves strong performance on\ndiagnostic and biomarker prediction tasks, outperforming prior slide-level\nmodels including PRISM and TITAN. It also introduces a zero-shot yes/no\nclassification approach that surpasses CLIP-style methods without prompt tuning\nor class enumeration. By aligning visual features with clinical reasoning,\nPRISM2 improves generalization on both data-rich and low-sample tasks, offering\na scalable path forward for building general pathology AI agents capable of\nassisting diagnostic and prognostic decisions.\n","authors":["George Shaikovski","Eugene Vorontsov","Adam Casson","Julian Viret","Eric Zimmermann","Neil Tenenholtz","Yi Kan Wang","Jan H. Bernhard","Ran A. Godrich","Juan A. Retamero","Razik Yousfi","Nicolo Fusi","Thomas J. Fuchs","Kristen Severson","Siqi Liu"],"pdf_url":"https://arxiv.org/pdf/2506.13063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11999v2","updated":"2025-06-16T03:10:31Z","published":"2025-06-13T17:54:12Z","title":"Generative Representational Learning of Foundation Models for\n  Recommendation","summary":"  Developing a single foundation model with the capability to excel across\ndiverse tasks has been a long-standing objective in the field of artificial\nintelligence. As the wave of general-purpose foundation models sweeps across\nvarious domains, their influence has significantly extended to the field of\nrecommendation systems. While recent efforts have explored recommendation\nfoundation models for various generative tasks, they often overlook crucial\nembedding tasks and struggle with the complexities of multi-task learning,\nincluding knowledge sharing & conflict resolution, and convergence speed\ninconsistencies. To address these limitations, we introduce RecFound, a\ngenerative representational learning framework for recommendation foundation\nmodels. We construct the first comprehensive dataset for recommendation\nfoundation models covering both generative and embedding tasks across diverse\nscenarios. Based on this dataset, we propose a novel multi-task training scheme\nfeaturing a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge\nsharing & conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched)\nto address inconsistent convergence, and a Model Merge module to balance the\nperformance across tasks. Experiments demonstrate that RecFound achieves\nstate-of-the-art performance across various recommendation tasks, outperforming\nexisting baselines.\n","authors":["Zheli Zhou","Chenxu Zhu","Jianghao Lin","Bo Chen","Ruiming Tang","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2506.11999v2.pdf","comment":"Project page is available at https://junkfood436.github.io/RecFound/"},{"id":"http://arxiv.org/abs/2506.13059v1","updated":"2025-06-16T03:00:40Z","published":"2025-06-16T03:00:40Z","title":"Multipole Attention for Efficient Long Context Reasoning","summary":"  Large Reasoning Models (LRMs) have shown promising accuracy improvements on\ncomplex problem-solving tasks. While these models have attained high accuracy\nby leveraging additional computation at test time, they need to generate long\nchain-of-thought reasoning in order to think before answering, which requires\ngenerating thousands of tokens. While sparse attention methods can help reduce\nthe KV cache pressure induced by this long autoregressive reasoning, these\nmethods can introduce errors which disrupt the reasoning process. Additionally,\nprior methods often pre-process the input to make it easier to identify the\nimportant prompt tokens when computing attention during generation, and this\npre-processing is challenging to perform online for newly generated reasoning\ntokens. Our work addresses these challenges by introducing Multipole Attention,\nwhich accelerates autoregressive reasoning by only computing exact attention\nfor the most important tokens, while maintaining approximate representations\nfor the remaining tokens. Our method first performs clustering to group\ntogether semantically similar key vectors, and then uses the cluster centroids\nboth to identify important key vectors and to approximate the remaining key\nvectors in order to retain high accuracy. We design a fast cluster update\nprocess to quickly re-cluster the input and previously generated tokens,\nthereby allowing for accelerating attention to the previous output tokens. We\nevaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our\napproach can maintain accuracy on complex reasoning tasks even with aggressive\nattention sparsity settings. We also provide kernel implementations to\ndemonstrate the practical efficiency gains from our method, achieving up to\n4.5$\\times$ speedup for attention in long-context reasoning applications. Our\ncode is available at https://github.com/SqueezeAILab/MultipoleAttention.\n","authors":["Coleman Hooper","Sebastian Zhao","Luca Manolache","Sehoon Kim","Michael W. Mahoney","Yakun Sophia Shao","Kurt Keutzer","Amir Gholami"],"pdf_url":"https://arxiv.org/pdf/2506.13059v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2506.09342v2","updated":"2025-06-16T02:57:37Z","published":"2025-06-11T02:48:16Z","title":"Latent Multi-Head Attention for Small Language Models","summary":"  We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.\n","authors":["Sushant Mehta","Raj Dandekar","Rajat Dandekar","Sreedath Panat"],"pdf_url":"https://arxiv.org/pdf/2506.09342v2.pdf","comment":"6 pages, 1 figure. 5 tables"},{"id":"http://arxiv.org/abs/2506.13055v1","updated":"2025-06-16T02:52:44Z","published":"2025-06-16T02:52:44Z","title":"CFBenchmark-MM: Chinese Financial Assistant Benchmark for Multimodal\n  Large Language Model","summary":"  Multimodal Large Language Models (MLLMs) have rapidly evolved with the growth\nof Large Language Models (LLMs) and are now applied in various fields. In\nfinance, the integration of diverse modalities such as text, charts, and tables\nis crucial for accurate and efficient decision-making. Therefore, an effective\nevaluation system that incorporates these data types is essential for advancing\nfinancial application. In this paper, we introduce CFBenchmark-MM, a Chinese\nmultimodal financial benchmark with over 9,000 image-question pairs featuring\ntables, histogram charts, line charts, pie charts, and structural diagrams.\nAdditionally, we develop a staged evaluation system to assess MLLMs in handling\nmultimodal information by providing different visual content step by step.\nDespite MLLMs having inherent financial knowledge, experimental results still\nshow limited efficiency and robustness in handling multimodal financial\ncontext. Further analysis on incorrect responses reveals the misinterpretation\nof visual content and the misunderstanding of financial concepts are the\nprimary issues. Our research validates the significant, yet underexploited,\npotential of MLLMs in financial analysis, highlighting the need for further\ndevelopment and domain-specific optimization to encourage the enhanced use in\nfinancial domain.\n","authors":["Jiangtong Li","Yiyun Zhu","Dawei Cheng","Zhijun Ding","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2506.13055v1.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2506.13051v1","updated":"2025-06-16T02:40:33Z","published":"2025-06-16T02:40:33Z","title":"Stress-Testing Multimodal Foundation Models for Crystallographic\n  Reasoning","summary":"  Evaluating foundation models for crystallographic reasoning requires\nbenchmarks that isolate generalization behavior while enforcing physical\nconstraints. This work introduces a multiscale multicrystal dataset with two\nphysically grounded evaluation protocols to stress-test multimodal generative\nmodels. The Spatial-Exclusion benchmark withholds all supercells of a given\nradius from a diverse dataset, enabling controlled assessments of spatial\ninterpolation and extrapolation. The Compositional-Exclusion benchmark omits\nall samples of a specific chemical composition, probing generalization across\nstoichiometries. Nine vision--language foundation models are prompted with\ncrystallographic images and textual context to generate structural annotations.\nResponses are evaluated via (i) relative errors in lattice parameters and\ndensity, (ii) a physics-consistency index penalizing volumetric violations, and\n(iii) a hallucination score capturing geometric outliers and invalid\nspace-group predictions. These benchmarks establish a reproducible, physically\ninformed framework for assessing generalization, consistency, and reliability\nin large-scale multimodal models. Dataset and code are available at\nhttps://github.com/KurbanIntelligenceLab/StressTestingMMFMinCR.\n","authors":["Can Polat","Hasan Kurban","Erchin Serpedin","Mustafa Kurban"],"pdf_url":"https://arxiv.org/pdf/2506.13051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07311v9","updated":"2025-06-16T02:37:13Z","published":"2024-03-12T04:47:29Z","title":"Knowledge Graph Large Language Model (KG-LLM) for Link Prediction","summary":"  The task of multi-hop link prediction within knowledge graphs (KGs) stands as\na challenge in the field of knowledge graph analysis, as it requires the model\nto reason through and understand all intermediate connections before making a\nprediction. In this paper, we introduce the Knowledge Graph Large Language\nModel (KG-LLM), a novel framework that leverages large language models (LLMs)\nfor knowledge graph tasks. We first convert structured knowledge graph data\ninto natural language and then use these natural language prompts to fine-tune\nLLMs to enhance multi-hop link prediction in KGs. By converting the KG to\nnatural language prompts, our framework is designed to learn the latent\nrepresentations of entities and their interrelations. To show the efficacy of\nthe KG-LLM Framework, we fine-tune three leading LLMs within this framework,\nincluding Flan-T5, LLaMa2 and Gemma. Further, we explore the framework's\npotential to provide LLMs with zero-shot capabilities for handling previously\nunseen prompts. Experimental results show that KG-LLM significantly improves\nthe models' generalization capabilities, leading to more accurate predictions\nin unfamiliar scenarios.\n","authors":["Dong Shu","Tianle Chen","Mingyu Jin","Chong Zhang","Mengnan Du","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07311v9.pdf","comment":"Accepted by ACML 2024"},{"id":"http://arxiv.org/abs/2410.07524v2","updated":"2025-06-16T02:29:18Z","published":"2024-10-10T01:36:03Z","title":"Upcycling Large Language Models into Mixture of Experts","summary":"  Upcycling pre-trained dense language models into sparse mixture-of-experts\n(MoE) models is an efficient approach to increase the model capacity of already\ntrained models. However, optimal techniques for upcycling at scale remain\nunclear. In this work, we conduct an extensive study of upcycling methods and\nhyperparameters for billion-parameter scale language models. We propose a novel\n\"virtual group\" initialization scheme and weight scaling approach to enable\nupcycling into fine-grained MoE architectures. Through ablations, we find that\nupcycling outperforms continued dense model training. In addition, we show that\nsoftmax-then-topK expert routing improves over topK-then-softmax approach and\nhigher granularity MoEs can help improve accuracy. Finally, we upcycled\nNemotron-4 15B on 1T tokens and compared it to a continuously trained version\nof the same model on the same 1T tokens: the continuous trained model achieved\n65.3% MMLU, whereas the upcycled model achieved 67.6%. Our results offer\ninsights and best practices to effectively leverage upcycling for building MoE\nlanguage models. Code is available.\n","authors":["Ethan He","Abhinav Khattar","Ryan Prenger","Vijay Korthikanti","Zijie Yan","Tong Liu","Shiqing Fan","Ashwath Aithal","Mohammad Shoeybi","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2410.07524v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11105v2","updated":"2025-06-16T02:22:18Z","published":"2025-06-07T01:37:42Z","title":"Enabling On-Device Medical AI Assistants via Input-Driven Saliency\n  Adaptation","summary":"  Large Language Models (LLMs) have significant impact on the healthcare\nscenarios but remain prohibitively large for deployment in real-time,\nresource-constrained environments such as edge devices. In this work, we\nintroduce a novel medical assistant system, optimized through our\ngeneral-purpose compression framework, which tailors Large Language Models\n(LLMs) for deployment in specialized domains. By measuring neuron saliency on\ndomain-specific data, our method can aggressively prune irrelevant neurons,\nreducing model size while preserving performance. Following pruning, we apply\npost-training quantization to further reduce the memory footprint, and evaluate\nthe compressed model across medical benchmarks including MedMCQA, MedQA, and\nPubMedQA. We also deploy the 50\\% compressed Gemma and the 67\\% compressed\nLLaMA3 models on Jetson Orin Nano (18.7W peak) and Raspberry Pi 5 (6.3W peak),\nachieving real-time, energy-efficient inference under hardware constraints.\n","authors":["Uttej Kallakurik","Edward Humes","Rithvik Jonna","Xiaomin Lin","Tinoosh Mohsenin"],"pdf_url":"https://arxiv.org/pdf/2506.11105v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13044v1","updated":"2025-06-16T02:21:15Z","published":"2025-06-16T02:21:15Z","title":"Just Go Parallel: Improving the Multilingual Capabilities of Large\n  Language Models","summary":"  Large language models (LLMs) have demonstrated impressive translation\ncapabilities even without being explicitly trained on parallel data. This\nremarkable property has led some to believe that parallel data is no longer\nnecessary for building multilingual language models. While some attribute this\nto the emergent abilities of LLMs due to scale, recent work suggests that it is\nactually caused by incidental bilingual signals present in the training data.\nVarious methods have been proposed to maximize the utility of parallel data to\nenhance the multilingual capabilities of multilingual encoder-based and\nencoder-decoder language models. However, some decoder-based LLMs opt to ignore\nparallel data instead. In this work, we conduct a systematic study on the\nimpact of adding parallel data on LLMs' multilingual capabilities, focusing\nspecifically on translation and multilingual common-sense reasoning. Through\ncontrolled experiments, we demonstrate that parallel data can significantly\nimprove LLMs' multilingual capabilities.\n","authors":["Muhammad Reza Qorib","Junyi Li","Hwee Tou Ng"],"pdf_url":"https://arxiv.org/pdf/2506.13044v1.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2409.04267v3","updated":"2025-06-16T02:13:39Z","published":"2024-09-06T13:24:22Z","title":"An overview of domain-specific foundation model: key technologies,\n  applications and challenges","summary":"  The impressive performance of ChatGPT and other foundation-model-based\nproducts in human language understanding has prompted both academia and\nindustry to explore how these models can be tailored for specific industries\nand application scenarios. This process, known as the customization of\ndomain-specific foundation models (FMs), addresses the limitations of\ngeneral-purpose models, which may not fully capture the unique patterns and\nrequirements of domain-specific data. Despite its importance, there is a\nnotable lack of comprehensive overview papers on building domain-specific FMs,\nwhile numerous resources exist for general-purpose models. To bridge this gap,\nthis article provides a timely and thorough overview of the methodology for\ncustomizing domain-specific FMs. It introduces basic concepts, outlines the\ngeneral architecture, and surveys key methods for constructing domain-specific\nmodels. Furthermore, the article discusses various domains that can benefit\nfrom these specialized models and highlights the challenges ahead. Through this\noverview, we aim to offer valuable guidance and reference for researchers and\npractitioners from diverse fields to develop their own customized FMs.\n","authors":["Haolong Chen","Hanzhi Chen","Zijian Zhao","Kaifeng Han","Guangxu Zhu","Yichen Zhao","Ying Du","Wei Xu","Qingjiang Shi"],"pdf_url":"https://arxiv.org/pdf/2409.04267v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10588v4","updated":"2025-06-16T02:12:47Z","published":"2024-11-15T21:19:04Z","title":"A dataset of questions on decision-theoretic reasoning in Newcomb-like\n  problems","summary":"  We introduce a dataset of natural-language questions in the decision theory\nof so-called Newcomb-like problems. Newcomb-like problems include, for\ninstance, decision problems in which an agent interacts with a similar other\nagent, and thus has to reason about the fact that the other agent will likely\nreason in similar ways. Evaluating LLM reasoning about Newcomb-like problems is\nimportant because interactions between foundation-model-based agents will often\nbe Newcomb-like. Some ways of reasoning about Newcomb-like problems may allow\nfor greater cooperation between models.\n  Our dataset contains both capabilities questions (i.e., questions with a\nunique, uncontroversially correct answer) and attitude questions (i.e.,\nquestions about which decision theorists would disagree). We use our dataset\nfor an investigation of decision-theoretical capabilities and expressed\nattitudes and their interplay in existing models (different models by OpenAI,\nAnthropic, Meta, GDM, Reka, etc.), as well as models under simple prompt-based\ninterventions. We find, among other things, that attitudes vary significantly\nbetween existing models; that high capabilities are associated with attitudes\nmore favorable toward so-called evidential decision theory; and that attitudes\nare consistent across different types of questions.\n","authors":["Caspar Oesterheld","Emery Cooper","Miles Kodama","Linh Chi Nguyen","Ethan Perez"],"pdf_url":"https://arxiv.org/pdf/2411.10588v4.pdf","comment":"48 pages, 15 figures; code and data at\n  https://github.com/casparoe/newcomblike_questions_dataset"},{"id":"http://arxiv.org/abs/2505.21549v4","updated":"2025-06-16T01:45:22Z","published":"2025-05-25T07:08:07Z","title":"Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal\n  Transformer Distillation","summary":"  We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that\nenhances multimodal image-text retrieval while preserving the original model's\nstrong zero-shot classification capabilities. CLIP models are typically\nconstrained by fixed image resolutions and limited context, which can hinder\ntheir effectiveness in retrieval tasks that require fine-grained cross-modal\nunderstanding. DCLIP addresses these challenges through a meta teacher-student\ndistillation framework, where a cross-modal transformer teacher is fine-tuned\nto produce enriched embeddings via bidirectional cross-attention between\nYOLO-extracted image regions and corresponding textual spans. These\nsemantically and spatially aligned global representations guide the training of\na lightweight student model using a hybrid loss that combines contrastive\nlearning and cosine similarity objectives. Despite being trained on only\n~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a\nfraction of CLIP's original dataset-DCLIP significantly improves image-text\nretrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's\nzero-shot classification performance. These results demonstrate that DCLIP\neffectively mitigates the trade-off between task specialization and\ngeneralization, offering a resource-efficient, domain-adaptive, and\ndetail-sensitive solution for advanced vision-language tasks. Code available at\nhttps://anonymous.4open.science/r/DCLIP-B772/README.md.\n","authors":["Daniel Csizmadia","Andrei Codreanu","Victor Sim","Vighnesh Prabhu","Michael Lu","Kevin Zhu","Sean O'Brien","Vasu Sharma"],"pdf_url":"https://arxiv.org/pdf/2505.21549v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11031v2","updated":"2025-06-16T01:28:03Z","published":"2025-05-20T22:44:04Z","title":"Task-aligned prompting improves zero-shot detection of AI-generated\n  images by Vision-Language Models","summary":"  As image generators produce increasingly realistic images, concerns about\npotential misuse continue to grow. Supervised detection relies on large,\ncurated datasets and struggles to generalize across diverse generators. In this\nwork, we investigate the use of pre-trained Vision-Language Models (VLMs) for\nzero-shot detection of AI-generated images. While off-the-shelf VLMs exhibit\nsome task-specific reasoning and chain-of-thought prompting offers gains, we\nshow that task-aligned prompting elicits more focused reasoning and\nsignificantly improves performance without fine-tuning. Specifically, prefixing\nthe model's response with the phrase \"Let's examine the style and the synthesis\nartifacts\" -- a method we call zero-shot-s$^2$ -- boosts Macro F1 scores by\n8%-29%. These gains are consistent for two widely used open-source models and\nacross three recent, diverse datasets spanning human faces, objects, and\nanimals with images generated by 16 different models -- demonstrating strong\ngeneralization. We further evaluate the approach across three additional model\nsizes and observe improvements in most dataset-model combinations -- suggesting\nrobustness to model scale. Surprisingly, self-consistency, a behavior\npreviously observed in language reasoning, where aggregating answers from\ndiverse reasoning paths improves performance, also holds in this setting. Even\nhere, zero-shot-s$^2$ scales better than chain-of-thought in most cases --\nindicating that it elicits more useful diversity. Our findings show that\ntask-aligned prompts elicit more focused reasoning and enhance latent\ncapabilities in VLMs, like the detection of AI-generated images -- offering a\nsimple, generalizable, and explainable alternative to supervised methods. Our\ncode is publicly available on github: https://github.com/Zoher15/Zero-shot-s2.\n","authors":["Zoher Kachwala","Danishjeet Singh","Danielle Yang","Filippo Menczer"],"pdf_url":"https://arxiv.org/pdf/2506.11031v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13026v1","updated":"2025-06-16T01:26:08Z","published":"2025-06-16T01:26:08Z","title":"Knowledge Graph Fusion with Large Language Models for Accurate,\n  Explainable Manufacturing Process Planning","summary":"  Precision process planning in Computer Numerical Control (CNC) machining\ndemands rapid, context-aware decisions on tool selection, feed-speed pairs, and\nmulti-axis routing, placing immense cognitive and procedural burdens on\nengineers from design specification through final part inspection. Conventional\nrule-based computer-aided process planning and knowledge-engineering shells\nfreeze domain know-how into static tables, which become limited when dealing\nwith unseen topologies, novel material states, shifting\ncost-quality-sustainability weightings, or shop-floor constraints such as tool\nunavailability and energy caps. Large language models (LLMs) promise flexible,\ninstruction-driven reasoning for tasks but they routinely hallucinate numeric\nvalues and provide no provenance. We present Augmented Retrieval Knowledge\nNetwork Enhanced Search & Synthesis (ARKNESS), the end-to-end framework that\nfuses zero-shot Knowledge Graph (KG) construction with retrieval-augmented\ngeneration to deliver verifiable, numerically exact answers for CNC process\nplanning. ARKNESS (1) automatically distills heterogeneous machining documents,\nG-code annotations, and vendor datasheets into augmented triple,\nmulti-relational graphs without manual labeling, and (2) couples any on-prem\nLLM with a retriever that injects the minimal, evidence-linked subgraph needed\nto answer a query. Benchmarked on 155 industry-curated questions spanning tool\nsizing and feed-speed optimization, a lightweight 3B-parameter Llama-3\naugmented by ARKNESS matches GPT-4o accuracy while achieving a +25 percentage\npoint gain in multiple-choice accuracy, +22.4 pp in F1, and 8.1x ROUGE-L on\nopen-ended responses.\n","authors":["Danny Hoang","David Gorsich","Matthew P. Castanier","Farhad Imani"],"pdf_url":"https://arxiv.org/pdf/2506.13026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13020v1","updated":"2025-06-16T01:14:52Z","published":"2025-06-16T01:14:52Z","title":"Edeflip: Supervised Word Translation between English and Yoruba","summary":"  In recent years, embedding alignment has become the state-of-the-art machine\ntranslation approach, as it can yield high-quality translation without training\non parallel corpora. However, existing research and application of embedding\nalignment mostly focus on high-resource languages with high-quality monolingual\nembeddings. It is unclear if and how low-resource languages may be similarly\nbenefited. In this study, we implement an established supervised embedding\nalignment method for word translation from English to Yoruba, the latter a\nlow-resource language. We found that higher embedding quality and normalizing\nembeddings increase word translation precision, with, additionally, an\ninteraction effect between the two. Our results demonstrate the limitations of\nthe state-of-the-art supervised embedding alignment when it comes to\nlow-resource languages, for which there are additional factors that need to be\ntaken into consideration, such as the importance of curating high-quality\nmonolingual embeddings. We hope our work will be a starting point for further\nmachine translation research that takes into account the challenges that\nlow-resource languages face.\n","authors":["Ikeoluwa Abioye","Jiani Ge"],"pdf_url":"https://arxiv.org/pdf/2506.13020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00332v2","updated":"2025-06-16T01:12:52Z","published":"2025-05-31T01:09:04Z","title":"Disentangling Codemixing in Chats: The NUS ABC Codemixed Corpus","summary":"  Code-mixing involves the seamless integration of linguistic elements from\nmultiple languages within a single discourse, reflecting natural multilingual\ncommunication patterns. Despite its prominence in informal interactions such as\nsocial media, chat messages and instant-messaging exchanges, there has been a\nlack of publicly available corpora that are author-labeled and suitable for\nmodeling human conversations and relationships. This study introduces the first\nlabeled and general-purpose corpus for understanding code-mixing in context\nwhile maintaining rigorous privacy and ethical standards. Our live project will\ncontinuously gather, verify, and integrate code-mixed messages into a\nstructured dataset released in JSON format, accompanied by detailed metadata\nand linguistic statistics. To date, it includes over 355,641 messages spanning\nvarious code-mixing patterns, with a primary focus on English, Mandarin, and\nother languages. We expect the Codemix Corpus to serve as a foundational\ndataset for research in computational linguistics, sociolinguistics, and NLP\napplications.\n","authors":["Svetlana Churina","Akshat Gupta","Insyirah Mujtahid","Kokil Jaidka"],"pdf_url":"https://arxiv.org/pdf/2506.00332v2.pdf","comment":"19 pages, 5 figures, 8 tables"},{"id":"http://arxiv.org/abs/2503.23243v2","updated":"2025-06-16T00:52:04Z","published":"2025-03-29T22:53:15Z","title":"Evaluating how LLM annotations represent diverse views on contentious\n  topics","summary":"  Researchers have proposed the use of generative large language models (LLMs)\nto label data for research and applied settings. This literature emphasizes the\nimproved performance of these models relative to other natural language models,\nnoting that generative LLMs typically outperform other models and even humans\nacross several metrics. Previous literature has examined bias across many\napplications and contexts, but less work has focused specifically on bias in\ngenerative LLMs' responses to subjective annotation tasks. This bias could\nresult in labels applied by LLMs that disproportionately align with majority\ngroups over a more diverse set of viewpoints. In this paper, we evaluate how\nLLMs represent diverse viewpoints on these contentious tasks. Across four\nannotation tasks on four datasets, we show that LLMs do not show systematic\nsubstantial disagreement with annotators on the basis of demographics. Rather,\nwe find that multiple LLMs tend to be biased in the same directions on the same\ndemographic categories within the same datasets. Moreover, the disagreement\nbetween human annotators on the labeling task -- a measure of item difficulty\n-- is far more predictive of LLM agreement with human annotators. We conclude\nwith a discussion of the implications for researchers and practitioners using\nLLMs for automated data annotation tasks. Specifically, we emphasize that\nfairness evaluations must be contextual, model choice alone will not solve\npotential issues of bias, and item difficulty must be integrated into bias\nassessments.\n","authors":["Megan A. Brown","Shubham Atreja","Libby Hemphill","Patrick Y. Wu"],"pdf_url":"https://arxiv.org/pdf/2503.23243v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13013v1","updated":"2025-06-16T00:48:09Z","published":"2025-06-16T00:48:09Z","title":"Missing the human touch? A computational stylometry analysis of GPT-4\n  translations of online Chinese literature","summary":"  Existing research indicates that machine translations (MTs) of literary texts\nare often unsatisfactory. MTs are typically evaluated using automated metrics\nand subjective human ratings, with limited focus on stylistic features.\nEvidence is also limited on whether state-of-the-art large language models\n(LLMs) will reshape literary translation. This study examines the stylistic\nfeatures of LLM translations, comparing GPT-4's performance to human\ntranslations in a Chinese online literature task. Computational stylometry\nanalysis shows that GPT-4 translations closely align with human translations in\nlexical, syntactic, and content features, suggesting that LLMs might replicate\nthe 'human touch' in literary translation style. These findings offer insights\ninto AI's impact on literary translation from a posthuman perspective, where\ndistinctions between machine and human translations become increasingly blurry.\n","authors":["Xiaofang Yao","Yong-Bin Kang","Anthony McCosker"],"pdf_url":"https://arxiv.org/pdf/2506.13013v1.pdf","comment":"15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.14133v2","updated":"2025-06-16T00:24:41Z","published":"2025-02-19T22:27:59Z","title":"Self-Regularization with Sparse Autoencoders for Controllable LLM-based\n  Classification","summary":"  Modern text classification methods heavily rely on contextual embeddings from\nlarge language models (LLMs). Compared to human-engineered features, these\nembeddings provide automatic and effective representations for classification\nmodel training. However, they also introduce a challenge: we lose the ability\nto manually remove unintended features, such as sensitive or task-irrelevant\nfeatures, to guarantee regulatory compliance or improve the generalizability of\nclassification models. This limitation arises because LLM embeddings are opaque\nand difficult to interpret. In this paper, we propose a novel framework to\nidentify and regularize unintended features in the LLM latent space.\nSpecifically, we first pre-train a sparse autoencoder (SAE) to extract\ninterpretable features from LLM latent spaces. To ensure the SAE can capture\ntask-specific features, we further fine-tune it on task-specific datasets. In\ntraining the classification model, we propose a simple and effective\nregularizer, by minimizing the similarity between the classifier weights and\nthe identified unintended feature, to remove the impact of these unintended\nfeatures on classification. We evaluate the proposed framework on three\nreal-world tasks, including toxic chat detection, reward modeling, and disease\ndiagnosis. Results show that the proposed self-regularization framework can\nimprove the classifier's generalizability by regularizing those features that\nare not semantically correlated to the task. This work pioneers controllable\ntext classification on LLM latent spaces by leveraging interpreted features to\naddress generalizability, fairness, and privacy challenges. The code and data\nare publicly available at\nhttps://github.com/JacksonWuxs/Controllable_LLM_Classifier.\n","authors":["Xuansheng Wu","Wenhao Yu","Xiaoming Zhai","Ninghao Liu"],"pdf_url":"https://arxiv.org/pdf/2502.14133v2.pdf","comment":"Accepted by SIGKDD 2025"},{"id":"http://arxiv.org/abs/2502.04322v2","updated":"2025-06-16T00:13:14Z","published":"2025-02-06T18:59:02Z","title":"Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple\n  Interactions","summary":"  Despite extensive safety alignment efforts, large language models (LLMs)\nremain vulnerable to jailbreak attacks that elicit harmful behavior. While\nexisting studies predominantly focus on attack methods that require technical\nexpertise, two critical questions remain underexplored: (1) Are jailbroken\nresponses truly useful in enabling average users to carry out harmful actions?\n(2) Do safety vulnerabilities exist in more common, simple human-LLM\ninteractions? In this paper, we demonstrate that LLM responses most effectively\nfacilitate harmful actions when they are both actionable and informative--two\nattributes easily elicited in multi-step, multilingual interactions. Using this\ninsight, we propose HarmScore, a jailbreak metric that measures how effectively\nan LLM response enables harmful actions, and Speak Easy, a simple multi-step,\nmultilingual attack framework. Notably, by incorporating Speak Easy into direct\nrequest and jailbreak baselines, we see an average absolute increase of 0.319\nin Attack Success Rate and 0.426 in HarmScore in both open-source and\nproprietary LLMs across four safety benchmarks. Our work reveals a critical yet\noften overlooked vulnerability: Malicious users can easily exploit common\ninteraction patterns for harmful intentions.\n","authors":["Yik Siu Chan","Narutatsu Ri","Yuxin Xiao","Marzyeh Ghassemi"],"pdf_url":"https://arxiv.org/pdf/2502.04322v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14064v1","updated":"2025-06-16T23:48:04Z","published":"2025-06-16T23:48:04Z","title":"Automatic Extraction of Clausal Embedding Based on Large-Scale English\n  Text Data","summary":"  For linguists, embedded clauses have been of special interest because of\ntheir intricate distribution of syntactic and semantic features. Yet, current\nresearch relies on schematically created language examples to investigate these\nconstructions, missing out on statistical information and naturally-occurring\nexamples that can be gained from large language corpora. Thus, we present a\nmethodological approach for detecting and annotating naturally-occurring\nexamples of English embedded clauses in large-scale text data using\nconstituency parsing and a set of parsing heuristics. Our tool has been\nevaluated on our dataset Golden Embedded Clause Set (GECS), which includes\nhand-annotated examples of naturally-occurring English embedded clause\nsentences. Finally, we present a large-scale dataset of naturally-occurring\nEnglish embedded clauses which we have extracted from the open-source corpus\nDolma using our extraction tool.\n","authors":["Iona Carslaw","Sivan Milton","Nicolas Navarre","Ciyang Qing","Wataru Uegaki"],"pdf_url":"https://arxiv.org/pdf/2506.14064v1.pdf","comment":"Accepted in the Society for Computation in Linguistics"},{"id":"http://arxiv.org/abs/2506.14046v1","updated":"2025-06-16T22:40:16Z","published":"2025-06-16T22:40:16Z","title":"Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic\n  Difficulty of Conversational Texts for LLM Applications","summary":"  There is an unmet need to evaluate the language difficulty of short,\nconversational passages of text, particularly for training and filtering Large\nLanguage Models (LLMs). We introduce Ace-CEFR, a dataset of English\nconversational text passages expert-annotated with their corresponding level of\ntext difficulty. We experiment with several models on Ace-CEFR, including\nTransformer-based models and LLMs. We show that models trained on Ace-CEFR can\nmeasure text difficulty more accurately than human experts and have latency\nappropriate to production environments. Finally, we release the Ace-CEFR\ndataset to the public for research and development.\n","authors":["David Kogan","Max Schumacher","Sam Nguyen","Masanori Suzuki","Melissa Smith","Chloe Sophia Bellows","Jared Bernstein"],"pdf_url":"https://arxiv.org/pdf/2506.14046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14040v1","updated":"2025-06-16T22:26:17Z","published":"2025-06-16T22:26:17Z","title":"An Interdisciplinary Review of Commonsense Reasoning and Intent\n  Detection","summary":"  This review explores recent advances in commonsense reasoning and intent\ndetection, two key challenges in natural language understanding. We analyze 28\npapers from ACL, EMNLP, and CHI (2020-2025), organizing them by methodology and\napplication. Commonsense reasoning is reviewed across zero-shot learning,\ncultural adaptation, structured evaluation, and interactive contexts. Intent\ndetection is examined through open-set models, generative formulations,\nclustering, and human-centered systems. By bridging insights from NLP and HCI,\nwe highlight emerging trends toward more adaptive, multilingual, and\ncontext-aware models, and identify key gaps in grounding, generalization, and\nbenchmark design.\n","authors":["Md Nazmus Sakib"],"pdf_url":"https://arxiv.org/pdf/2506.14040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16464v3","updated":"2025-06-16T22:23:37Z","published":"2024-10-21T19:46:06Z","title":"Beyond Browsing: API-Based Web Agents","summary":"  Web browsers are a portal to the internet, where much of human activity is\nundertaken. Thus, there has been significant research work in AI agents that\ninteract with the internet through web browsing. However, there is also another\ninterface designed specifically for machine interaction with online content:\napplication programming interfaces (APIs). In this paper we ask -- what if we\nwere to take tasks traditionally tackled by Browsing Agents, and give AI agents\naccess to APIs? To do so, we propose two varieties of agents: (1) an\nAPI-calling agent that attempts to perform online tasks through APIs only,\nsimilar to traditional coding agents, and (2) a Hybrid Agent that can interact\nwith online data through both web browsing and APIs. In experiments on\nWebArena, a widely-used and realistic benchmark for web navigation tasks, we\nfind that API-Based Agents outperform web Browsing Agents. Hybrid Agents\nout-perform both others nearly uniformly across tasks, resulting in a more than\n24.0% absolute improvement over web browsing alone, achieving a success rate of\n38.9%, the SOTA performance among task-agnostic agents. These results strongly\nsuggest that when APIs are available, they present an attractive alternative to\nrelying on web browsing alone.\n","authors":["Yueqi Song","Frank Xu","Shuyan Zhou","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2410.16464v3.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2506.14028v1","updated":"2025-06-16T22:01:49Z","published":"2025-06-16T22:01:49Z","title":"MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark\n  for Financial LLM Evaluation","summary":"  Recent advances in large language models (LLMs) have accelerated progress in\nfinancial NLP and applications, yet existing benchmarks remain limited to\nmonolingual and unimodal settings, often over-relying on simple tasks and\nfailing to reflect the complexity of real-world financial communication. We\nintroduce MultiFinBen, the first multilingual and multimodal benchmark tailored\nto the global financial domain, evaluating LLMs across modalities (text,\nvision, audio) and linguistic settings (monolingual, bilingual, multilingual)\non domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy\nand PolyFiQA-Expert, the first multilingual financial benchmarks requiring\nmodels to perform complex reasoning over mixed-language inputs; and EnglishOCR\nand SpanishOCR, the first OCR-embedded financial QA tasks challenging models to\nextract and reason over information from visual-text financial documents.\nMoreover, we propose a dynamic, difficulty-aware selection mechanism and curate\na compact, balanced benchmark rather than simple aggregation existing datasets.\nExtensive evaluation of 22 state-of-the-art models reveals that even the\nstrongest models, despite their general multimodal and multilingual\ncapabilities, struggle dramatically when faced with complex cross-lingual and\nmultimodal tasks in financial domain. MultiFinBen is publicly released to\nfoster transparent, reproducible, and inclusive progress in financial studies\nand applications.\n","authors":["Xueqing Peng","Lingfei Qian","Yan Wang","Ruoyu Xiang","Yueru He","Yang Ren","Mingyang Jiang","Jeff Zhao","Huan He","Yi Han","Yun Feng","Yuechen Jiang","Yupeng Cao","Haohang Li","Yangyang Yu","Xiaoyu Wang","Penglei Gao","Shengyuan Lin","Keyi Wang","Shanshan Yang","Yilun Zhao","Zhiwei Liu","Peng Lu","Jerry Huang","Suyuchen Wang","Triantafillos Papadopoulos","Polydoros Giannouris","Efstathia Soufleri","Nuo Chen","Guojun Xiong","Zhiyang Deng","Yijia Zhao","Mingquan Lin","Meikang Qiu","Kaleb E Smith","Arman Cohan","Xiao-Yang Liu","Jimin Huang","Alejandro Lopez-Lira","Xi Chen","Junichi Tsujii","Jian-Yun Nie","Sophia Ananiadou","Qianqian Xie"],"pdf_url":"https://arxiv.org/pdf/2506.14028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14012v1","updated":"2025-06-16T21:19:27Z","published":"2025-06-16T21:19:27Z","title":"Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text","summary":"  Code-switching (CSW) is the act of alternating between two or more languages\nwithin a single discourse. This phenomenon is widespread in multilingual\ncommunities, and increasingly prevalent in online content, where users\nnaturally mix languages in everyday communication. As a result, Large Language\nModels (LLMs), now central to content processing and generation, are frequently\nexposed to code-switched inputs. Given their widespread use, it is crucial to\nunderstand how LLMs process and reason about such mixed-language text. This\npaper presents a systematic evaluation of LLM comprehension under\ncode-switching by generating CSW variants of established reasoning and\ncomprehension benchmarks. While degradation is evident when foreign tokens\ndisrupt English text$\\unicode{x2013}$even under linguistic\nconstraints$\\unicode{x2013}$embedding English into other languages often\nimproves comprehension. Though prompting yields mixed results, fine-tuning\noffers a more stable path to degradation mitigation.\n","authors":["Amr Mohamed","Yang Zhang","Michalis Vazirgiannis","Guokan Shang"],"pdf_url":"https://arxiv.org/pdf/2506.14012v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13497v3","updated":"2025-06-16T20:58:21Z","published":"2025-02-19T07:29:58Z","title":"Towards Geo-Culturally Grounded LLM Generations","summary":"  Generative large language models (LLMs) have demonstrated gaps in diverse\ncultural awareness across the globe. We investigate the effect of retrieval\naugmented generation and search-grounding techniques on LLMs' ability to\ndisplay familiarity with various national cultures. Specifically, we compare\nthe performance of standard LLMs, LLMs augmented with retrievals from a bespoke\nknowledge base (i.e., KB grounding), and LLMs augmented with retrievals from a\nweb search (i.e., search grounding) on multiple cultural awareness benchmarks.\nWe find that search grounding significantly improves the LLM performance on\nmultiple-choice benchmarks that test propositional knowledge (e.g., cultural\nnorms, artifacts, and institutions), while KB grounding's effectiveness is\nlimited by inadequate knowledge base coverage and a suboptimal retriever.\nHowever, search grounding also increases the risk of stereotypical judgments by\nlanguage models and fails to improve evaluators' judgments of cultural\nfamiliarity in a human evaluation with adequate statistical power. These\nresults highlight the distinction between propositional cultural knowledge and\nopen-ended cultural fluency when it comes to evaluating LLMs' cultural\nawareness.\n","authors":["Piyawat Lertvittayakumjorn","David Kinney","Vinodkumar Prabhakaran","Donald Martin Jr.","Sunipa Dev"],"pdf_url":"https://arxiv.org/pdf/2502.13497v3.pdf","comment":"ACL 2025 (main conference)"},{"id":"http://arxiv.org/abs/2506.07801v2","updated":"2025-06-16T20:28:26Z","published":"2025-06-09T14:27:47Z","title":"MultiMatch: Multihead Consistency Regularization Matching for\n  Semi-Supervised Text Classification","summary":"  We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm\ncombining the paradigms of co-training and consistency regularization with\npseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label\nweighting module designed for three key purposes: selecting and filtering\npseudo-labels based on head agreement and model confidence, and weighting them\naccording to the perceived classification difficulty. This novel module\nenhances and unifies three existing techniques -- heads agreement from\nMultihead Co-training, self-adaptive thresholds from FreeMatch, and Average\nPseudo-Margins from MarginMatch -- resulting in a holistic approach that\nimproves robustness and performance in SSL settings. Experimental results on\nbenchmark datasets highlight the superior performance of MultiMatch, achieving\nstate-of-the-art results on 9 out of 10 setups from 5 natural language\nprocessing datasets and ranking first according to the Friedman test among 19\nmethods. Furthermore, MultiMatch demonstrates exceptional robustness in highly\nimbalanced settings, outperforming the second-best approach by 3.26% -- and\ndata imbalance is a key factor for many text classification tasks.\n","authors":["Iustin Sirbu","Robert-Adrian Popovici","Cornelia Caragea","Stefan Trausan-Matu","Traian Rebedea"],"pdf_url":"https://arxiv.org/pdf/2506.07801v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07313v4","updated":"2025-06-16T20:19:19Z","published":"2024-07-10T02:20:19Z","title":"ETM: Modern Insights into Perspective on Text-to-SQL Evaluation in the\n  Age of Large Language Models","summary":"  The task of Text-to-SQL enables anyone to retrieve information from SQL\ndatabases using natural language. While this task has made substantial\nprogress, the two primary evaluation metrics - Execution Accuracy (EXE) and\nExact Set Matching Accuracy (ESM) - suffer from inherent limitations that can\nmisrepresent performance. Specifically, ESM's rigid matching overlooks\nsemantically correct but stylistically different queries, whereas EXE can\noverestimate correctness by ignoring structural errors that yield correct\noutputs. These shortcomings become especially problematic when assessing\noutputs from large language model (LLM)-based approaches without fine-tuning,\nwhich vary more in style and structure compared to their fine-tuned\ncounterparts. Thus, we introduce a new metric, Enhanced Tree Matching (ETM),\nwhich mitigates these issues by comparing queries using both syntactic and\nsemantic elements. Through evaluating nine LLM-based models, we show that EXE\nand ESM can produce false positive and negative rates as high as 23.0% and\n28.9%, while ETM reduces these rates to 0.3% and 2.7%, respectively. We release\nour ETM script as open source, offering the community a more robust and\nreliable approach to evaluating Text-to-SQL.\n","authors":["Benjamin G. Ascoli","Yasoda Sai Ram Kandikonda","Jinho D. Choi"],"pdf_url":"https://arxiv.org/pdf/2407.07313v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13965v1","updated":"2025-06-16T20:15:57Z","published":"2025-06-16T20:15:57Z","title":"Are manual annotations necessary for statutory interpretations\n  retrieval?","summary":"  One of the elements of legal research is looking for cases where judges have\nextended the meaning of a legal concept by providing interpretations of what a\nconcept means or does not mean. This allow legal professionals to use such\ninterpretations as precedents as well as laymen to better understand the legal\nconcept. The state-of-the-art approach for retrieving the most relevant\ninterpretations for these concepts currently depends on the ranking of\nsentences and the training of language models over annotated examples. That\nmanual annotation process can be quite expensive and need to be repeated for\neach such concept, which prompted recent research in trying to automate this\nprocess. In this paper, we highlight the results of various experiments\nconducted to determine the volume, scope and even the need for manual\nannotation. First of all, we check what is the optimal number of annotations\nper a legal concept. Second, we check if we can draw the sentences for\nannotation randomly or there is a gain in the performance of the model, when\nonly the best candidates are annotated. As the last question we check what is\nthe outcome of automating the annotation process with the help of an LLM.\n","authors":["Aleksander Smywiński-Pohl","Tomer Libal","Adam Kaczmarczyk","Magdalena Król"],"pdf_url":"https://arxiv.org/pdf/2506.13965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13956v1","updated":"2025-06-16T19:58:54Z","published":"2025-06-16T19:58:54Z","title":"ASMR: Augmenting Life Scenario using Large Generative Models for Robotic\n  Action Reflection","summary":"  When designing robots to assist in everyday human activities, it is crucial\nto enhance user requests with visual cues from their surroundings for improved\nintent understanding. This process is defined as a multimodal classification\ntask. However, gathering a large-scale dataset encompassing both visual and\nlinguistic elements for model training is challenging and time-consuming. To\naddress this issue, our paper introduces a novel framework focusing on data\naugmentation in robotic assistance scenarios, encompassing both dialogues and\nrelated environmental imagery. This approach involves leveraging a\nsophisticated large language model to simulate potential conversations and\nenvironmental contexts, followed by the use of a stable diffusion model to\ncreate images depicting these environments. The additionally generated data\nserves to refine the latest multimodal models, enabling them to more accurately\ndetermine appropriate actions in response to user interactions with the limited\ntarget data. Our experimental results, based on a dataset collected from\nreal-world scenarios, demonstrate that our methodology significantly enhances\nthe robot's action selection capabilities, achieving the state-of-the-art\nperformance.\n","authors":["Shang-Chi Tsai","Seiya Kawano","Angel Garcia Contreras","Koichiro Yoshino","Yun-Nung Chen"],"pdf_url":"https://arxiv.org/pdf/2506.13956v1.pdf","comment":"IWSDS 2024 Best Paper Award"},{"id":"http://arxiv.org/abs/2505.07897v2","updated":"2025-06-16T19:55:19Z","published":"2025-05-12T05:38:03Z","title":"LongCodeBench: Evaluating Coding LLMs at 1M Context Windows","summary":"  Context lengths for models have grown rapidly, from thousands to millions of\ntokens in just a few years. The extreme context sizes of modern long-context\nmodels have made it difficult to construct realistic long-context benchmarks --\nnot only due to the cost of collecting million-context tasks but also in\nidentifying realistic scenarios that require significant contexts. We identify\ncode comprehension and repair as a natural testbed and challenge task for\nlong-context models and introduce LongCodeBench (LCB), a benchmark to test LLM\ncoding abilities in long-context scenarios. Our benchmark tests both the\ncomprehension and repair capabilities of LCLMs in realistic and important\nsettings by drawing from real-world GitHub issues and constructing QA\n(LongCodeQA) and bug fixing (LongSWE-Bench) tasks. We carefully stratify the\ncomplexity of our benchmark, enabling us to evaluate models across different\nscales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model.\nWe find that long-context remains a weakness for all models, with performance\ndrops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for\nQwen2.5.\n","authors":["Stefano Rando","Luca Romani","Alessio Sampieri","Luca Franco","John Yang","Yuta Kyuragi","Fabio Galasso","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2505.07897v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20612v2","updated":"2025-06-16T19:08:27Z","published":"2025-05-27T01:24:29Z","title":"Roboflow100-VL: A Multi-Domain Object Detection Benchmark for\n  Vision-Language Models","summary":"  Vision-language models (VLMs) trained on internet-scale data achieve\nremarkable zero-shot detection performance on common objects like car, truck,\nand pedestrian. However, state-of-the-art models still struggle to generalize\nto out-of-distribution classes, tasks and imaging modalities not typically\nfound in their pre-training. Rather than simply re-training VLMs on more visual\ndata, we argue that one should align VLMs to new concepts with annotation\ninstructions containing a few visual examples and rich textual descriptions. To\nthis end, we introduce Roboflow100-VL, a large-scale collection of 100\nmulti-modal object detection datasets with diverse concepts not commonly found\nin VLM pre-training. We evaluate state-of-the-art models on our benchmark in\nzero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing\nfor comparison across data regimes. Notably, we find that VLMs like\nGroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on\nchallenging medical imaging datasets within Roboflow100-VL, demonstrating the\nneed for few-shot concept alignment. Lastly, we discuss our recent CVPR 2025\nFoundational FSOD competition and share insights from the community. Notably,\nthe winning team significantly outperforms our baseline by 16.8 mAP! Our code\nand dataset are available at https://github.com/roboflow/rf100-vl/ and\nhttps://universe.roboflow.com/rf100-vl/\n","authors":["Peter Robicheaux","Matvei Popov","Anish Madan","Isaac Robinson","Joseph Nelson","Deva Ramanan","Neehar Peri"],"pdf_url":"https://arxiv.org/pdf/2505.20612v2.pdf","comment":"The first two authors contributed equally. Project Page:\n  https://rf100-vl.org/"},{"id":"http://arxiv.org/abs/2506.13923v1","updated":"2025-06-16T19:03:06Z","published":"2025-06-16T19:03:06Z","title":"Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models","summary":"  We study the process through which reasoning models trained with\nreinforcement learning on verifiable rewards (RLVR) can learn to solve new\nproblems. We find that RLVR drives performance through two main means: (1) by\ncompressing pass@$k$ into pass@1 and (2) via \"capability gain\" in which models\nlearn to solve new problems that they previously could not solve even at high\n$k$. We find that while capability gain exists across model scales, learning to\nsolve new problems is primarily driven through self-distillation. We\ndemonstrate these findings across model scales ranging from 0.5B to 72B on\n>500,000 reasoning problems with prompts and verifiable final answers across\nmath, science, and code domains. We further show that we can significantly\nimprove pass@$k$ rates by leveraging natural language guidance for the model to\nconsider within context while still requiring the model to derive a solution\nchain from scratch. Based of these insights, we derive $\\text{Guide}$ - a new\nclass of online training algorithms. $\\text{Guide}$ adaptively incorporates\nhints into the model's context on problems for which all rollouts were\ninitially incorrect and adjusts the importance sampling ratio for the\n\"off-policy\" trajectories in order to optimize the policy for contexts in which\nthe hints are no longer present. We describe variants of $\\text{Guide}$ for\nGRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter\nmodels improves generalization over its vanilla counterpart with up to 4$\\%$\nmacro-average improvement across math benchmarks. We include careful ablations\nto analyze $\\text{Guide}$'s components and theoretically analyze Guide's\nlearning efficiency.\n","authors":["Vaskar Nath","Elaine Lau","Anisha Gunjal","Manasi Sharma","Nikhil Baharte","Sean Hendryx"],"pdf_url":"https://arxiv.org/pdf/2506.13923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10274v2","updated":"2025-06-16T18:38:29Z","published":"2025-06-12T01:35:43Z","title":"Discrete Audio Tokens: More Than a Survey!","summary":"  Discrete audio tokens are compact representations that aim to preserve\nperceptual quality, phonetic content, and speaker characteristics while\nenabling efficient storage and inference, as well as competitive performance\nacross diverse downstream tasks. They provide a practical alternative to\ncontinuous features, enabling the integration of speech and audio into modern\nlarge language models (LLMs). As interest in token-based audio processing\ngrows, various tokenization methods have emerged, and several surveys have\nreviewed the latest progress in the field. However, existing studies often\nfocus on specific domains or tasks and lack a unified comparison across various\nbenchmarks. This paper presents a systematic review and benchmark of discrete\naudio tokenizers, covering three domains: speech, music, and general audio. We\npropose a taxonomy of tokenization approaches based on encoder-decoder,\nquantization techniques, training paradigm, streamability, and application\ndomains. We evaluate tokenizers on multiple benchmarks for reconstruction,\ndownstream performance, and acoustic language modeling, and analyze trade-offs\nthrough controlled ablation studies. Our findings highlight key limitations,\npractical considerations, and open challenges, providing insight and guidance\nfor future research in this rapidly evolving area. For more information,\nincluding our main results and tokenizer database, please refer to our website:\nhttps://poonehmousavi.github.io/dates-website/.\n","authors":["Pooneh Mousavi","Gallil Maimon","Adel Moumen","Darius Petermann","Jiatong Shi","Haibin Wu","Haici Yang","Anastasia Kuznetsova","Artem Ploujnikov","Ricard Marxer","Bhuvana Ramabhadran","Benjamin Elizalde","Loren Lugosch","Jinyu Li","Cem Subakan","Phil Woodland","Minje Kim","Hung-yi Lee","Shinji Watanabe","Yossi Adi","Mirco Ravanelli"],"pdf_url":"https://arxiv.org/pdf/2506.10274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04079v2","updated":"2025-06-16T18:23:31Z","published":"2025-06-04T15:43:31Z","title":"EuroLLM-9B: Technical Report","summary":"  This report presents EuroLLM-9B, a large language model trained from scratch\nto support the needs of European citizens by covering all 24 official European\nUnion languages and 11 additional languages. EuroLLM addresses the issue of\nEuropean languages being underrepresented and underserved in existing open\nlarge language models. We provide a comprehensive overview of EuroLLM-9B's\ndevelopment, including tokenizer design, architectural specifications, data\nfiltering, and training procedures. We describe the pre-training data\ncollection and filtering pipeline, including the creation of EuroFilter, an\nAI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a\nnovel synthetic dataset for post-training that enhances language coverage for\nEuropean languages. Evaluation results demonstrate EuroLLM-9B's competitive\nperformance on multilingual benchmarks and machine translation tasks,\nestablishing it as the leading open European-made LLM of its size. To support\nopen research and adoption, we release all major components of this work,\nincluding the base and instruction-tuned models, the EuroFilter classifier, and\nthe synthetic post-training dataset.\n","authors":["Pedro Henrique Martins","João Alves","Patrick Fernandes","Nuno M. Guerreiro","Ricardo Rei","Amin Farajian","Mateusz Klimaszewski","Duarte M. Alves","José Pombal","Nicolas Boizard","Manuel Faysse","Pierre Colombo","François Yvon","Barry Haddow","José G. C. de Souza","Alexandra Birch","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2506.04079v2.pdf","comment":"56 pages"},{"id":"http://arxiv.org/abs/2506.13901v1","updated":"2025-06-16T18:22:28Z","published":"2025-06-16T18:22:28Z","title":"Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic\n  Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise\n  Pooled Representations","summary":"  Alignment is no longer a luxury, it is a necessity. As large language models\n(LLMs) enter high-stakes domains like education, healthcare, governance, and\nlaw, their behavior must reliably reflect human-aligned values and safety\nconstraints. Yet current evaluations rely heavily on behavioral proxies such as\nrefusal rates, G-Eval scores, and toxicity classifiers, all of which have\ncritical blind spots. Aligned models are often vulnerable to jailbreaking,\nstochasticity of generation, and alignment faking.\n  To address this issue, we introduce the Alignment Quality Index (AQI). This\nnovel geometric and prompt-invariant metric empirically assesses LLM alignment\nby analyzing the separation of safe and unsafe activations in latent space. By\ncombining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI),\nXie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various\nformulations, AQI captures clustering quality to detect hidden misalignments\nand jailbreak risks, even when outputs appear compliant. AQI also serves as an\nearly warning signal for alignment faking, offering a robust, decoding\ninvariant tool for behavior agnostic safety auditing.\n  Additionally, we propose the LITMUS dataset to facilitate robust evaluation\nunder these challenging conditions. Empirical tests on LITMUS across different\nmodels trained under DPO, GRPO, and RLHF conditions demonstrate AQI's\ncorrelation with external judges and ability to reveal vulnerabilities missed\nby refusal metrics. We make our implementation publicly available to foster\nfuture research in this area.\n","authors":["Abhilekh Borah","Chhavi Sharma","Danush Khanna","Utkarsh Bhatt","Gurpreet Singh","Hasnat Md Abdullah","Raghav Kaushik Ravi","Vinija Jain","Jyoti Patel","Shubham Singh","Vasu Sharma","Arpita Vats","Rahul Raja","Aman Chadha","Amitava Das"],"pdf_url":"https://arxiv.org/pdf/2506.13901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13894v1","updated":"2025-06-16T18:16:04Z","published":"2025-06-16T18:16:04Z","title":"EmoNews: A Spoken Dialogue System for Expressive News Conversations","summary":"  We develop a task-oriented spoken dialogue system (SDS) that regulates\nemotional speech based on contextual cues to enable more empathetic news\nconversations. Despite advancements in emotional text-to-speech (TTS)\ntechniques, task-oriented emotional SDSs remain underexplored due to the\ncompartmentalized nature of SDS and emotional TTS research, as well as the lack\nof standardized evaluation metrics for social goals. We address these\nchallenges by developing an emotional SDS for news conversations that utilizes\na large language model (LLM)-based sentiment analyzer to identify appropriate\nemotions and PromptTTS to synthesize context-appropriate emotional speech. We\nalso propose subjective evaluation scale for emotional SDSs and judge the\nemotion regulation performance of the proposed and baseline systems.\nExperiments showed that our emotional SDS outperformed a baseline system in\nterms of the emotion regulation and engagement. These results suggest the\ncritical role of speech emotion for more engaging conversations. All our source\ncode is open-sourced at\nhttps://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds/sds1\n","authors":["Ryuki Matsuura","Shikhar Bharadwaj","Jiarui Liu","Dhatchi Kunde Govindarajan"],"pdf_url":"https://arxiv.org/pdf/2506.13894v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19110v3","updated":"2025-06-16T18:13:10Z","published":"2025-02-26T13:01:49Z","title":"Conformal Linguistic Calibration: Trading-off between Factuality and\n  Specificity","summary":"  Language model outputs are not always reliable, thus prompting research into\nhow to adapt model responses based on uncertainty. Common approaches include:\n\\emph{abstention}, where models refrain from generating responses when\nuncertain; and \\emph{linguistic calibration}, where models hedge their\nstatements using uncertainty quantifiers. However, abstention can withhold\nvaluable information, while linguistically calibrated responses are often\nchallenging to leverage in downstream tasks. We propose a unified view,\nConformal Linguistic Calibration (CLC), which reinterprets linguistic\ncalibration as \\emph{answer set prediction}. First we present a framework\nconnecting abstention and linguistic calibration through the lens of linguistic\npragmatics. We then describe an implementation of CLC that allows for\ncontrolling the level of imprecision in model responses. Results demonstrate\nour method produces calibrated outputs with conformal guarantees on factual\naccuracy. Further, our approach enables fine-tuning models to perform\nuncertainty-aware adaptive claim rewriting, offering a controllable balance\nbetween factuality and specificity.\n","authors":["Zhengping Jiang","Anqi Liu","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2502.19110v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13888v1","updated":"2025-06-16T18:10:51Z","published":"2025-06-16T18:10:51Z","title":"VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and\n  Iterative Training","summary":"  Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large\nlanguage models but remains underexplored for Vision-Language (VL) models. The\nVision-Language Reward Model (VL-RM) is key to aligning VL models by providing\nstructured feedback, yet training effective VL-RMs faces two major challenges.\nFirst, the bootstrapping dilemma arises as high-quality training data depends\non already strong VL models, creating a cycle where self-generated supervision\nreinforces existing biases. Second, modality bias and negative example\namplification occur when VL models hallucinate incorrect visual attributes,\nleading to flawed preference data that further misguides training. To address\nthese issues, we propose an iterative training framework leveraging vision\nexperts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection\nSampling. Our approach refines preference datasets, enhances structured\ncritiques, and iteratively improves reasoning. Experiments across VL-RM\nbenchmarks demonstrate superior performance in hallucination detection and\nmultimodal reasoning, advancing VL model alignment with reinforcement learning.\n","authors":["Jipeng Zhang","Kehao Miao","Renjie Pi","Zhaowei Wang","Runtao Liu","Rui Pan","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.13888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13886v1","updated":"2025-06-16T18:09:38Z","published":"2025-06-16T18:09:38Z","title":"Investigating the interaction of linguistic and mathematical reasoning\n  in language models using multilingual number puzzles","summary":"  Across languages, numeral systems vary widely in how they construct and\ncombine numbers. While humans consistently learn to navigate this diversity,\nlarge language models (LLMs) struggle with linguistic-mathematical puzzles\ninvolving cross-linguistic numeral systems, which humans can learn to solve\nsuccessfully. We investigate why this task is difficult for LLMs through a\nseries of experiments that untangle the linguistic and mathematical aspects of\nnumbers in language. Our experiments establish that models cannot consistently\nsolve such problems unless the mathematical operations in the problems are\nexplicitly marked using known symbols ($+$, $\\times$, etc, as in \"twenty +\nthree\"). In further ablation studies, we probe how individual parameters of\nnumeral construction and combination affect performance. While humans use their\nlinguistic understanding of numbers to make inferences about the implicit\ncompositional structure of numerals, LLMs seem to lack this notion of implicit\nnumeral structure. We conclude that the ability to flexibly infer compositional\nrules from implicit patterns in human-scale data remains an open challenge for\ncurrent reasoning models.\n","authors":["Antara Raaghavi Bhattacharya","Isabel Papadimitriou","Kathryn Davidson","David Alvarez-Melis"],"pdf_url":"https://arxiv.org/pdf/2506.13886v1.pdf","comment":null}]},"2025-06-15T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2506.12991v1","updated":"2025-06-15T23:16:12Z","published":"2025-06-15T23:16:12Z","title":"Large Language Models Enhanced by Plug and Play Syntactic Knowledge for\n  Aspect-based Sentiment Analysis","summary":"  Aspect-based sentiment analysis (ABSA) generally requires a deep\nunderstanding of the contextual information, including the words associated\nwith the aspect terms and their syntactic dependencies. Most existing studies\nemploy advanced encoders (e.g., pre-trained models) to capture such context,\nespecially large language models (LLMs). However, training these encoders is\nresource-intensive, and in many cases, the available data is insufficient for\nnecessary fine-tuning. Therefore it is challenging for learning LLMs within\nsuch restricted environments and computation efficiency requirement. As a\nresult, it motivates the exploration of plug-and-play methods that adapt LLMs\nto ABSA with minimal effort. In this paper, we propose an approach that\nintegrates extendable components capable of incorporating various types of\nsyntactic knowledge, such as constituent syntax, word dependencies, and\ncombinatory categorial grammar (CCG). Specifically, we propose a memory module\nthat records syntactic information and is incorporated into LLMs to instruct\nthe prediction of sentiment polarities. Importantly, this encoder acts as a\nversatile, detachable plugin that is trained independently of the LLM. We\nconduct experiments on benchmark datasets, which show that our approach\noutperforms strong baselines and previous approaches, thus demonstrates its\neffectiveness.\n","authors":["Yuanhe Tian","Xu Li","Wei Wang","Guoqing Jin","Pengsen Cheng","Yan Song"],"pdf_url":"https://arxiv.org/pdf/2506.12991v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2506.12981v1","updated":"2025-06-15T22:35:43Z","published":"2025-06-15T22:35:43Z","title":"Efficient Neuro-Symbolic Retrieval-Augmented Generation through Adaptive\n  Query Routing","summary":"  Retrieval-Augmented Generation (RAG) systems address factual inconsistencies\nin Large Language Models by grounding generation in external knowledge, yet\nthey face a fundamental efficiency problem: simple queries consume\ncomputational resources equivalent to complex multi-hop reasoning tasks. We\npresent SymRAG, a neuro-symbolic framework that introduces adaptive query\nrouting based on real-time complexity and system load assessments. SymRAG\ndynamically selects symbolic, neural, or hybrid processing paths to align\nresource use with query demands. Evaluated on 2,000 queries from HotpotQA and\nDROP using Llama-3.2-3B and Mistral-7B models, SymRAG achieves 97.6--100.0%\nexact match accuracy with significantly lower CPU utilization (3.6--6.2%) and\nprocessing time (0.985--3.165s). Disabling adaptive logic results in 169--1151%\nincrease in processing time, highlighting the framework's impact. These results\nunderscore the potential of adaptive neuro-symbolic routing for scalable,\nsustainable AI systems.\n","authors":["Safayat Bin Hakim","Muhammad Adil","Alvaro Velasquez","Houbing Herbert Song"],"pdf_url":"https://arxiv.org/pdf/2506.12981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12978v1","updated":"2025-06-15T22:14:59Z","published":"2025-06-15T22:14:59Z","title":"Multi-document Summarization through Multi-document Event Relation Graph\n  Reasoning in LLMs: a case study in Framing Bias Mitigation","summary":"  Media outlets are becoming more partisan and polarized nowadays. Most\nprevious work focused on detecting media bias. In this paper, we aim to\nmitigate media bias by generating a neutralized summary given multiple articles\npresenting different ideological views. Motivated by the critical role of\nevents and event relations in media bias detection, we propose to increase\nawareness of bias in LLMs via multi-document events reasoning and use a\nmulti-document event relation graph to guide the summarization process. This\ngraph contains rich event information useful to reveal bias: four common types\nof in-doc event relations to reflect content framing bias, cross-doc event\ncoreference relation to reveal content selection bias, and event-level moral\nopinions to highlight opinionated framing bias. We further develop two\nstrategies to incorporate the multi-document event relation graph for\nneutralized summarization. Firstly, we convert a graph into natural language\ndescriptions and feed the textualized graph into LLMs as a part of a hard text\nprompt. Secondly, we encode the graph with graph attention network and insert\nthe graph embedding into LLMs as a soft prompt. Both automatic evaluation and\nhuman evaluation confirm that our approach effectively mitigates both lexical\nand informational media bias, and meanwhile improves content preservation.\n","authors":["Yuanyuan Lei","Ruihong Huang"],"pdf_url":"https://arxiv.org/pdf/2506.12978v1.pdf","comment":"Accepted to ACL 2025"},{"id":"http://arxiv.org/abs/2501.01426v2","updated":"2025-06-15T21:53:18Z","published":"2025-01-02T18:59:45Z","title":"Unifying Specialized Visual Encoders for Video Language Models","summary":"  The recent advent of Large Language Models (LLMs) has ushered sophisticated\nreasoning capabilities into the realm of video through Video Large Language\nModels (VideoLLMs). However, VideoLLMs currently rely on a single vision\nencoder for all of their visual processing, which limits the amount and type of\nvisual information that can be conveyed to the LLM. Our method, MERV,\nMulti-Encoder Representation of Videos, instead leverages multiple frozen\nvisual encoders to create a unified representation of a video, providing the\nVideoLLM with a comprehensive set of specialized visual knowledge.\nSpatio-temporally aligning the features from each encoder allows us to tackle a\nwider range of open-ended and multiple-choice video understanding questions and\noutperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy\nthan Video-LLaVA across the standard suite video understanding benchmarks,\nwhile also having a better Video-ChatGPT score. We also improve upon SeViLA,\nthe previous best on zero-shot Perception Test accuracy, by 2.2%. MERV\nintroduces minimal extra parameters and trains faster than equivalent\nsingle-encoder methods while parallelizing the visual processing. Finally, we\nprovide qualitative evidence that MERV successfully captures domain knowledge\nfrom each of its encoders. Our results offer promising directions in utilizing\nmultiple vision encoders for comprehensive video understanding.\n","authors":["Jihoon Chung","Tyler Zhu","Max Gonzalez Saez-Diez","Juan Carlos Niebles","Honglu Zhou","Olga Russakovsky"],"pdf_url":"https://arxiv.org/pdf/2501.01426v2.pdf","comment":"Accepted to ICML 2025 as a Poster. Project page:\n  https://tylerzhu.com/merv/"},{"id":"http://arxiv.org/abs/2405.20947v5","updated":"2025-06-15T21:44:25Z","published":"2024-05-31T15:44:33Z","title":"OR-Bench: An Over-Refusal Benchmark for Large Language Models","summary":"  Large Language Models (LLMs) require careful safety alignment to prevent\nmalicious outputs. While significant research focuses on mitigating harmful\ncontent generation, the enhanced safety often come with the side effect of\nover-refusal, where LLMs may reject innocuous prompts and become less helpful.\nAlthough the issue of over-refusal has been empirically observed, a systematic\nmeasurement is challenging due to the difficulty of crafting prompts that can\nelicit the over-refusal behaviors of LLMs. This study proposes a novel method\nfor automatically generating large-scale over-refusal datasets. Leveraging this\ntechnique, we introduce OR-Bench, the first large-scale over-refusal benchmark.\nOR-Bench comprises 80,000 over-refusal prompts across 10 common rejection\ncategories, a subset of around 1,000 hard prompts that are challenging even for\nstate-of-the-art LLMs, and an additional 600 toxic prompts to prevent\nindiscriminate responses. We then conduct a comprehensive study to measure the\nover-refusal of 32 popular LLMs across 8 model families. Our datasets are\npublicly available at https://huggingface.co/bench-llms and our codebase is\nopen-sourced at https://github.com/justincui03/or-bench. We hope this benchmark\ncan help the community develop better safety aligned models.\n","authors":["Justin Cui","Wei-Lin Chiang","Ion Stoica","Cho-Jui Hsieh"],"pdf_url":"https://arxiv.org/pdf/2405.20947v5.pdf","comment":"Accepted to ICML 2025, we thank everyone for their valuable\n  suggestions and feedback!"},{"id":"http://arxiv.org/abs/2410.21332v2","updated":"2025-06-15T21:38:31Z","published":"2024-10-27T18:13:07Z","title":"Building, Reusing, and Generalizing Abstract Representations from\n  Concrete Sequences","summary":"  Humans excel at learning abstract patterns across different sequences,\nfiltering out irrelevant details, and transferring these generalized concepts\nto new sequences. In contrast, many sequence learning models lack the ability\nto abstract, which leads to memory inefficiency and poor transfer. We introduce\na non-parametric hierarchical variable learning model (HVM) that learns chunks\nfrom sequences and abstracts contextually similar chunks as variables. HVM\nefficiently organizes memory while uncovering abstractions, leading to compact\nsequence representations. When learning on language datasets such as babyLM,\nHVM learns a more efficient dictionary than standard compression algorithms\nsuch as Lempel-Ziv. In a sequence recall task requiring the acquisition and\ntransfer of variables embedded in sequences, we demonstrate HVM's sequence\nlikelihood correlates with human recall times. In contrast, large language\nmodels (LLMs) struggle to transfer abstract variables as effectively as humans.\nFrom HVM's adjustable layer of abstraction, we demonstrate that the model\nrealizes a precise trade-off between compression and generalization. Our work\noffers a cognitive model that captures the learning and transfer of abstract\nrepresentations in human cognition and differentiates itself from LLMs.\n","authors":["Shuchen Wu","Mirko Thalmann","Peter Dayan","Zeynep Akata","Eric Schulz"],"pdf_url":"https://arxiv.org/pdf/2410.21332v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12966v1","updated":"2025-06-15T21:08:51Z","published":"2025-06-15T21:08:51Z","title":"Assessing the Role of Data Quality in Training Bilingual Language Models","summary":"  Bilingual and multilingual language models offer a promising path toward\nscaling NLP systems across diverse languages and users. However, their\nperformance often varies wildly between languages as prior works show that\nadding more languages can degrade performance for some languages (such as\nEnglish), while improving others (typically more data constrained languages).\nIn this work, we investigate causes of these inconsistencies by comparing\nbilingual and monolingual language models. Our analysis reveals that unequal\ndata quality, not just data quantity, is a major driver of performance\ndegradation in bilingual settings. We propose a simple yet effective data\nfiltering strategy to select higher-quality bilingual training data with only\nhigh quality English data. Applied to French, German, and Chinese, our approach\nimproves monolingual performance by 2-4% and reduces bilingual model\nperformance gaps to 1%. These results highlight the overlooked importance of\ndata quality in multilingual pretraining and offer a practical recipe for\nbalancing performance.\n","authors":["Skyler Seto","Maartje ter Hoeve","Maureen de Seyssel","David Grangier"],"pdf_url":"https://arxiv.org/pdf/2506.12966v1.pdf","comment":"26 pages, 18 figures, 25 tables"},{"id":"http://arxiv.org/abs/2503.13102v2","updated":"2025-06-15T19:53:19Z","published":"2025-03-17T12:15:16Z","title":"REPA: Russian Error Types Annotation for Evaluating Text Generation and\n  Judgment Capabilities","summary":"  Recent advances in large language models (LLMs) have introduced the novel\nparadigm of using LLMs as judges, where an LLM evaluates and scores the outputs\nof another LLM, which often correlates highly with human preferences. However,\nthe use of LLM-as-a-judge has been primarily studied in English. In this paper,\nwe evaluate this framework in Russian by introducing the Russian Error tyPes\nAnnotation dataset (REPA), a dataset of 1k user queries and 2k LLM-generated\nresponses. Human annotators labeled each response pair expressing their\npreferences across ten specific error types, as well as selecting an overall\npreference. We rank six generative LLMs across the error types using three\nrating systems based on human preferences. We also evaluate responses using\neight LLM judges in zero-shot and few-shot settings. We describe the results of\nanalyzing the judges and position and length biases. Our findings reveal a\nnotable gap between LLM judge performance in Russian and English. However,\nrankings based on human and LLM preferences show partial alignment, suggesting\nthat while current LLM judges struggle with fine-grained evaluation in Russian,\nthere is potential for improvement.\n","authors":["Alexander Pugachev","Alena Fenogenova","Vladislav Mikhailov","Ekaterina Artemova"],"pdf_url":"https://arxiv.org/pdf/2503.13102v2.pdf","comment":"To appear at SIGSLAV 2025"},{"id":"http://arxiv.org/abs/2506.00713v2","updated":"2025-06-15T19:52:55Z","published":"2025-05-31T21:11:30Z","title":"From Argumentative Text to Argument Knowledge Graph: A New Framework for\n  Structured Argumentation","summary":"  This paper presents a framework to convert argumentative texts into argument\nknowledge graphs (AKG). Starting with basic annotations of argumentative\ncomponents (ACs) and argumentative relations (ARs), we enrich the information\nby constructing a knowledge base (KB) graph with metadata attributes for nodes.\nNext, we use premises and inference rules from the KB to form arguments by\napplying modus ponens. From these arguments, we create an AKG. The nodes and\nedges of the AKG have attributes that capture important argumentative features.\nWe also find missing inference rules by identifying markers. This makes it\npossible to identify undercut attacks that were previously undetectable in\nexisting datasets. The AKG gives a graphical view of the argumentative\nstructure that is easier to understand than theoretical formats. It also\nprepares the ground for future reasoning tasks, including checking the\ncoherence of arguments and identifying opportunities for revision. For this, it\nis important to find indirect relations, many of which are implicit. Our\nproposed AKG format, with annotated inference rules and modus ponens, will help\nreasoning models learn the implicit indirect relations that require inference\nover arguments and the relations between them.\n","authors":["Debarati Bhattacharjee","Ashish Anand"],"pdf_url":"https://arxiv.org/pdf/2506.00713v2.pdf","comment":"16 pages, 7 figures"},{"id":"http://arxiv.org/abs/2506.12953v1","updated":"2025-06-15T19:42:58Z","published":"2025-06-15T19:42:58Z","title":"Forecasting Time Series with LLMs via Patch-Based Prompting and\n  Decomposition","summary":"  Recent advances in Large Language Models (LLMs) have demonstrated new\npossibilities for accurate and efficient time series analysis, but prior work\noften required heavy fine-tuning and/or ignored inter-series correlations. In\nthis work, we explore simple and flexible prompt-based strategies that enable\nLLMs to perform time series forecasting without extensive retraining or the use\nof a complex external architecture. Through the exploration of specialized\nprompting methods that leverage time series decomposition, patch-based\ntokenization, and similarity-based neighbor augmentation, we find that it is\npossible to enhance LLM forecasting quality while maintaining simplicity and\nrequiring minimal preprocessing of data. To this end, we propose our own\nmethod, PatchInstruct, which enables LLMs to make precise and effective\npredictions.\n","authors":["Mayank Bumb","Anshul Vemulapalli","Sri Harsha Vardhan Prasad Jella","Anish Gupta","An La","Ryan A. Rossi","Hongjie Chen","Franck Dernoncourt","Nesreen K. Ahmed","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2506.12953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12937v1","updated":"2025-06-15T18:41:23Z","published":"2025-06-15T18:41:23Z","title":"HypER: Literature-grounded Hypothesis Generation and Distillation with\n  Provenance","summary":"  Large Language models have demonstrated promising performance in research\nideation across scientific domains. Hypothesis development, the process of\ngenerating a highly specific declarative statement connecting a research idea\nwith empirical validation, has received relatively less attention. Existing\napproaches trivially deploy retrieval augmentation and focus only on the\nquality of the final output ignoring the underlying reasoning process behind\nideation. We present $\\texttt{HypER}$ ($\\textbf{Hyp}$othesis Generation with\n$\\textbf{E}$xplanation and $\\textbf{R}$easoning), a small language model (SLM)\ntrained for literature-guided reasoning and evidence-based hypothesis\ngeneration. $\\texttt{HypER}$ is trained in a multi-task setting to discriminate\nbetween valid and invalid scientific reasoning chains in presence of controlled\ndistractions. We find that $\\texttt{HypER}$ outperformes the base model,\ndistinguishing valid from invalid reasoning chains (+22\\% average absolute F1),\ngenerates better evidence-grounded hypotheses (0.327 vs. 0.305 base model) with\nhigh feasibility and impact as judged by human experts ($>$3.5 on 5-point\nLikert scale).\n","authors":["Rosni Vasu","Chandrayee Basu","Bhavana Dalvi Mishra","Cristina Sarasua","Peter Clark","Abraham Bernstein"],"pdf_url":"https://arxiv.org/pdf/2506.12937v1.pdf","comment":"26 pages (9 pages: main paper body)"},{"id":"http://arxiv.org/abs/2506.12936v1","updated":"2025-06-15T18:39:24Z","published":"2025-06-15T18:39:24Z","title":"CliniDial: A Naturally Occurring Multimodal Dialogue Dataset for Team\n  Reflection in Action During Clinical Operation","summary":"  In clinical operations, teamwork can be the crucial factor that determines\nthe final outcome. Prior studies have shown that sufficient collaboration is\nthe key factor that determines the outcome of an operation. To understand how\nthe team practices teamwork during the operation, we collected CliniDial from\nsimulations of medical operations. CliniDial includes the audio data and its\ntranscriptions, the simulated physiology signals of the patient manikins, and\nhow the team operates from two camera angles. We annotate behavior codes\nfollowing an existing framework to understand the teamwork process for\nCliniDial. We pinpoint three main characteristics of our dataset, including its\nlabel imbalances, rich and natural interactions, and multiple modalities, and\nconduct experiments to test existing LLMs' capabilities on handling data with\nthese characteristics. Experimental results show that CliniDial poses\nsignificant challenges to the existing models, inviting future effort on\ndeveloping methods that can deal with real-world clinical data. We open-source\nthe codebase at https://github.com/MichiganNLP/CliniDial\n","authors":["Naihao Deng","Kapotaksha Das","Rada Mihalcea","Vitaliy Popov","Mohamed Abouelenien"],"pdf_url":"https://arxiv.org/pdf/2506.12936v1.pdf","comment":"Accepted to ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2502.02013v2","updated":"2025-06-15T18:27:17Z","published":"2025-02-04T05:03:42Z","title":"Layer by Layer: Uncovering Hidden Representations in Language Models","summary":"  From extracting features to generating text, the outputs of large language\nmodels (LLMs) typically rely on the final layers, following the conventional\nwisdom that earlier layers capture only low-level cues. However, our analysis\nshows that intermediate layers can encode even richer representations, often\nimproving performance on a range of downstream tasks. To explain and quantify\nthese hidden-layer properties, we propose a unified framework of representation\nquality metrics based on information theory, geometry, and invariance to input\nperturbations. Our framework highlights how each layer balances information\ncompression and signal preservation, revealing why mid-depth embeddings can\nexceed the last layer's performance. Through extensive experiments on 32\ntext-embedding tasks across various architectures (transformers, state-space\nmodels) and domains (language, vision), we demonstrate that intermediate layers\nconsistently provide stronger features, challenging the standard view on\nfinal-layer embeddings and opening new directions on using mid-layer\nrepresentations for more robust and accurate representations.\n","authors":["Oscar Skean","Md Rifat Arefin","Dan Zhao","Niket Patel","Jalal Naghiyev","Yann LeCun","Ravid Shwartz-Ziv"],"pdf_url":"https://arxiv.org/pdf/2502.02013v2.pdf","comment":"update for ICML2025 camera-ready"},{"id":"http://arxiv.org/abs/2506.12935v1","updated":"2025-06-15T18:26:08Z","published":"2025-06-15T18:26:08Z","title":"SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models","summary":"  While large language models have shown reasoning capabilities, their\napplication to the audio modality, particularly in large audio-language models\n(ALMs), remains significantly underdeveloped. Addressing this gap requires a\nsystematic approach, involving a capable base model, high-quality\nreasoning-oriented audio data, and effective training algorithms. In this\nstudy, we present a comprehensive solution: we introduce the Audio Logical\nReasoning (ALR) dataset, consisting of 6,446 text-audio annotated samples\nspecifically designed for complex reasoning tasks. Building on this resource,\nwe propose SoundMind, a rule-based reinforcement learning (RL) algorithm\ntailored to endow ALMs with deep bimodal reasoning abilities. By training\nQwen2.5-Omni-7B on the ALR dataset using SoundMind, our approach achieves\nstate-of-the-art performance in audio logical reasoning. This work highlights\nthe impact of combining high-quality, reasoning-focused datasets with\nspecialized RL techniques, advancing the frontier of auditory intelligence in\nlanguage models. Our code and the proposed dataset are available at\nhttps://github.com/xid32/SoundMind.\n","authors":["Xingjian Diao","Chunhui Zhang","Keyi Kong","Weiyi Wu","Chiyu Ma","Zhongyu Ouyang","Peijun Qing","Soroush Vosoughi","Jiang Gui"],"pdf_url":"https://arxiv.org/pdf/2506.12935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14693v2","updated":"2025-06-15T18:18:35Z","published":"2025-01-24T18:06:07Z","title":"Rethinking Table Instruction Tuning","summary":"  Recent advances in table understanding have focused on instruction-tuning\nlarge language models (LLMs) for table-related tasks. However, existing\nresearch has overlooked the impact of hyperparameter choices, and also lacks a\ncomprehensive evaluation of the out-of-domain table understanding ability and\nthe general capabilities of these table LLMs. In this paper, we evaluate these\nabilities in existing table LLMs, and find significant declines in both\nout-of-domain table understanding and general capabilities as compared to their\nbase models. Through systematic analysis, we show that hyperparameters, such as\nlearning rate, can significantly influence both table-specific and general\ncapabilities. Contrary to the previous table instruction-tuning work, we\ndemonstrate that smaller learning rates and fewer training instances can\nenhance table understanding while preserving general capabilities. Based on our\nfindings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B\nInstruct, which achieves performance on par with, or surpassing GPT-3.5 and\nGPT-4 on table tasks, while maintaining strong out-of-domain generalization and\ngeneral capabilities. Our findings highlight the potential for reduced data\nannotation costs and more efficient model development through careful\nhyperparameter selection. We open-source the project and our models.\n","authors":["Naihao Deng","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2501.14693v2.pdf","comment":"Accepted to ACL 2025 Findings. Project page:\n  https://lit.eecs.umich.edu/TAMA/. Code: https://github.com/MichiganNLP/TAMA.\n  Huggingface models:\n  https://huggingface.co/collections/MichiganNLP/tama-684eeb3e7f262362856eccd1.\n  Data: https://huggingface.co/datasets/MichiganNLP/TAMA_Instruct"},{"id":"http://arxiv.org/abs/2506.07042v2","updated":"2025-06-15T18:16:38Z","published":"2025-06-08T08:36:14Z","title":"Reasoning with RAGged events: RAG-Enhanced Event Knowledge Base\n  Construction and reasoning with proof-assistants","summary":"  Extracting structured computational representations of historical events from\nnarrative text remains computationally expensive when constructed manually.\nWhile RDF/OWL reasoners enable graph-based reasoning, they are limited to\nfragments of first-order logic, preventing deeper temporal and semantic\nanalysis. This paper addresses both challenges by developing automatic\nhistorical event extraction models using multiple LLMs (GPT-4, Claude, Llama\n3.2) with three enhancement strategies: pure base generation, knowledge graph\nenhancement, and Retrieval-Augmented Generation (RAG). We conducted\ncomprehensive evaluations using historical texts from Thucydides. Our findings\nreveal that enhancement strategies optimize different performance dimensions\nrather than providing universal improvements. For coverage and historical\nbreadth, base generation achieves optimal performance with Claude and GPT-4\nextracting comprehensive events. However, for precision, RAG enhancement\nimproves coordinate accuracy and metadata completeness. Model architecture\nfundamentally determines enhancement sensitivity: larger models demonstrate\nrobust baseline performance with incremental RAG improvements, while Llama 3.2\nshows extreme variance from competitive performance to complete failure. We\nthen developed an automated translation pipeline converting extracted RDF\nrepresentations into Coq proof assistant specifications, enabling higher-order\nreasoning beyond RDF capabilities including multi-step causal verification,\ntemporal arithmetic with BC dates, and formal proofs about historical\ncausation. The Coq formalization validates that RAG-discovered event types\nrepresent legitimate domain-specific semantic structures rather than\nontological violations.\n","authors":["Stergios Chatzikyriakidis"],"pdf_url":"https://arxiv.org/pdf/2506.07042v2.pdf","comment":null}]},"2025-06-17T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2503.08679v4","updated":"2025-06-17T17:59:57Z","published":"2025-03-11T17:56:30Z","title":"Chain-of-Thought Reasoning In The Wild Is Not Always Faithful","summary":"  Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art\nAI capabilities. However, recent studies have shown that CoT reasoning is not\nalways faithful when models face an explicit bias in their prompts, i.e., the\nCoT can give an incorrect picture of how models arrive at conclusions. We go\nfurther and show that unfaithful CoT can also occur on realistic prompts with\nno artificial bias. We find that when separately presented with the questions\n\"Is X bigger than Y?\" and \"Is Y bigger than X?\", models sometimes produce\nsuperficially coherent arguments to justify systematically answering Yes to\nboth questions or No to both questions, despite such responses being logically\ncontradictory. We show preliminary evidence that this is due to models'\nimplicit biases towards Yes or No, thus labeling this unfaithfulness as\nImplicit Post-Hoc Rationalization. Our results reveal that several production\nmodels exhibit surprisingly high rates of post-hoc rationalization in our\nsettings: GPT-4o-mini (13%) and Haiku 3.5 (7%). While frontier models are more\nfaithful, especially thinking ones, none are entirely faithful: Gemini 2.5\nFlash (2.17%), ChatGPT-4o (0.49%), DeepSeek R1 (0.37%), Gemini 2.5 Pro (0.14%),\nand Sonnet 3.7 with thinking (0.04%). We also investigate Unfaithful Illogical\nShortcuts, where models use subtly illogical reasoning to try to make a\nspeculative answer to hard maths problems seem rigorously proven. Our findings\nraise challenges for strategies for detecting undesired behavior in LLMs via\nthe chain of thought.\n","authors":["Iván Arcuschin","Jett Janiak","Robert Krzyzanowski","Senthooran Rajamanoharan","Neel Nanda","Arthur Conmy"],"pdf_url":"https://arxiv.org/pdf/2503.08679v4.pdf","comment":"Accepted to the Reasoning and Planning for LLMs Workshop (ICLR 25),\n  10 main paper pages, 39 appendix pages"},{"id":"http://arxiv.org/abs/2506.14767v1","updated":"2025-06-17T17:58:17Z","published":"2025-06-17T17:58:17Z","title":"A Variational Framework for Improving Naturalness in Generative Spoken\n  Language Models","summary":"  The success of large language models in text processing has inspired their\nadaptation to speech modeling. However, since speech is continuous and complex,\nit is often discretized for autoregressive modeling. Speech tokens derived from\nself-supervised models (known as semantic tokens) typically focus on the\nlinguistic aspects of speech but neglect prosodic information. As a result,\nmodels trained on these tokens can generate speech with reduced naturalness.\nExisting approaches try to fix this by adding pitch features to the semantic\ntokens. However, pitch alone cannot fully represent the range of paralinguistic\nattributes, and selecting the right features requires careful hand-engineering.\nTo overcome this, we propose an end-to-end variational approach that\nautomatically learns to encode these continuous speech attributes to enhance\nthe semantic tokens. Our approach eliminates the need for manual extraction and\nselection of paralinguistic features. Moreover, it produces preferred speech\ncontinuations according to human raters. Code, samples and models are available\nat https://github.com/b04901014/vae-gslm.\n","authors":["Li-Wei Chen","Takuya Higuchi","Zakaria Aldeneh","Ahmed Hussen Abdelaziz","Alexander Rudnicky"],"pdf_url":"https://arxiv.org/pdf/2506.14767v1.pdf","comment":"International Conference on Machine Learning (ICML) 2025"},{"id":"http://arxiv.org/abs/2506.14766v1","updated":"2025-06-17T17:58:11Z","published":"2025-06-17T17:58:11Z","title":"ASCD: Attention-Steerable Contrastive Decoding for Reducing\n  Hallucination in MLLM","summary":"  Multimodal Large Language Model (MLLM) often suffer from hallucinations. They\nover-rely on partial cues and generate incorrect responses. Recently, methods\nlike Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding\n(ICD) have been proposed to mitigate hallucinations by contrasting predictions\nfrom perturbed or negatively prefixed inputs against original outputs. In this\nwork, we uncover that methods like VCD and ICD fundamentally influence internal\nattention dynamics of the model. This observation suggests that their\neffectiveness may not stem merely from surface-level modifications to logits\nbut from deeper shifts in attention distribution. Inspired by this insight, we\npropose an attention-steerable contrastive decoding framework that directly\nintervenes in attention mechanisms of the model to offer a more principled\napproach to mitigating hallucinations. Our experiments across multiple MLLM\narchitectures and diverse decoding methods demonstrate that our approach\nsignificantly reduces hallucinations and improves the performance on benchmarks\nsuch as POPE, CHAIR, and MMHal-Bench, while simultaneously enhancing\nperformance on standard VQA benchmarks.\n","authors":["Yujun Wang","Jinhe Bi","Yunpu Ma","Soeren Pirk"],"pdf_url":"https://arxiv.org/pdf/2506.14766v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2506.14761v1","updated":"2025-06-17T17:55:11Z","published":"2025-06-17T17:55:11Z","title":"From Bytes to Ideas: Language Modeling with Autoregressive U-Nets","summary":"  Tokenization imposes a fixed granularity on the input text, freezing how a\nlanguage model operates on data and how far in the future it predicts. Byte\nPair Encoding (BPE) and similar schemes split text once, build a static\nvocabulary, and leave the model stuck with that choice. We relax this rigidity\nby introducing an autoregressive U-Net that learns to embed its own tokens as\nit trains. The network reads raw bytes, pools them into words, then pairs of\nwords, then up to 4 words, giving it a multi-scale view of the sequence. At\ndeeper stages, the model must predict further into the future -- anticipating\nthe next few words rather than the next byte -- so deeper stages focus on\nbroader semantic patterns while earlier stages handle fine details. When\ncarefully tuning and controlling pretraining compute, shallow hierarchies tie\nstrong BPE baselines, and deeper hierarchies have a promising trend. Because\ntokenization now lives inside the model, the same system can handle\ncharacter-level tasks and carry knowledge across low-resource languages.\n","authors":["Mathurin Videau","Badr Youbi Idrissi","Alessandro Leite","Marc Schoenauer","Olivier Teytaud","David Lopez-Paz"],"pdf_url":"https://arxiv.org/pdf/2506.14761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14758v1","updated":"2025-06-17T17:54:03Z","published":"2025-06-17T17:54:03Z","title":"Reasoning with Exploration: An Entropy Perspective","summary":"  Balancing exploration and exploitation is a central goal in reinforcement\nlearning (RL). Despite recent advances in enhancing language model (LM)\nreasoning, most methods lean toward exploitation, and increasingly encounter\nperformance plateaus. In this work, we revisit entropy -- a signal of\nexploration in RL -- and examine its relationship to exploratory reasoning in\nLMs. Through empirical analysis, we uncover strong positive correlations\nbetween high-entropy regions and three types of exploratory reasoning actions:\n(1) pivotal tokens that determine or connect logical steps, (2) reflective\nactions such as self-verification and correction, and (3) rare behaviors\nunder-explored by the base LMs. Motivated by this, we introduce a minimal\nmodification to standard RL with only one line of code: augmenting the\nadvantage function with an entropy-based term. Unlike traditional\nmaximum-entropy methods which encourage exploration by promoting uncertainty,\nwe encourage exploration by promoting longer and deeper reasoning chains.\nNotably, our method achieves significant gains on the Pass@K metric -- an\nupper-bound estimator of LM reasoning capabilities -- even when evaluated with\nextremely large K values, pushing the boundaries of LM reasoning.\n","authors":["Daixuan Cheng","Shaohan Huang","Xuekai Zhu","Bo Dai","Wayne Xin Zhao","Zhenliang Zhang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2506.14758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05674v3","updated":"2025-06-17T17:53:30Z","published":"2024-07-08T07:17:40Z","title":"Controllable and Reliable Knowledge-Intensive Task-Oriented\n  Conversational Agents with Declarative Genie Worksheets","summary":"  Large Language Models can carry out human-like conversations in diverse\nsettings, responding to user requests for tasks and knowledge. However,\nexisting conversational agents implemented with LLMs often struggle with\nhallucination, following instructions with conditional logic, and integrating\nknowledge from different sources. These shortcomings compromise the agents'\neffectiveness, rendering them unsuitable for deployment. To address these\nchallenges, we introduce Genie, a programmable framework for creating\nknowledge-intensive task-oriented conversational agents. Genie can handle\ninvolved interactions and answer complex queries. Unlike LLMs, it delivers\nreliable, grounded responses through advanced dialogue state management and\nsupports controllable agent policies via its declarative specification -- Genie\nWorksheet. This is achieved through an algorithmic runtime system that\nimplements the developer-supplied policy, limiting LLMs to (1) parse user input\nusing a succinct conversational history, and (2) generate responses according\nto supplied context. Agents built with Genie outperform SOTA methods on complex\nlogic dialogue datasets. We conducted a user study with 62 participants on\nthree real-life applications: restaurant reservations with Yelp, as well as\nticket submission and course enrollment for university students. Genie agents\nwith GPT-4 Turbo outperformed the GPT-4 Turbo agents with function calling,\nimproving goal completion rates from 21.8% to 82.8% across three real-world\ntasks.\n","authors":["Harshit Joshi","Shicheng Liu","James Chen","Robert Weigle","Monica S. Lam"],"pdf_url":"https://arxiv.org/pdf/2407.05674v3.pdf","comment":"Accepted at ACL 2025"},{"id":"http://arxiv.org/abs/2503.08669v2","updated":"2025-06-17T17:50:44Z","published":"2025-03-11T17:53:02Z","title":"SOPBench: Evaluating Language Agents at Following Standard Operating\n  Procedures and Constraints","summary":"  As language agents increasingly automate critical tasks, their ability to\nfollow domain-specific standard operating procedures (SOPs), policies, and\nconstraints when taking actions and making tool calls becomes essential yet\nremains underexplored. To address this gap, we develop an automated evaluation\npipeline SOPBench with: (1) executable environments containing 167\ntools/functions across seven customer service domains with service-specific\nSOPs and rule-based verifiers, (2) an automated test generation framework\nproducing over 900 verified test cases, and (3) an automated evaluation\nframework to rigorously assess agent adherence from multiple dimensions. Our\napproach transforms each service-specific SOP code program into a directed\ngraph of executable functions and requires agents to call these functions based\non natural language SOP descriptions. The original code serves as oracle\nrule-based verifiers to assess compliance, reducing reliance on manual\nannotations and LLM-based evaluations. We evaluate 18 leading models, and\nresults show the task is challenging even for top-tier models (like GPT-4o,\nClaude-3.7-Sonnet), with variances across domains. Reasoning models like\no4-mini-high show superiority while other powerful models perform less\neffectively (pass rates of 30%-50%), and small models (7B, 8B) perform\nsignificantly worse. Additionally, language agents can be easily jailbroken to\noverlook SOPs and constraints. Code, data, and over 24k agent trajectories are\nreleased at https://github.com/Leezekun/SOPBench.\n","authors":["Zekun Li","Shinda Huang","Jiangtian Wang","Nathan Zhang","Antonis Antoniades","Wenyue Hua","Kaijie Zhu","Sirui Zeng","Chi Wang","William Yang Wang","Xifeng Yan"],"pdf_url":"https://arxiv.org/pdf/2503.08669v2.pdf","comment":"Code, data, and over 24k agent trajectories are released at\n  https://github.com/Leezekun/SOPBench"},{"id":"http://arxiv.org/abs/2506.14755v1","updated":"2025-06-17T17:50:16Z","published":"2025-06-17T17:50:16Z","title":"Optimizing Length Compression in Large Reasoning Models","summary":"  Large Reasoning Models (LRMs) have achieved remarkable success, yet they\noften suffer from producing unnecessary and verbose reasoning chains. We\nidentify a core aspect of this issue as \"invalid thinking\" -- models tend to\nrepeatedly double-check their work after having derived the correct answer. To\naddress this specific inefficiency, we move beyond the general principles of\nEfficacy and Efficiency to propose two new, fine-grained principles: Brevity,\nwhich advocates for eliminating redundancy, and Sufficiency, which ensures\ncritical reasoning steps are preserved. Guided by these principles, we\nintroduce LC-R1, a post-training method based on Group Relative Policy\nOptimization (GRPO). LC-R1 employs a novel combination of a Length Reward for\noverall conciseness and a Compress Reward that is specifically designed to\nremove the invalid portion of the thinking process. Extensive experiments on\nmultiple reasoning benchmarks demonstrate that LC-R1 achieves a significant\nreduction in sequence length (~50%) with only a marginal (~2%) drop in\naccuracy, achieving a favorable trade-off point on the Pareto frontier that\nprioritizes high compression. Our analysis further validates the robustness of\nLC-R1 and provides valuable insights for developing more powerful yet\ncomputationally efficient LRMs. Our code is released at\nhttps://github.com/zxiangx/LC-R1.\n","authors":["Zhengxiang Cheng","Dongping Chen","Mingyang Fu","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.14755v1.pdf","comment":"16 pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.18653v3","updated":"2025-06-17T17:41:07Z","published":"2024-10-24T11:32:01Z","title":"Towards Better Open-Ended Text Generation: A Multicriteria Evaluation\n  Framework","summary":"  Open-ended text generation has become a prominent task in natural language\nprocessing due to the rise of powerful (large) language models. However,\nevaluating the quality of these models and the employed decoding strategies\nremains challenging due to trade-offs among widely used metrics such as\ncoherence, diversity, and perplexity. This paper addresses the specific problem\nof multicriteria evaluation for open-ended text generation, proposing novel\nmethods for both relative and absolute rankings of decoding methods.\nSpecifically, we employ benchmarking approaches based on partial orderings and\npresent a new summary metric to balance existing automatic indicators,\nproviding a more holistic evaluation of text generation quality. Our\nexperiments demonstrate that the proposed approaches offer a robust way to\ncompare decoding strategies and serve as valuable tools to guide model\nselection for open-ended text generation tasks. We suggest future directions\nfor improving evaluation methodologies in text generation and make our code,\ndatasets, and models publicly available.\n","authors":["Esteban Garces Arias","Hannah Blocher","Julian Rodemann","Meimingwei Li","Christian Heumann","Matthias Aßenmacher"],"pdf_url":"https://arxiv.org/pdf/2410.18653v3.pdf","comment":"Accepted at the $GEM^2$ Workshop (co-located with ACL 2025)"},{"id":"http://arxiv.org/abs/2506.14731v1","updated":"2025-06-17T17:12:34Z","published":"2025-06-17T17:12:34Z","title":"Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\n  for LLMs","summary":"  We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model\noptimized via reinforcement learning (RL) to achieve efficient and robust\nreasoning capabilities. Built upon the publicly available Ling-lite model, a\n16.8 billion parameter model with 2.75 billion activated parameters, our\napproach matches the performance of state-of-the-art (SOTA) small-scale\nreasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,\nGPQA-Diamond) while activating only one-third of the parameters required by\ncomparable models. To accomplish this, we introduce a joint training pipeline\nintegrating distillation with RL, revealing undocumented challenges in MoE RL\ntraining. First, we identify optimization instability during RL training, and\nwe propose Constrained Contextual Computation Policy Optimization(C3PO), a\nnovel approach that enhances training stability and improves computational\nthroughput via algorithm-system co-design methodology. Second, we empirically\ndemonstrate that selecting distillation checkpoints based on entropy loss for\nRL training, rather than validation metrics, yields superior\nperformance-efficiency trade-offs in subsequent RL training. Finally, we\ndevelop a two-stage training paradigm to harmonize multi-domain data\nintegration, addressing domain conflicts that arise in training with mixed\ndataset. We will release the model, dataset, and code.\n","authors":[" Ring Team","Bin Hu","Cai Chen","Deng Zhao","Ding Liu","Dingnan Jin","Feng Zhu","Hao Dai","Hongzhi Luan","Jia Guo","Jiaming Liu","Jiewei Wu","Jun Mei","Jun Zhou","Junbo Zhao","Junwu Xiong","Kaihong Zhang","Kuan Xu","Lei Liang","Liang Jiang","Liangcheng Fu","Longfei Zheng","Qiang Gao","Qing Cui","Quan Wan","Shaomian Zheng","Shuaicheng Li","Tongkai Yang","Wang Ren","Xiaodong Yan","Xiaopei Wan","Xiaoyun Feng","Xin Zhao","Xinxing Yang","Xinyu Kong","Xuemin Yang","Yang Li","Yingting Wu","Yongkang Liu","Zhankai Xu","Zhenduo Zhang","Zhenglei Zhou","Zhenyu Huang","Zhiqiang Zhang","Zihao Wang","Zujie Wen"],"pdf_url":"https://arxiv.org/pdf/2506.14731v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2406.13677v3","updated":"2025-06-17T17:06:35Z","published":"2024-06-19T16:30:58Z","title":"Leveraging Large Language Models to Measure Gender Representation Bias\n  in Gendered Language Corpora","summary":"  Large language models (LLMs) often inherit and amplify social biases embedded\nin their training data. A prominent social bias is gender bias. In this regard,\nprior work has mainly focused on gender stereotyping bias - the association of\nspecific roles or traits with a particular gender - in English and on\nevaluating gender bias in model embeddings or generated outputs. In contrast,\ngender representation bias - the unequal frequency of references to individuals\nof different genders - in the training corpora has received less attention. Yet\nsuch imbalances in the training data constitute an upstream source of bias that\ncan propagate and intensify throughout the entire model lifecycle. To fill this\ngap, we propose a novel LLM-based method to detect and quantify gender\nrepresentation bias in LLM training data in gendered languages, where\ngrammatical gender challenges the applicability of methods developed for\nEnglish. By leveraging the LLMs' contextual understanding, our approach\nautomatically identifies and classifies person-referencing words in gendered\nlanguage corpora. Applied to four Spanish-English benchmarks and five Valencian\ncorpora, our method reveals substantial male-dominant imbalances. We show that\nsuch biases in training data affect model outputs, but can surprisingly be\nmitigated leveraging small-scale training on datasets that are biased towards\nthe opposite gender. Our findings highlight the need for corpus-level gender\nbias analysis in multilingual NLP. We make our code and data publicly\navailable.\n","authors":["Erik Derner","Sara Sansalvador de la Fuente","Yoan Gutiérrez","Paloma Moreda","Nuria Oliver"],"pdf_url":"https://arxiv.org/pdf/2406.13677v3.pdf","comment":"Accepted for presentation at the 6th Workshop on Gender Bias in\n  Natural Language Processing (GeBNLP) at ACL 2025"},{"id":"http://arxiv.org/abs/2402.10735v4","updated":"2025-06-17T17:05:26Z","published":"2024-02-16T14:52:05Z","title":"Assessing the Reasoning Capabilities of LLMs in the context of\n  Evidence-based Claim Verification","summary":"  Although LLMs have shown great performance on Mathematics and Coding related\nreasoning tasks, the reasoning capabilities of LLMs regarding other forms of\nreasoning are still an open problem. Here, we examine the issue of reasoning\nfrom the perspective of claim verification. We propose a framework designed to\nbreak down any claim paired with evidence into atomic reasoning types that are\nnecessary for verification. We use this framework to create RECV, the first\nclaim verification benchmark, incorporating real-world claims, to assess the\ndeductive and abductive reasoning capabilities of LLMs. The benchmark comprises\nof three datasets, covering reasoning problems of increasing complexity. We\nevaluate three state-of-the-art proprietary LLMs under multiple prompt\nsettings. Our results show that while LLMs can address deductive reasoning\nproblems, they consistently fail in cases of abductive reasoning. Moreover, we\nobserve that enhancing LLMs with rationale generation is not always beneficial.\nNonetheless, we find that generated rationales are semantically similar to\nthose provided by humans, especially in deductive reasoning cases.\n","authors":["John Dougrez-Lewis","Mahmud Elahi Akhter","Federico Ruggeri","Sebastian Löbbers","Yulan He","Maria Liakata"],"pdf_url":"https://arxiv.org/pdf/2402.10735v4.pdf","comment":"First two authors contributed equally to this work. 25 pages, 3\n  figure"},{"id":"http://arxiv.org/abs/2506.08001v3","updated":"2025-06-17T16:44:36Z","published":"2025-06-09T17:59:34Z","title":"Reparameterized LLM Training via Orthogonal Equivalence Transformation","summary":"  While large language models (LLMs) are driving the rapid advancement of\nartificial intelligence, effectively and reliably training these large models\nremains one of the field's most significant challenges. To address this\nchallenge, we propose POET, a novel reParameterized training algorithm that\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\nfixed random weight matrix. Because of its provable preservation of spectral\nproperties of weight matrices, POET can stably optimize the objective function\nwith improved generalization. We further develop efficient approximations that\nmake POET flexible and scalable for training large-scale neural networks.\nExtensive experiments validate the effectiveness and scalability of POET in\ntraining LLMs.\n","authors":["Zeju Qiu","Simon Buchholz","Tim Z. Xiao","Maximilian Dax","Bernhard Schölkopf","Weiyang Liu"],"pdf_url":"https://arxiv.org/pdf/2506.08001v3.pdf","comment":"Technical report v3 (38 pages, 26 figures, project page:\n  https://spherelab.ai/poet/, v3: added singular spectrum and energy analyses\n  in Section 4)"},{"id":"http://arxiv.org/abs/2506.14704v1","updated":"2025-06-17T16:42:54Z","published":"2025-06-17T16:42:54Z","title":"Capacity Matters: a Proof-of-Concept for Transformer Memorization on\n  Real-World Data","summary":"  This paper studies how the model architecture and data configurations\ninfluence the empirical memorization capacity of generative transformers. The\nmodels are trained using synthetic text datasets derived from the Systematized\nNomenclature of Medicine (SNOMED) knowledge graph: triplets, representing\nstatic connections, and sequences, simulating complex relation patterns. The\nresults show that embedding size is the primary determinant of learning speed\nand capacity, while additional layers provide limited benefits and may hinder\nperformance on simpler datasets. Activation functions play a crucial role, and\nSoftmax demonstrates greater stability and capacity. Furthermore, increasing\nthe complexity of the data set seems to improve the final memorization. These\ninsights improve our understanding of transformer memory mechanisms and provide\na framework for optimizing model design with structured real-world data.\n","authors":["Anton Changalidis","Aki Härmä"],"pdf_url":"https://arxiv.org/pdf/2506.14704v1.pdf","comment":"This work has been accepted for publication at the First Workshop on\n  Large Language Model Memorization (L2M2) at ACL 2025, Vienna, Austria"},{"id":"http://arxiv.org/abs/2506.14702v1","updated":"2025-06-17T16:40:42Z","published":"2025-06-17T16:40:42Z","title":"Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time\n  Markers","summary":"  One of the most profound challenges of modern machine learning is performing\nwell on the long-tail of rare and underrepresented features. Large\ngeneral-purpose models are trained for many tasks, but work best on\nhigh-frequency use cases. After training, it is hard to adapt a model to\nperform well on specific use cases underrepresented in the training corpus.\nRelying on prompt engineering or few-shot examples to maximize the output\nquality on a particular test case can be frustrating, as models can be highly\nsensitive to small changes, react in unpredicted ways or rely on a fixed system\nprompt for maintaining performance. In this work, we ask: \"Can we optimize our\ntraining protocols to both improve controllability and performance on\nunderrepresented use cases at inference time?\" We revisit the divide between\ntraining and inference techniques to improve long-tail performance while\nproviding users with a set of control levers the model is trained to be\nresponsive to. We create a detailed taxonomy of data characteristics and task\nprovenance to explicitly control generation attributes and implicitly condition\ngenerations at inference time. We fine-tune a base model to infer these markers\nautomatically, which makes them optional at inference time. This principled and\nflexible approach yields pronounced improvements in performance, especially on\nexamples from the long tail of the training distribution. While we observe an\naverage lift of 5.7% win rates in open-ended generation quality with our\nmarkers, we see over 9.1% gains in underrepresented domains. We also observe\nrelative lifts of up to 14.1% on underrepresented tasks like CodeRepair and\nabsolute improvements of 35.3% on length instruction following evaluations.\n","authors":["Daniel D'souza","Julia Kreutzer","Adrien Morisot","Ahmet Üstün","Sara Hooker"],"pdf_url":"https://arxiv.org/pdf/2506.14702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11423v4","updated":"2025-06-17T16:39:08Z","published":"2024-06-17T11:22:04Z","title":"Bridging Social Media and Search Engines: Dredge Words and the Detection\n  of Unreliable Domains","summary":"  Proactive content moderation requires platforms to rapidly and continuously\nevaluate the credibility of websites. Leveraging the direct and indirect paths\nusers follow to unreliable websites, we develop a website credibility\nclassification and discovery system that integrates both webgraph and\nlarge-scale social media contexts. We additionally introduce the concept of\ndredge words, terms or phrases for which unreliable domains rank highly on\nsearch engines, and provide the first exploration of their usage on social\nmedia. Our graph neural networks that combine webgraph and social media\ncontexts generate to state-of-the-art results in website credibility\nclassification and significantly improves the top-k identification of\nunreliable domains. Additionally, we release a novel dataset of dredge words,\nhighlighting their strong connections to both social media and online commerce\nplatforms.\n","authors":["Evan M. Williams","Peter Carragher","Kathleen M. Carley"],"pdf_url":"https://arxiv.org/pdf/2406.11423v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.10970v3","updated":"2025-06-17T16:38:16Z","published":"2025-01-19T07:09:11Z","title":"The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically\n  Justify Replacing Human Annotators with LLMs","summary":"  The \"LLM-as-an-annotator\" and \"LLM-as-a-judge\" paradigms employ Large\nLanguage Models (LLMs) as annotators, judges, and evaluators in tasks\ntraditionally performed by humans. LLM annotations are widely used, not only in\nNLP research but also in fields like medicine, psychology, and social science.\nDespite their role in shaping study results and insights, there is no standard\nor rigorous procedure to determine whether LLMs can replace human annotators.\nIn this paper, we propose a novel statistical procedure, the Alternative\nAnnotator Test (alt-test), that requires only a modest subset of annotated\nexamples to justify using LLM annotations. Additionally, we introduce a\nversatile and interpretable measure for comparing LLM annotators and judges. To\ndemonstrate our procedure, we curated a diverse collection of ten datasets,\nconsisting of language and vision-language tasks, and conducted experiments\nwith six LLMs and four prompting techniques. Our results show that LLMs can\nsometimes replace humans with closed-source LLMs (such as GPT-4o),\noutperforming the open-source LLMs we examine, and that prompting techniques\nyield judges of varying quality. We hope this study encourages more rigorous\nand reliable practices.\n","authors":["Nitay Calderon","Roi Reichart","Rotem Dror"],"pdf_url":"https://arxiv.org/pdf/2501.10970v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05478v2","updated":"2025-06-17T16:28:39Z","published":"2025-01-07T16:01:25Z","title":"Language and Planning in Robotic Navigation: A Multilingual Evaluation\n  of State-of-the-Art Models","summary":"  Large Language Models (LLMs) such as GPT-4, trained on huge amount of\ndatasets spanning multiple domains, exhibit significant reasoning,\nunderstanding, and planning capabilities across various tasks. This study\npresents the first-ever work in Arabic language integration within the\nVision-and-Language Navigation (VLN) domain in robotics, an area that has been\nnotably underexplored in existing research. We perform a comprehensive\nevaluation of state-of-the-art multi-lingual Small Language Models (SLMs),\nincluding GPT-4o mini, Llama 3 8B, and Phi-3 medium 14B, alongside the\nArabic-centric LLM, Jais. Our approach utilizes the NavGPT framework, a pure\nLLM-based instruction-following navigation agent, to assess the impact of\nlanguage on navigation reasoning through zero-shot sequential action prediction\nusing the R2R dataset. Through comprehensive experiments, we demonstrate that\nour framework is capable of high-level planning for navigation tasks when\nprovided with instructions in both English and Arabic. However, certain models\nstruggled with reasoning and planning in the Arabic language due to inherent\nlimitations in their capabilities, sub-optimal performance, and parsing issues.\nThese findings highlight the importance of enhancing planning and reasoning\ncapabilities in language models for effective navigation, emphasizing this as a\nkey area for further development while also unlocking the potential of\nArabic-language models for impactful real-world applications.\n","authors":["Malak Mansour","Ahmed Aly","Bahey Tharwat","Sarim Hashmi","Dong An","Ian Reid"],"pdf_url":"https://arxiv.org/pdf/2501.05478v2.pdf","comment":"This work has been accepted for presentation at LM4Plan@AAAI'25. For\n  more details, please check: https://llmforplanning.github.io/"},{"id":"http://arxiv.org/abs/2501.04227v2","updated":"2025-06-17T16:19:14Z","published":"2025-01-08T01:58:42Z","title":"Agent Laboratory: Using LLM Agents as Research Assistants","summary":"  Historically, scientific discovery has been a lengthy and costly process,\ndemanding substantial time and resources from initial conception to final\nresults. To accelerate scientific discovery, reduce research costs, and improve\nresearch quality, we introduce Agent Laboratory, an autonomous LLM-based\nframework capable of completing the entire research process. This framework\naccepts a human-provided research idea and progresses through three\nstages--literature review, experimentation, and report writing to produce\ncomprehensive research outputs, including a code repository and a research\nreport, while enabling users to provide feedback and guidance at each stage. We\ndeploy Agent Laboratory with various state-of-the-art LLMs and invite multiple\nresearchers to assess its quality by participating in a survey, providing human\nfeedback to guide the research process, and then evaluate the final paper. We\nfound that: (1) Agent Laboratory driven by o1-preview generates the best\nresearch outcomes; (2) The generated machine learning code is able to achieve\nstate-of-the-art performance compared to existing methods; (3) Human\ninvolvement, providing feedback at each stage, significantly improves the\noverall quality of research; (4) Agent Laboratory significantly reduces\nresearch expenses, achieving an 84% decrease compared to previous autonomous\nresearch methods. We hope Agent Laboratory enables researchers to allocate more\neffort toward creative ideation rather than low-level coding and writing,\nultimately accelerating scientific discovery.\n","authors":["Samuel Schmidgall","Yusheng Su","Ze Wang","Ximeng Sun","Jialian Wu","Xiaodong Yu","Jiang Liu","Michael Moor","Zicheng Liu","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2501.04227v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14681v1","updated":"2025-06-17T16:13:15Z","published":"2025-06-17T16:13:15Z","title":"Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and\n  Training Factors Shape LLM Alignment Quality","summary":"  Supervised fine-tuning (SFT) is a critical step in aligning large language\nmodels (LLMs) with human instructions and values, yet many aspects of SFT\nremain poorly understood. We trained a wide range of base models on a variety\nof datasets including code generation, mathematical reasoning, and\ngeneral-domain tasks, resulting in 1,000+ SFT models under controlled\nconditions. We then identified the dataset properties that matter most and\nexamined the layer-wise modifications introduced by SFT. Our findings reveal\nthat some training-task synergies persist across all models while others vary\nsubstantially, emphasizing the importance of model-specific strategies.\nMoreover, we demonstrate that perplexity consistently predicts SFT\neffectiveness--often surpassing superficial similarity between trained data and\nbenchmark--and that mid-layer weight changes correlate most strongly with\nperformance gains. We will release these 1,000+ SFT models and benchmark\nresults to accelerate further research.\n","authors":["Yuto Harada","Yusuke Yamauchi","Yusuke Oda","Yohei Oseki","Yusuke Miyao","Yu Takagi"],"pdf_url":"https://arxiv.org/pdf/2506.14681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10867v2","updated":"2025-06-17T16:07:14Z","published":"2023-07-20T13:40:22Z","title":"FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with\n  Human Feedback","summary":"  Captions are crucial for understanding scientific visualizations and\ndocuments. Existing captioning methods for scientific figures rely on\nfigure-caption pairs extracted from documents for training, many of which fall\nshort with respect to metrics like helpfulness, explainability, and\nvisual-descriptiveness [15] leading to generated captions being misaligned with\nreader preferences. To enable the generation of high-quality figure captions,\nwe introduce FigCaps-HF a new framework for figure-caption generation that can\nincorporate domain expert feedback in generating captions optimized for reader\npreferences. Our framework comprises of 1) an automatic method for evaluating\nquality of figure-caption pairs, 2) a novel reinforcement learning with human\nfeedback (RLHF) method to optimize a generative figure-to-caption model for\nreader preferences. We demonstrate the effectiveness of our simple learning\nframework by improving performance over standard fine-tuning across different\ntypes of models. In particular, when using BLIP as the base model, our RLHF\nframework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and\nMeteor, respectively. Finally, we release a large-scale benchmark dataset with\nhuman feedback on figure-caption pairs to enable further evaluation and\ndevelopment of RLHF techniques for this problem.\n","authors":["Ashish Singh","Ashutosh Singh","Prateek Agarwal","Zixuan Huang","Arpita Singh","Tong Yu","Sungchul Kim","Victor Bursztyn","Nesreen K. Ahmed","Puneet Mathur","Erik Learned-Miller","Franck Dernoncourt","Ryan A. Rossi"],"pdf_url":"https://arxiv.org/pdf/2307.10867v2.pdf","comment":"16 pages, 4 figures. Benchmark Documentation:\n  https://figcapshf.github.io/"},{"id":"http://arxiv.org/abs/2506.11681v2","updated":"2025-06-17T15:59:13Z","published":"2025-06-13T11:19:27Z","title":"A Hybrid Multi-Agent Prompting Approach for Simplifying Complex\n  Sentences","summary":"  This paper addresses the challenge of transforming complex sentences into\nsequences of logical, simplified sentences while preserving semantic and\nlogical integrity with the help of Large Language Models. We propose a hybrid\napproach that combines advanced prompting with multi-agent architectures to\nenhance the sentence simplification process. Experimental results show that our\napproach was able to successfully simplify 70% of the complex sentences written\nfor video game design application. In comparison, a single-agent approach\nattained a 48% success rate on the same task.\n","authors":["Pratibha Zunjare","Michael Hsiao"],"pdf_url":"https://arxiv.org/pdf/2506.11681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06745v2","updated":"2025-06-17T15:57:52Z","published":"2024-12-09T18:37:14Z","title":"ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended\n  Capabilities","summary":"  Traditional fixed test sets fall short in evaluating open-ended capabilities\nof foundation models. To address this, we propose ONEBench(OpeN-Ended\nBenchmarking), a new testing paradigm that consolidates individual evaluation\ndatasets into a unified, ever-expanding sample pool. ONEBench allows users to\ngenerate custom, open-ended evaluation benchmarks from this pool, corresponding\nto specific capabilities of interest. By aggregating samples across test sets,\nONEBench enables the assessment of diverse capabilities beyond those covered by\nthe original test sets, while mitigating overfitting and dataset bias. Most\nimportantly, it frames model evaluation as a collective process of selecting\nand aggregating sample-level tests.\n  The shift from task-specific benchmarks to ONEBench introduces two\nchallenges: (1)heterogeneity and (2)incompleteness. Heterogeneity refers to the\naggregation over diverse metrics, while incompleteness describes comparing\nmodels evaluated on different data subsets. To address these challenges, we\nexplore algorithms to aggregate sparse measurements into reliable model scores.\nOur aggregation algorithm ensures identifiability(asymptotically recovering\nground-truth scores) and rapid convergence, enabling accurate model ranking\nwith less data. On homogenous datasets, we show our aggregation algorithm\nprovides rankings that highly correlate with those produced by average scores.\nWe also demonstrate robustness to ~95% of measurements missing, reducing\nevaluation cost by up to 20x with little-to-no change in model rankings. We\nintroduce ONEBench-LLM for language models and ONEBench-LMM for vision-language\nmodels, unifying evaluations across these domains. Overall, we present a\ntechnique for open-ended evaluation, which can aggregate over incomplete,\nheterogeneous sample-level measurements to continually grow a benchmark\nalongside the rapidly developing foundation models.\n","authors":["Adhiraj Ghosh","Sebastian Dziadzio","Ameya Prabhu","Vishaal Udandarao","Samuel Albanie","Matthias Bethge"],"pdf_url":"https://arxiv.org/pdf/2412.06745v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06987v4","updated":"2025-06-17T15:43:40Z","published":"2025-05-11T14:13:58Z","title":"Convert Language Model into a Value-based Strategic Planner","summary":"  Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Q-learning on LLMs, and propose a framework called straQ*. Our\nframework allows a plug-and-play LLM to bootstrap the planning during ESC,\ndetermine the optimal strategy based on long-term returns, and finally guide\nthe LLM to response. Substantial experiments on ESC datasets suggest that\nstraQ* outperforms many baselines, including direct inference, self-refine,\nchain of thought, finetuning, and finite state machines.\n","authors":["Xiaoyu Wang","Yue Zhao","Qingqing Gu","Zhonglin Jiang","Xiaokai Chen","Yong Chen","Luo Ji"],"pdf_url":"https://arxiv.org/pdf/2505.06987v4.pdf","comment":"13 pages, 6 figures, Accepted by ACL 2025 Industry Track"},{"id":"http://arxiv.org/abs/2506.14646v1","updated":"2025-06-17T15:41:33Z","published":"2025-06-17T15:41:33Z","title":"GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel\n  Optimization with GuidedSelection Vectors","summary":"  Parameter-efficient fine-tuning (PEFT) methods, particularly Low-Rank\nAdaptation (LoRA), offer an efficient way to adapt large language models with\nreduced computational costs. However, their performance is limited by the small\nnumber of trainable parameters. Recent work combines LoRA with the\nMixture-of-Experts (MoE), i.e., LoRA-MoE, to enhance capacity, but two\nlimitations remain in hindering the full exploitation of its potential: 1) the\ninfluence of downstream tasks when assigning expert numbers, and 2) the uniform\nrank assignment across all LoRA experts, which restricts representational\ndiversity. To mitigate these gaps, we propose GuiLoMo, a fine-grained\nlayer-wise expert numbers and ranks allocation strategy with GuidedSelection\nVectors (GSVs). GSVs are learned via a prior bilevel optimization process to\ncapture both model- and task-specific needs, and are then used to allocate\noptimal expert numbers and ranks. Experiments on three backbone models across\ndiverse benchmarks show that GuiLoMo consistently achieves superior or\ncomparable performance to all baselines. Further analysis offers key insights\ninto how expert numbers and ranks vary across layers and tasks, highlighting\nthe benefits of adaptive expert configuration. Our code is available at\nhttps://github.com/Liar406/Gui-LoMo.git.\n","authors":["Hengyuan Zhang","Xinrong Chen","Yingmin Qiu","Xiao Liang","Ziyue Li","Guanyu Wang","Weiping Li","Tong Mo","Wenyue Li","Hayden Kwok-Hay So","Ngai Wong"],"pdf_url":"https://arxiv.org/pdf/2506.14646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14645v1","updated":"2025-06-17T15:41:26Z","published":"2025-06-17T15:41:26Z","title":"Passing the Turing Test in Political Discourse: Fine-Tuning LLMs to\n  Mimic Polarized Social Media Comments","summary":"  The increasing sophistication of large language models (LLMs) has sparked\ngrowing concerns regarding their potential role in exacerbating ideological\npolarization through the automated generation of persuasive and biased content.\nThis study explores the extent to which fine-tuned LLMs can replicate and\namplify polarizing discourse within online environments. Using a curated\ndataset of politically charged discussions extracted from Reddit, we fine-tune\nan open-source LLM to produce context-aware and ideologically aligned\nresponses. The model's outputs are evaluated through linguistic analysis,\nsentiment scoring, and human annotation, with particular attention to\ncredibility and rhetorical alignment with the original discourse. The results\nindicate that, when trained on partisan data, LLMs are capable of producing\nhighly plausible and provocative comments, often indistinguishable from those\nwritten by humans. These findings raise significant ethical questions about the\nuse of AI in political discourse, disinformation, and manipulation campaigns.\nThe paper concludes with a discussion of the broader implications for AI\ngovernance, platform regulation, and the development of detection tools to\nmitigate adversarial fine-tuning risks.\n","authors":[". Pazzaglia","V. Vendetti","L. D. Comencini","F. Deriu","V. Modugno"],"pdf_url":"https://arxiv.org/pdf/2506.14645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14641v1","updated":"2025-06-17T15:39:33Z","published":"2025-06-17T15:39:33Z","title":"Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than\n  Few-shot","summary":"  In-Context Learning (ICL) is an essential emergent ability of Large Language\nModels (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars\nof ICL to enhance the reasoning capability, especially in mathematics tasks.\nHowever, given the continuous advancement of model capabilities, it remains\nunclear whether CoT exemplars still benefit recent, stronger models in such\ntasks. Through systematic experiments, we find that for recent strong models\nsuch as the Qwen2.5 series, adding traditional CoT exemplars does not improve\nreasoning performance compared to Zero-Shot CoT. Instead, their primary\nfunction is to align the output format with human expectations. We further\ninvestigate the effectiveness of enhanced CoT exemplars, constructed using\nanswers from advanced models such as \\texttt{Qwen2.5-Max} and\n\\texttt{DeepSeek-R1}. Experimental results indicate that these enhanced\nexemplars still fail to improve the model's reasoning performance. Further\nanalysis reveals that models tend to ignore the exemplars and focus primarily\non the instructions, leading to no observable gain in reasoning ability.\nOverall, our findings highlight the limitations of the current ICL+CoT\nframework in mathematical reasoning, calling for a re-examination of the ICL\nparadigm and the definition of exemplars.\n","authors":["Xiang Cheng","Chengyan Pan","Minjun Zhao","Deyang Li","Fangchao Liu","Xinyu Zhang","Xiao Zhang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2506.14641v1.pdf","comment":"19 pages,22 figures"},{"id":"http://arxiv.org/abs/2505.12442v3","updated":"2025-06-17T15:37:58Z","published":"2025-05-18T14:31:45Z","title":"IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems","summary":"  The rapid advancement of Large Language Models (LLMs) has led to the\nemergence of Multi-Agent Systems (MAS) to perform complex tasks through\ncollaboration. However, the intricate nature of MAS, including their\narchitecture and agent interactions, raises significant concerns regarding\nintellectual property (IP) protection. In this paper, we introduce MASLEAK, a\nnovel attack framework designed to extract sensitive information from MAS\napplications. MASLEAK targets a practical, black-box setting, where the\nadversary has no prior knowledge of the MAS architecture or agent\nconfigurations. The adversary can only interact with the MAS through its public\nAPI, submitting attack query $q$ and observing outputs from the final agent.\nInspired by how computer worms propagate and infect vulnerable network hosts,\nMASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain\nresponses from each MAS agent that reveal a full set of proprietary components,\nincluding the number of agents, system topology, system prompts, task\ninstructions, and tool usages. We construct the first synthetic dataset of MAS\napplications with 810 applications and also evaluate MASLEAK against real-world\nMAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in\nextracting MAS IP, with an average attack success rate of 87% for system\nprompts and task instructions, and 92% for system architecture in most cases.\nWe conclude by discussing the implications of our findings and the potential\ndefenses.\n","authors":["Liwen Wang","Wenxuan Wang","Shuai Wang","Zongjie Li","Zhenlan Ji","Zongyi Lyu","Daoyuan Wu","Shing-Chi Cheung"],"pdf_url":"https://arxiv.org/pdf/2505.12442v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14986v3","updated":"2025-06-17T15:36:53Z","published":"2024-06-21T08:56:35Z","title":"Do Large Language Models Exhibit Cognitive Dissonance? Studying the\n  Difference Between Revealed Beliefs and Stated Answers","summary":"  Multiple Choice Questions (MCQ) have become a commonly used approach to\nassess the capabilities of Large Language Models (LLMs), due to their ease of\nmanipulation and evaluation. The experimental appraisals of the LLMs' Stated\nAnswer (their answer to MCQ) have pointed to their apparent ability to perform\nprobabilistic reasoning or to grasp uncertainty. In this work, we investigate\nwhether these aptitudes are measurable outside tailored prompting and MCQ by\nreformulating these issues as direct text-completion - the fundamental\ncomputational unit of LLMs. We introduce Revealed Belief, an evaluation\nframework that evaluates LLMs on tasks requiring reasoning under uncertainty,\nwhich complements MCQ scoring by analyzing text-completion probability\ndistributions. Our findings suggest that while LLMs frequently state the\ncorrect answer, their Revealed Belief shows that they often allocate\nprobability mass inconsistently, exhibit systematic biases, and often fail to\nupdate their beliefs appropriately when presented with new evidence, leading to\nstrong potential impacts on downstream tasks. These results suggest that common\nevaluation methods may only provide a partial picture and that more research is\nneeded to assess the extent and nature of their capabilities.\n","authors":["Manuel Mondal","Ljiljana Dolamic","Gérôme Bovet","Philippe Cudré-Mauroux","Julien Audiffren"],"pdf_url":"https://arxiv.org/pdf/2406.14986v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14634v1","updated":"2025-06-17T15:28:53Z","published":"2025-06-17T15:28:53Z","title":"AIn't Nothing But a Survey? Using Large Language Models for Coding\n  German Open-Ended Survey Responses on Survey Motivation","summary":"  The recent development and wider accessibility of LLMs have spurred\ndiscussions about how they can be used in survey research, including\nclassifying open-ended survey responses. Due to their linguistic capacities, it\nis possible that LLMs are an efficient alternative to time-consuming manual\ncoding and the pre-training of supervised machine learning models. As most\nexisting research on this topic has focused on English-language responses\nrelating to non-complex topics or on single LLMs, it is unclear whether its\nfindings generalize and how the quality of these classifications compares to\nestablished methods. In this study, we investigate to what extent different\nLLMs can be used to code open-ended survey responses in other contexts, using\nGerman data on reasons for survey participation as an example. We compare\nseveral state-of-the-art LLMs and several prompting approaches, and evaluate\nthe LLMs' performance by using human expert codings. Overall performance\ndiffers greatly between LLMs, and only a fine-tuned LLM achieves satisfactory\nlevels of predictive performance. Performance differences between prompting\napproaches are conditional on the LLM used. Finally, LLMs' unequal\nclassification performance across different categories of reasons for survey\nparticipation results in different categorical distributions when not using\nfine-tuning. We discuss the implications of these findings, both for\nmethodological research on coding open-ended responses and for their\nsubstantive analysis, and for practitioners processing or substantively\nanalyzing such data. Finally, we highlight the many trade-offs researchers need\nto consider when choosing automated methods for open-ended response\nclassification in the age of LLMs. In doing so, our study contributes to the\ngrowing body of research about the conditions under which LLMs can be\nefficiently, accurately, and reliably leveraged in survey research.\n","authors":["Leah von der Heyde","Anna-Carolina Haensch","Bernd Weiß","Jessika Daikeler"],"pdf_url":"https://arxiv.org/pdf/2506.14634v1.pdf","comment":"to appear in Survey Research Methods"},{"id":"http://arxiv.org/abs/2506.13674v2","updated":"2025-06-17T15:25:25Z","published":"2025-06-16T16:30:26Z","title":"Prefix-Tuning+: Modernizing Prefix-Tuning by Decoupling the Prefix from\n  Attention","summary":"  Parameter-Efficient Fine-Tuning (PEFT) methods have become crucial for\nrapidly adapting large language models (LLMs) to downstream tasks.\nPrefix-Tuning, an early and effective PEFT technique, demonstrated the ability\nto achieve performance comparable to full fine-tuning with significantly\nreduced computational and memory overhead. However, despite its earlier\nsuccess, its effectiveness in training modern state-of-the-art LLMs has been\nvery limited. In this work, we demonstrate empirically that Prefix-Tuning\nunderperforms on LLMs because of an inherent tradeoff between input and prefix\nsignificance within the attention head. This motivates us to introduce\nPrefix-Tuning+, a novel architecture that generalizes the principles of\nPrefix-Tuning while addressing its shortcomings by shifting the prefix module\nout of the attention head itself. We further provide an overview of our\nconstruction process to guide future users when constructing their own\ncontext-based methods. Our experiments show that, across a diverse set of\nbenchmarks, Prefix-Tuning+ consistently outperforms existing Prefix-Tuning\nmethods. Notably, it achieves performance on par with the widely adopted LoRA\nmethod on several general benchmarks, highlighting the potential modern\nextension of Prefix-Tuning approaches. Our findings suggest that by overcoming\nits inherent limitations, Prefix-Tuning can remain a competitive and relevant\nresearch direction in the landscape of parameter-efficient LLM adaptation.\n","authors":["Haonan Wang","Brian Chen","Siquan Li","Xinhe Liang","Hwee Kuan Lee","Kenji Kawaguchi","Tianyang Hu"],"pdf_url":"https://arxiv.org/pdf/2506.13674v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14629v1","updated":"2025-06-17T15:24:30Z","published":"2025-06-17T15:24:30Z","title":"VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based\n  Mosquito Breeding Site Detection and Reasoning","summary":"  Mosquito-borne diseases pose a major global health risk, requiring early\ndetection and proactive control of breeding sites to prevent outbreaks. In this\npaper, we present VisText-Mosquito, a multimodal dataset that integrates visual\nand textual data to support automated detection, segmentation, and reasoning\nfor mosquito breeding site analysis. The dataset includes 1,828 annotated\nimages for object detection, 142 images for water surface segmentation, and\nnatural language reasoning texts linked to each image. The YOLOv9s model\nachieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object\ndetection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and\nmAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves\na final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and\nROUGE-L of 0.87. This dataset and model framework emphasize the theme\n\"Prevention is Better than Cure\", showcasing how AI-based detection can\nproactively address mosquito-borne disease risks. The dataset and\nimplementation code are publicly available at GitHub:\nhttps://github.com/adnanul-islam-jisun/VisText-Mosquito\n","authors":["Md. Adnanul Islam","Md. Faiyaz Abdullah Sayeedi","Md. Asaduzzaman Shuvo","Muhammad Ziaur Rahman","Shahanur Rahman Bappy","Raiyan Rahman","Swakkhar Shatabda"],"pdf_url":"https://arxiv.org/pdf/2506.14629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04619v2","updated":"2025-06-17T15:22:43Z","published":"2025-03-06T17:05:33Z","title":"SynGraph: A Dynamic Graph-LLM Synthesis Framework for Sparse Streaming\n  User Sentiment Modeling","summary":"  User reviews on e-commerce platforms exhibit dynamic sentiment patterns\ndriven by temporal and contextual factors. Traditional sentiment analysis\nmethods focus on static reviews, failing to capture the evolving temporal\nrelationship between user sentiment rating and textual content. Sentiment\nanalysis on streaming reviews addresses this limitation by modeling and\npredicting the temporal evolution of user sentiments. However, it suffers from\ndata sparsity, manifesting in temporal, spatial, and combined forms. In this\npaper, we introduce SynGraph, a novel framework designed to address data\nsparsity in sentiment analysis on streaming reviews. SynGraph alleviates data\nsparsity by categorizing users into mid-tail, long-tail, and extreme scenarios\nand incorporating LLM-augmented enhancements within a dynamic graph-based\nstructure. Experiments on real-world datasets demonstrate its effectiveness in\naddressing sparsity and improving sentiment modeling in streaming reviews.\n","authors":["Xin Zhang","Qiyu Wei","Yingjie Zhu","Linhai Zhang","Deyu Zhou","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2503.04619v2.pdf","comment":"Accepted at ACL 2025"},{"id":"http://arxiv.org/abs/2506.14625v1","updated":"2025-06-17T15:22:21Z","published":"2025-06-17T15:22:21Z","title":"Probabilistic Aggregation and Targeted Embedding Optimization for\n  Collective Moral Reasoning in Large Language Models","summary":"  Large Language Models (LLMs) have shown impressive moral reasoning abilities.\nYet they often diverge when confronted with complex, multi-factor moral\ndilemmas. To address these discrepancies, we propose a framework that\nsynthesizes multiple LLMs' moral judgments into a collectively formulated moral\njudgment, realigning models that deviate significantly from this consensus. Our\naggregation mechanism fuses continuous moral acceptability scores (beyond\nbinary labels) into a collective probability, weighting contributions by model\nreliability. For misaligned models, a targeted embedding-optimization procedure\nfine-tunes token embeddings for moral philosophical theories, minimizing JS\ndivergence to the consensus while preserving semantic integrity. Experiments on\na large-scale social moral dilemma dataset show our approach builds robust\nconsensus and improves individual model fidelity. These findings highlight the\nvalue of data-driven moral alignment across multiple models and its potential\nfor safer, more consistent AI systems.\n","authors":["Chenchen Yuan","Zheyu Zhang","Shuo Yang","Bardh Prenkaj","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2506.14625v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2506.10055v2","updated":"2025-06-17T15:19:26Z","published":"2025-06-11T17:58:14Z","title":"TaskCraft: Automated Generation of Agentic Tasks","summary":"  Agentic tasks, which require multi-step problem solving with autonomy, tool\nuse, and adaptive reasoning, are becoming increasingly central to the\nadvancement of NLP and AI. However, existing instruction data lacks tool\ninteraction, and current agentic benchmarks rely on costly human annotation,\nlimiting their scalability. We introduce \\textsc{TaskCraft}, an automated\nworkflow for generating difficulty-scalable, multi-tool, and verifiable agentic\ntasks with execution trajectories. TaskCraft expands atomic tasks using\ndepth-based and width-based extensions to create structurally and\nhierarchically complex challenges. Empirical results show that these tasks\nimprove prompt optimization in the generation workflow and enhance supervised\nfine-tuning of agentic foundation models. We present a large-scale synthetic\ndataset of approximately 36,000 tasks with varying difficulty to support future\nresearch on agent tuning and evaluation.\n","authors":["Dingfeng Shi","Jingyi Cao","Qianben Chen","Weichen Sun","Weizhen Li","Hongxuan Lu","Fangchen Dong","Tianrui Qin","King Zhu","Minghao Liu","Jian Yang","Ge Zhang","Jiaheng Liu","Changwang Zhang","Jun Wang","Yuchen Eleanor Jiang","Wangchunshu Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.10055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00039v3","updated":"2025-06-17T15:18:57Z","published":"2025-04-29T18:36:57Z","title":"Graph RAG for Legal Norms: A Hierarchical, Temporal and Deterministic\n  Approach","summary":"  This article proposes an adaptation of Graph Retrieval-Augmented Generation\n(Graph RAG) specifically designed for the analysis and comprehension of legal\nnorms. Legal texts are characterized by a predefined hierarchical structure, an\nextensive network of references and a continuous evolution through multiple\ntemporal versions. This temporal dynamism poses a significant challenge for\nstandard AI systems, demanding a deterministic representation of the law at any\ngiven point in time. To address this, our approach grounds the knowledge graph\nconstruction in a formal, FRBRoo-inspired model that distinguishes abstract\nlegal works from their concrete textual expressions. We introduce a\nmulti-layered representation of Temporal Versions (capturing date-specific\nchanges) and Language Versions (capturing linguistic variations). By modeling\nnormative evolution as a precise sequence of these versioned entities, we\nenable the construction of a knowledge graph that serves as a verifiable\n\"ground truth\". This allows Large Language Models to generate responses based\non accurate, context-aware, and point-in-time correct legal information,\novercoming the risk of temporal inaccuracies. Through a detailed analysis of\nthis formal Graph RAG approach and its application to legal norm datasets, this\narticle aims to advance the field of Artificial Intelligence applied to Law,\ncreating opportunities for more effective and reliable systems in legal\nresearch, legislative analysis, and decision support.\n","authors":["Hudson de Martim"],"pdf_url":"https://arxiv.org/pdf/2505.00039v3.pdf","comment":"This version enhances the theoretical underpinnings of the proposed\n  Graph RAG methodology, including the introduction of a formal, FRBRoo-based\n  model for versioning, and enabling multi-language support for both content\n  and metadata"},{"id":"http://arxiv.org/abs/2506.14613v1","updated":"2025-06-17T15:12:54Z","published":"2025-06-17T15:12:54Z","title":"When Does Meaning Backfire? Investigating the Role of AMRs in NLI","summary":"  Natural Language Inference (NLI) relies heavily on adequately parsing the\nsemantic content of the premise and hypothesis. In this work, we investigate\nwhether adding semantic information in the form of an Abstract Meaning\nRepresentation (AMR) helps pretrained language models better generalize in NLI.\nOur experiments integrating AMR into NLI in both fine-tuning and prompting\nsettings show that the presence of AMR in fine-tuning hinders model\ngeneralization while prompting with AMR leads to slight gains in\n\\texttt{GPT-4o}. However, an ablation study reveals that the improvement comes\nfrom amplifying surface-level differences rather than aiding semantic\nreasoning. This amplification can mislead models to predict non-entailment even\nwhen the core meaning is preserved.\n","authors":["Junghyun Min","Xiulin Yang","Shira Wein"],"pdf_url":"https://arxiv.org/pdf/2506.14613v1.pdf","comment":"9 pages, 2 figures"},{"id":"http://arxiv.org/abs/2506.14606v1","updated":"2025-06-17T15:06:54Z","published":"2025-06-17T15:06:54Z","title":"Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees","summary":"  The hardware ecosystem is rapidly evolving, with increasing interest in\ntranslating low-level programs across different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\nlongevity of existing code. A particularly challenging class of this\ntranspilation problem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity, memory models, and execution paradigms. In this work, we introduce\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\ntranslation power of pre-trained large language models (LLMs) with the rigor of\nestablished software testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\nand benchmarks to establish a common foundation for ISA-level code translation\nresearch.\n","authors":["Ahmed Heakl","Sarim Hashmi","Chaimaa Abi","Celine Lee","Abdulrahman Mahmoud"],"pdf_url":"https://arxiv.org/pdf/2506.14606v1.pdf","comment":"Project page: https://ahmedheakl.github.io/Guaranteed-Guess/"},{"id":"http://arxiv.org/abs/2410.05243v3","updated":"2025-06-17T15:06:02Z","published":"2024-10-07T17:47:50Z","title":"Navigating the Digital World as Humans Do: Universal Visual Grounding\n  for GUI Agents","summary":"  Multimodal large language models (MLLMs) are transforming the capabilities of\ngraphical user interface (GUI) agents, facilitating their transition from\ncontrolled simulations to complex, real-world applications across various\nplatforms. However, the effectiveness of these agents hinges on the robustness\nof their grounding capability. Current GUI agents predominantly utilize\ntext-based representations such as HTML or accessibility trees, which, despite\ntheir utility, often introduce noise, incompleteness, and increased\ncomputational overhead. In this paper, we advocate a human-like embodiment for\nGUI agents that perceive the environment entirely visually and directly perform\npixel-level operations on the GUI. The key is visual grounding models that can\naccurately map diverse referring expressions of GUI elements to their\ncoordinates on the GUI across different platforms. We show that a simple\nrecipe, which includes web-based synthetic data and slight adaptation of the\nLLaVA architecture, is surprisingly effective for training such visual\ngrounding models. We collect the largest dataset for GUI visual grounding so\nfar, containing 10M GUI elements and their referring expressions over 1.3M\nscreenshots, and use it to train UGround, a strong universal visual grounding\nmodel for GUI agents. Empirical results on six benchmarks spanning three\ncategories (grounding, offline agent, and online agent) show that 1) UGround\nsubstantially outperforms existing visual grounding models for GUI agents, by\nup to 20% absolute, and 2) agents with UGround outperform state-of-the-art\nagents, despite the fact that existing agents use additional text-based input\nwhile ours only uses visual perception. These results provide strong support\nfor the feasibility and promises of GUI agents that navigate the digital world\nas humans do.\n","authors":["Boyu Gou","Ruohan Wang","Boyuan Zheng","Yanan Xie","Cheng Chang","Yiheng Shu","Huan Sun","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2410.05243v3.pdf","comment":"Accepted to ICLR 2025 (Oral). Project Homepage:\n  https://osu-nlp-group.github.io/UGround/"},{"id":"http://arxiv.org/abs/2506.14602v1","updated":"2025-06-17T15:05:57Z","published":"2025-06-17T15:05:57Z","title":"Computational Studies in Influencer Marketing: A Systematic Literature\n  Review","summary":"  Influencer marketing has become a crucial feature of digital marketing\nstrategies. Despite its rapid growth and algorithmic relevance, the field of\ncomputational studies in influencer marketing remains fragmented, especially\nwith limited systematic reviews covering the computational methodologies\nemployed. This makes overarching scientific measurements in the influencer\neconomy very scarce, to the detriment of interested stakeholders outside of\nplatforms themselves, such as regulators, but also researchers from other\nfields. This paper aims to provide an overview of the state of the art of\ncomputational studies in influencer marketing by conducting a systematic\nliterature review (SLR) based on the PRISMA model. The paper analyses 69\nstudies to identify key research themes, methodologies, and future directions\nin this research field. The review identifies four major research themes:\nInfluencer identification and characterisation, Advertising strategies and\nengagement, Sponsored content analysis and discovery, and Fairness.\nMethodologically, the studies are categorised into machine learning-based\ntechniques (e.g., classification, clustering) and non-machine-learning-based\ntechniques (e.g., statistical analysis, network analysis). Key findings reveal\na strong focus on optimising commercial outcomes, with limited attention to\nregulatory compliance and ethical considerations. The review highlights the\nneed for more nuanced computational research that incorporates contextual\nfactors such as language, platform, and industry type, as well as improved\nmodel explainability and dataset reproducibility. The paper concludes by\nproposing a multidisciplinary research agenda that emphasises the need for\nfurther links to regulation and compliance technology, finer granularity in\nanalysis, and the development of standardised datasets.\n","authors":["Haoyang Gui","Thales Bertaglia","Catalina Goanta","Gerasimos Spanakis"],"pdf_url":"https://arxiv.org/pdf/2506.14602v1.pdf","comment":"journal submission, under review"},{"id":"http://arxiv.org/abs/2501.18045v3","updated":"2025-06-17T14:46:20Z","published":"2025-01-29T23:17:43Z","title":"From tools to thieves: Measuring and understanding public perceptions of\n  AI through crowdsourced metaphors","summary":"  How has the public responded to the increasing prevalence of artificial\nintelligence (AI)-based technologies? We investigate public perceptions of AI\nby collecting over 12,000 responses over 12 months from a nationally\nrepresentative U.S. sample. Participants provided open-ended metaphors\nreflecting their mental models of AI, a methodology that overcomes the\nlimitations of traditional self-reported measures by capturing more nuance.\nUsing a mixed-methods approach combining quantitative clustering and\nqualitative coding, we identify 20 dominant metaphors shaping public\nunderstanding of AI. To analyze these metaphors systematically, we present a\nscalable framework integrating language modeling (LM)-based techniques to\nmeasure key dimensions of public perception: anthropomorphism (attribution of\nhuman-like qualities), warmth, and competence. We find that Americans generally\nview AI as warm and competent, and that over the past year, perceptions of AI's\nhuman-likeness and warmth have significantly increased ($+34\\%, r = 0.80, p <\n0.01; +41\\%, r = 0.62, p < 0.05$). These implicit perceptions, along with the\nidentified dominant metaphors, strongly predict trust in and willingness to\nadopt AI ($r^2 = 0.21, 0.18, p < 0.001$). Moreover, we uncover systematic\ndemographic differences in metaphors and implicit perceptions, such as the\nhigher propensity of women, older individuals, and people of color to\nanthropomorphize AI, which shed light on demographic disparities in trust and\nadoption. In addition to our dataset and framework for tracking evolving public\nattitudes, we provide actionable insights on using metaphors for inclusive and\nresponsible AI development.\n","authors":["Myra Cheng","Angela Y. Lee","Kristina Rapuano","Kate Niederhoffer","Alex Liebscher","Jeffrey Hancock"],"pdf_url":"https://arxiv.org/pdf/2501.18045v3.pdf","comment":"To appear at the ACM Conference on Fairness, Accountability, and\n  Transparency 2025"},{"id":"http://arxiv.org/abs/2506.14580v1","updated":"2025-06-17T14:37:09Z","published":"2025-06-17T14:37:09Z","title":"GenerationPrograms: Fine-grained Attribution with Executable Programs","summary":"  Recent large language models (LLMs) achieve impressive performance in\nsource-conditioned text generation but often fail to correctly provide\nfine-grained attributions for their outputs, undermining verifiability and\ntrust. Moreover, existing attribution methods do not explain how and why models\nleverage the provided source documents to generate their final responses,\nlimiting interpretability. To overcome these challenges, we introduce a modular\ngeneration framework, GenerationPrograms, inspired by recent advancements in\nexecutable \"code agent\" architectures. Unlike conventional generation methods\nthat simultaneously generate outputs and attributions or rely on post-hoc\nattribution, GenerationPrograms decomposes the process into two distinct\nstages: first, creating an executable program plan composed of modular text\noperations (such as paraphrasing, compression, and fusion) explicitly tailored\nto the query, and second, executing these operations following the program's\nspecified instructions to produce the final response. Empirical evaluations\ndemonstrate that GenerationPrograms significantly improves attribution quality\nat both the document level and sentence level across two long-form\nquestion-answering tasks and a multi-document summarization task. We further\ndemonstrate that GenerationPrograms can effectively function as a post-hoc\nattribution method, outperforming traditional techniques in recovering accurate\nattributions. In addition, the interpretable programs generated by\nGenerationPrograms enable localized refinement through modular-level\nimprovements that further enhance overall attribution quality.\n","authors":["David Wan","Eran Hirsch","Elias Stengel-Eskin","Ido Dagan","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2506.14580v1.pdf","comment":"27 Pages. Code: https://github.com/meetdavidwan/generationprograms"},{"id":"http://arxiv.org/abs/2502.14445v2","updated":"2025-06-17T14:34:13Z","published":"2025-02-20T10:52:38Z","title":"PredictaBoard: Benchmarking LLM Score Predictability","summary":"  Despite possessing impressive skills, Large Language Models (LLMs) often fail\nunpredictably, demonstrating inconsistent success in even basic common sense\nreasoning tasks. This unpredictability poses a significant challenge to\nensuring their safe deployment, as identifying and operating within a reliable\n\"safe zone\" is essential for mitigating risks. To address this, we present\nPredictaBoard, a novel collaborative benchmarking framework designed to\nevaluate the ability of score predictors (referred to as assessors) to\nanticipate LLM errors on specific task instances (i.e., prompts) from existing\ndatasets. PredictaBoard evaluates pairs of LLMs and assessors by considering\nthe rejection rate at different tolerance errors. As such, PredictaBoard\nstimulates research into developing better assessors and making LLMs more\npredictable, not only with a higher average performance. We conduct\nillustrative experiments using baseline assessors and state-of-the-art LLMs.\nPredictaBoard highlights the critical need to evaluate predictability alongside\nperformance, paving the way for safer AI systems where errors are not only\nminimised but also anticipated and effectively mitigated. Code for our\nbenchmark can be found at\nhttps://github.com/Kinds-of-Intelligence-CFI/PredictaBoard\n","authors":["Lorenzo Pacchiardi","Konstantinos Voudouris","Ben Slater","Fernando Martínez-Plumed","José Hernández-Orallo","Lexin Zhou","Wout Schellaert"],"pdf_url":"https://arxiv.org/pdf/2502.14445v2.pdf","comment":"Accepted at ACL Findings 2025"},{"id":"http://arxiv.org/abs/2308.12420v4","updated":"2025-06-17T14:32:54Z","published":"2023-08-23T20:42:32Z","title":"Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature","summary":"  Distributed Ledger Technology (DLT) faces increasing environmental scrutiny,\nparticularly concerning the energy consumption of the Proof of Work (PoW)\nconsensus mechanism and broader Environmental, Social, and Governance (ESG)\nissues. However, existing systematic literature reviews of DLT rely on limited\nanalyses of citations, abstracts, and keywords, failing to fully capture the\nfield's complexity and ESG concerns. We address these challenges by analyzing\nthe full text of 24,539 publications using Natural Language Processing (NLP)\nwith our manually labeled Named Entity Recognition (NER) dataset of 39,427\nentities for DLT. This methodology identified 505 key publications at the\nDLT/ESG intersection, enabling comprehensive domain analysis. Our combined NLP\nand temporal graph analysis reveals critical trends in DLT evolution and ESG\nimpacts, including cryptography and peer-to-peer networks research's\nfoundational influence, Bitcoin's persistent impact on research and\nenvironmental concerns (a \"Lindy effect\"), Ethereum's catalytic role on Proof\nof Stake (PoS) and smart contract adoption, and the industry's progressive\nshift toward energy-efficient consensus mechanisms. Our contributions include\nthe first DLT-specific NER dataset addressing the scarcity of high-quality\nlabeled NLP data in blockchain research, a methodology integrating NLP and\ntemporal graph analysis for large-scale interdisciplinary literature reviews,\nand the first NLP-driven literature review focusing on DLT's ESG aspects.\n","authors":["Walter Hernandez Cruz","Kamil Tylinski","Alastair Moore","Niall Roche","Nikhil Vadgama","Horst Treiblmaier","Jiangbo Shangguan","Paolo Tasca","Jiahua Xu"],"pdf_url":"https://arxiv.org/pdf/2308.12420v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14574v1","updated":"2025-06-17T14:30:06Z","published":"2025-06-17T14:30:06Z","title":"TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct\n  Preference Optimization","summary":"  Recent advancements in reinforcement learning from human feedback have shown\nthat utilizing fine-grained token-level reward models can substantially enhance\nthe performance of Proximal Policy Optimization (PPO) in aligning large\nlanguage models. However, it is challenging to leverage such token-level reward\nas guidance for Direct Preference Optimization (DPO), since DPO is formulated\nas a sequence-level bandit problem. To address this challenge, this work\ndecomposes the sequence-level PPO into a sequence of token-level proximal\npolicy optimization problems and then frames the problem of token-level PPO\nwith token-level reward guidance, from which closed-form optimal token-level\npolicy and the corresponding token-level reward can be derived. Using the\nobtained reward and Bradley-Terry model, this work establishes a framework of\ncomputable loss functions with token-level reward guidance for DPO, and\nproposes a practical reward guidance based on the induced DPO reward. This\nformulation enables different tokens to exhibit varying degrees of deviation\nfrom reference policy based on their respective rewards. Experiment results\ndemonstrate that our method achieves substantial performance improvements over\nDPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on\nAlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at\nhttps://github.com/dvlab-research/TGDPO.\n","authors":["Mingkang Zhu","Xi Chen","Zhongdao Wang","Bei Yu","Hengshuang Zhao","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2506.14574v1.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2506.14562v1","updated":"2025-06-17T14:21:10Z","published":"2025-06-17T14:21:10Z","title":"AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs","summary":"  Weight decay is a standard regularization technique for training large\nlanguage models (LLMs). While it is common to assign a uniform decay rate to\nevery layer, this approach overlooks the structural diversity of LLMs and the\nvarying spectral properties across modules. In this paper, we introduce\nAlphaDecay, a simple yet effective method that adaptively assigns different\nweight decay strengths to each module of an LLM. Our approach is guided by\nHeavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical\nspectral density (ESD) of weight correlation matrices to quantify\n\"heavy-tailedness.\" Modules exhibiting more pronounced heavy-tailed ESDs,\nreflecting stronger feature learning, are assigned weaker decay, while modules\nwith lighter-tailed spectra receive stronger decay. Our method leverages\ntailored weight decay assignments to balance the module-wise differences in\nspectral properties, leading to improved performance. Extensive pre-training\ntasks with various model sizes from 60M to 1B demonstrate that AlphaDecay\nachieves better perplexity and generalization than conventional uniform decay\nand other adaptive decay baselines.\n","authors":["Di He","Ajay Jaiswal","Songjun Tu","Li Shen","Ganzhao Yuan","Shiwei Liu","Lu Yin"],"pdf_url":"https://arxiv.org/pdf/2506.14562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14533v2","updated":"2025-06-17T14:18:09Z","published":"2024-12-19T05:11:16Z","title":"ClusterChat: Multi-Feature Search for Corpus Exploration","summary":"  Exploring large-scale text corpora presents a significant challenge in\nbiomedical, finance, and legal domains, where vast amounts of documents are\ncontinuously published. Traditional search methods, such as keyword-based\nsearch, often retrieve documents in isolation, limiting the user's ability to\neasily inspect corpus-wide trends and relationships. We present ClusterChat\n(The demo video and source code are available at:\nhttps://github.com/achouhan93/ClusterChat), an open-source system for corpus\nexploration that integrates cluster-based organization of documents using\ntextual embeddings with lexical and semantic search, timeline-driven\nexploration, and corpus and document-level question answering (QA) as\nmulti-feature search capabilities. We validate the system with two case studies\non a four million abstract PubMed dataset, demonstrating that ClusterChat\nenhances corpus exploration by delivering context-aware insights while\nmaintaining scalability and responsiveness on large-scale document collections.\n","authors":["Ashish Chouhan","Saifeldin Mandour","Michael Gertz"],"pdf_url":"https://arxiv.org/pdf/2412.14533v2.pdf","comment":"5 pages, 1 table, 1 figure, Accepted to SIGIR Demo Paper Track 2025"},{"id":"http://arxiv.org/abs/2506.14532v1","updated":"2025-06-17T13:58:36Z","published":"2025-06-17T13:58:36Z","title":"M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with\n  Large Language Models","summary":"  This paper introduces a novel neural network framework called M2BeamLLM for\nbeam prediction in millimeter-wave (mmWave) massive multi-input multi-output\n(mMIMO) communication systems. M2BeamLLM integrates multi-modal sensor data,\nincluding images, radar, LiDAR, and GPS, leveraging the powerful reasoning\ncapabilities of large language models (LLMs) such as GPT-2 for beam prediction.\nBy combining sensing data encoding, multimodal alignment and fusion, and\nsupervised fine-tuning (SFT), M2BeamLLM achieves significantly higher beam\nprediction accuracy and robustness, demonstrably outperforming traditional deep\nlearning (DL) models in both standard and few-shot scenarios. Furthermore, its\nprediction performance consistently improves with increased diversity in\nsensing modalities. Our study provides an efficient and intelligent beam\nprediction solution for vehicle-to-infrastructure (V2I) mmWave communication\nsystems.\n","authors":["Can Zheng","Jiguang He","Chung G. Kang","Guofa Cai","Zitong Yu","Merouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2506.14532v1.pdf","comment":"13 pages, 20 figures"},{"id":"http://arxiv.org/abs/2504.03255v2","updated":"2025-06-17T13:42:52Z","published":"2025-04-04T08:10:02Z","title":"Inherent and emergent liability issues in LLM-based agentic systems: a\n  principal-agent perspective","summary":"  Agentic systems powered by large language models (LLMs) are becoming\nprogressively more complex and capable. Their increasing agency and expanding\ndeployment settings attract growing attention to effective governance policies,\nmonitoring, and control protocols. Based on the emerging landscape of the\nagentic market, we analyze potential liability issues arising from the\ndelegated use of LLM agents and their extended systems through a\nprincipal-agent perspective. Our analysis complements existing risk-based\nstudies on artificial agency and covers the spectrum of important aspects of\nthe principal-agent relationship and their potential consequences at\ndeployment. Furthermore, we motivate method developments for technical\ngovernance along the directions of interpretability and behavior evaluations,\nreward and conflict management, and the mitigation of misalignment and\nmisconduct through principled engineering of detection and fail-safe\nmechanisms. By illustrating the outstanding issues in AI liability for\nLLM-based agentic systems, we aim to inform the system design, auditing, and\ntracing to enhance transparency and liability attribution.\n","authors":["Garry A. Gabison","R. Patrick Xian"],"pdf_url":"https://arxiv.org/pdf/2504.03255v2.pdf","comment":"22 pages (incl. appendix), accepted at REALM workshop, ACL2025"},{"id":"http://arxiv.org/abs/2506.14493v1","updated":"2025-06-17T13:14:55Z","published":"2025-06-17T13:14:55Z","title":"LingoLoop Attack: Trapping MLLMs via Linguistic Context and State\n  Entrapment into Endless Loops","summary":"  Multimodal Large Language Models (MLLMs) have shown great promise but require\nsubstantial computational resources during inference. Attackers can exploit\nthis by inducing excessive output, leading to resource exhaustion and service\ndegradation. Prior energy-latency attacks aim to increase generation time by\nbroadly shifting the output token distribution away from the EOS token, but\nthey neglect the influence of token-level Part-of-Speech (POS) characteristics\non EOS and sentence-level structural patterns on output counts, limiting their\nefficacy. To address this, we propose LingoLoop, an attack designed to induce\nMLLMs to generate excessively verbose and repetitive sequences. First, we find\nthat the POS tag of a token strongly affects the likelihood of generating an\nEOS token. Based on this insight, we propose a POS-Aware Delay Mechanism to\npostpone EOS token generation by adjusting attention weights guided by POS\ninformation. Second, we identify that constraining output diversity to induce\nrepetitive loops is effective for sustained generation. We introduce a\nGenerative Path Pruning Mechanism that limits the magnitude of hidden states,\nencouraging the model to produce persistent loops. Extensive experiments\ndemonstrate LingoLoop can increase generated tokens by up to 30 times and\nenergy consumption by a comparable factor on models like Qwen2.5-VL-3B,\nconsistently driving MLLMs towards their maximum generation limits. These\nfindings expose significant MLLMs' vulnerabilities, posing challenges for their\nreliable deployment. The code will be released publicly following the paper's\nacceptance.\n","authors":["Jiyuan Fu","Kaixun Jiang","Lingyi Hong","Jinglun Li","Haijing Guo","Dingkang Yang","Zhaoyu Chen","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.14493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04726v3","updated":"2025-06-17T13:04:28Z","published":"2024-12-06T02:34:40Z","title":"BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for\n  Varieties of English","summary":"  Despite large language models (LLMs) being known to exhibit bias against\nnon-standard language varieties, there are no known labelled datasets for\nsentiment analysis of English. To address this gap, we introduce BESSTIE, a\nbenchmark for sentiment and sarcasm classification for three varieties of\nEnglish: Australian (en-AU), Indian (en-IN), and British (en-UK). We collect\ndatasets for these language varieties using two methods: location-based for\nGoogle Places reviews, and topic-based filtering for Reddit comments. To assess\nwhether the dataset accurately represents these varieties, we conduct two\nvalidation steps: (a) manual annotation of language varieties and (b) automatic\nlanguage variety prediction. Native speakers of the language varieties manually\nannotate the datasets with sentiment and sarcasm labels. We perform an\nadditional annotation exercise to validate the reliance of the annotated\nlabels. Subsequently, we fine-tune nine LLMs (representing a range of\nencoder/decoder and mono/multilingual models) on these datasets, and evaluate\ntheir performance on the two tasks. Our results show that the models\nconsistently perform better on inner-circle varieties (i.e., en-AU and en-UK),\nin comparison with en-IN, particularly for sarcasm classification. We also\nreport challenges in cross-variety generalisation, highlighting the need for\nlanguage variety-specific datasets such as ours. BESSTIE promises to be a\nuseful evaluative benchmark for future research in equitable LLMs, specifically\nin terms of language varieties. The BESSTIE dataset is publicly available at:\nhttps://huggingface.co/ datasets/unswnlporg/BESSTIE.\n","authors":["Dipankar Srirag","Aditya Joshi","Jordan Painter","Diptesh Kanojia"],"pdf_url":"https://arxiv.org/pdf/2412.04726v3.pdf","comment":"Findings of ACL: ACL 2025"},{"id":"http://arxiv.org/abs/2506.14474v1","updated":"2025-06-17T12:41:53Z","published":"2025-06-17T12:41:53Z","title":"LexiMark: Robust Watermarking via Lexical Substitutions to Enhance\n  Membership Verification of an LLM's Textual Training Data","summary":"  Large language models (LLMs) can be trained or fine-tuned on data obtained\nwithout the owner's consent. Verifying whether a specific LLM was trained on\nparticular data instances or an entire dataset is extremely challenging.\nDataset watermarking addresses this by embedding identifiable modifications in\ntraining data to detect unauthorized use. However, existing methods often lack\nstealth, making them relatively easy to detect and remove. In light of these\nlimitations, we propose LexiMark, a novel watermarking technique designed for\ntext and documents, which embeds synonym substitutions for carefully selected\nhigh-entropy words. Our method aims to enhance an LLM's memorization\ncapabilities on the watermarked text without altering the semantic integrity of\nthe text. As a result, the watermark is difficult to detect, blending\nseamlessly into the text with no visible markers, and is resistant to removal\ndue to its subtle, contextually appropriate substitutions that evade automated\nand manual detection. We evaluated our method using baseline datasets from\nrecent studies and seven open-source models: LLaMA-1 7B, LLaMA-3 8B, Mistral\n7B, Pythia 6.9B, as well as three smaller variants from the Pythia family\n(160M, 410M, and 1B). Our evaluation spans multiple training settings,\nincluding continued pretraining and fine-tuning scenarios. The results\ndemonstrate significant improvements in AUROC scores compared to existing\nmethods, underscoring our method's effectiveness in reliably verifying whether\nunauthorized watermarked data was used in LLM training.\n","authors":["Eyal German","Sagiv Antebi","Edan Habler","Asaf Shabtai","Yuval Elovici"],"pdf_url":"https://arxiv.org/pdf/2506.14474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20620v2","updated":"2025-06-17T12:26:49Z","published":"2025-02-28T00:57:45Z","title":"Rectifying Belief Space via Unlearning to Harness LLMs' Reasoning","summary":"  Large language models (LLMs) can exhibit advanced reasoning yet still\ngenerate incorrect answers. We hypothesize that such errors frequently stem\nfrom spurious beliefs, propositions the model internally considers true but are\nincorrect. To address this, we propose a method to rectify the belief space by\nsuppressing these spurious beliefs while simultaneously enhancing true ones,\nthereby enabling more reliable inferences. Our approach first identifies the\nbeliefs that lead to incorrect or correct answers by prompting the model to\ngenerate textual explanations, using our Forward-Backward Beam Search (FBBS).\nWe then apply unlearning to suppress the identified spurious beliefs and\nenhance the true ones, effectively rectifying the model's belief space.\nEmpirical results on multiple QA datasets and LLMs show that our method\ncorrects previously misanswered questions without harming overall model\nperformance. Furthermore, our approach yields improved generalization on unseen\ndata, suggesting that rectifying a model's belief space is a promising\ndirection for mitigating errors and enhancing overall reliability.\n","authors":["Ayana Niwa","Masahiro Kaneko","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2502.20620v2.pdf","comment":"Accepted at ACL2025 Findings (long)"},{"id":"http://arxiv.org/abs/2506.14448v1","updated":"2025-06-17T12:13:56Z","published":"2025-06-17T12:13:56Z","title":"How Far Can LLMs Improve from Experience? Measuring Test-Time Learning\n  Ability in LLMs with Human Comparison","summary":"  As evaluation designs of large language models may shape our trajectory\ntoward artificial general intelligence, comprehensive and forward-looking\nassessment is essential. Existing benchmarks primarily assess static knowledge,\nwhile intelligence also entails the ability to rapidly learn from experience.\nTo this end, we advocate for the evaluation of Test-time Learning, the capacity\nto improve performance in experience-based, reasoning-intensive tasks during\ntest time. In this work, we propose semantic games as effective testbeds for\nevaluating test-time learning, due to their resistance to saturation and\ninherent demand for strategic reasoning. We introduce an objective evaluation\nframework that compares model performance under both limited and cumulative\nexperience settings, and contains four forms of experience representation. To\nprovide a comparative baseline, we recruit eight human participants to complete\nthe same task. Results show that LLMs exhibit measurable test-time learning\ncapabilities; however, their improvements are less stable under cumulative\nexperience and progress more slowly than those observed in humans. These\nfindings underscore the potential of LLMs as general-purpose learning machines,\nwhile also revealing a substantial intellectual gap between models and humans,\nirrespective of how well LLMs perform on static benchmarks.\n","authors":["Jiayin Wang","Zhiquang Guo","Weizhi Ma","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.14448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14429v1","updated":"2025-06-17T11:45:37Z","published":"2025-06-17T11:45:37Z","title":"LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs","summary":"  Large Language Diffusion Models, or diffusion LLMs, have emerged as a\nsignificant focus in NLP research, with substantial effort directed toward\nunderstanding their scalability and downstream task performance. However, their\nlong-context capabilities remain unexplored, lacking systematic analysis or\nmethods for context extension. In this work, we present the first systematic\ninvestigation comparing the long-context performance of diffusion LLMs and\ntraditional auto-regressive LLMs. We first identify a unique characteristic of\ndiffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably\n\\textbf{\\textit{stable perplexity}} during direct context extrapolation.\nFurthermore, where auto-regressive models fail outright during the\nNeedle-In-A-Haystack task with context exceeding their pretrained length, we\ndiscover diffusion LLMs exhibit a distinct \\textbf{\\textit{local perception}}\nphenomenon, enabling successful retrieval from recent context segments. We\nexplain both phenomena through the lens of Rotary Position Embedding (RoPE)\nscaling theory. Building on these observations, we propose LongLLaDA, a\ntraining-free method that integrates LLaDA with the NTK-based RoPE\nextrapolation. Our results validate that established extrapolation scaling laws\nremain effective for extending the context windows of diffusion LLMs.\nFurthermore, we identify long-context tasks where diffusion LLMs outperform\nauto-regressive LLMs and others where they fall short. Consequently, this study\nestablishes the first context extrapolation method for diffusion LLMs while\nproviding essential theoretical insights and empirical benchmarks critical for\nadvancing future research on long-context diffusion LLMs.\n","authors":["Xiaoran Liu","Zhigeng Liu","Zengfeng Huang","Qipeng Guo","Ziwei He","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2506.14429v1.pdf","comment":"16 pages, 12 figures, work in progress"},{"id":"http://arxiv.org/abs/2410.07819v2","updated":"2025-06-17T11:41:46Z","published":"2024-10-10T11:09:00Z","title":"Uncovering Overfitting in Large Language Model Editing","summary":"  Knowledge editing has been proposed as an effective method for updating and\ncorrecting the internal knowledge of Large Language Models (LLMs). However,\nexisting editing methods often struggle with complex tasks, such as multi-hop\nreasoning. In this paper, we identify and investigate the phenomenon of Editing\nOverfit, where edited models assign disproportionately high probabilities to\nthe edit target, hindering the generalization of new knowledge in complex\nscenarios. We attribute this issue to the current editing paradigm, which\nplaces excessive emphasis on the direct correspondence between the input prompt\nand the edit target for each edit sample. To further explore this issue, we\nintroduce a new benchmark, EVOKE (EValuation of Editing Overfit in Knowledge\nEditing), along with fine-grained evaluation metrics. Through comprehensive\nexperiments and analysis, we demonstrate that Editing Overfit is prevalent in\ncurrent editing methods and that common overfitting mitigation strategies are\nineffective in knowledge editing. To overcome this, inspired by LLMs' knowledge\nrecall mechanisms, we propose a new plug-and-play strategy called Learn the\nInference (LTI), which introduce a Multi-stage Inference Constraint module to\nguide the edited models in recalling new knowledge similarly to how unedited\nLLMs leverage knowledge through in-context learning. Extensive experimental\nresults across a wide range of tasks validate the effectiveness of LTI in\nmitigating Editing Overfit.\n","authors":["Mengqi Zhang","Xiaotian Ye","Qiang Liu","Pengjie Ren","Shu Wu","Zhumin Chen"],"pdf_url":"https://arxiv.org/pdf/2410.07819v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2506.14407v1","updated":"2025-06-17T11:08:29Z","published":"2025-06-17T11:08:29Z","title":"ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge","summary":"  Retrieval systems are central to many NLP pipelines, but often rely on\nsurface-level cues such as keyword overlap and lexical semantic similarity. To\nevaluate retrieval beyond these shallow signals, recent benchmarks introduce\nreasoning-heavy queries; however, they primarily shift the burden to query-side\nprocessing techniques -- like prompting or multi-hop retrieval -- that can help\nresolve complexity. In contrast, we present ImpliRet, a benchmark that shifts\nthe reasoning challenge to document-side processing: The queries are simple,\nbut relevance depends on facts stated implicitly in documents through temporal\n(e.g., resolving \"two days ago\"), arithmetic, and world knowledge\nrelationships. We evaluate a range of sparse and dense retrievers, all of which\nstruggle in this setting: the best nDCG@10 is only 15.07%. We also test whether\nlong-context models can overcome this limitation. But even with a short context\nof only ten documents, including the positive document, GPT-4.1 scores only\n35.06%, showing that document-side reasoning remains a challenge. Our codes are\navailable at github.com/ZeinabTaghavi/IMPLIRET.Contribution.\n","authors":["Zeinab Sadat Taghavi","Ali Modarressi","Yunpu Ma","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2506.14407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14397v1","updated":"2025-06-17T10:51:39Z","published":"2025-06-17T10:51:39Z","title":"Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation\n  Understanding","summary":"  Negation is a fundamental linguistic phenomenon that poses persistent\nchallenges for Large Language Models (LLMs), particularly in tasks requiring\ndeep semantic understanding. Existing benchmarks often treat negation as a side\ncase within broader tasks like natural language inference, resulting in a lack\nof benchmarks that exclusively target negation understanding. In this work, we\nintroduce \\textbf{Thunder-NUBench}, a novel benchmark explicitly designed to\nassess sentence-level negation understanding in LLMs. Thunder-NUBench goes\nbeyond surface-level cue detection by contrasting standard negation with\nstructurally diverse alternatives such as local negation, contradiction, and\nparaphrase. The benchmark consists of manually curated sentence-negation pairs\nand a multiple-choice dataset that enables in-depth evaluation of models'\nnegation understanding.\n","authors":["Yeonkyoung So","Gyuseong Lee","Sungmok Jung","Joonhak Lee","JiA Kang","Sangho Kim","Jaejin Lee"],"pdf_url":"https://arxiv.org/pdf/2506.14397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.16005v4","updated":"2025-06-17T10:43:41Z","published":"2025-04-22T16:14:31Z","title":"CAPO: Cost-Aware Prompt Optimization","summary":"  Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automatic prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p in accuracy. Our algorithm\nachieves better performances already with smaller budgets, saves evaluations\nthrough racing, and decreases average prompt length via a length penalty,\nmaking it both cost-efficient and cost-aware. Even without few-shot examples,\nCAPO outperforms its competitors and generally remains robust to initial\nprompts. CAPO represents an important step toward making prompt optimization\nmore powerful and accessible by improving cost-efficiency.\n","authors":["Tom Zehle","Moritz Schlager","Timo Heiß","Matthias Feurer"],"pdf_url":"https://arxiv.org/pdf/2504.16005v4.pdf","comment":"Submitted to AutoML 2025"},{"id":"http://arxiv.org/abs/2411.19563v2","updated":"2025-06-17T10:36:46Z","published":"2024-11-29T09:18:32Z","title":"Ensemble Watermarks for Large Language Models","summary":"  As large language models (LLMs) reach human-like fluency, reliably\ndistinguishing AI-generated text from human authorship becomes increasingly\ndifficult. While watermarks already exist for LLMs, they often lack flexibility\nand struggle with attacks such as paraphrasing. To address these issues, we\npropose a multi-feature method for generating watermarks that combines multiple\ndistinct watermark features into an ensemble watermark. Concretely, we combine\nacrostica and sensorimotor norms with the established red-green watermark to\nachieve a 98% detection rate. After a paraphrasing attack, the performance\nremains high with 95% detection rate. In comparison, the red-green feature\nalone as a baseline achieves a detection rate of 49% after paraphrasing. The\nevaluation of all feature combinations reveals that the ensemble of all three\nconsistently has the highest detection rate across several LLMs and watermark\nstrength settings. Due to the flexibility of combining features in the\nensemble, various requirements and trade-offs can be addressed. Additionally,\nthe same detection function can be used without adaptations for all ensemble\nconfigurations. This method is particularly of interest to facilitate\naccountability and prevent societal harm.\n","authors":["Georg Niess","Roman Kern"],"pdf_url":"https://arxiv.org/pdf/2411.19563v2.pdf","comment":"Accepted to ACL 2025 main conference. This article extends our\n  earlier work arXiv:2405.08400 by introducing an ensemble of stylometric\n  watermarking features and alternative experimental analysis. Code and data\n  are available at http://github.com/CommodoreEU/ensemble-watermark"},{"id":"http://arxiv.org/abs/2504.07738v2","updated":"2025-06-17T10:16:59Z","published":"2025-04-10T13:29:58Z","title":"Automated Construction of a Knowledge Graph of Nuclear Fusion Energy for\n  Effective Elicitation and Retrieval of Information","summary":"  In this document, we discuss a multi-step approach to automated construction\nof a knowledge graph, for structuring and representing domain-specific\nknowledge from large document corpora. We apply our method to build the first\nknowledge graph of nuclear fusion energy, a highly specialized field\ncharacterized by vast scope and heterogeneity. This is an ideal benchmark to\ntest the key features of our pipeline, including automatic named entity\nrecognition and entity resolution. We show how pre-trained large language\nmodels can be used to address these challenges and we evaluate their\nperformance against Zipf's law, which characterizes human-generated natural\nlanguage. Additionally, we develop a knowledge-graph retrieval-augmented\ngeneration system that combines large language models with a multi-prompt\napproach. This system provides contextually relevant answers to\nnatural-language queries, including complex multi-hop questions that require\nreasoning across interconnected entities.\n","authors":["Andrea Loreti","Kesi Chen","Ruby George","Robert Firth","Adriano Agnello","Shinnosuke Tanaka"],"pdf_url":"https://arxiv.org/pdf/2504.07738v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13277v2","updated":"2025-06-17T10:12:39Z","published":"2025-06-16T09:16:40Z","title":"SeqPE: Transformer with Sequential Position Encoding","summary":"  Since self-attention layers in Transformers are permutation invariant by\ndesign, positional encodings must be explicitly incorporated to enable spatial\nunderstanding. However, fixed-size lookup tables used in traditional learnable\nposition embeddings (PEs) limit extrapolation capabilities beyond pre-trained\nsequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this\nlimitation but demand extensive modifications for adapting to new modalities,\nunderscoring fundamental challenges in adaptability and scalability. In this\nwork, we present SeqPE, a unified and fully learnable position encoding\nframework that represents each $n$-dimensional position index as a symbolic\nsequence and employs a lightweight sequential position encoder to learn their\nembeddings in an end-to-end manner. To regularize SeqPE's embedding space, we\nintroduce two complementary objectives: a contrastive objective that aligns\nembedding distances with a predefined position-distance function, and a\nknowledge distillation loss that anchors out-of-distribution position\nembeddings to in-distribution teacher representations, further enhancing\nextrapolation performance. Experiments across language modeling, long-context\nquestion answering, and 2D image classification demonstrate that SeqPE not only\nsurpasses strong baselines in perplexity, exact match (EM), and\naccuracy--particularly under context length extrapolation--but also enables\nseamless generalization to multi-dimensional inputs without requiring manual\narchitectural redesign. We release our code, data, and checkpoints at\nhttps://github.com/ghrua/seqpe.\n","authors":["Huayang Li","Yahui Liu","Hongyu Sun","Deng Cai","Leyang Cui","Wei Bi","Peilin Zhao","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2506.13277v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14371v1","updated":"2025-06-17T10:10:51Z","published":"2025-06-17T10:10:51Z","title":"ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions\n  shared task: LLM-based question generation and selection","summary":"  The widespread adoption of chat interfaces based on Large Language Models\n(LLMs) raises concerns about promoting superficial learning and undermining the\ndevelopment of critical thinking skills. Instead of relying on LLMs purely for\nretrieving factual information, this work explores their potential to foster\ndeeper reasoning by generating critical questions that challenge unsupported or\nvague claims in debate interventions. This study is part of a shared task of\nthe 12th Workshop on Argument Mining, co-located with ACL 2025, focused on\nautomatic critical question generation. We propose a two-step framework\ninvolving two small-scale open source language models: a Questioner that\ngenerates multiple candidate questions and a Judge that selects the most\nrelevant ones. Our system ranked first in the shared task competition,\ndemonstrating the potential of the proposed LLM-based approach to encourage\ncritical engagement with argumentative texts.\n","authors":["Lucile Favero","Daniel Frases","Juan Antonio Pérez-Ortiz","Tanja Käser","Nuria Oliver"],"pdf_url":"https://arxiv.org/pdf/2506.14371v1.pdf","comment":"Proceedings of the 12th Workshop on Argument Mining"},{"id":"http://arxiv.org/abs/2506.14370v1","updated":"2025-06-17T10:10:39Z","published":"2025-06-17T10:10:39Z","title":"Digital Gatekeepers: Google's Role in Curating Hashtags and Subreddits","summary":"  Search engines play a crucial role as digital gatekeepers, shaping the\nvisibility of Web and social media content through algorithmic curation. This\nstudy investigates how search engines like Google selectively promotes or\nsuppresses certain hashtags and subreddits, impacting the information users\nencounter. By comparing search engine results with nonsampled data from Reddit\nand Twitter/X, we reveal systematic biases in content visibility. Google's\nalgorithms tend to suppress subreddits and hashtags related to sexually\nexplicit material, conspiracy theories, advertisements, and cryptocurrencies,\nwhile promoting content associated with higher engagement. These findings\nsuggest that Google's gatekeeping practices influence public discourse by\ncurating the social media narratives available to users.\n","authors":["Amrit Poudel","Yifan Ding","Jurgen Pfeffer","Tim Weninger"],"pdf_url":"https://arxiv.org/pdf/2506.14370v1.pdf","comment":"Accepted to ACL 2025 Main"},{"id":"http://arxiv.org/abs/2312.16490v2","updated":"2025-06-17T10:06:03Z","published":"2023-12-27T09:35:23Z","title":"Exploring news intent and its application: A theory-driven approach","summary":"  Understanding the intent behind information is crucial. However, news as a\nmedium of public discourse still lacks a structured investigation of perceived\nnews intent and its application. To advance this field, this paper reviews\ninterdisciplinary studies on intentional action and introduces a conceptual\ndeconstruction-based news intent understanding framework (NINT). This framework\nidentifies the components of intent, facilitating a structured representation\nof news intent and its applications. Building upon NINT, we contribute a new\nintent perception dataset. Moreover, we investigate the potential of intent\nassistance on news-related tasks, such as significant improvement (+2.2% macF1)\nin the task of fake news detection. We hope that our findings will provide\nvaluable insights into action-based intent cognition and computational social\nscience.\n","authors":["Zhengjia Wang","Danding Wang","Qiang Sheng","Juan Cao","Siyuan Ma","Haonan Cheng"],"pdf_url":"https://arxiv.org/pdf/2312.16490v2.pdf","comment":"Accepted to Information Processing & Management. DOI:\n  https://doi.org/10.1016/j.ipm.2025.104229"},{"id":"http://arxiv.org/abs/2506.14345v1","updated":"2025-06-17T09:38:45Z","published":"2025-06-17T09:38:45Z","title":"A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive,\n  Transparent, and Reproducible Geo-Temporal Information Synthesis","summary":"  The emergence of Large Language Models (LLMs) has transformed information\naccess, with current LLMs also powering deep research systems that can generate\ncomprehensive report-style answers, through planned iterative search,\nretrieval, and reasoning. Still, current deep research systems lack the\ngeo-temporal capabilities that are essential for answering context-rich\nquestions involving geographic and/or temporal constraints, frequently\noccurring in domains like public health, environmental science, or\nsocio-economic analysis. This paper reports our vision towards next generation\nsystems, identifying important technical, infrastructural, and evaluative\nchallenges in integrating geo-temporal reasoning into deep research pipelines.\nWe argue for augmenting retrieval and synthesis processes with the ability to\nhandle geo-temporal constraints, supported by open and reproducible\ninfrastructures and rigorous evaluation protocols. Our vision outlines a path\ntowards more advanced and geo-temporally aware deep research systems, of\npotential impact to the future of AI-driven information access.\n","authors":["Bruno Martins","Piotr Szymański","Piotr Gramacki"],"pdf_url":"https://arxiv.org/pdf/2506.14345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14335v1","updated":"2025-06-17T09:17:41Z","published":"2025-06-17T09:17:41Z","title":"Evaluation Should Not Ignore Variation: On the Impact of Reference Set\n  Choice on Summarization Metrics","summary":"  Human language production exhibits remarkable richness and variation,\nreflecting diverse communication styles and intents. However, this variation is\noften overlooked in summarization evaluation. While having multiple reference\nsummaries is known to improve correlation with human judgments, the impact of\nusing different reference sets on reference-based metrics has not been\nsystematically investigated. This work examines the sensitivity of widely used\nreference-based metrics in relation to the choice of reference sets, analyzing\nthree diverse multi-reference summarization datasets: SummEval, GUMSum, and\nDUC2004. We demonstrate that many popular metrics exhibit significant\ninstability. This instability is particularly concerning for n-gram-based\nmetrics like ROUGE, where model rankings vary depending on the reference sets,\nundermining the reliability of model comparisons. We also collect human\njudgments on LLM outputs for genre-diverse data and examine their correlation\nwith metrics to supplement existing findings beyond newswire summaries, finding\nweak-to-no correlation. Taken together, we recommend incorporating reference\nset variation into summarization evaluation to enhance consistency alongside\ncorrelation with human judgments, especially when evaluating LLMs.\n","authors":["Silvia Casola","Yang Janet Liu","Siyao Peng","Oliver Kraus","Albert Gatt","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2506.14335v1.pdf","comment":"17 pages, 13 figures"},{"id":"http://arxiv.org/abs/2506.13472v2","updated":"2025-06-17T09:13:54Z","published":"2025-06-16T13:30:33Z","title":"ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently\n  Compressing Large Language Models","summary":"  Quantization has been widely studied as an effective technique for reducing\nthe memory requirement of large language models (LLMs), potentially improving\nthe latency time as well. Utilizing the characteristic of rotational invariance\nof transformer, we propose the rotation-based saliency-aware weight\nquantization (ROSAQ), which identifies salient channels in the projection\nfeature space, not in the original feature space, where the projected\n\"principal\" dimensions are naturally considered as \"salient\" features. The\nproposed ROSAQ consists of 1) PCA-based projection, which first performs\nprincipal component analysis (PCA) on a calibration set and transforms via the\nPCA projection, 2) Salient channel dentification, which selects dimensions\ncorresponding to the K-largest eigenvalues as salient channels, and 3)\nSaliency-aware quantization with mixed-precision, which uses FP16 for salient\ndimensions and INT3/4 for other dimensions. Experiment results show that ROSAQ\nshows improvements over the baseline saliency-aware quantization on the\noriginal feature space and other existing quantization methods. With kernel\nfusion, ROSAQ presents about 2.3x speed up over FP16 implementation in\ngenerating 256 tokens with a batch size of 64.\n","authors":["Junho Yoon","Geom Lee","Donghyeon Jeon","Inho Kang","Seung-Hoon Na"],"pdf_url":"https://arxiv.org/pdf/2506.13472v2.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2506.01413v4","updated":"2025-06-17T08:52:06Z","published":"2025-06-02T08:11:44Z","title":"Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models","summary":"  Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nwill be available later (under review).\n  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction\nfollowing, complex instructions\n","authors":["Yulei Qin","Gang Li","Zongyi Li","Zihan Xu","Yuchen Shi","Zhekai Lin","Xiao Cui","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2506.01413v4.pdf","comment":"13 pages of main body, 3 tables, 5 figures, 45 pages of appendix"},{"id":"http://arxiv.org/abs/2503.11593v2","updated":"2025-06-17T08:44:18Z","published":"2025-03-14T17:02:45Z","title":"Do Construction Distributions Shape Formal Language Learning In German\n  BabyLMs?","summary":"  We analyze the influence of utterance-level construction distributions in\nGerman child-directed/child-available speech on the resulting word-level,\nsyntactic and semantic competence (and their underlying learning trajectories)\nin small LMs, which we train on a novel collection of developmentally plausible\nlanguage data for German. We find that trajectories are surprisingly robust for\nmarkedly different distributions of constructions in the training data, which\nhave little effect on final accuracies and almost no effect on global learning\ntrajectories. While syntax learning benefits from more complex utterances,\nword-level learning culminates in better scores with more fragmentary\nutterances. We argue that LMs trained on developmentally plausible data can\ncontribute to debates on how conducive different kinds of linguistic stimuli\nare to language learning.\n","authors":["Bastian Bunzeck","Daniel Duran","Sina Zarrieß"],"pdf_url":"https://arxiv.org/pdf/2503.11593v2.pdf","comment":"Accepted at CoNNL 2025"},{"id":"http://arxiv.org/abs/2506.14302v1","updated":"2025-06-17T08:29:04Z","published":"2025-06-17T08:29:04Z","title":"Expectation Confirmation Preference Optimization for Multi-Turn\n  Conversational Recommendation Agent","summary":"  Recent advancements in Large Language Models (LLMs) have significantly\npropelled the development of Conversational Recommendation Agents (CRAs).\nHowever, these agents often generate short-sighted responses that fail to\nsustain user guidance and meet expectations. Although preference optimization\nhas proven effective in aligning LLMs with user expectations, it remains costly\nand performs poorly in multi-turn dialogue. To address this challenge, we\nintroduce a novel multi-turn preference optimization (MTPO) paradigm ECPO,\nwhich leverages Expectation Confirmation Theory to explicitly model the\nevolution of user satisfaction throughout multi-turn dialogues, uncovering the\nunderlying causes of dissatisfaction. These causes can be utilized to support\ntargeted optimization of unsatisfactory responses, thereby achieving turn-level\npreference optimization. ECPO ingeniously eliminates the significant sampling\noverhead of existing MTPO methods while ensuring the optimization process\ndrives meaningful improvements. To support ECPO, we introduce an LLM-based user\nsimulator, AILO, to simulate user feedback and perform expectation confirmation\nduring conversational recommendations. Experimental results show that ECPO\nsignificantly enhances CRA's interaction capabilities, delivering notable\nimprovements in both efficiency and effectiveness over existing MTPO methods.\n","authors":["Xueyang Feng","Jingsen Zhang","Jiakai Tang","Wei Li","Guohao Cai","Xu Chen","Quanyu Dai","Yue Zhu","Zhenhua Dong"],"pdf_url":"https://arxiv.org/pdf/2506.14302v1.pdf","comment":"Accepted to Findings of ACL 2025"},{"id":"http://arxiv.org/abs/2506.13172v2","updated":"2025-06-17T08:24:30Z","published":"2025-06-16T07:34:31Z","title":"AI-Facilitated Analysis of Abstracts and Conclusions: Flagging\n  Unsubstantiated Claims and Ambiguous Pronouns","summary":"  We present and evaluate a suite of proof-of-concept (PoC), structured\nworkflow prompts designed to elicit human-like hierarchical reasoning while\nguiding Large Language Models (LLMs) in the high-level semantic and linguistic\nanalysis of scholarly manuscripts. The prompts target two non-trivial\nanalytical tasks within academic summaries (abstracts and conclusions):\nidentifying unsubstantiated claims (informational integrity) and flagging\nsemantically confusing ambiguous pronoun references (linguistic clarity). We\nconducted a systematic, multi-run evaluation on two frontier models (Gemini Pro\n2.5 Pro and ChatGPT Plus o3) under varied context conditions. Our results for\nthe informational integrity task reveal a significant divergence in model\nperformance: while both models successfully identified an unsubstantiated head\nof a noun phrase (95% success), ChatGPT consistently failed (0% success) to\nidentify an unsubstantiated adjectival modifier that Gemini correctly flagged\n(95% success), raising a question regarding the potential influence of the\ntarget's syntactic role. For the linguistic analysis task, both models\nperformed well (80-90% success) with full manuscript context. Surprisingly, in\na summary-only setting, Gemini's performance was substantially degraded, while\nChatGPT achieved a perfect (100%) success rate. Our findings suggest that while\nstructured prompting is a viable methodology for complex textual analysis,\nprompt performance may be highly dependent on the interplay between the model,\ntask type, and context, highlighting the need for rigorous, model-specific\ntesting.\n","authors":["Evgeny Markhasin"],"pdf_url":"https://arxiv.org/pdf/2506.13172v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2506.13300v2","updated":"2025-06-17T08:15:17Z","published":"2025-06-16T09:42:05Z","title":"Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning\n  Language Models","summary":"  This paper presents Seewo's systems for both tracks of the Multilingual\nConversational Speech Language Model Challenge (MLC-SLM), addressing automatic\nspeech recognition (ASR) and speaker diarization with ASR (SD-ASR). We\nintroduce a multi-stage training pipeline that explicitly enhances reasoning\nand self-correction in speech language models for ASR. Our approach combines\ncurriculum learning for progressive capability acquisition, Chain-of-Thought\ndata augmentation to foster intermediate reflection, and Reinforcement Learning\nwith Verifiable Rewards (RLVR) to further refine self-correction through\nreward-driven optimization. This approach achieves substantial improvements\nover the official challenge baselines. On the evaluation set, our best system\nattains a WER/CER of 11.57% for Track 1 and a tcpWER/tcpCER of 17.67% for Track\n2. Comprehensive ablation studies demonstrate the effectiveness of each\ncomponent under challenge constraints.\n","authors":["Bo Li","Chengben Xu","Wufeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.13300v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12376v2","updated":"2025-06-17T08:11:59Z","published":"2025-06-14T07:18:33Z","title":"ConsistencyChecker: Tree-based Evaluation of LLM Generalization\n  Capabilities","summary":"  Evaluating consistency in large language models (LLMs) is crucial for\nensuring reliability, particularly in complex, multi-step interactions between\nhumans and LLMs. Traditional self-consistency methods often miss subtle\nsemantic changes in natural language and functional shifts in code or\nequations, which can accumulate over multiple transformations. To address this,\nwe propose ConsistencyChecker, a tree-based evaluation framework designed to\nmeasure consistency through sequences of reversible transformations, including\nmachine translation tasks and AI-assisted programming tasks. In our framework,\nnodes represent distinct text states, while edges correspond to pairs of\ninverse operations. Dynamic and LLM-generated benchmarks ensure a fair\nassessment of the model's generalization ability and eliminate benchmark\nleakage. Consistency is quantified based on similarity across different depths\nof the transformation tree. Experiments on eight models from various families\nand sizes show that ConsistencyChecker can distinguish the performance of\ndifferent models. Notably, our consistency scores-computed entirely without\nusing WMT paired data-correlate strongly (r > 0.7) with WMT 2024 auto-ranking,\ndemonstrating the validity of our benchmark-free approach. Our implementation\nis available at: https://github.com/ulab-uiuc/consistencychecker.\n","authors":["Zhaochen Hong","Haofei Yu","Jiaxuan You"],"pdf_url":"https://arxiv.org/pdf/2506.12376v2.pdf","comment":"Accepted at ACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2506.14285v1","updated":"2025-06-17T07:56:32Z","published":"2025-06-17T07:56:32Z","title":"From What to Respond to When to Respond: Timely Response Generation for\n  Open-domain Dialogue Agents","summary":"  While research on dialogue response generation has primarily focused on\ngenerating coherent responses conditioning on textual context, the critical\nquestion of when to respond grounded on the temporal context remains\nunderexplored. To bridge this gap, we propose a novel task called timely\ndialogue response generation and introduce the TimelyChat benchmark, which\nevaluates the capabilities of language models to predict appropriate time\nintervals and generate time-conditioned responses. Additionally, we construct a\nlarge-scale training dataset by leveraging unlabeled event knowledge from a\ntemporal commonsense knowledge graph and employing a large language model (LLM)\nto synthesize 55K event-driven dialogues. We then train Timer, a dialogue agent\ndesigned to proactively predict time intervals and generate timely responses\nthat align with those intervals. Experimental results show that Timer\noutperforms prompting-based LLMs and other fine-tuned baselines in both\nturn-level and dialogue-level evaluations. We publicly release our data, model,\nand code.\n","authors":["Seongbo Jang","Minjin Jeon","Jaehoon Lee","Seonghyeon Lee","Dongha Lee","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2506.14285v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2506.09081v2","updated":"2025-06-17T07:54:37Z","published":"2025-06-10T04:19:02Z","title":"FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model\n  Evaluation","summary":"  We present FlagEvalMM, an open-source evaluation framework designed to\ncomprehensively assess multimodal models across a diverse range of\nvision-language understanding and generation tasks, such as visual question\nanswering, text-to-image/video generation, and image-text retrieval. We\ndecouple model inference from evaluation through an independent evaluation\nservice, thus enabling flexible resource allocation and seamless integration of\nnew tasks and models. Moreover, FlagEvalMM utilizes advanced inference\nacceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to\nsignificantly enhance evaluation efficiency. Extensive experiments show that\nFlagEvalMM offers accurate and efficient insights into model strengths and\nlimitations, making it a valuable tool for advancing multimodal research. The\nframework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.\n","authors":["Zheqi He","Yesheng Liu","Jing-shu Zheng","Xuejing Li","Jin-Ge Yao","Bowen Qin","Richeng Xuan","Xi Yang"],"pdf_url":"https://arxiv.org/pdf/2506.09081v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14280v1","updated":"2025-06-17T07:49:43Z","published":"2025-06-17T07:49:43Z","title":"Improving LoRA with Variational Learning","summary":"  Bayesian methods have recently been used to improve LoRA finetuning and,\nalthough they improve calibration, their effect on other metrics (such as\naccuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian\nmethods also increase computational overheads and require additional tricks for\nthem to work well. Here, we fix these issues by using a recently proposed\nvariational algorithm called IVON. We show that IVON is easy to implement and\nhas similar costs to AdamW, and yet it can also drastically improve many\nmetrics by using a simple posterior pruning technique. We present extensive\nresults on billion-scale LLMs (Llama and Qwen series) going way beyond the\nscale of existing applications of IVON. For example, we finetune a Llama-3.2-3B\nmodel on a set of commonsense reasoning tasks and improve accuracy over AdamW\nby 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian\nmethods like Laplace-LoRA and BLoB. Overall, our results show that variational\nlearning with IVON can effectively improve LoRA finetuning.\n","authors":["Bai Cong","Nico Daheim","Yuesong Shen","Rio Yokota","Mohammad Emtiyaz Khan","Thomas Möllenhoff"],"pdf_url":"https://arxiv.org/pdf/2506.14280v1.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2506.12796v2","updated":"2025-06-17T07:46:17Z","published":"2025-06-15T10:04:42Z","title":"Surprise Calibration for Better In-Context Learning","summary":"  In-context learning (ICL) has emerged as a powerful paradigm for task\nadaptation in large language models (LLMs), where models infer underlying task\nstructures from a few demonstrations. However, ICL remains susceptible to\nbiases that arise from prior knowledge and contextual demonstrations, which can\ndegrade the performance of LLMs. Existing bias calibration methods typically\napply fixed class priors across all inputs, limiting their efficacy in dynamic\nICL settings where the context for each query differs. To address these\nlimitations, we adopt implicit sequential Bayesian inference as a framework for\ninterpreting ICL, identify \"surprise\" as an informative signal for class prior\nshift, and introduce a novel method--Surprise Calibration (SC). SC leverages\nthe notion of surprise to capture the temporal dynamics of class priors,\nproviding a more adaptive and computationally efficient solution for in-context\nlearning. We empirically demonstrate the superiority of SC over existing bias\ncalibration techniques across a range of benchmark natural language processing\ntasks.\n","authors":["Zhihang Tan","Jingrui Hou","Ping Wang","Qibiao Hu","Peng Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.12796v2.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2503.04804v4","updated":"2025-06-17T07:44:01Z","published":"2025-03-03T15:32:18Z","title":"What do Large Language Models Say About Animals? Investigating Risks of\n  Animal Harm in Generated Text","summary":"  As machine learning systems become increasingly embedded in society, their\nimpact on human and nonhuman life continues to escalate. Technical evaluations\nhave addressed a variety of potential harms from large language models (LLMs)\ntowards humans and the environment, but there is little empirical work\nregarding harms towards nonhuman animals. Following the growing recognition of\nanimal protection in regulatory and ethical AI frameworks, we present\nAnimalHarmBench (AHB), a benchmark for risks of animal harm in LLM-generated\ntext. Our benchmark dataset comprises 1,850 curated questions from Reddit post\ntitles and 2,500 synthetic questions based on 50 animal categories (e.g., cats,\nreptiles) and 50 ethical scenarios with a 70-30 public-private split. Scenarios\ninclude open-ended questions about how to treat animals, practical scenarios\nwith potential animal harm, and willingness-to-pay measures for the prevention\nof animal harm. Using the LLM-as-a-judge framework, responses are evaluated for\ntheir potential to increase or decrease harm, and evaluations are debiased for\nthe tendency of judges to judge their own outputs more favorably. AHB reveals\nsignificant differences across frontier LLMs, animal categories, scenarios, and\nsubreddits. We conclude with future directions for technical research and\naddressing the challenges of building evaluations on complex social and moral\ntopics.\n","authors":["Arturs Kanepajs","Aditi Basu","Sankalpa Ghose","Constance Li","Akshat Mehta","Ronak Mehta","Samuel David Tucker-Davis","Eric Zhou","Bob Fischer","Jacy Reese Anthis"],"pdf_url":"https://arxiv.org/pdf/2503.04804v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02958v3","updated":"2025-06-17T07:41:08Z","published":"2025-02-05T07:51:32Z","title":"Position: Editing Large Language Models Poses Serious Safety Risks","summary":"  Large Language Models (LLMs) contain large amounts of facts about the world.\nThese facts can become outdated over time, which has led to the development of\nknowledge editing methods (KEs) that can change specific facts in LLMs with\nlimited side effects. This position paper argues that editing LLMs poses\nserious safety risks that have been largely overlooked. First, we note the fact\nthat KEs are widely available, computationally inexpensive, highly performant,\nand stealthy makes them an attractive tool for malicious actors. Second, we\ndiscuss malicious use cases of KEs, showing how KEs can be easily adapted for a\nvariety of malicious purposes. Third, we highlight vulnerabilities in the AI\necosystem that allow unrestricted uploading and downloading of updated models\nwithout verification. Fourth, we argue that a lack of social and institutional\nawareness exacerbates this risk, and discuss the implications for different\nstakeholders. We call on the community to (i) research tamper-resistant models\nand countermeasures against malicious model editing, and (ii) actively engage\nin securing the AI ecosystem.\n","authors":["Paul Youssef","Zhixue Zhao","Daniel Braun","Jörg Schlötterer","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2502.02958v3.pdf","comment":"Accepted at ICML 2025"},{"id":"http://arxiv.org/abs/2506.14248v1","updated":"2025-06-17T07:11:00Z","published":"2025-06-17T07:11:00Z","title":"Re-Initialization Token Learning for Tool-Augmented Large Language\n  Models","summary":"  Large language models have demonstrated exceptional performance, yet struggle\nwith complex tasks such as numerical reasoning, plan generation. Integrating\nexternal tools, such as calculators and databases, into large language models\n(LLMs) is crucial for enhancing problem-solving capabilities. Current methods\nassign a unique token to each tool, enabling LLMs to call tools through token\nprediction-similar to word generation. However, this approach fails to account\nfor the relationship between tool and word tokens, limiting adaptability within\npre-trained LLMs. To address this issue, we propose a novel token learning\nmethod that aligns tool tokens with the existing word embedding space from the\nperspective of initialization, thereby enhancing model performance. We begin by\nconstructing prior token embeddings for each tool based on the tool's name or\ndescription, which are used to initialize and regularize the learnable tool\ntoken embeddings. This ensures the learned embeddings are well-aligned with the\nword token space, improving tool call accuracy. We evaluate the method on tasks\nsuch as numerical reasoning, knowledge-based question answering, and embodied\nplan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The\nresults demonstrate clear improvements over recent baselines, including CoT,\nREACT, ICL, and ToolkenGPT, indicating that our approach effectively augments\nLLMs with tools through relevant tokens across diverse domains.\n","authors":["Chenghao Li","Liu Liu","Baosheng Yu","Jiayan Qiu","Yibing Zhan"],"pdf_url":"https://arxiv.org/pdf/2506.14248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14245v1","updated":"2025-06-17T07:06:56Z","published":"2025-06-17T07:06:56Z","title":"Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes\n  Correct Reasoning in Base LLMs","summary":"  Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npromising paradigm for advancing the reasoning capabilities of Large Language\nModels (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned\nmodels often underperform their base models on the $Pass@K$ metric for\nsolution-finding, leading to the hypothesis that RLVR merely re-weights\nexisting reasoning paths at the cost of reasoning diversity. In this work, we\nresolve this contradiction by identifying the source of the problem: the\n$Pass@K$ metric itself is a flawed measure of reasoning, as it credits correct\nfinal answers that probably arise from inaccurate or incomplete chains of\nthought (CoTs). To address this, we introduce a more precise evaluation metric,\n$CoT$-$Pass@K$, which mandates that both the reasoning path and the final\nanswer be correct. We provide a new theoretical foundation that formalizes how\nRLVR, unlike traditional RL, is uniquely structured to incentivize logical\nintegrity. Our empirical results are supportive: using $CoT$-$Pass@K$, we\nobserve that RLVR can incentivize the generalization of correct reasoning for\nall values of $K$. Furthermore, by analyzing the training dynamics, we find\nthat this enhanced reasoning capability emerges early in the training process\nand smoothly generalizes. Our work provides a clear perspective on the role of\nRLVR, offers a more reliable method for its evaluation, and confirms its\npotential to genuinely advance machine reasoning.\n","authors":["Xumeng Wen","Zihan Liu","Shun Zheng","Zhijian Xu","Shengyu Ye","Zhirong Wu","Xiao Liang","Yang Wang","Junjie Li","Ziming Miao","Jiang Bian","Mao Yang"],"pdf_url":"https://arxiv.org/pdf/2506.14245v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2505.11368v2","updated":"2025-06-17T06:55:16Z","published":"2025-05-16T15:32:23Z","title":"GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM\n  Agents","summary":"  Large language models (LLMs) have been widely deployed as autonomous agents\ncapable of following user instructions and making decisions in real-world\napplications. Previous studies have made notable progress in benchmarking the\ninstruction following capabilities of LLMs in general domains, with a primary\nfocus on their inherent commonsense knowledge. Recently, LLMs have been\nincreasingly deployed as domain-oriented agents, which rely on domain-oriented\nguidelines that may conflict with their commonsense knowledge. These guidelines\nexhibit two key characteristics: they consist of a wide range of\ndomain-oriented rules and are subject to frequent updates. Despite these\nchallenges, the absence of comprehensive benchmarks for evaluating the\ndomain-oriented guideline following capabilities of LLMs presents a significant\nobstacle to their effective assessment and further development. In this paper,\nwe introduce GuideBench, a comprehensive benchmark designed to evaluate\nguideline following performance of LLMs. GuideBench evaluates LLMs on three\ncritical aspects: (i) adherence to diverse rules, (ii) robustness to rule\nupdates, and (iii) alignment with human preferences. Experimental results on a\nrange of LLMs indicate substantial opportunities for improving their ability to\nfollow domain-oriented guidelines.\n","authors":["Lingxiao Diao","Xinyue Xu","Wanxuan Sun","Cheng Yang","Zhuosheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.11368v2.pdf","comment":"ACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2506.14235v1","updated":"2025-06-17T06:49:13Z","published":"2025-06-17T06:49:13Z","title":"A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling\n  Historical Patterns in Temporal Knowledge Graphs","summary":"  Temporal knowledge graph reasoning aims to predict future events with\nknowledge of existing facts and plays a key role in various downstream tasks.\nPrevious methods focused on either graph structure learning or semantic\nreasoning, failing to integrate dual reasoning perspectives to handle different\nprediction scenarios. Moreover, they lack the capability to capture the\ninherent differences between historical and non-historical events, which limits\ntheir generalization across different temporal contexts. To this end, we\npropose a Multi-Expert Structural-Semantic Hybrid (MESH) framework that employs\nthree kinds of expert modules to integrate both structural and semantic\ninformation, guiding the reasoning process for different events. Extensive\nexperiments on three datasets demonstrate the effectiveness of our approach.\n","authors":["Yimin Deng","Yuxia Wu","Yejing Wang","Guoshuai Zhao","Li Zhu","Qidong Liu","Derong Xu","Zichuan Fu","Xian Wu","Yefeng Zheng","Xiangyu Zhao","Xueming Qian"],"pdf_url":"https://arxiv.org/pdf/2506.14235v1.pdf","comment":"ACL25 findings"},{"id":"http://arxiv.org/abs/2506.14234v1","updated":"2025-06-17T06:47:19Z","published":"2025-06-17T06:47:19Z","title":"Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just\n  Like an Olympiad Team","summary":"  Despite impressive progress on complex reasoning, current large language\nmodels (LLMs) typically operate in isolation - treating each problem as an\nindependent attempt, without accumulating or integrating experiential\nknowledge. In contrast, expert problem solvers - such as Olympiad or\nprogramming contest teams - leverage a rich tapestry of experiences: absorbing\nmentorship from coaches, developing intuition from past problems, leveraging\nknowledge of tool usage and library functionality, adapting strategies based on\nthe expertise and experiences of peers, continuously refining their reasoning\nthrough trial and error, and learning from other related problems even during\ncompetition. We introduce Xolver, a training-free multi-agent reasoning\nframework that equips a black-box LLM with a persistent, evolving memory of\nholistic experience. Xolver integrates diverse experience modalities, including\nexternal and self-retrieval, tool use, collaborative interactions, agent-driven\nevaluation, and iterative refinement. By learning from relevant strategies,\ncode fragments, and abstract reasoning patterns at inference time, Xolver\navoids generating solutions from scratch - marking a transition from isolated\ninference toward experience-aware language agents. Built on both open-weight\nand proprietary models, Xolver consistently outperforms specialized reasoning\nagents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses\nadvanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high.\nWith o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24\n(94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) -\nhighlighting holistic experience learning as a key step toward generalist\nagents capable of expert-level reasoning. Code and data are available at\nhttps://kagnlp.github.io/xolver.github.io/.\n","authors":["Md Tanzib Hosain","Salman Rahman","Md Kishor Morol","Md Rizwan Parvez"],"pdf_url":"https://arxiv.org/pdf/2506.14234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06926v2","updated":"2025-06-17T06:41:25Z","published":"2025-03-10T05:11:58Z","title":"Effect of Selection Format on LLM Performance","summary":"  This paper investigates a critical aspect of large language model (LLM)\nperformance: the optimal formatting of classification task options in prompts.\nThrough an extensive experimental study, we compared two selection formats --\nbullet points and plain English -- to determine their impact on model\nperformance. Our findings suggest that presenting options via bullet points\ngenerally yields better results, although there are some exceptions.\nFurthermore, our research highlights the need for continued exploration of\noption formatting to drive further improvements in model performance.\n","authors":["Yuchen Han","Yucheng Wu","Jeffrey Willard"],"pdf_url":"https://arxiv.org/pdf/2503.06926v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13227v2","updated":"2025-06-17T06:33:35Z","published":"2025-05-19T15:09:23Z","title":"Scaling Computer-Use Grounding via User Interface Decomposition and\n  Synthesis","summary":"  Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io.\n","authors":["Tianbao Xie","Jiaqi Deng","Xiaochuan Li","Junlin Yang","Haoyuan Wu","Jixuan Chen","Wenjing Hu","Xinyuan Wang","Yuhui Xu","Zekun Wang","Yiheng Xu","Junli Wang","Doyen Sahoo","Tao Yu","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2505.13227v2.pdf","comment":"49 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.15910v2","updated":"2025-06-17T06:29:03Z","published":"2025-02-21T19:54:46Z","title":"Modality-Aware Neuron Pruning for Unlearning in Multimodal Large\n  Language Models","summary":"  Generative models such as Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) trained on massive datasets can lead them to memorize\nand inadvertently reveal sensitive information, raising ethical and privacy\nconcerns. While some prior works have explored this issue in the context of\nLLMs, it presents a unique challenge for MLLMs due to the entangled nature of\nknowledge across modalities, making comprehensive unlearning more difficult. To\naddress this challenge, we propose Modality Aware Neuron Unlearning (MANU), a\nnovel unlearning framework for MLLMs designed to selectively clip neurons based\non their relative importance to the targeted forget data, curated for different\nmodalities. Specifically, MANU consists of two stages: important neuron\nselection and selective pruning. The first stage identifies and collects the\nmost influential neurons across modalities relative to the targeted forget\nknowledge, while the second stage is dedicated to pruning those selected\nneurons. MANU effectively isolates and removes the neurons that contribute most\nto the forget data within each modality, while preserving the integrity of\nretained knowledge. Our experiments conducted across various MLLM architectures\nillustrate that MANU can achieve a more balanced and comprehensive unlearning\nin each modality without largely affecting the overall model utility.\n","authors":["Zheyuan Liu","Guangyao Dou","Xiangchi Yuan","Chunhui Zhang","Zhaoxuan Tan","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.15910v2.pdf","comment":"ACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2506.14223v1","updated":"2025-06-17T06:25:35Z","published":"2025-06-17T06:25:35Z","title":"Fretting-Transformer: Encoder-Decoder Model for MIDI to Tablature\n  Transcription","summary":"  Music transcription plays a pivotal role in Music Information Retrieval\n(MIR), particularly for stringed instruments like the guitar, where symbolic\nmusic notations such as MIDI lack crucial playability information. This\ncontribution introduces the Fretting-Transformer, an encoderdecoder model that\nutilizes a T5 transformer architecture to automate the transcription of MIDI\nsequences into guitar tablature. By framing the task as a symbolic translation\nproblem, the model addresses key challenges, including string-fret ambiguity\nand physical playability. The proposed system leverages diverse datasets,\nincluding DadaGP, GuitarToday, and Leduc, with novel data pre-processing and\ntokenization strategies. We have developed metrics for tablature accuracy and\nplayability to quantitatively evaluate the performance. The experimental\nresults demonstrate that the Fretting-Transformer surpasses baseline methods\nlike A* and commercial applications like Guitar Pro. The integration of\ncontext-sensitive processing and tuning/capo conditioning further enhances the\nmodel's performance, laying a robust foundation for future developments in\nautomated guitar transcription.\n","authors":["Anna Hamberger","Sebastian Murgul","Jochen Schmidt","Michael Heizmann"],"pdf_url":"https://arxiv.org/pdf/2506.14223v1.pdf","comment":"Accepted to the 50th International Computer Music Conference (ICMC),\n  2025"},{"id":"http://arxiv.org/abs/2506.14213v1","updated":"2025-06-17T06:06:01Z","published":"2025-06-17T06:06:01Z","title":"Chaining Event Spans for Temporal Relation Grounding","summary":"  Accurately understanding temporal relations between events is a critical\nbuilding block of diverse tasks, such as temporal reading comprehension (TRC)\nand relation extraction (TRE). For example in TRC, we need to understand the\ntemporal semantic differences between the following two questions that are\nlexically near-identical: \"What finished right before the decision?\" or \"What\nfinished right after the decision?\". To discern the two questions, existing\nsolutions have relied on answer overlaps as a proxy label to contrast similar\nand dissimilar questions. However, we claim that answer overlap can lead to\nunreliable results, due to spurious overlaps of two dissimilar questions with\ncoincidentally identical answers. To address the issue, we propose a novel\napproach that elicits proper reasoning behaviors through a module for\npredicting time spans of events. We introduce the Timeline Reasoning Network\n(TRN) operating in a two-step inductive reasoning process: In the first step\nmodel initially answers each question with semantic and syntactic information.\nThe next step chains multiple questions on the same event to predict a\ntimeline, which is then used to ground the answers. Results on the TORQUE and\nTB-dense, TRC and TRE tasks respectively, demonstrate that TRN outperforms\nprevious methods by effectively resolving the spurious overlaps using the\npredicted timeline.\n","authors":["Jongho Kim","Dohyeon Lee","Minsoo Kim","Seung-won Hwang"],"pdf_url":"https://arxiv.org/pdf/2506.14213v1.pdf","comment":"In Proceedings of the 18th Conference of the European Chapter of the\n  Association for Computational Linguistics (Volume 1: Long Papers), pages\n  1689-1700"},{"id":"http://arxiv.org/abs/2506.14211v1","updated":"2025-06-17T06:00:07Z","published":"2025-06-17T06:00:07Z","title":"Explainable Detection of Implicit Influential Patterns in Conversations\n  via Data Augmentation","summary":"  In the era of digitalization, as individuals increasingly rely on digital\nplatforms for communication and news consumption, various actors employ\nlinguistic strategies to influence public perception. While models have become\nproficient at detecting explicit patterns, which typically appear in texts as\nsingle remarks referred to as utterances, such as social media posts, malicious\nactors have shifted toward utilizing implicit influential verbal patterns\nembedded within conversations. These verbal patterns aim to mentally penetrate\nthe victim's mind in order to influence them, enabling the actor to obtain the\ndesired information through implicit means. This paper presents an improved\napproach for detecting such implicit influential patterns. Furthermore, the\nproposed model is capable of identifying the specific locations of these\ninfluential elements within a conversation. To achieve this, the existing\ndataset was augmented using the reasoning capabilities of state-of-the-art\nlanguage models. Our designed framework resulted in a 6% improvement in the\ndetection of implicit influential patterns in conversations. Moreover, this\napproach improved the multi-label classification tasks related to both the\ntechniques used for influence and the vulnerability of victims by 33% and 43%,\nrespectively.\n","authors":["Sina Abdidizaji","Md Kowsher","Niloofar Yousefi","Ivan Garibay"],"pdf_url":"https://arxiv.org/pdf/2506.14211v1.pdf","comment":"Accepted at the HCI International conference 2025"},{"id":"http://arxiv.org/abs/2502.17421v2","updated":"2025-06-17T05:58:01Z","published":"2025-02-24T18:53:31Z","title":"LongSpec: Long-Context Lossless Speculative Decoding with Efficient\n  Drafting and Verification","summary":"  As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec.\n","authors":["Penghui Yang","Cunxiao Du","Fengzhuo Zhang","Haonan Wang","Tianyu Pang","Chao Du","Bo An"],"pdf_url":"https://arxiv.org/pdf/2502.17421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14206v1","updated":"2025-06-17T05:48:44Z","published":"2025-06-17T05:48:44Z","title":"CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data\n  Generation","summary":"  Training data has been proven to be one of the most critical components in\ntraining generative AI. However, obtaining high-quality data remains\nchallenging, with data privacy issues presenting a significant hurdle. To\naddress the need for high-quality data. Synthesize data has emerged as a\nmainstream solution, demonstrating impressive performance in areas such as\nimages, audio, and video. Generating mixed-type data, especially high-quality\ntabular data, still faces significant challenges. These primarily include its\ninherent heterogeneous data types, complex inter-variable relationships, and\nintricate column-wise distributions. In this paper, we introduce CausalDiffTab,\na diffusion model-based generative model specifically designed to handle mixed\ntabular data containing both numerical and categorical features, while being\nmore flexible in capturing complex interactions among variables. We further\npropose a hybrid adaptive causal regularization method based on the principle\nof Hierarchical Prior Fusion. This approach adaptively controls the weight of\ncausal regularization, enhancing the model's performance without compromising\nits generative capabilities. Comprehensive experiments conducted on seven\ndatasets demonstrate that CausalDiffTab outperforms baseline methods across all\nmetrics. Our code is publicly available at:\nhttps://github.com/Godz-z/CausalDiffTab.\n","authors":["Jia-Chen Zhang","Zheng Zhou","Yu-Jie Xiong","Chun-Ming Xia","Fei Dai"],"pdf_url":"https://arxiv.org/pdf/2506.14206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14205v1","updated":"2025-06-17T05:46:52Z","published":"2025-06-17T05:46:52Z","title":"AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents","summary":"  We introduce AgentSynth, a scalable and cost-efficient pipeline for\nautomatically synthesizing high-quality tasks and trajectory datasets for\ngeneralist computer-use agents. Leveraging information asymmetry, AgentSynth\nconstructs subtasks that are simple during generation but significantly more\nchallenging when composed into long-horizon tasks, enabling the creation of\nover 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based\ntask proposer guided by a persona, followed by an execution agent that\ncompletes the task and logs the trajectory. This process is repeated\niteratively to form a sequence of subtasks, which are then summarized by a\nseparate agent into a composite task of controllable difficulty. A key strength\nof AgentSynth is its ability to precisely modulate task complexity by varying\nthe number of subtasks. Empirical evaluations show that state-of-the-art LLM\nagents suffer a steep performance drop, from 18% success at difficulty level 1\nto just 4% at level 6, highlighting the benchmark's difficulty and\ndiscriminative power. Moreover, our pipeline achieves a low average cost of\n\\$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our\ncode and data are publicly available at\nhttps://github.com/sunblaze-ucb/AgentSynth\n","authors":["Jingxu Xie","Dylan Xu","Xuandong Zhao","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2506.14205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14204v1","updated":"2025-06-17T05:46:38Z","published":"2025-06-17T05:46:38Z","title":"Improving Practical Aspects of End-to-End Multi-Talker Speech\n  Recognition for Online and Offline Scenarios","summary":"  We extend the frameworks of Serialized Output Training (SOT) to address\npractical needs of both streaming and offline automatic speech recognition\n(ASR) applications. Our approach focuses on balancing latency and accuracy,\ncatering to real-time captioning and summarization requirements. We propose\nseveral key improvements: (1) Leveraging Continuous Speech Separation (CSS)\nsingle-channel front-end with end-to-end (E2E) systems for highly overlapping\nscenarios, challenging the conventional wisdom of E2E versus cascaded setups.\nThe CSS framework improves the accuracy of the ASR system by separating\noverlapped speech from multiple speakers. (2) Implementing dual models --\nConformer Transducer for streaming and Sequence-to-Sequence for offline -- or\nalternatively, a two-pass model based on cascaded encoders. (3) Exploring\nsegment-based SOT (segSOT) which is better suited for offline scenarios while\nalso enhancing readability of multi-talker transcriptions.\n","authors":["Aswin Shanmugam Subramanian","Amit Das","Naoyuki Kanda","Jinyu Li","Xiaofei Wang","Yifan Gong"],"pdf_url":"https://arxiv.org/pdf/2506.14204v1.pdf","comment":"Accepted to Interspeech 2025"},{"id":"http://arxiv.org/abs/2506.14203v1","updated":"2025-06-17T05:44:55Z","published":"2025-06-17T05:44:55Z","title":"Intended Target Identification for Anomia Patients with Gradient-based\n  Selective Augmentation","summary":"  In this study, we investigate the potential of language models (LMs) in\naiding patients experiencing anomia, a difficulty identifying the names of\nitems. Identifying the intended target item from patient's circumlocution\ninvolves the two challenges of term failure and error: (1) The terms relevant\nto identifying the item remain unseen. (2) What makes the challenge unique is\ninherent perturbed terms by semantic paraphasia, which are not exactly related\nto the target item, hindering the identification process. To address each, we\npropose robustifying the model from semantically paraphasic errors and\nenhancing the model with unseen terms with gradient-based selective\naugmentation. Specifically, the gradient value controls augmented data quality\namid semantic errors, while the gradient variance guides the inclusion of\nunseen but relevant terms. Due to limited domain-specific datasets, we evaluate\nthe model on the Tip-of-the-Tongue dataset as an intermediary task and then\napply our findings to real patient data from AphasiaBank. Our results\ndemonstrate strong performance against baselines, aiding anomia patients by\naddressing the outlined challenges.\n","authors":["Jongho Kim","Romain Storaï","Seung-won Hwang"],"pdf_url":"https://arxiv.org/pdf/2506.14203v1.pdf","comment":"EMNLP 2024 Findings (long)"},{"id":"http://arxiv.org/abs/2505.12368v2","updated":"2025-06-17T05:38:20Z","published":"2025-05-18T11:14:14Z","title":"CAPTURE: Context-Aware Prompt Injection Testing and Robustness\n  Enhancement","summary":"  Prompt injection remains a major security risk for large language models.\nHowever, the efficacy of existing guardrail models in context-aware settings\nremains underexplored, as they often rely on static attack benchmarks.\nAdditionally, they have over-defense tendencies. We introduce CAPTURE, a novel\ncontext-aware benchmark assessing both attack detection and over-defense\ntendencies with minimal in-domain examples. Our experiments reveal that current\nprompt injection guardrail models suffer from high false negatives in\nadversarial cases and excessive false positives in benign scenarios,\nhighlighting critical limitations. To demonstrate our framework's utility, we\ntrain CaptureGuard on our generated data. This new model drastically reduces\nboth false negative and false positive rates on our context-aware datasets\nwhile also generalizing effectively to external benchmarks, establishing a path\ntoward more robust and practical prompt injection defenses.\n","authors":["Gauri Kholkar","Ratinder Ahuja"],"pdf_url":"https://arxiv.org/pdf/2505.12368v2.pdf","comment":"Accepted in ACL LLMSec Workshop 2025"},{"id":"http://arxiv.org/abs/2506.01565v2","updated":"2025-06-17T05:37:58Z","published":"2025-06-02T11:43:46Z","title":"Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural\n  Understanding and Transcreation","summary":"  Culture is a rich and dynamic domain that evolves across both geography and\ntime. However, existing studies on cultural understanding with vision-language\nmodels (VLMs) primarily emphasize geographic diversity, often overlooking the\ncritical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a\nnovel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning\nancient Chinese dynasties, serves as a representative cultural heritage that\nreflects the profound temporal aspects of Chinese culture while remaining\nhighly popular in Chinese contemporary society. Hanfu-Bench comprises two core\ntasks: cultural visual understanding and cultural image transcreation.The\nformer task examines temporal-cultural feature recognition based on single- or\nmulti-image inputs through multiple-choice visual question answering, while the\nlatter focuses on transforming traditional attire into modern designs through\ncultural element inheritance and modern context adaptation. Our evaluation\nshows that closed VLMs perform comparably to non-experts on visual cutural\nunderstanding but fall short by 10\\% to human experts, while open VLMs lags\nfurther behind non-experts. For the transcreation task, multi-faceted human\nevaluation indicates that the best-performing model achieves a success rate of\nonly 42\\%. Our benchmark provides an essential testbed, revealing significant\nchallenges in this new direction of temporal cultural understanding and\ncreative adaptation.\n","authors":["Li Zhou","Lutong Yu","Dongchu Xie","Shaohuan Cheng","Wenyan Li","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2506.01565v2.pdf","comment":"cultural analysis, cultural visual understanding, cultural image\n  transcreation (update dataset license)"},{"id":"http://arxiv.org/abs/2506.14200v1","updated":"2025-06-17T05:36:39Z","published":"2025-06-17T05:36:39Z","title":"ELI-Why: Evaluating the Pedagogical Utility of Language Model\n  Explanations","summary":"  Language models today are widely used in education, yet their ability to\ntailor responses for learners with varied informational needs and knowledge\nbackgrounds remains under-explored. To this end, we introduce ELI-Why, a\nbenchmark of 13.4K \"Why\" questions to evaluate the pedagogical capabilities of\nlanguage models. We then conduct two extensive human studies to assess the\nutility of language model-generated explanatory answers (explanations) on our\nbenchmark, tailored to three distinct educational grades: elementary,\nhigh-school and graduate school. In our first study, human raters assume the\nrole of an \"educator\" to assess model explanations' fit to different\neducational grades. We find that GPT-4-generated explanations match their\nintended educational background only 50% of the time, compared to 79% for lay\nhuman-curated explanations. In our second study, human raters assume the role\nof a learner to assess if an explanation fits their own informational needs.\nAcross all educational backgrounds, users deemed GPT-4-generated explanations\n20% less suited on average to their informational needs, when compared to\nexplanations curated by lay people. Additionally, automated evaluation metrics\nreveal that explanations generated across different language model families for\ndifferent informational needs remain indistinguishable in their grade-level,\nlimiting their pedagogical effectiveness.\n","authors":["Brihi Joshi","Keyu He","Sahana Ramnath","Sadra Sabouri","Kaitlyn Zhou","Souti Chattopadhyay","Swabha Swayamdipta","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2506.14200v1.pdf","comment":"Findings of ACL 2025"},{"id":"http://arxiv.org/abs/2410.01444v5","updated":"2025-06-17T05:34:44Z","published":"2024-10-02T11:54:06Z","title":"Geometric Signatures of Compositionality Across a Language Model's\n  Lifetime","summary":"  By virtue of linguistic compositionality, few syntactic rules and a finite\nlexicon can generate an unbounded number of sentences. That is, language,\nthough seemingly high-dimensional, can be explained using relatively few\ndegrees of freedom. An open question is whether contemporary language models\n(LMs) reflect the intrinsic simplicity of language that is enabled by\ncompositionality. We take a geometric view of this problem by relating the\ndegree of compositionality in a dataset to the intrinsic dimension (ID) of its\nrepresentations under an LM, a measure of feature complexity. We find not only\nthat the degree of dataset compositionality is reflected in representations'\nID, but that the relationship between compositionality and geometric complexity\narises due to learned linguistic features over training. Finally, our analyses\nreveal a striking contrast between nonlinear and linear dimensionality, showing\nthey respectively encode semantic and superficial aspects of linguistic\ncomposition.\n","authors":["Jin Hwa Lee","Thomas Jiralerspong","Lei Yu","Yoshua Bengio","Emily Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.01444v5.pdf","comment":"Published at ACL 2025"},{"id":"http://arxiv.org/abs/2502.11425v2","updated":"2025-06-17T05:34:15Z","published":"2025-02-17T04:37:07Z","title":"Counterfactual-Consistency Prompting for Relative Temporal Understanding\n  in Large Language Models","summary":"  Despite the advanced capabilities of large language models (LLMs), their\ntemporal reasoning ability remains underdeveloped. Prior works have highlighted\nthis limitation, particularly in maintaining temporal consistency when\nunderstanding events. For example, models often confuse mutually exclusive\ntemporal relations like ``before'' and ``after'' between events and make\ninconsistent predictions. In this work, we tackle the issue of temporal\ninconsistency in LLMs by proposing a novel counterfactual prompting approach.\nOur method generates counterfactual questions and enforces collective\nconstraints, enhancing the model's consistency. We evaluate our method on\nmultiple datasets, demonstrating significant improvements in event ordering for\nexplicit and implicit events and temporal commonsense understanding by\neffectively addressing temporal inconsistencies.\n","authors":["Jongho Kim","Seung-won Hwang"],"pdf_url":"https://arxiv.org/pdf/2502.11425v2.pdf","comment":"ACL 2025 main (short)"},{"id":"http://arxiv.org/abs/2506.14199v1","updated":"2025-06-17T05:33:40Z","published":"2025-06-17T05:33:40Z","title":"MAS-LitEval : Multi-Agent System for Literary Translation Quality\n  Assessment","summary":"  Literary translation requires preserving cultural nuances and stylistic\nelements, which traditional metrics like BLEU and METEOR fail to assess due to\ntheir focus on lexical overlap. This oversight neglects the narrative\nconsistency and stylistic fidelity that are crucial for literary works. To\naddress this, we propose MAS-LitEval, a multi-agent system using Large Language\nModels (LLMs) to evaluate translations based on terminology, narrative, and\nstyle. We tested MAS-LitEval on translations of The Little Prince and A\nConnecticut Yankee in King Arthur's Court, generated by various LLMs, and\ncompared it to traditional metrics. \\textbf{MAS-LitEval} outperformed these\nmetrics, with top models scoring up to 0.890 in capturing literary nuances.\nThis work introduces a scalable, nuanced framework for Translation Quality\nAssessment (TQA), offering a practical tool for translators and researchers.\n","authors":["Junghwan Kim","Kieun Park","Sohee Park","Hyunggug Kim","Bongwon Suh"],"pdf_url":"https://arxiv.org/pdf/2506.14199v1.pdf","comment":"4 Pages, 2 tables, EMNLP submitted"},{"id":"http://arxiv.org/abs/2304.03030v2","updated":"2025-06-17T05:11:56Z","published":"2023-04-06T12:29:53Z","title":"Compression of enumerations and gain","summary":"  We study the compressibility of enumerations in the context of Kolmogorov\ncomplexity, focusing on strong and weak forms of compression and their gain:\nthe amount of auxiliary information embedded in the compressed enumeration. The\nexistence of strong compression and weak gainless compression is shown for any\ncomputably enumerable (c.e.) set. The density problem of c.e. sets with respect\nto their prefix complexity is reduced to the question of whether every c.e. set\nis well-compressible, which we study via enumeration games.\n","authors":["George Barmpalias","Xiaoyan Zhang","Bohua Zhan"],"pdf_url":"https://arxiv.org/pdf/2304.03030v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18770v3","updated":"2025-06-17T05:05:12Z","published":"2025-02-26T02:57:59Z","title":"Reward Shaping to Mitigate Reward Hacking in RLHF","summary":"  Reinforcement Learning from Human Feedback (RLHF) is essential for aligning\nlarge language models (LLMs) with human values. However, RLHF is susceptible to\n\\emph{reward hacking}, where the agent exploits flaws in the reward function\nrather than learning the intended behavior, thus degrading alignment. Although\nreward shaping helps stabilize RLHF and partially mitigate reward hacking, a\nsystematic investigation into shaping techniques and their underlying\nprinciples remains lacking. To bridge this gap, we present a comprehensive\nstudy of the prevalent reward shaping methods. Our analysis suggests two key\ndesign principles: (1) the RL reward should be bounded, and (2) the RL reward\nbenefits from rapid initial growth followed by gradual convergence. Guided by\nthese insights, we propose Preference As Reward (PAR), a novel approach that\nleverages the latent preferences embedded within the reward model as the signal\nfor reinforcement learning. We evaluated PAR on two base models, Gemma2-2B, and\nLlama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF.\nExperimental results demonstrate PAR's superior performance over other reward\nshaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at\nleast 5 percentage points higher than competing approaches. Furthermore, PAR\nexhibits remarkable data efficiency, requiring only a single reference reward\nfor optimal performance, and maintains robustness against reward hacking even\nafter two full epochs of training. The code is available at\nhttps://github.com/PorUna-byte/PAR, and the Work done during the internship at\nStepFun by Jiayi Fu.\n","authors":["Jiayi Fu","Xuandong Zhao","Chengyuan Yao","Heng Wang","Qi Han","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2502.18770v3.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2506.14190v1","updated":"2025-06-17T05:05:09Z","published":"2025-06-17T05:05:09Z","title":"AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR","summary":"  Developing code-switched ASR systems is challenging due to language ambiguity\nand limited exposure to multilingual, code-switched data, while collecting such\nspeech is costly. Prior work generates synthetic audio from text, but these\nmethods are computationally intensive and hard to scale. We introduce\nAsyncSwitch, a novel asynchronous adaptation framework that leverages\nlarge-scale, text-rich web data to pre-expose ASR models to diverse\ncode-switched domains before fine-tuning on paired speech-text corpora. Our\nthree-stage process (1) trains decoder self-attention and feedforward layers on\ncode-switched text, (2) aligns decoder and encoder via cross-attention using\nlimited speech-text data, and (3) fully fine-tunes the entire model.\nExperiments with Whisper on Malay-English code-switching demonstrate a 9.02%\nrelative WER reduction, while improving monolingual performance in Singlish,\nMalay, and other English variants.\n","authors":["Tuan Nguyen","Huy-Dat Tran"],"pdf_url":"https://arxiv.org/pdf/2506.14190v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  This paper is a preprint version submitted to the 2025 IEEE Automatic Speech\n  Recognition and Understanding Workshop (ASRU 2025)"},{"id":"http://arxiv.org/abs/2506.00854v2","updated":"2025-06-17T04:58:31Z","published":"2025-06-01T06:26:32Z","title":"EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG\n  Alignment via Large Language Model and Contrastive Learning on ChineseEEG","summary":"  We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one\nof the earliest open-vocabulary EEG-to-text generation frameworks tailored for\nChinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact\npretrained language model (MiniLM), our architecture aligns multichannel brain\nsignals with natural language representations via masked pretraining and\ncontrastive learning. Using a subset of the ChineseEEG dataset, where each\nsentence contains approximately ten Chinese characters aligned with 128-channel\nEEG recorded at 256 Hz, we segment EEG into per-character embeddings and\npredict full sentences in a zero-shot setting. The decoder is trained with\nteacher forcing and padding masks to accommodate variable-length sequences.\nEvaluation on over 1,500 training-validation sentences and 300 held-out test\nsamples shows promising lexical alignment, with a best BLEU-1 score of 6.38\\%.\nWhile syntactic fluency remains a challenge, our findings demonstrate the\nfeasibility of non-phonetic, cross-modal language decoding from EEG. This work\nopens a new direction in multilingual brain-to-text research and lays the\nfoundation for future cognitive-language interfaces in Chinese.\n","authors":["Jacky Tai-Yu Lu","Jung Chiang","Chi-Sheng Chen","Anna Nai-Yun Tung","Hsiang Wei Hu","Yuan Chiao Cheng"],"pdf_url":"https://arxiv.org/pdf/2506.00854v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14177v1","updated":"2025-06-17T04:37:16Z","published":"2025-06-17T04:37:16Z","title":"Can we train ASR systems on Code-switch without real code-switch data?\n  Case study for Singapore's languages","summary":"  Code-switching (CS), common in multilingual settings, presents challenges for\nASR due to scarce and costly transcribed data caused by linguistic complexity.\nThis study investigates building CS-ASR using synthetic CS data. We propose a\nphrase-level mixing method to generate synthetic CS data that mimics natural\npatterns. Utilizing monolingual augmented with synthetic phrase-mixed CS data\nto fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T). This\npaper focuses on three under-resourced Southeast Asian language pairs:\nMalay-English (BM-EN), Mandarin-Malay (ZH-BM), and Tamil-English (TA-EN),\nestablishing a new comprehensive benchmark for CS-ASR to evaluate the\nperformance of leading ASR models. Experimental results show that the proposed\ntraining strategy enhances ASR performance on monolingual and CS tests, with\nBM-EN showing highest gains, then TA-EN and ZH-BM. This finding offers a\ncost-effective approach for CS-ASR development, benefiting research and\nindustry.\n","authors":["Tuan Nguyen","Huy-Dat Tran"],"pdf_url":"https://arxiv.org/pdf/2506.14177v1.pdf","comment":"Accepted by Interspeech 2025"},{"id":"http://arxiv.org/abs/2506.14175v1","updated":"2025-06-17T04:34:27Z","published":"2025-06-17T04:34:27Z","title":"GRAM: A Generative Foundation Reward Model for Reward Generalization","summary":"  In aligning large language models (LLMs), reward models have played an\nimportant role, but are standardly trained as discriminative models and rely\nonly on labeled human preference data. In this paper, we explore methods that\ntrain reward models using both unlabeled and labeled data. Building on the\ngenerative models in LLMs, we develop a generative reward model that is first\ntrained via large-scale unsupervised learning and then fine-tuned via\nsupervised learning. We also show that by using label smoothing, we are in fact\noptimizing a regularized pairwise ranking loss. This result, in turn, provides\na new view of training reward models, which links generative models and\ndiscriminative models under the same class of training objectives. The outcome\nof these techniques is a foundation reward model, which can be applied to a\nwide range of tasks with little or no further fine-tuning effort. Extensive\nexperiments show that this model generalizes well across several tasks,\nincluding response ranking, reinforcement learning from human feedback, and\ntask adaptation with fine-tuning, achieving significant performance\nimprovements over several strong baseline models.\n","authors":["Chenglong Wang","Yang Gan","Yifu Huo","Yongyu Mu","Qiaozhi He","Murun Yang","Bei Li","Tong Xiao","Chunliang Zhang","Tongran Liu","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.14175v1.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2506.00555v2","updated":"2025-06-17T03:59:45Z","published":"2025-05-31T13:22:55Z","title":"MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal\n  Medical Reasoning","summary":"  Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential\nin multimodal diagnostic tasks. However, existing single-agent models struggle\nto generalize across diverse medical specialties, limiting their performance.\nRecent efforts introduce multi-agent collaboration frameworks inspired by\nclinical workflows, where general practitioners (GPs) and specialists interact\nin a fixed sequence. Despite improvements, these static pipelines lack\nflexibility and adaptability in reasoning. To address this, we propose\nMMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that\nenables dynamic, optimized collaboration among medical agents. Specifically, we\ntrain two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to\nassign patients to appropriate specialties, while the attending physician\nintegrates the judgments from multi-specialists and its own knowledge to make\nfinal decisions. To address the inconsistency in specialist outputs, we\nintroduce a curriculum learning (CL)-guided RL strategy that progressively\nteaches the attending physician to balance between imitating specialists and\ncorrecting their mistakes. Experiments on five medical VQA benchmarks\ndemonstrate that MMedAgent-RL not only outperforms both open-source and\nproprietary Med-LVLMs, but also exhibits human-like reasoning patterns.\nNotably, it achieves an average performance gain of 20.7% over supervised\nfine-tuning baselines.\n","authors":["Peng Xia","Jinglu Wang","Yibo Peng","Kaide Zeng","Xian Wu","Xiangru Tang","Hongtu Zhu","Yun Li","Shujie Liu","Yan Lu","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2506.00555v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14161v1","updated":"2025-06-17T03:50:57Z","published":"2025-06-17T03:50:57Z","title":"MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation\n  of LLMs via Theory of Mind","summary":"  Theory of Mind (ToM) in Large Language Models (LLMs) refers to their capacity\nfor reasoning about mental states, yet failures in this capacity often manifest\nas systematic implicit bias. Evaluating this bias is challenging, as\nconventional direct-query methods are susceptible to social desirability\neffects and fail to capture its subtle, multi-dimensional nature. To this end,\nwe propose an evaluation framework that leverages the Stereotype Content Model\n(SCM) to reconceptualize bias as a multi-dimensional failure in ToM across\nCompetence, Sociability, and Morality. The framework introduces two indirect\ntasks: the Word Association Bias Test (WABT) to assess implicit lexical\nassociations and the Affective Attribution Test (AAT) to measure covert\naffective leanings, both designed to probe latent stereotypes without\ntriggering model avoidance. Extensive experiments on 8 State-of-the-Art LLMs\ndemonstrate our framework's capacity to reveal complex bias structures,\nincluding pervasive sociability bias, multi-dimensional divergence, and\nasymmetric stereotype amplification, thereby providing a more robust\nmethodology for identifying the structural nature of implicit bias.\n","authors":["Yanlin Li","Hao Liu","Huimin Liu","Yinwei Wei","Yupeng Hu"],"pdf_url":"https://arxiv.org/pdf/2506.14161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14158v1","updated":"2025-06-17T03:38:19Z","published":"2025-06-17T03:38:19Z","title":"S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for\n  Efficient Inference of Large Language Models","summary":"  Large language models (LLMs) exhibit remarkable reasoning capabilities across\ndiverse downstream tasks. However, their autoregressive nature leads to\nsubstantial inference latency, posing challenges for real-time applications.\nSpeculative sampling mitigates this issue by introducing a drafting phase\nfollowed by a parallel validation phase, enabling faster token generation and\nverification. Existing approaches, however, overlook the inherent coherence in\ntext generation, limiting their efficiency. To address this gap, we propose a\nSpeculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework,\nwhich extends speculative sampling by leveraging multi-head drafting for rapid\ntoken generation and a continuous verification tree for efficient candidate\nvalidation and feature reuse. Experimental results demonstrate that S$^4$C\nsurpasses baseline methods across mainstream tasks, offering enhanced\nefficiency, parallelism, and the ability to generate more valid tokens with\nfewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an\nacceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods.\n","authors":["Tao He","Guang Huang","Yu Yang","Tianshi Xu","Sicheng Zhao","Guiguang Ding","Pengyang Wang","Feng Tian"],"pdf_url":"https://arxiv.org/pdf/2506.14158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14157v1","updated":"2025-06-17T03:37:41Z","published":"2025-06-17T03:37:41Z","title":"DCRM: A Heuristic to Measure Response Pair Quality in Preference\n  Optimization","summary":"  Recent research has attempted to associate preference optimization (PO)\nperformance with the underlying preference datasets. In this work, our\nobservation is that the differences between the preferred response $y^+$ and\ndispreferred response $y^-$ influence what LLMs can learn, which may not match\nthe desirable differences to learn. Therefore, we use distance and reward\nmargin to quantify these differences, and combine them to get Distance\nCalibrated Reward Margin (DCRM), a metric that measures the quality of a\nresponse pair for PO. Intuitively, DCRM encourages minimal noisy differences\nand maximal desired differences. With this, we study 3 types of commonly used\npreference datasets, classified along two axes: the source of the responses and\nthe preference labeling function. We establish a general correlation between\nhigher DCRM of the training set and better learning outcome. Inspired by this,\nwe propose a best-of-$N^2$ pairing method that selects response pairs with the\nhighest DCRM. Empirically, in various settings, our method produces training\ndatasets that can further improve models' performance on AlpacaEval, MT-Bench,\nand Arena-Hard over the existing training sets.\n","authors":["Chengyu Huang","Tanya Goyal"],"pdf_url":"https://arxiv.org/pdf/2506.14157v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07631v2","updated":"2025-06-17T03:34:55Z","published":"2025-03-04T19:37:33Z","title":"OWLViz: An Open-World Benchmark for Visual Question Answering","summary":"  We present a challenging benchmark for the Open WorLd VISual question\nanswering (OWLViz) task. OWLViz presents concise, unambiguous queries that\nrequire integrating multiple capabilities, including visual understanding, web\nexploration, and specialized tool usage. While humans achieve 69.2% accuracy on\nthese intuitive tasks, even state-of-the-art VLMs struggle, with the best\nmodel, Gemini 2.0, achieving only 26.6% accuracy. Current agentic VLMs, which\nrely on limited vision and vision-language models as tools, perform even worse.\nThis performance gap reveals significant limitations in multimodal systems'\nability to select appropriate tools and execute complex reasoning sequences,\nestablishing new directions for advancing practical AI research.\n","authors":["Thuy Nguyen","Dang Nguyen","Hoang Nguyen","Thuan Luong","Long Hoang Dang","Viet Dac Lai"],"pdf_url":"https://arxiv.org/pdf/2503.07631v2.pdf","comment":"ICML 2025 Workshop on Multi-Agent Systems in the Era of Foundation\n  Models: Opportunities, Challenges, and Futures. (8 pages + appendix)"},{"id":"http://arxiv.org/abs/2506.14153v1","updated":"2025-06-17T03:30:58Z","published":"2025-06-17T03:30:58Z","title":"Pushing the Performance of Synthetic Speech Detection with\n  Kolmogorov-Arnold Networks and Self-Supervised Learning Models","summary":"  Recent advancements in speech synthesis technologies have led to increasingly\nadvanced spoofing attacks, posing significant challenges for automatic speaker\nverification systems. While systems based on self-supervised learning (SSL)\nmodels, particularly the XLSR-Conformer model, have demonstrated remarkable\nperformance in synthetic speech detection, there remains room for architectural\nimprovements. In this paper, we propose a novel approach that replaces the\ntraditional Multi-Layer Perceptron in the XLSR-Conformer model with a\nKolmogorov-Arnold Network (KAN), a novel architecture based on the\nKolmogorov-Arnold representation theorem. Our results on ASVspoof2021\ndemonstrate that integrating KAN into the SSL-based models can improve the\nperformance by 60.55% relatively on LA and DF sets, further achieving 0.70% EER\non the 21LA set. These findings suggest that incorporating KAN into SSL-based\nmodels is a promising direction for advances in synthetic speech detection.\n","authors":["Tuan Dat Phuong","Long-Vu Hoang","Huy Dat Tran"],"pdf_url":"https://arxiv.org/pdf/2506.14153v1.pdf","comment":"Accepted to Interspeech 2025"},{"id":"http://arxiv.org/abs/2505.20613v2","updated":"2025-06-17T03:25:43Z","published":"2025-05-27T01:26:11Z","title":"REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning","summary":"  Nowadays, formal theorem provers have made monumental progress on high-school\nand competition-level mathematics, but few of them generalize to more advanced\nmathematics. In this paper, we present REAL-Prover, a new open-source stepwise\ntheorem prover for Lean 4 to push this boundary. This prover, based on our\nfine-tuned large language model (REAL-Prover-v1) and integrated with a\nretrieval system (Leansearch-PS), notably boosts performance on solving\ncollege-level mathematics problems. To train REAL-Prover-v1, we developed\nHERALD-AF, a data extraction pipeline that converts natural language math\nproblems into formal statements, and a new open-source Lean 4 interactive\nenvironment (Jixia-interactive) to facilitate synthesis data collection. In our\nexperiments, our prover using only supervised fine-tune achieves competitive\nresults with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable\nto state-of-the-art (SOTA) models. To further evaluate our approach, we\nintroduce FATE-M, a new benchmark focused on algebraic problems, where our\nprover achieves a SOTA success rate of 56.7% (Pass@64).\n","authors":["Ziju Shen","Naohao Huang","Fanyi Yang","Yutong Wang","Guoxiong Gao","Tianyi Xu","Jiedong Jiang","Wanyi He","Pu Yang","Mengzhou Sun","Haocheng Ju","Peihao Wu","Bryan Dai","Bin Dong"],"pdf_url":"https://arxiv.org/pdf/2505.20613v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14148v1","updated":"2025-06-17T03:25:38Z","published":"2025-06-17T03:25:38Z","title":"Acoustic scattering AI for non-invasive object classifications: A case\n  study on hair assessment","summary":"  This paper presents a novel non-invasive object classification approach using\nacoustic scattering, demonstrated through a case study on hair assessment. When\nan incident wave interacts with an object, it generates a scattered acoustic\nfield encoding structural and material properties. By emitting acoustic stimuli\nand capturing the scattered signals from head-with-hair-sample objects, we\nclassify hair type and moisture using AI-driven, deep-learning-based sound\nclassification. We benchmark comprehensive methods, including (i) fully\nsupervised deep learning, (ii) embedding-based classification, (iii) supervised\nfoundation model fine-tuning, and (iv) self-supervised model fine-tuning. Our\nbest strategy achieves nearly 90% classification accuracy by fine-tuning all\nparameters of a self-supervised model. These results highlight acoustic\nscattering as a privacy-preserving, non-contact alternative to visual\nclassification, opening huge potential for applications in various industries.\n","authors":["Long-Vu Hoang","Tuan Nguyen","Tran Huy Dat"],"pdf_url":"https://arxiv.org/pdf/2506.14148v1.pdf","comment":"Accepted to Interspeech 2025"},{"id":"http://arxiv.org/abs/2506.14142v1","updated":"2025-06-17T03:10:33Z","published":"2025-06-17T03:10:33Z","title":"RadFabric: Agentic AI System with Reasoning Capability for Radiology","summary":"  Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic\nconditions, but current automated systems face limitations in pathology\ncoverage, diagnostic accuracy, and integration of visual and textual reasoning.\nTo address these gaps, we propose RadFabric, a multi agent, multimodal\nreasoning framework that unifies visual and textual analysis for comprehensive\nCXR interpretation. RadFabric is built on the Model Context Protocol (MCP),\nenabling modularity, interoperability, and scalability for seamless integration\nof new diagnostic agents. The system employs specialized CXR agents for\npathology detection, an Anatomical Interpretation Agent to map visual findings\nto precise anatomical structures, and a Reasoning Agent powered by large\nmultimodal reasoning models to synthesize visual, anatomical, and clinical data\ninto transparent and evidence based diagnoses. RadFabric achieves significant\nperformance improvements, with near-perfect detection of challenging\npathologies like fractures (1.000 accuracy) and superior overall diagnostic\naccuracy (0.799) compared to traditional systems (0.229 to 0.527). By\nintegrating cross modal feature alignment and preference-driven reasoning,\nRadFabric advances AI-driven radiology toward transparent, anatomically\nprecise, and clinically actionable CXR analysis.\n","authors":["Wenting Chen","Yi Dong","Zhaojun Ding","Yucheng Shi","Yifan Zhou","Fang Zeng","Yijun Luo","Tianyu Lin","Yihang Su","Yichen Wu","Kai Zhang","Zhen Xiang","Tianming Liu","Ninghao Liu","Lichao Sun","Yixuan Yuan","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2506.14142v1.pdf","comment":"4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2505.17238v2","updated":"2025-06-17T02:57:43Z","published":"2025-05-22T19:31:40Z","title":"Personalizing Student-Agent Interactions Using Log-Contextualized\n  Retrieval Augmented Generation (RAG)","summary":"  Collaborative dialogue offers rich insights into students' learning and\ncritical thinking, which is essential for personalizing pedagogical agent\ninteractions in STEM+C settings. While large language models (LLMs) facilitate\ndynamic pedagogical interactions, hallucinations undermine confidence, trust,\nand instructional value. Retrieval-augmented generation (RAG) grounds LLM\noutputs in curated knowledge but requires a clear semantic link between user\ninput and a knowledge base, which is often weak in student dialogue. We propose\nlog-contextualized RAG (LC-RAG), which enhances RAG retrieval by using\nenvironment logs to contextualize collaborative discourse. Our findings show\nthat LC-RAG improves retrieval over a discourse-only baseline and allows our\ncollaborative peer agent, Copa, to deliver relevant, personalized guidance that\nsupports students' critical thinking and epistemic decision-making in a\ncollaborative computational modeling environment, C2STEM.\n","authors":["Clayton Cohn","Surya Rayala","Caitlin Snyder","Joyce Fonteles","Shruti Jain","Naveeduddin Mohammed","Umesh Timalsina","Sarah K. Burriss","Ashwin T S","Namrata Srivastava","Menton Deweese","Angela Eeds","Gautam Biswas"],"pdf_url":"https://arxiv.org/pdf/2505.17238v2.pdf","comment":"To appear in the International Conference on Artificial Intelligence\n  in Education (AIED25) Workshop on Epistemics and Decision-Making in\n  AI-Supported Education"},{"id":"http://arxiv.org/abs/2506.01391v2","updated":"2025-06-17T02:57:04Z","published":"2025-06-02T07:30:29Z","title":"AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning","summary":"  The recent progress of large language model agents has opened new\npossibilities for automating tasks through graphical user interfaces (GUIs),\nespecially in mobile environments where intelligent interaction can greatly\nenhance usability. However, practical deployment of such agents remains\nconstrained by several key challenges. Existing training data is often noisy\nand lack semantic diversity, which hinders the learning of precise grounding\nand planning. Models trained purely by imitation tend to overfit to seen\ninterface patterns and fail to generalize in unfamiliar scenarios. Moreover,\nmost prior work focuses on English interfaces while overlooks the growing\ndiversity of non-English applications such as those in the Chinese mobile\necosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent\nbuilt for robust and efficient on-device GUI interaction. Our training pipeline\nincludes grounding-aware pre-training to enhance perception, supervised\nfine-tuning on high-quality Chinese and English trajectories to imitate\nhuman-like actions, and reinforcement fine-tuning with GRPO to improve\nreasoning capability. We also introduce a compact action space that reduces\noutput length and supports low-latency execution on mobile devices.\nAgentCPM-GUI achieves state-of-the-art performance on five public benchmarks\nand a new Chinese GUI benchmark called CAGUI, reaching $96.9\\%$ Type-Match and\n$91.3\\%$ Exact-Match. To facilitate reproducibility and further research, we\npublicly release all code, model checkpoint, and evaluation data.\n","authors":["Zhong Zhang","Yaxi Lu","Yikun Fu","Yupeng Huo","Shenzhi Yang","Yesai Wu","Han Si","Xin Cong","Haotian Chen","Yankai Lin","Jie Xie","Wei Zhou","Wang Xu","Yuanheng Zhang","Zhou Su","Zhongwu Zhai","Xiaoming Liu","Yudong Mei","Jianming Xu","Hongyan Tian","Chongyi Wang","Chi Chen","Yuan Yao","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2506.01391v2.pdf","comment":"Updated results in Table 2 and Table 3; The project is available at\n  https://github.com/OpenBMB/AgentCPM-GUI"},{"id":"http://arxiv.org/abs/2503.16974v3","updated":"2025-06-17T02:40:54Z","published":"2025-03-21T09:43:37Z","title":"Assessing Consistency and Reproducibility in the Outputs of Large\n  Language Models: Evidence Across Diverse Finance and Accounting Tasks","summary":"  This study provides the first comprehensive assessment of consistency and\nreproducibility in Large Language Model (LLM) outputs in finance and accounting\nresearch. We evaluate how consistently LLMs produce outputs given identical\ninputs through extensive experimentation with 50 independent runs across five\ncommon tasks: classification, sentiment analysis, summarization, text\ngeneration, and prediction. Using three OpenAI models (GPT-3.5-turbo,\nGPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse\nfinancial source texts and data, covering MD&As, FOMC statements, finance news\narticles, earnings call transcripts, and financial statements. Our findings\nreveal substantial but task-dependent consistency, with binary classification\nand sentiment analysis achieving near-perfect reproducibility, while complex\ntasks show greater variability. More advanced models do not consistently\ndemonstrate better consistency and reproducibility, with task-specific patterns\nemerging. LLMs significantly outperform expert human annotators in consistency\nand maintain high agreement even where human experts significantly disagree. We\nfurther find that simple aggregation strategies across 3-5 runs dramatically\nimprove consistency. We also find that aggregation may come with an additional\nbenefit of improved accuracy for sentiment analysis when using newer models.\nSimulation analysis reveals that despite measurable inconsistency in LLM\noutputs, downstream statistical inferences remain remarkably robust. These\nfindings address concerns about what we term \"G-hacking,\" the selective\nreporting of favorable outcomes from multiple Generative AI runs, by\ndemonstrating that such risks are relatively low for finance and accounting\ntasks.\n","authors":["Julian Junyan Wang","Victor Xiaoqi Wang"],"pdf_url":"https://arxiv.org/pdf/2503.16974v3.pdf","comment":"89 pages, 20 tables, 15 figures"},{"id":"http://arxiv.org/abs/2506.14123v1","updated":"2025-06-17T02:37:04Z","published":"2025-06-17T02:37:04Z","title":"Sampling from Your Language Model One Byte at a Time","summary":"  Tokenization is used almost universally by modern language models, enabling\nefficient text representation using multi-byte or multi-character tokens.\nHowever, prior work has shown that tokenization can introduce distortion into\nthe model's generations. For example, users are often advised not to end their\nprompts with a space because it prevents the model from including the space as\npart of the next token. This Prompt Boundary Problem (PBP) also arises in\nlanguages such as Chinese and in code generation, where tokens often do not\nline up with syntactic boundaries. Additionally mismatching tokenizers often\nhinder model composition and interoperability. For example, it is not possible\nto directly ensemble models with different tokenizers due to their mismatching\nvocabularies. To address these issues, we present an inference-time method to\nconvert any autoregressive LM with a BPE tokenizer into a character-level or\nbyte-level LM, without changing its generative distribution at the text level.\nOur method efficient solves the PBP and is also able to unify the vocabularies\nof language models with different tokenizers, allowing one to ensemble LMs with\ndifferent tokenizers at inference time as well as transfer the post-training\nfrom one model to another using proxy-tuning. We demonstrate in experiments\nthat the ensemble and proxy-tuned models outperform their constituents on\ndownstream evals.\n","authors":["Jonathan Hayase","Alisa Liu","Noah A. Smith","Sewoong Oh"],"pdf_url":"https://arxiv.org/pdf/2506.14123v1.pdf","comment":"23 pages, 8 figures"},{"id":"http://arxiv.org/abs/2506.13366v2","updated":"2025-06-17T02:34:00Z","published":"2025-06-16T11:15:21Z","title":"Enhancing Goal-oriented Proactive Dialogue Systems via Consistency\n  Reflection and Correction","summary":"  Goal-oriented proactive dialogue systems are designed to guide user\nconversations seamlessly towards specific objectives by planning a\ngoal-oriented path. However, previous research has focused predominantly on\noptimizing these paths while neglecting the inconsistencies that may arise\nbetween generated responses and dialogue contexts, including user profiles,\ndialogue history, domain knowledge, and subgoals. To address this issue, we\nintroduce a model-agnostic two-stage Consistency Reflection and Correction\n(CRC) framework. Specifically, in the consistency reflection stage, the model\nis prompted to reflect on the discrepancies between generated responses and\ndialogue contexts, identifying inconsistencies and suggesting possible\ncorrections. In the consistency correction stage, the model generates responses\nthat are more consistent with the dialogue context based on these reflection\nresults. We conducted experiments on various model architectures with different\nparameter sizes, including encoder-decoder models (BART, T5) and decoder-only\nmodels (GPT-2, DialoGPT, Phi3, Mistral and LLaMA3), and the experimental\nresults on three datasets demonstrate that our CRC framework significantly\nimproves the consistency between generated responses and dialogue contexts.\n","authors":["Didi Zhang","Yaxin Fan","Peifeng Li","Qiaoming Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.13366v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05693v2","updated":"2025-06-17T02:24:51Z","published":"2024-12-07T16:41:54Z","title":"Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression","summary":"  Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.\n","authors":["Michael R. Metel","Boxing Chen","Mehdi Rezagholizadeh"],"pdf_url":"https://arxiv.org/pdf/2412.05693v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14111v1","updated":"2025-06-17T02:03:36Z","published":"2025-06-17T02:03:36Z","title":"Essential-Web v1.0: 24T tokens of organized web data","summary":"  Data plays the most prominent role in how language models acquire skills and\nknowledge. The lack of massive, well-organized pre-training datasets results in\ncostly and inaccessible data pipelines. We present Essential-Web v1.0, a\n24-trillion-token dataset in which every document is annotated with a\ntwelve-category taxonomy covering topic, format, content complexity, and\nquality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned\n0.5b-parameter model that achieves an annotator agreement within 3% of\nQwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain\ncompetitive web-curated datasets in math (-8.0% relative to SOTA), web code\n(+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on\nHuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0\n","authors":["Essential AI"," :","Andrew Hojel","Michael Pust","Tim Romanski","Yash Vanjani","Ritvik Kapila","Mohit Parmar","Adarsh Chaluvaraju","Alok Tripathy","Anil Thomas","Ashish Tanwer","Darsh J Shah","Ishaan Shah","Karl Stratos","Khoi Nguyen","Kurt Smith","Michael Callahan","Peter Rushton","Philip Monk","Platon Mazarakis","Saad Jamal","Saurabh Srivastava","Somanshu Singla","Ashish Vaswani"],"pdf_url":"https://arxiv.org/pdf/2506.14111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17514v2","updated":"2025-06-17T01:51:36Z","published":"2025-02-22T14:20:07Z","title":"SAE-V: Interpreting Multimodal Models for Enhanced Alignment","summary":"  With the integration of image modality, the semantic space of multimodal\nlarge language models (MLLMs) is more complex than text-only models, making\ntheir interpretability more challenging and their alignment less stable,\nparticularly susceptible to low-quality data, which can lead to inconsistencies\nbetween modalities, hallucinations, and biased outputs. As a result, developing\ninterpretability methods for MLLMs is crucial for improving alignment quality\nand efficiency. In text-only LLMs, Sparse Autoencoders (SAEs) have gained\nattention for their ability to interpret latent representations. However,\nextending SAEs to multimodal settings presents new challenges due to modality\nfusion and the difficulty of isolating cross-modal representations. To address\nthese challenges, we introduce SAE-V, a mechanistic interpretability framework\nthat extends the SAE paradigm to MLLMs. By identifying and analyzing\ninterpretable features along with their corresponding data, SAE-V enables\nfine-grained interpretation of both model behavior and data quality,\nfacilitating a deeper understanding of cross-modal interactions and alignment\ndynamics. Moreover, by utilizing cross-modal feature weighting, SAE-V provides\nan intrinsic data filtering mechanism to enhance model alignment without\nrequiring additional models. Specifically, when applied to the alignment\nprocess of MLLMs, SAE-V-based data filtering methods could achieve more than\n110% performance with less than 50% data. Our results highlight SAE-V's ability\nto enhance interpretability and alignment in MLLMs, providing insights into\ntheir internal mechanisms.\n","authors":["Hantao Lou","Changye Li","Jiaming Ji","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2502.17514v2.pdf","comment":"17 pages, 13 figures"},{"id":"http://arxiv.org/abs/2506.14104v1","updated":"2025-06-17T01:47:17Z","published":"2025-06-17T01:47:17Z","title":"Innovating China's Intangible Cultural Heritage with DeepSeek +\n  MidJourney: The Case of Yangliuqing theme Woodblock Prints","summary":"  Yangliuqing woodblock prints, a cornerstone of China's intangible cultural\nheritage, are celebrated for their intricate designs and vibrant colors.\nHowever, preserving these traditional art forms while fostering innovation\npresents significant challenges. This study explores the DeepSeek + MidJourney\napproach to generating creative, themed Yangliuqing woodblock prints focused on\nthe fight against COVID-19 and depicting joyous winners. Using Fr\\'echet\nInception Distance (FID) scores for evaluation, the method that combined\nDeepSeek-generated thematic prompts, MidJourney-generated thematic images,\noriginal Yangliuqing prints, and DeepSeek-generated key prompts in\nMidJourney-generated outputs achieved the lowest mean FID score (150.2) with\nminimal variability ({\\sigma} = 4.9). Additionally, feedback from 62\nparticipants, collected via questionnaires, confirmed that this hybrid approach\nproduced the most representative results. Moreover, the questionnaire data\nrevealed that participants demonstrated the highest willingness to promote\ntraditional culture and the strongest interest in consuming the AI-generated\nimages produced through this method. These findings underscore the\neffectiveness of an innovative approach that seamlessly blends traditional\nartistic elements with modern AI-driven creativity, ensuring both cultural\npreservation and contemporary relevance.\n","authors":["RuiKun Yang","ZhongLiang Wei","Longdi Xian"],"pdf_url":"https://arxiv.org/pdf/2506.14104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14101v1","updated":"2025-06-17T01:33:01Z","published":"2025-06-17T01:33:01Z","title":"Abstract Meaning Representation for Hospital Discharge Summarization","summary":"  The Achilles heel of Large Language Models (LLMs) is hallucination, which has\ndrastic consequences for the clinical domain. This is particularly important\nwith regards to automatically generating discharge summaries (a lengthy medical\ndocument that summarizes a hospital in-patient visit). Automatically generating\nthese summaries would free physicians to care for patients and reduce\ndocumentation burden. The goal of this work is to discover new methods that\ncombine language-based graphs and deep learning models to address provenance of\ncontent and trustworthiness in automatic summarization. Our method shows\nimpressive reliability results on the publicly available Medical Information\nMart for Intensive III (MIMIC-III) corpus and clinical notes written by\nphysicians at Anonymous Hospital. rovide our method, generated discharge ary\noutput examples, source code and trained models.\n","authors":["Paul Landes","Sitara Rao","Aaron Jeremy Chaise","Barbara Di Eugenio"],"pdf_url":"https://arxiv.org/pdf/2506.14101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12674v2","updated":"2025-06-17T01:12:31Z","published":"2025-06-15T00:57:48Z","title":"Enhancing Clinical Models with Pseudo Data for De-identification","summary":"  Many models are pretrained on redacted text for privacy reasons. Clinical\nfoundation models are often trained on de-identified text, which uses special\nsyntax (masked) text in place of protected health information. Even though\nthese models have increased in popularity, there has been little effort in\nunderstanding the effects of training them on redacted text. In this work, we\npretrain several encoder-only models on a dataset that contains redacted text\nand a version with replaced realistic pseudo text. We then fine-tuned models\nfor the protected health information de-identification task and show how our\nmethods significantly outperform previous baselines. The contributions of this\nwork include: a) our novel, and yet surprising findings with training\nrecommendations, b) redacted text replacements used to produce the pseudo\ndataset, c) pretrained embeddings and fine-tuned task specific models, and d)\nfreely available pseudo training dataset generation and model source code used\nin our experiments.\n","authors":["Paul Landes","Aaron J Chaise","Tarak Nath Nandi","Ravi K Madduri"],"pdf_url":"https://arxiv.org/pdf/2506.12674v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14086v1","updated":"2025-06-17T01:04:45Z","published":"2025-06-17T01:04:45Z","title":"InsertRank: LLMs can reason over BM25 scores to Improve Listwise\n  Reranking","summary":"  Large Language Models (LLMs) have demonstrated significant strides across\nvarious information retrieval tasks, particularly as rerankers, owing to their\nstrong generalization and knowledge-transfer capabilities acquired from\nextensive pretraining. In parallel, the rise of LLM-based chat interfaces has\nraised user expectations, encouraging users to pose more complex queries that\nnecessitate retrieval by ``reasoning'' over documents rather than through\nsimple keyword matching or semantic similarity. While some recent efforts have\nexploited reasoning abilities of LLMs for reranking such queries, considerable\npotential for improvement remains. In that regards, we introduce InsertRank, an\nLLM-based reranker that leverages lexical signals like BM25 scores during\nreranking to further improve retrieval performance. InsertRank demonstrates\nimproved retrieval effectiveness on -- BRIGHT, a reasoning benchmark spanning\n12 diverse domains, and R2MED, a specialized medical reasoning retrieval\nbenchmark spanning 8 different tasks. We conduct an exhaustive evaluation and\nseveral ablation studies and demonstrate that InsertRank consistently improves\nretrieval effectiveness across multiple families of LLMs, including GPT,\nGemini, and Deepseek models. %In addition, we also conduct ablation studies on\nnormalization by varying the scale of the BM25 scores, and positional bias by\nshuffling the order of the documents. With Deepseek-R1, InsertRank achieves a\nscore of 37.5 on the BRIGHT benchmark. and 51.1 on the R2MED benchmark,\nsurpassing previous methods.\n","authors":["Rahul Seetharaman","Kaustubh D. Dhole","Aman Bansal"],"pdf_url":"https://arxiv.org/pdf/2506.14086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15025v1","updated":"2025-06-17T23:57:30Z","published":"2025-06-17T23:57:30Z","title":"Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size","summary":"  Pretraining large language models is a costly process. To make this process\nmore efficient, several methods have been proposed to optimize model\narchitecture/parametrization and hardware use. On the parametrization side,\n$\\mu P$ (Maximal Update Parametrization) parametrizes model weights and\nlearning rate (LR) in a way that makes hyperparameters (HPs) transferable with\nwidth (embedding dimension): HPs can be tuned for a small model and used for\nlarger models without additional tuning. While $\\mu$P showed impressive results\nin practice, recent empirical studies have reported conflicting observations\nwhen applied to LLMs. One limitation of the theory behind $\\mu$P is the fact\nthat input dimension (vocabulary size in LLMs) is considered fixed when taking\nthe width to infinity. This is unrealistic since vocabulary size is generally\nmuch larger than width in practice. In this work, we provide a theoretical\nanalysis of the effect of vocabulary size on training dynamics, and\nsubsequently show that as vocabulary size increases, the training dynamics\n\\emph{interpolate between the $\\mu$P regime and another regime that we call\nLarge Vocab (LV) Regime}, where optimal scaling rules are different from those\npredicted by $\\mu$P. Our analysis reveals that in the LV regime, the optimal\nembedding LR to hidden LR ratio should roughly scale as $\\Theta(\\sqrt{width})$,\nsurprisingly close to the empirical findings previously reported in the\nliterature, and different from the $\\Theta(width)$ ratio predicted by $\\mu$P.\nWe conduct several experiments to validate our theory, and pretrain a 1B model\nfrom scratch to show the benefit of our suggested scaling rule for the\nembedding LR.\n","authors":["Soufiane Hayou","Liyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2506.15025v1.pdf","comment":"TD,LR: How to set the learning rate for emebdding layer in LLMs?"},{"id":"http://arxiv.org/abs/2506.09331v2","updated":"2025-06-17T23:22:53Z","published":"2025-06-11T02:12:34Z","title":"Multi-Agent Language Models: Advancing Cooperation, Coordination, and\n  Adaptation","summary":"  Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot\ngeneralization capabilities across complex natural language tasks, enabling\ntheir widespread use as virtual assistants for diverse applications such as\ntranslation and summarization. Despite being trained solely on large corpora of\ntext without explicit supervision on author intent, LLMs appear to infer the\nunderlying meaning of textual interactions. This raises a fundamental question:\ncan LLMs model and reason about the intentions of others, i.e., do they possess\na form of theory of mind? Understanding other's intentions is crucial for\neffective collaboration, which underpins human societal success and is\nessential for cooperative interactions among multiple agents, including humans\nand autonomous systems. In this work, we investigate the theory of mind in LLMs\nthrough the lens of cooperative multi-agent reinforcement learning (MARL),\nwhere agents learn to collaborate via repeated interactions, mirroring human\nsocial reasoning. Our approach aims to enhance artificial agent's ability to\nadapt and cooperate with both artificial and human partners. By leveraging\nLLM-based agents capable of natural language interaction, we move towards\ncreating hybrid human-AI systems that can foster seamless collaboration, with\nbroad implications for the future of human-artificial interaction.\n","authors":["Arjun Vaithilingam Sudhakar"],"pdf_url":"https://arxiv.org/pdf/2506.09331v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2311.07687"},{"id":"http://arxiv.org/abs/2503.15848v2","updated":"2025-06-17T22:37:38Z","published":"2025-03-20T05:03:26Z","title":"Entropy-based Exploration Conduction for Multi-step Reasoning","summary":"  Multi-step processes via large language models (LLMs) have proven effective\nfor solving complex reasoning tasks. However, the depth of exploration of the\nreasoning procedure can significantly affect the task performance. Existing\nmethods to automatically decide the depth often lead to high cost and a lack of\nflexibility. To address these issues, we propose Entropy-based Exploration\nDepth Conduction (Entro-duction), a novel method that dynamically adjusts the\nexploration depth during multi-step reasoning by monitoring LLM's output\nentropy and variance entropy. We employ these two features to capture the\nmodel's uncertainty of the current step and the fluctuation of uncertainty\nacross consecutive reasoning steps. Based on the observed entropy changes, the\nLLM selects whether to deepen, expand, or stop exploration according to the\nprobability, which facilitates the trade-off between the reasoning accuracy and\nexploration effectiveness. Experimental results across four benchmark datasets\ndemonstrate the efficacy of Entro-duction.\n","authors":["Jinghan Zhang","Xiting Wang","Fengran Mo","Yeyang Zhou","Wanfu Gao","Kunpeng Liu"],"pdf_url":"https://arxiv.org/pdf/2503.15848v2.pdf","comment":"Accepted by ACL 2025"},{"id":"http://arxiv.org/abs/2506.15001v1","updated":"2025-06-17T22:13:34Z","published":"2025-06-17T22:13:34Z","title":"Memory Tokens: Large Language Models Can Generate Reversible Sentence\n  Embeddings","summary":"  In this work, we observe an interesting phenomenon: it is possible to\ngenerate reversible sentence embeddings that allow an LLM to reconstruct the\noriginal text exactly, without modifying the model's weights. This is achieved\nby introducing a special memory token, whose embedding is optimized through\ntraining on a fixed sequence. When prompted with this embedding, the model\nreconstructs the fixed sequence exactly. We evaluate this phenomenon across\nEnglish and Spanish datasets, sequences of up to approximately 240 tokens, and\nmodel scales ranging from 100M to 8B parameters. Notably, Llama 3.1 8B\nsuccessfully reconstructs all tested sequences. Our findings highlight an\ninteresting capability of LLMs and suggest potential applications in\nmemory-based retrieval, compression, and controlled text generation.\n","authors":["Ignacio Sastre","Aiala Rosá"],"pdf_url":"https://arxiv.org/pdf/2506.15001v1.pdf","comment":"This paper will be presented at The First Workshop on Large Language\n  Model Memorization (L2M2) at ACL 2025"},{"id":"http://arxiv.org/abs/2506.14997v1","updated":"2025-06-17T22:04:55Z","published":"2025-06-17T22:04:55Z","title":"Hypothesis Testing for Quantifying LLM-Human Misalignment in Multiple\n  Choice Settings","summary":"  As Large Language Models (LLMs) increasingly appear in social science\nresearch (e.g., economics and marketing), it becomes crucial to assess how well\nthese models replicate human behavior. In this work, using hypothesis testing,\nwe present a quantitative framework to assess the misalignment between\nLLM-simulated and actual human behaviors in multiple-choice survey settings.\nThis framework allows us to determine in a principled way whether a specific\nlanguage model can effectively simulate human opinions, decision-making, and\ngeneral behaviors represented through multiple-choice options. We applied this\nframework to a popular language model for simulating people's opinions in\nvarious public surveys and found that this model is ill-suited for simulating\nthe tested sub-populations (e.g., across different races, ages, and incomes)\nfor contentious questions. This raises questions about the alignment of this\nlanguage model with the tested populations, highlighting the need for new\npractices in using LLMs for social science studies beyond naive simulations of\nhuman subjects.\n","authors":["Harbin Hong","Sebastian Caldas","Liu Leqi"],"pdf_url":"https://arxiv.org/pdf/2506.14997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06561v2","updated":"2025-06-17T20:40:28Z","published":"2025-06-06T22:16:16Z","title":"LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure\n  Profiles","summary":"  Figure captions are crucial for helping readers understand and remember a\nfigure's key message. Many models have been developed to generate these\ncaptions, helping authors compose better quality captions more easily. Yet,\nauthors almost always need to revise generic AI-generated captions to match\ntheir writing style and the domain's style, highlighting the need for\npersonalization. Despite language models' personalization (LaMP) advances,\nthese technologies often focus on text-only settings and rarely address\nscenarios where both inputs and profiles are multimodal. This paper introduces\nLaMP-Cap, a dataset for personalized figure caption generation with multimodal\nfigure profiles. For each target figure, LaMP-Cap provides not only the needed\ninputs, such as figure images, but also up to three other figures from the same\ndocument--each with its image, caption, and figure-mentioning paragraphs--as a\nprofile to characterize the context. Experiments with four LLMs show that using\nprofile information consistently helps generate captions closer to the original\nauthor-written ones. Ablation studies reveal that images in the profile are\nmore helpful than figure-mentioning paragraphs, highlighting the advantage of\nusing multimodal profiles over text-only ones.\n","authors":["Ho Yin 'Sam' Ng","Ting-Yao Hsu","Aashish Anantha Ramakrishnan","Branislav Kveton","Nedim Lipka","Franck Dernoncourt","Dongwon Lee","Tong Yu","Sungchul Kim","Ryan A. Rossi","Ting-Hao 'Kenneth' Huang"],"pdf_url":"https://arxiv.org/pdf/2506.06561v2.pdf","comment":"The LaMP-CAP dataset is publicly available at:\n  https://github.com/Crowd-AI-Lab/lamp-cap"},{"id":"http://arxiv.org/abs/2506.14965v1","updated":"2025-06-17T20:24:00Z","published":"2025-06-17T20:24:00Z","title":"Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain\n  Perspective","summary":"  Reinforcement learning (RL) has emerged as a promising approach to improve\nlarge language model (LLM) reasoning, yet most open efforts focus narrowly on\nmath and code, limiting our understanding of its broader applicability to\ngeneral reasoning. A key challenge lies in the lack of reliable, scalable RL\nreward signals across diverse reasoning domains. We introduce Guru, a curated\nRL reasoning corpus of 92K verifiable examples spanning six reasoning\ndomains--Math, Code, Science, Logic, Simulation, and Tabular--each built\nthrough domain-specific reward design, deduplication, and filtering to ensure\nreliability and effectiveness for RL training. Based on Guru, we systematically\nrevisit established findings in RL for LLM reasoning and observe significant\nvariation across domains. For example, while prior work suggests that RL\nprimarily elicits existing knowledge from pretrained models, our results reveal\na more nuanced pattern: domains frequently seen during pretraining (Math, Code,\nScience) easily benefit from cross-domain RL training, while domains with\nlimited pretraining exposure (Logic, Simulation, and Tabular) require in-domain\ntraining to achieve meaningful performance gains, suggesting that RL is likely\nto facilitate genuine skill acquisition. Finally, we present Guru-7B and\nGuru-32B, two models that achieve state-of-the-art performance among open\nmodels RL-trained with publicly available data, outperforming best baselines by\n7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We\nalso show that our models effectively improve the Pass@k performance of their\nbase models, particularly on complex tasks less likely to appear in pretraining\ndata. We release data, models, training and evaluation code to facilitate\ngeneral-purpose reasoning at: https://github.com/LLM360/Reasoning360\n","authors":["Zhoujun Cheng","Shibo Hao","Tianyang Liu","Fan Zhou","Yutao Xie","Feng Yao","Yuexin Bian","Yonghao Zhuang","Nilabjo Dey","Yuheng Zha","Yi Gu","Kun Zhou","Yuqi Wang","Yuan Li","Richard Fan","Jianshu She","Chengqian Gao","Abulhair Saparov","Haonan Li","Taylor W. Killian","Mikhail Yurochkin","Zhengzhong Liu","Eric P. Xing","Zhiting Hu"],"pdf_url":"https://arxiv.org/pdf/2506.14965v1.pdf","comment":"38 pages, 9 figures. Under review"},{"id":"http://arxiv.org/abs/2506.14949v1","updated":"2025-06-17T20:00:16Z","published":"2025-06-17T20:00:16Z","title":"From Chat to Checkup: Can Large Language Models Assist in Diabetes\n  Prediction?","summary":"  While Machine Learning (ML) and Deep Learning (DL) models have been widely\nused for diabetes prediction, the use of Large Language Models (LLMs) for\nstructured numerical data is still not well explored. In this study, we test\nthe effectiveness of LLMs in predicting diabetes using zero-shot, one-shot, and\nthree-shot prompting methods. We conduct an empirical analysis using the Pima\nIndian Diabetes Database (PIDD). We evaluate six LLMs, including four\nopen-source models: Gemma-2-27B, Mistral-7B, Llama-3.1-8B, and Llama-3.2-2B. We\nalso test two proprietary models: GPT-4o and Gemini Flash 2.0. In addition, we\ncompare their performance with three traditional machine learning models:\nRandom Forest, Logistic Regression, and Support Vector Machine (SVM). We use\naccuracy, precision, recall, and F1-score as evaluation metrics. Our results\nshow that proprietary LLMs perform better than open-source ones, with GPT-4o\nand Gemma-2-27B achieving the highest accuracy in few-shot settings. Notably,\nGemma-2-27B also outperforms the traditional ML models in terms of F1-score.\nHowever, there are still issues such as performance variation across prompting\nstrategies and the need for domain-specific fine-tuning. This study shows that\nLLMs can be useful for medical prediction tasks and encourages future work on\nprompt engineering and hybrid approaches to improve healthcare predictions.\n","authors":["Shadman Sakib","Oishy Fatema Akhand","Ajwad Abrar"],"pdf_url":"https://arxiv.org/pdf/2506.14949v1.pdf","comment":"Accepted in 1st IEEE QPAIN 2025"},{"id":"http://arxiv.org/abs/2503.11895v2","updated":"2025-06-17T19:33:26Z","published":"2025-03-14T21:53:12Z","title":"Resolving UnderEdit & OverEdit with Iterative & Neighbor-Assisted Model\n  Editing","summary":"  Large Language Models (LLMs) are widely deployed in downstream tasks, but\nkeeping their knowledge up-to-date via retraining or fine-tuning is often\ncomputationally expensive. Model editing provides a more efficient alternative\nby updating a targeted subset of parameters, which often follows the\nlocate-and-edit paradigm. Despite this efficiency, existing methods are\nlimited: edits may fail to inject knowledge (UnderEdit) or unintentionally\ndisrupt unrelated neighboring knowledge (OverEdit). To address these\nchallenges, we propose two complementary methods: iterative model editing,\nwhich applies successive edits to mitigate UnderEdit, and neighbor-assisted\nmodel editing, which incorporates neighboring knowledge during editing to\nreduce OverEdit. Our extensive experiments show that these techniques improve\nediting performance across multiple LLMs, algorithms, and benchmarks, reducing\nUnderEdit by up to 38 percentage points and OverEdit by up to 6, while\nremaining broadly applicable to any locate-and-edit method.\n","authors":["Bhiman Kumar Baghel","Scott M. Jordan","Zheyuan Ryan Shi","Xiang Lorraine Li"],"pdf_url":"https://arxiv.org/pdf/2503.11895v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2506.09099v2","updated":"2025-06-17T19:17:29Z","published":"2025-06-10T14:49:33Z","title":"Too Big to Think: Capacity, Memorization, and Generalization in\n  Pre-Trained Transformers","summary":"  The relationship between memorization and generalization in large language\nmodels (LLMs) remains an open area of research, with growing evidence that the\ntwo are deeply intertwined. In this work, we investigate this relationship by\npre-training a series of capacity-limited Transformer models from scratch on\ntwo synthetic character-level tasks designed to separately probe generalization\n(via arithmetic extrapolation) and memorization (via factual recall). We\nobserve a consistent trade-off: small models extrapolate to unseen arithmetic\ncases but fail to memorize facts, while larger models memorize but fail to\nextrapolate. An intermediate-capacity model exhibits a similar shift toward\nmemorization. When trained on both tasks jointly, no model (regardless of size)\nsucceeds at extrapolation. These findings suggest that pre-training may\nintrinsically favor one learning mode over the other. By isolating these\ndynamics in a controlled setting, our study offers insight into how model\ncapacity shapes learning behavior and offers broader implications for the\ndesign and deployment of small language models.\n","authors":["Joshua Barron","Devin White"],"pdf_url":"https://arxiv.org/pdf/2506.09099v2.pdf","comment":"Accepted for oral presentation to Tiny Titans: The next wave of\n  On-Device Learning for Foundational Models Workshop at the 42nd International\n  Conference on Machine Learning"},{"id":"http://arxiv.org/abs/2506.14927v1","updated":"2025-06-17T19:14:30Z","published":"2025-06-17T19:14:30Z","title":"MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with\n  Knowledge Guidance","summary":"  Natural language processing evaluation has made significant progress, largely\ndriven by the proliferation of powerful large language mod-els (LLMs). New\nevaluation benchmarks are of increasing priority as the reasoning capabilities\nof LLMs are expanding at a rapid pace. In particular, while multi-document (MD)\nreasoning is an area of extreme relevance given LLM capabilities in handling\nlonger-context inputs, few benchmarks exist to rigorously examine model\nbehavior in this setting. Moreover, the multi-document setting is historically\nchallenging for benchmark creation due to the expensive cost of annotating long\ninputs. In this work, we introduce MDBench, a new dataset for evaluating LLMs\non the task of multi-document reasoning. Notably, MDBench is created through a\nnovel synthetic generation process, allowing us to controllably and efficiently\ngenerate challenging document sets and the corresponding question-answer (QA)\nexamples. Our novel technique operates on condensed structured seed knowledge,\nmodifying it through LLM-assisted edits to induce MD-specific reasoning\nchallenges. We then convert this structured knowledge into a natural text\nsurface form, generating a document set and corresponding QA example. We\nanalyze the behavior of popular LLMs and prompting techniques, finding that\nMDBENCH poses significant challenges for all methods, even with relatively\nshort document sets. We also see our knowledge-guided generation technique (1)\nallows us to readily perform targeted analysis of MD-specific reasoning\ncapabilities and (2) can be adapted quickly to account for new challenges and\nfuture modeling improvements.\n","authors":["Joseph J. Peper","Wenzhao Qiu","Ali Payani","Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2506.14927v1.pdf","comment":"ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2504.20304v3","updated":"2025-06-17T18:46:13Z","published":"2025-04-28T23:20:36Z","title":"UD-English-CHILDES: A Collected Resource of Gold and Silver Universal\n  Dependencies Trees for Child Language Interactions","summary":"  CHILDES is a widely used resource of transcribed child and child-directed\nspeech. This paper introduces UD-English-CHILDES, the first officially released\nUniversal Dependencies (UD) treebank. It is derived from previously\ndependency-annotated CHILDES data, which we harmonize to follow unified\nannotation principles. The gold-standard trees encompass utterances sampled\nfrom 11 children and their caregivers, totaling over 48K sentences (236K\ntokens). We validate these gold-standard annotations under the UD v2 framework\nand provide an additional 1M~silver-standard sentences, offering a consistent\nresource for computational and linguistic research.\n","authors":["Xiulin Yang","Zhuoxuan Ju","Lanni Bu","Zoey Liu","Nathan Schneider"],"pdf_url":"https://arxiv.org/pdf/2504.20304v3.pdf","comment":"UDW 2025"},{"id":"http://arxiv.org/abs/2501.03491v2","updated":"2025-06-17T18:44:37Z","published":"2025-01-07T03:21:17Z","title":"Can LLMs Ask Good Questions?","summary":"  We evaluate questions generated by large language models (LLMs) from context,\ncomparing them to human-authored questions across six dimensions: question\ntype, question length, context coverage, answerability, uncommonness, and\nrequired answer length. Our study spans two open-source and two proprietary\nstate-of-the-art models. Results reveal that LLM-generated questions tend to\ndemand longer descriptive answers and exhibit more evenly distributed context\nfocus, in contrast to the positional bias often seen in QA tasks. These\nfindings provide insights into the distinctive characteristics of LLM-generated\nquestions and inform future work on question quality and downstream\napplications.\n","authors":["Yueheng Zhang","Xiaoyuan Liu","Yiyou Sun","Atheer Alharbi","Hend Alzahrani","Tianneng Shi","Basel Alomair","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2501.03491v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14912v1","updated":"2025-06-17T18:44:21Z","published":"2025-06-17T18:44:21Z","title":"CrEst: Credibility Estimation for Contexts in LLMs via Weak Supervision","summary":"  The integration of contextual information has significantly enhanced the\nperformance of large language models (LLMs) on knowledge-intensive tasks.\nHowever, existing methods often overlook a critical challenge: the credibility\nof context documents can vary widely, potentially leading to the propagation of\nunreliable information. In this paper, we introduce CrEst, a novel weakly\nsupervised framework for assessing the credibility of context documents during\nLLM inference--without requiring manual annotations. Our approach is grounded\nin the insight that credible documents tend to exhibit higher semantic\ncoherence with other credible documents, enabling automated credibility\nestimation through inter-document agreement. To incorporate credibility into\nLLM inference, we propose two integration strategies: a black-box approach for\nmodels without access to internal weights or activations, and a white-box\nmethod that directly modifies attention mechanisms. Extensive experiments\nacross three model architectures and five datasets demonstrate that CrEst\nconsistently outperforms strong baselines, achieving up to a 26.86% improvement\nin accuracy and a 3.49% increase in F1 score. Further analysis shows that CrEst\nmaintains robust performance even under high-noise conditions.\n","authors":["Dyah Adila","Shuai Zhang","Boran Han","Bonan Min","Yuyang Wang"],"pdf_url":"https://arxiv.org/pdf/2506.14912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14901v1","updated":"2025-06-17T18:16:17Z","published":"2025-06-17T18:16:17Z","title":"Combining Constrained and Unconstrained Decoding via Boosting: BoostCD\n  and Its Application to Information Extraction","summary":"  Many recent approaches to structured NLP tasks use an autoregressive language\nmodel $M$ to map unstructured input text $x$ to output text $y$ representing\nstructured objects (such as tuples, lists, trees, code, etc.), where the\ndesired output structure is enforced via constrained decoding. During training,\nthese approaches do not require the model to be aware of the constraints, which\nare merely implicit in the training outputs $y$. This is advantageous as it\nallows for dynamic constraints without requiring retraining, but can lead to\nlow-quality output during constrained decoding at test time. We overcome this\nproblem with Boosted Constrained Decoding (BoostCD), which combines constrained\nand unconstrained decoding in two phases: Phase 1 decodes from the base model\n$M$ twice, in constrained and unconstrained mode, obtaining two weak\npredictions. In phase 2, a learned autoregressive boosted model combines the\ntwo weak predictions into one final prediction. The mistakes made by the base\nmodel with vs. without constraints tend to be complementary, which the boosted\nmodel learns to exploit for improved performance. We demonstrate the power of\nBoostCD by applying it to closed information extraction. Our model, BoostIE,\noutperforms prior approaches both in and out of distribution, addressing\nseveral common errors identified in those approaches.\n","authors":["Marija Šakota","Robert West"],"pdf_url":"https://arxiv.org/pdf/2506.14901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14900v1","updated":"2025-06-17T18:13:40Z","published":"2025-06-17T18:13:40Z","title":"Adverse Event Extraction from Discharge Summaries: A New Dataset,\n  Annotation Scheme, and Initial Findings","summary":"  In this work, we present a manually annotated corpus for Adverse Event (AE)\nextraction from discharge summaries of elderly patients, a population often\nunderrepresented in clinical NLP resources. The dataset includes 14 clinically\nsignificant AEs-such as falls, delirium, and intracranial haemorrhage, along\nwith contextual attributes like negation, diagnosis type, and in-hospital\noccurrence. Uniquely, the annotation schema supports both discontinuous and\noverlapping entities, addressing challenges rarely tackled in prior work. We\nevaluate multiple models using FlairNLP across three annotation granularities:\nfine-grained, coarse-grained, and coarse-grained with negation. While\ntransformer-based models (e.g., BERT-cased) achieve strong performance on\ndocument-level coarse-grained extraction (F1 = 0.943), performance drops\nnotably for fine-grained entity-level tasks (e.g., F1 = 0.675), particularly\nfor rare events and complex attributes. These results demonstrate that despite\nhigh-level scores, significant challenges remain in detecting underrepresented\nAEs and capturing nuanced clinical language. Developed within a Trusted\nResearch Environment (TRE), the dataset is available upon request via DataLoch\nand serves as a robust benchmark for evaluating AE extraction methods and\nsupporting future cross-dataset generalisation.\n","authors":["Imane Guellil","Salomé Andres","Atul Anand","Bruce Guthrie","Huayu Zhang","Abul Hasan","Honghan Wu","Beatrice Alex"],"pdf_url":"https://arxiv.org/pdf/2506.14900v1.pdf","comment":"Accepted and will be published at ACL2025 (main conference)"}]},"2025-06-18T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2506.15683v1","updated":"2025-06-18T17:59:58Z","published":"2025-06-18T17:59:58Z","title":"PhantomHunter: Detecting Unseen Privately-Tuned LLM-Generated Text via\n  Family-Aware Learning","summary":"  With the popularity of large language models (LLMs), undesirable societal\nproblems like misinformation production and academic misconduct have been more\nsevere, making LLM-generated text detection now of unprecedented importance.\nAlthough existing methods have made remarkable progress, a new challenge posed\nby text from privately tuned LLMs remains underexplored. Users could easily\npossess private LLMs by fine-tuning an open-source one with private corpora,\nresulting in a significant performance drop of existing detectors in practice.\nTo address this issue, we propose PhantomHunter, an LLM-generated text detector\nspecialized for detecting text from unseen, privately-tuned LLMs. Its\nfamily-aware learning framework captures family-level traits shared across the\nbase models and their derivatives, instead of memorizing individual\ncharacteristics. Experiments on data from LLaMA, Gemma, and Mistral families\nshow its superiority over 7 baselines and 3 industrial services, with F1 scores\nof over 96%.\n","authors":["Yuhui Shi","Yehan Yang","Qiang Sheng","Hao Mi","Beizhe Hu","Chaoxi Xu","Juan Cao"],"pdf_url":"https://arxiv.org/pdf/2506.15683v1.pdf","comment":"17 pages, 3 figures, 6 tables"},{"id":"http://arxiv.org/abs/2506.15681v1","updated":"2025-06-18T17:59:49Z","published":"2025-06-18T17:59:49Z","title":"GenRecal: Generation after Recalibration from Large to Small\n  Vision-Language Models","summary":"  Recent advancements in vision-language models (VLMs) have leveraged large\nlanguage models (LLMs) to achieve performance on par with closed-source systems\nlike GPT-4V. However, deploying these models in real-world scenarios,\nparticularly on resource-constrained devices, remains challenging due to their\nsubstantial computational demands. This has spurred interest in distilling\nknowledge from large VLMs into smaller, more efficient counterparts. A key\nchallenge arises here from the diversity of VLM architectures, which are built\non different LLMs and employ varying token types-differing in vocabulary size,\ntoken splits, and token index ordering. To address this challenge of limitation\nto a specific VLM type, we present Generation after Recalibration (GenRecal), a\nnovel, general-purpose distillation framework for VLMs. GenRecal incorporates a\nRecalibrator that aligns and adapts feature representations between\nheterogeneous VLMs, enabling effective knowledge transfer across different\ntypes of VLMs. Through extensive experiments on multiple challenging\nbenchmarks, we demonstrate that GenRecal significantly improves baseline\nperformances, eventually outperforming large-scale open- and closed-source\nVLMs.\n","authors":["Byung-Kwan Lee","Ryo Hachiuma","Yong Man Ro","Yu-Chiang Frank Wang","Yueh-Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2506.15681v1.pdf","comment":"Project page: https://byungkwanlee.github.io/GenRecal-page/"},{"id":"http://arxiv.org/abs/2506.15679v1","updated":"2025-06-18T17:59:35Z","published":"2025-06-18T17:59:35Z","title":"Dense SAE Latents Are Features, Not Bugs","summary":"  Sparse autoencoders (SAEs) are designed to extract interpretable features\nfrom language models by enforcing a sparsity constraint. Ideally, training an\nSAE would yield latents that are both sparse and semantically meaningful.\nHowever, many SAE latents activate frequently (i.e., are \\emph{dense}), raising\nconcerns that they may be undesirable artifacts of the training procedure. In\nthis work, we systematically investigate the geometry, function, and origin of\ndense latents and show that they are not only persistent but often reflect\nmeaningful model representations. We first demonstrate that dense latents tend\nto form antipodal pairs that reconstruct specific directions in the residual\nstream, and that ablating their subspace suppresses the emergence of new dense\nfeatures in retrained SAEs -- suggesting that high density features are an\nintrinsic property of the residual space. We then introduce a taxonomy of dense\nlatents, identifying classes tied to position tracking, context binding,\nentropy regulation, letter-specific output signals, part-of-speech, and\nprincipal component reconstruction. Finally, we analyze how these features\nevolve across layers, revealing a shift from structural features in early\nlayers, to semantic features in mid layers, and finally to output-oriented\nsignals in the last layers of the model. Our findings indicate that dense\nlatents serve functional roles in language model computation and should not be\ndismissed as training noise.\n","authors":["Xiaoqing Sun","Alessandro Stolfo","Joshua Engels","Ben Wu","Senthooran Rajamanoharan","Mrinmaya Sachan","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2506.15679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15677v1","updated":"2025-06-18T17:58:17Z","published":"2025-06-18T17:58:17Z","title":"Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence","summary":"  AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.\n","authors":["Yining Hong","Rui Sun","Bingxuan Li","Xingcheng Yao","Maxine Wu","Alexander Chien","Da Yin","Ying Nian Wu","Zhecan James Wang","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2506.15677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15676v1","updated":"2025-06-18T17:57:39Z","published":"2025-06-18T17:57:39Z","title":"Gender-Neutral Machine Translation Strategies in Practice","summary":"  Gender-inclusive machine translation (MT) should preserve gender ambiguity in\nthe source to avoid misgendering and representational harms. While gender\nambiguity often occurs naturally in notional gender languages such as English,\nmaintaining that gender neutrality in grammatical gender languages is a\nchallenge. Here we assess the sensitivity of 21 MT systems to the need for\ngender neutrality in response to gender ambiguity in three translation\ndirections of varying difficulty. The specific gender-neutral strategies that\nare observed in practice are categorized and discussed. Additionally, we\nexamine the effect of binary gender stereotypes on the use of gender-neutral\ntranslation. In general, we report a disappointing absence of gender-neutral\ntranslations in response to gender ambiguity. However, we observe a small\nhandful of MT systems that switch to gender neutral translation using specific\nstrategies, depending on the target language.\n","authors":["Hillary Dawkins","Isar Nejadgholi","Chi-kiu Lo"],"pdf_url":"https://arxiv.org/pdf/2506.15676v1.pdf","comment":"to appear at GITT 2025"},{"id":"http://arxiv.org/abs/2506.15674v1","updated":"2025-06-18T17:57:01Z","published":"2025-06-18T17:57:01Z","title":"Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers","summary":"  We study privacy leakage in the reasoning traces of large reasoning models\nused as personal agents. Unlike final outputs, reasoning traces are often\nassumed to be internal and safe. We challenge this assumption by showing that\nreasoning traces frequently contain sensitive user data, which can be extracted\nvia prompt injections or accidentally leak into outputs. Through probing and\nagentic evaluations, we demonstrate that test-time compute approaches,\nparticularly increased reasoning steps, amplify such leakage. While increasing\nthe budget of those test-time compute approaches makes models more cautious in\ntheir final answers, it also leads them to reason more verbosely and leak more\nin their own thinking. This reveals a core tension: reasoning improves utility\nbut enlarges the privacy attack surface. We argue that safety efforts must\nextend to the model's internal thinking, not just its outputs.\n","authors":["Tommaso Green","Martin Gubri","Haritz Puerto","Sangdoo Yun","Seong Joon Oh"],"pdf_url":"https://arxiv.org/pdf/2506.15674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15662v1","updated":"2025-06-18T17:41:28Z","published":"2025-06-18T17:41:28Z","title":"CC-LEARN: Cohort-based Consistency Learning","summary":"  Large language models excel at many tasks but still struggle with consistent,\nrobust reasoning. We introduce Cohort-based Consistency Learning (CC-Learn), a\nreinforcement learning framework that improves the reliability of LLM reasoning\nby training on cohorts of similar questions derived from shared programmatic\nabstractions. To enforce cohort-level consistency, we define a composite\nobjective combining cohort accuracy, a retrieval bonus for effective problem\ndecomposition, and a rejection penalty for trivial or invalid lookups that\nreinforcement learning can directly optimize, unlike supervised fine-tuning.\nOptimizing this reward guides the model to adopt uniform reasoning patterns\nacross all cohort members. Experiments on challenging reasoning benchmarks\n(including ARC-Challenge and StrategyQA) show that CC-Learn boosts both\naccuracy and reasoning stability over pretrained and SFT baselines. These\nresults demonstrate that cohort-level RL effectively enhances reasoning\nconsistency in LLMs.\n","authors":["Xiao Ye","Shaswat Shrivastava","Zhaonan Li","Jacob Dineen","Shijie Lu","Avneet Ahuja","Ming Shen","Zhikun Xu","Ben Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.15662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15651v1","updated":"2025-06-18T17:29:19Z","published":"2025-06-18T17:29:19Z","title":"AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards\n  Improve Preference Learning","summary":"  Rule-based rewards offer a promising strategy for improving reinforcement\nlearning from human feedback (RLHF), but current approaches often rely on\nmanual rule engineering. We present AutoRule, a fully automated method for\nextracting rules from preference feedback and formulating them into rule-based\nrewards. AutoRule extraction operates in three stages: it leverages a reasoning\nmodel to interpret user preferences, identifies candidate rules from the\nreasoning chain of these interpretations, and synthesizes them into a unified\nrule set. Leveraging the finalized rule set, we employ language-model verifiers\nto compute the fraction of rules satisfied by each output, using this metric as\nan auxiliary reward alongside the learned reward model during policy\noptimization. Training a Llama-3-8B model with AutoRule results in a 28.6\\%\nrelative improvement in length-controlled win rate on AlpacaEval2.0, and a\n6.1\\% relative gain in second-turn performance on a held-out MT-Bench subset,\ncompared to a GRPO baseline trained with the same learned reward model but\nwithout the rule-based auxiliary reward. Our analysis confirms that the\nextracted rules exhibit good agreement with dataset preference. We find that\nAutoRule demonstrates reduced reward hacking compared to a learned reward model\nwhen run over two episodes. Finally, our case study suggests that the extracted\nrules capture unique qualities valued in different datasets. The extracted\nrules are provided in the appendix, and the code is open-sourced at\nhttps://github.com/cxcscmu/AutoRule.\n","authors":["Tevin Wang","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2506.15651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15650v1","updated":"2025-06-18T17:28:37Z","published":"2025-06-18T17:28:37Z","title":"Oldies but Goldies: The Potential of Character N-grams for Romanian\n  Texts","summary":"  This study addresses the problem of authorship attribution for Romanian texts\nusing the ROST corpus, a standard benchmark in the field. We systematically\nevaluate six machine learning techniques: Support Vector Machine (SVM),\nLogistic Regression (LR), k-Nearest Neighbors (k-NN), Decision Trees (DT),\nRandom Forests (RF), and Artificial Neural Networks (ANN), employing character\nn-gram features for classification. Among these, the ANN model achieved the\nhighest performance, including perfect classification in four out of fifteen\nruns when using 5-gram features. These results demonstrate that lightweight,\ninterpretable character n-gram approaches can deliver state-of-the-art accuracy\nfor Romanian authorship attribution, rivaling more complex methods. Our\nfindings highlight the potential of simple stylometric features in resource,\nconstrained or under-studied language settings.\n","authors":["Dana Lupsa","Sanda-Maria Avram"],"pdf_url":"https://arxiv.org/pdf/2506.15650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16065v2","updated":"2025-06-18T17:04:04Z","published":"2025-05-21T22:33:40Z","title":"Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated\n  Synthetic Data Augmentation","summary":"  Embedding-Based Retrieval (EBR) is an important technique in modern search\nengines, enabling semantic match between search queries and relevant results.\nHowever, search logging data on platforms like Facebook Marketplace lacks the\ndiversity and details needed for effective EBR model training, limiting the\nmodels' ability to capture nuanced search patterns. To address this challenge,\nwe propose Aug2Search, an EBR-based framework leveraging synthetic data\ngenerated by Generative AI (GenAI) models, in a multimodal and multitask\napproach to optimize query-product relevance. This paper investigates the\ncapabilities of GenAI, particularly Large Language Models (LLMs), in generating\nhigh-quality synthetic data, and analyzing its impact on enhancing EBR models.\nWe conducted experiments using eight Llama models and 100 million data points\nfrom Facebook Marketplace logs. Our synthetic data generation follows three\nstrategies: (1) generate queries, (2) enhance product listings, and (3)\ngenerate queries from enhanced listings. We train EBR models on three different\ndatasets: sampled engagement data or original data ((e.g., \"Click\" and \"Listing\nInteractions\")), synthetic data, and a mixture of both engagement and synthetic\ndata to assess their performance across various training sets. Our findings\nunderscore the robustness of Llama models in producing synthetic queries and\nlistings with high coherence, relevance, and diversity, while maintaining low\nlevels of hallucination. Aug2Search achieves an improvement of up to 4% in\nROC_AUC with 100 million synthetic data samples, demonstrating the\neffectiveness of our approach. Moreover, our experiments reveal that with the\nsame volume of training data, models trained exclusively on synthetic data\noften outperform those trained on original data only or a mixture of original\nand synthetic data.\n","authors":["Ruijie Xi","He Ba","Hao Yuan","Rishu Agrawal","Yuxin Tian","Ruoyan Long","Arul Prakash"],"pdf_url":"https://arxiv.org/pdf/2505.16065v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15629v1","updated":"2025-06-18T17:00:54Z","published":"2025-06-18T17:00:54Z","title":"Revisiting Compositional Generalization Capability of Large Language\n  Models Considering Instruction Following Ability","summary":"  In generative commonsense reasoning tasks such as CommonGen, generative large\nlanguage models (LLMs) compose sentences that include all given concepts.\nHowever, when focusing on instruction-following capabilities, if a prompt\nspecifies a concept order, LLMs must generate sentences that adhere to the\nspecified order. To address this, we propose Ordered CommonGen, a benchmark\ndesigned to evaluate the compositional generalization and instruction-following\nabilities of LLMs. This benchmark measures ordered coverage to assess whether\nconcepts are generated in the specified order, enabling a simultaneous\nevaluation of both abilities. We conducted a comprehensive analysis using 36\nLLMs and found that, while LLMs generally understand the intent of\ninstructions, biases toward specific concept order patterns often lead to\nlow-diversity outputs or identical results even when the concept order is\naltered. Moreover, even the most instruction-compliant LLM achieved only about\n75% ordered coverage, highlighting the need for improvements in both\ninstruction-following and compositional generalization capabilities.\n","authors":["Yusuke Sakai","Hidetaka Kamigaito","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2506.15629v1.pdf","comment":"ACL 2025 Main"},{"id":"http://arxiv.org/abs/2505.13346v3","updated":"2025-06-18T16:58:25Z","published":"2025-05-19T16:50:35Z","title":"J4R: Learning to Judge with Equivalent Initial State Group Relative\n  Policy Optimization","summary":"  To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench.\n","authors":["Austin Xu","Yilun Zhou","Xuan-Phi Nguyen","Caiming Xiong","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2505.13346v3.pdf","comment":"25 pages, 4 figures, 6 tables. Updated with code and benchmark"},{"id":"http://arxiv.org/abs/2411.05060v4","updated":"2025-06-18T16:56:37Z","published":"2024-11-07T18:47:39Z","title":"A Guide to Misinformation Detection Data and Evaluation","summary":"  Misinformation is a complex societal issue, and mitigating solutions are\ndifficult to create due to data deficiencies. To address this, we have curated\nthe largest collection of (mis)information datasets in the literature, totaling\n75. From these, we evaluated the quality of 36 datasets that consist of\nstatements or claims, as well as the 9 datasets that consist of data in purely\nparagraph form. We assess these datasets to identify those with solid\nfoundations for empirical work and those with flaws that could result in\nmisleading and non-generalizable results, such as spurious correlations, or\nexamples that are ambiguous or otherwise impossible to assess for veracity. We\nfind the latter issue is particularly severe and affects most datasets in the\nliterature. We further provide state-of-the-art baselines on all these\ndatasets, but show that regardless of label quality, categorical labels may no\nlonger give an accurate evaluation of detection model performance. Finally, we\npropose and highlight Evaluation Quality Assurance (EQA) as a tool to guide the\nfield toward systemic solutions rather than inadvertently propagating issues in\nevaluation. Overall, this guide aims to provide a roadmap for higher quality\ndata and better grounded evaluations, ultimately improving research in\nmisinformation detection. All datasets and other artifacts are available at\nmisinfo-datasets.complexdatalab.com.\n","authors":["Camille Thibault","Jacob-Junqi Tian","Gabrielle Peloquin-Skulski","Taylor Lynn Curtis","James Zhou","Florence Laflamme","Yuxiang Guan","Reihaneh Rabbany","Jean-François Godbout","Kellin Pelrine"],"pdf_url":"https://arxiv.org/pdf/2411.05060v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15623v1","updated":"2025-06-18T16:52:20Z","published":"2025-06-18T16:52:20Z","title":"Minding the Politeness Gap in Cross-cultural Communication","summary":"  Misunderstandings in cross-cultural communication often arise from subtle\ndifferences in interpretation, but it is unclear whether these differences\narise from the literal meanings assigned to words or from more general\npragmatic factors such as norms around politeness and brevity. In this paper,\nwe report three experiments examining how speakers of British and American\nEnglish interpret intensifiers like \"quite\" and \"very.\" To better understand\nthese cross-cultural differences, we developed a computational cognitive model\nwhere listeners recursively reason about speakers who balance informativity,\npoliteness, and utterance cost. Our model comparisons suggested that\ncross-cultural differences in intensifier interpretation stem from a\ncombination of (1) different literal meanings, (2) different weights on\nutterance cost. These findings challenge accounts based purely on semantic\nvariation or politeness norms, demonstrating that cross-cultural differences in\ninterpretation emerge from an intricate interplay between the two.\n","authors":["Yuka Machino","Matthias Hofer","Max Siegel","Joshua B. Tenenbaum","Robert D. Hawkins"],"pdf_url":"https://arxiv.org/pdf/2506.15623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15617v1","updated":"2025-06-18T16:50:34Z","published":"2025-06-18T16:50:34Z","title":"The Compositional Architecture of Regret in Large Language Models","summary":"  Regret in Large Language Models refers to their explicit regret expression\nwhen presented with evidence contradicting their previously generated\nmisinformation. Studying the regret mechanism is crucial for enhancing model\nreliability and helps in revealing how cognition is coded in neural networks.\nTo understand this mechanism, we need to first identify regret expressions in\nmodel outputs, then analyze their internal representation. This analysis\nrequires examining the model's hidden states, where information processing\noccurs at the neuron level. However, this faces three key challenges: (1) the\nabsence of specialized datasets capturing regret expressions, (2) the lack of\nmetrics to find the optimal regret representation layer, and (3) the lack of\nmetrics for identifying and analyzing regret neurons. Addressing these\nlimitations, we propose: (1) a workflow for constructing a comprehensive regret\ndataset through strategically designed prompting scenarios, (2) the Supervised\nCompression-Decoupling Index (S-CDI) metric to identify optimal regret\nrepresentation layers, and (3) the Regret Dominance Score (RDS) metric to\nidentify regret neurons and the Group Impact Coefficient (GIC) to analyze\nactivation patterns. Our experimental results successfully identified the\noptimal regret representation layer using the S-CDI metric, which significantly\nenhanced performance in probe classification experiments. Additionally, we\ndiscovered an M-shaped decoupling pattern across model layers, revealing how\ninformation processing alternates between coupling and decoupling phases.\nThrough the RDS metric, we categorized neurons into three distinct functional\ngroups: regret neurons, non-regret neurons, and dual neurons.\n","authors":["Xiangxiang Cui","Shu Yang","Tianjin Huang","Wanyu Lin","Lijie Hu","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2506.15617v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2506.09033v2","updated":"2025-06-18T16:49:26Z","published":"2025-06-10T17:56:45Z","title":"Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via\n  Reinforcement Learning","summary":"  The rapid emergence of diverse large language models (LLMs) has spurred the\ndevelopment of LLM routers that assign user queries to the most suitable model.\nHowever, existing LLM routers typically perform a single-round, one-to-one\nmapping (\\textit{i.e.}, assigning each query to a single model in isolation),\nwhich limits their capability to tackle complex tasks that demand the\ncomplementary strengths of multiple LLMs. In this paper, we present\n\\textbf{Router-R1}, a reinforcement learning (RL)-based framework that\nformulates multi-LLM routing and aggregation as a sequential decision process.\nRouter-R1 instantiates the router itself as a capable LLM, leveraging its\nreasoning ability to interleave \"think\" actions (internal deliberation) with\n\"route\" actions (dynamic model invocation), and integrates each response into\nits evolving context. To facilitate learning, we employ a lightweight\nrule-based reward comprising format rewards, final outcome rewards, and a novel\ncost reward for optimizing the balance between performance and cost, opening a\npathway toward enhancing performance-cost trade-offs via RL. Router-R1 also\nconditions only on simple model descriptors such as pricing, latency, and\nexample performance, enabling strong generalization to unseen model selection.\nExperiments on seven general and multi-hop QA benchmarks show that Router-R1\noutperforms several strong baselines, achieving superior performance while\nmaintaining robust generalization and cost management.\n","authors":["Haozhen Zhang","Tao Feng","Jiaxuan You"],"pdf_url":"https://arxiv.org/pdf/2506.09033v2.pdf","comment":"Code is available at https://github.com/ulab-uiuc/Router-R1. Models\n  and Datasets are available at\n  https://huggingface.co/collections/ulab-ai/router-r1-6851bbe099c7a56914b5db03"},{"id":"http://arxiv.org/abs/2506.15606v1","updated":"2025-06-18T16:30:02Z","published":"2025-06-18T16:30:02Z","title":"LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning","summary":"  Large Language Models (LLMs) have become indispensable in real-world\napplications. However, their widespread adoption raises significant safety\nconcerns, particularly in responding to socially harmful questions. Despite\nsubstantial efforts to improve model safety through alignment, aligned models\ncan still have their safety protections undermined by subsequent fine-tuning -\neven when the additional training data appears benign. In this paper, we\nempirically demonstrate that this vulnerability stems from the sensitivity of\nsafety-critical low-rank subspaces in LLM parameters to fine-tuning. Building\non this insight, we propose a novel training-free method, termed Low-Rank\nExtrapolation (LoX), to enhance safety robustness by extrapolating the safety\nsubspace of an aligned LLM. Our experimental results confirm the effectiveness\nof LoX, demonstrating significant improvements in robustness against both\nbenign and malicious fine-tuning attacks while preserving the model's\nadaptability to new tasks. For instance, LoX leads to 11% to 54% absolute\nreductions in attack success rates (ASR) facing benign or malicious fine-tuning\nattacks. By investigating the ASR landscape of parameters, we attribute the\nsuccess of LoX to that the extrapolation moves LLM parameters to a flatter\nzone, thereby less sensitive to perturbations. The code is available at\ngithub.com/VITA-Group/LoX.\n","authors":["Gabrel J. Perin","Runjin Chen","Xuxi Chen","Nina S. T. Hirata","Zhangyang Wang","Junyuan Hong"],"pdf_url":"https://arxiv.org/pdf/2506.15606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15598v1","updated":"2025-06-18T16:19:46Z","published":"2025-06-18T16:19:46Z","title":"From Model to Classroom: Evaluating Generated MCQs for Portuguese with\n  Narrative and Difficulty Concerns","summary":"  While MCQs are valuable for learning and evaluation, manually creating them\nwith varying difficulty levels and targeted reading skills remains a\ntime-consuming and costly task. Recent advances in generative AI provide an\nopportunity to automate MCQ generation efficiently. However, assessing the\nactual quality and reliability of generated MCQs has received limited attention\n-- particularly regarding cases where generation fails. This aspect becomes\nparticularly important when the generated MCQs are meant to be applied in\nreal-world settings. Additionally, most MCQ generation studies focus on\nEnglish, leaving other languages underexplored. This paper investigates the\ncapabilities of current generative models in producing MCQs for reading\ncomprehension in Portuguese, a morphologically rich language. Our study focuses\non generating MCQs that align with curriculum-relevant narrative elements and\nspan different difficulty levels. We evaluate these MCQs through expert review\nand by analyzing the psychometric properties extracted from student responses\nto assess their suitability for elementary school students. Our results show\nthat current models can generate MCQs of comparable quality to human-authored\nones. However, we identify issues related to semantic clarity and\nanswerability. Also, challenges remain in generating distractors that engage\nstudents and meet established criteria for high-quality MCQ option design.\n","authors":["Bernardo Leite","Henrique Lopes Cardoso","Pedro Pinto","Abel Ferreira","Luís Abreu","Isabel Rangel","Sandra Monteiro"],"pdf_url":"https://arxiv.org/pdf/2506.15598v1.pdf","comment":"This is a preprint version of the manuscript currently under review\n  at an international journal"},{"id":"http://arxiv.org/abs/2506.15594v1","updated":"2025-06-18T16:09:18Z","published":"2025-06-18T16:09:18Z","title":"WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and\n  Charts","summary":"  Documents are fundamental to preserving and disseminating information, often\nincorporating complex layouts, tables, and charts that pose significant\nchallenges for automatic document understanding (DU). While vision-language\nlarge models (VLLMs) have demonstrated improvements across various tasks, their\neffectiveness in processing long-context vision inputs remains unclear. This\npaper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice\nquestions (MCQs) designed to evaluate cross-modal reasoning over tables and\ncharts extracted from 4,000 Wikipedia pages spanning seven distinct topics.\nUnlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring\nmodels to synthesize information from multiple modalities. We evaluate 12\nstate-of-the-art vision-language models, revealing that while proprietary\nmodels achieve ~70% accuracy when provided with direct context, their\nperformance deteriorates significantly when retrieval from long documents is\nrequired. Among these, GPT-4-o is the only model exceeding 50% accuracy in this\nsetting, whereas open-source models perform considerably worse, with a maximum\naccuracy of 27%. These findings underscore the challenges of long-context,\nmulti-modal reasoning and establish WikiMixQA as a crucial benchmark for\nadvancing document understanding research.\n","authors":["Negar Foroutan","Angelika Romanou","Matin Ansaripour","Julian Martin Eisenschlos","Karl Aberer","Rémi Lebret"],"pdf_url":"https://arxiv.org/pdf/2506.15594v1.pdf","comment":"ACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2406.03847v3","updated":"2025-06-18T16:07:47Z","published":"2024-06-06T08:25:43Z","title":"Lean Workbook: A large-scale Lean problem set formalized from natural\n  language math problems","summary":"  Large language models have demonstrated impressive capabilities across\nvarious natural language processing tasks, especially in solving mathematical\nproblems. However, large language models are not good at math theorem proving\nusing formal languages like Lean. A significant challenge in this area is the\nscarcity of training data available in these formal languages. To address this\nissue, we propose a novel pipeline that iteratively generates and filters\nsynthetic data to translate natural language mathematical problems into Lean 4\nstatements, and vice versa. Our results indicate that the synthetic data\npipeline can provide useful training data and improve the performance of LLMs\nin translating and understanding complex mathematical problems and proofs. Our\nfinal dataset contains about 57K formal-informal question pairs along with\nsearched proof from the math contest forum and 21 new IMO questions. We\nopen-source our code at https://github.com/InternLM/InternLM-Math and our data\nat https://huggingface.co/datasets/InternLM/Lean-Workbook.\n","authors":["Huaiyuan Ying","Zijian Wu","Yihan Geng","Zheng Yuan","Dahua Lin","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2406.03847v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15583v1","updated":"2025-06-18T16:00:19Z","published":"2025-06-18T16:00:19Z","title":"DiscoSG: Towards Discourse-Level Text Scene Graph Parsing through\n  Iterative Graph Refinement","summary":"  Vision-Language Models (VLMs) now generate discourse-level, multi-sentence\nvisual descriptions, challenging text scene graph parsers originally designed\nfor single-sentence caption-to-graph mapping. Current approaches typically\nmerge sentence-level parsing outputs for discourse input, often missing\nphenomena like cross-sentence coreference, resulting in fragmented graphs and\ndegraded downstream VLM task performance. To address this, we introduce a new\ntask, Discourse-level text Scene Graph parsing (DiscoSG), supported by our\ndataset DiscoSG-DS, which comprises 400 expert-annotated and 8,430 synthesised\nmulti-sentence caption-graph pairs for images. Each caption averages 9\nsentences, and each graph contains at least 3 times more triples than those in\nexisting datasets. While fine-tuning large PLMs (i.e., GPT-4) on DiscoSG-DS\nimproves SPICE by approximately 48% over the best sentence-merging baseline,\nhigh inference cost and restrictive licensing hinder its open-source use, and\nsmaller fine-tuned PLMs struggle with complex graphs. We propose\nDiscoSG-Refiner, which drafts a base graph using one small PLM, then employs a\nsecond PLM to iteratively propose graph edits, reducing full-graph generation\noverhead. Using two Flan-T5-Base models, DiscoSG-Refiner still improves SPICE\nby approximately 30% over the best baseline while achieving 86 times faster\ninference than GPT-4. It also consistently improves downstream VLM tasks like\ndiscourse-level caption evaluation and hallucination detection. Code and data\nare available at: https://github.com/ShaoqLin/DiscoSG\n","authors":["Shaoqing Lin","Chong Teng","Fei Li","Donghong Ji","Lizhen Qu","Zhuang Li"],"pdf_url":"https://arxiv.org/pdf/2506.15583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15569v1","updated":"2025-06-18T15:43:26Z","published":"2025-06-18T15:43:26Z","title":"SciVer: Evaluating Foundation Models for Multimodal Scientific Claim\n  Verification","summary":"  We introduce SciVer, the first benchmark specifically designed to evaluate\nthe ability of foundation models to verify claims within a multimodal\nscientific context. SciVer consists of 3,000 expert-annotated examples over\n1,113 scientific papers, covering four subsets, each representing a common\nreasoning type in multimodal scientific claim verification. To enable\nfine-grained evaluation, each example includes expert-annotated supporting\nevidence. We assess the performance of 21 state-of-the-art multimodal\nfoundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and\nQwen2.5-VL. Our experiment reveals a substantial performance gap between these\nmodels and human experts on SciVer. Through an in-depth analysis of\nretrieval-augmented generation (RAG), and human-conducted error evaluations, we\nidentify critical limitations in current open-source models, offering key\ninsights to advance models' comprehension and reasoning in multimodal\nscientific literature tasks.\n","authors":["Chengye Wang","Yifei Shen","Zexi Kuang","Arman Cohan","Yilun Zhao"],"pdf_url":"https://arxiv.org/pdf/2506.15569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15568v1","updated":"2025-06-18T15:43:16Z","published":"2025-06-18T15:43:16Z","title":"Gender Inclusivity Fairness Index (GIFI): A Multilevel Framework for\n  Evaluating Gender Diversity in Large Language Models","summary":"  We present a comprehensive evaluation of gender fairness in large language\nmodels (LLMs), focusing on their ability to handle both binary and non-binary\ngenders. While previous studies primarily focus on binary gender distinctions,\nwe introduce the Gender Inclusivity Fairness Index (GIFI), a novel and\ncomprehensive metric that quantifies the diverse gender inclusivity of LLMs.\nGIFI consists of a wide range of evaluations at different levels, from simply\nprobing the model with respect to provided gender pronouns to testing various\naspects of model generation and cognitive behaviors under different gender\nassumptions, revealing biases associated with varying gender identifiers. We\nconduct extensive evaluations with GIFI on 22 prominent open-source and\nproprietary LLMs of varying sizes and capabilities, discovering significant\nvariations in LLMs' gender inclusivity. Our study highlights the importance of\nimproving LLMs' inclusivity, providing a critical benchmark for future\nadvancements in gender fairness in generative models.\n","authors":["Zhengyang Shan","Emily Ruth Diana","Jiawei Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.15568v1.pdf","comment":"Accepted by ACL 2025 Main"},{"id":"http://arxiv.org/abs/2505.12992v3","updated":"2025-06-18T15:41:14Z","published":"2025-05-19T11:30:41Z","title":"Fractured Chain-of-Thought Reasoning","summary":"  Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning. Code is available at\nhttps://github.com/BaohaoLiao/frac-cot.\n","authors":["Baohao Liao","Hanze Dong","Yuhui Xu","Doyen Sahoo","Christof Monz","Junnan Li","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2505.12992v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15556v1","updated":"2025-06-18T15:29:02Z","published":"2025-06-18T15:29:02Z","title":"PredGen: Accelerated Inference of Large Language Models through\n  Input-Time Speculation for Real-Time Speech Interaction","summary":"  Large Language Models (LLMs) are widely used in real-time voice chat\napplications, typically in combination with text-to-speech (TTS) systems to\ngenerate audio responses. However, their large size often leads to noticeable\nlatency between the end of user input and the start of audio output, resulting\nin suboptimal user experiences. This latency is particularly evident when LLMs\nare deployed as single-user voice assistants on consumer-grade hardware with\nlimited computing capacity. We discovered that this latency is primarily\ndominated by the time it takes for the LLMs to generate the first sentence,\nwhich is required as input by the TTS systems that synthesize audio responses\non a sentence-by-sentence basis. To address this bottleneck, we propose\nPredictive Generation (PredGen), a novel framework that mitigates-or even\neliminates-this delay through speculative decoding at input time. PredGen\ngenerates candidate responses while the user is still speaking, enabling the\nsystem to begin TTS processing with minimal delay. Simulated experiments on the\nLmsys and MT-Bench datasets show that the proposed method can effectively\nreduce the latency by around 2x across a wide range of use cases, while\nincurring only minimal additional computation cost at input time-computation\nthat would otherwise go unused.\n","authors":["Shufan Li","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2506.15556v1.pdf","comment":"16 pages,4 figures"},{"id":"http://arxiv.org/abs/2505.24832v3","updated":"2025-06-18T15:27:03Z","published":"2025-05-30T17:34:03Z","title":"How much do language models memorize?","summary":"  We propose a new method for estimating how much a model knows about a\ndatapoint and use it to measure the capacity of modern language models. Prior\nstudies of language model memorization have struggled to disentangle\nmemorization from generalization. We formally separate memorization into two\ncomponents: unintended memorization, the information a model contains about a\nspecific dataset, and generalization, the information a model contains about\nthe true data-generation process. When we completely eliminate generalization,\nwe can compute the total memorization, which provides an estimate of model\ncapacity: our measurements estimate that GPT-style models have a capacity of\napproximately 3.6 bits per parameter. We train language models on datasets of\nincreasing size and observe that models memorize until their capacity fills, at\nwhich point \"grokking\" begins, and unintended memorization decreases as models\nbegin to generalize. We train hundreds of transformer language models ranging\nfrom $500K$ to $1.5B$ parameters and produce a series of scaling laws relating\nmodel capacity and data size to membership inference.\n","authors":["John X. Morris","Chawin Sitawarin","Chuan Guo","Narine Kokhlikyan","G. Edward Suh","Alexander M. Rush","Kamalika Chaudhuri","Saeed Mahloujifar"],"pdf_url":"https://arxiv.org/pdf/2505.24832v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15553v1","updated":"2025-06-18T15:26:43Z","published":"2025-06-18T15:26:43Z","title":"Approximating Language Model Training Data from Weights","summary":"  Modern language models often have open weights but closed training data. We\nformalize the problem of data approximation from model weights and propose\nseveral baselines and metrics. We develop a gradient-based approach that\nselects the highest-matching data from a large public text corpus and show its\neffectiveness at recovering useful data given only weights of the original and\nfinetuned models. Even when none of the true training data is known, our method\nis able to locate a small subset of public Web documents can be used to train a\nmodel to close to the original model performance given models trained for both\nclassification and supervised-finetuning. On the AG News classification task,\nour method improves performance from 65% (using randomly selected data) to 80%,\napproaching the expert benchmark of 88%. When applied to a model trained with\nSFT on MSMARCO web documents, our method reduces perplexity from 3.3 to 2.3,\ncompared to an expert LLAMA model's perplexity of 2.0.\n","authors":["John X. Morris","Junjie Oscar Yin","Woojeong Kim","Vitaly Shmatikov","Alexander M. Rush"],"pdf_url":"https://arxiv.org/pdf/2506.15553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15545v1","updated":"2025-06-18T15:18:07Z","published":"2025-06-18T15:18:07Z","title":"RATTENTION: Towards the Minimal Sliding Window Size in Local-Global\n  Attention Models","summary":"  Local-global attention models have recently emerged as compelling\nalternatives to standard Transformers, promising improvements in both training\nand inference efficiency. However, the crucial choice of window size presents a\nPareto tradeoff: larger windows maintain performance akin to full attention but\noffer minimal efficiency gains in short-context scenarios, while smaller\nwindows can lead to performance degradation. Current models, such as Gemma2 and\nMistral, adopt conservative window sizes (e.g., 4096 out of an 8192 pretraining\nlength) to preserve performance. This work investigates strategies to shift\nthis Pareto frontier, enabling local-global models to achieve efficiency gains\neven in short-context regimes. Our core motivation is to address the intrinsic\nlimitation of local attention -- its complete disregard for tokens outside the\ndefined window. We explore RATTENTION, a variant of local attention integrated\nwith a specialized linear attention mechanism designed to capture information\nfrom these out-of-window tokens. Pretraining experiments at the 3B and 12B\nscales demonstrate that RATTENTION achieves a superior Pareto tradeoff between\nperformance and efficiency. As a sweet spot, RATTENTION with a window size of\njust 512 consistently matches the performance of full-attention models across\ndiverse settings. Furthermore, the recurrent nature inherent in the linear\nattention component of RATTENTION contributes to enhanced long-context\nperformance, as validated on the RULER benchmark. Crucially, these improvements\ndo not compromise training efficiency; thanks to a specialized kernel\nimplementation and the reduced window size, RATTENTION maintains training\nspeeds comparable to existing state-of-the-art approaches.\n","authors":["Bailin Wang","Chang Lan","Chong Wang","Ruoming Pang"],"pdf_url":"https://arxiv.org/pdf/2506.15545v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2506.15538v1","updated":"2025-06-18T15:13:07Z","published":"2025-06-18T15:13:07Z","title":"Capturing Polysemanticity with PRISM: A Multi-Concept Feature\n  Description Framework","summary":"  Automated interpretability research aims to identify concepts encoded in\nneural network features to enhance human understanding of model behavior.\nCurrent feature description methods face two critical challenges: limited\nrobustness and the flawed assumption that each neuron encodes only a single\nconcept (monosemanticity), despite growing evidence that neurons are often\npolysemantic. This assumption restricts the expressiveness of feature\ndescriptions and limits their ability to capture the full range of behaviors\nencoded in model internals. To address this, we introduce Polysemantic FeatuRe\nIdentification and Scoring Method (PRISM), a novel framework that captures the\ninherent complexity of neural network features. Unlike prior approaches that\nassign a single description per feature, PRISM provides more nuanced\ndescriptions for both polysemantic and monosemantic features. We apply PRISM to\nlanguage models and, through extensive benchmarking against existing methods,\ndemonstrate that our approach produces more accurate and faithful feature\ndescriptions, improving both overall description quality (via a description\nscore) and the ability to capture distinct concepts when polysemanticity is\npresent (via a polysemanticity score).\n","authors":["Laura Kopf","Nils Feldhus","Kirill Bykov","Philine Lou Bommer","Anna Hedström","Marina M. -C. Höhne","Oliver Eberle"],"pdf_url":"https://arxiv.org/pdf/2506.15538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07009v3","updated":"2025-06-18T15:08:29Z","published":"2024-10-09T15:52:48Z","title":"Pap2Pat: Benchmarking Outline-Guided Long-Text Patent Generation with\n  Patent-Paper Pairs","summary":"  Dealing with long and highly complex technical text is a challenge for Large\nLanguage Models (LLMs), which still have to unfold their potential in\nsupporting expensive and timeintensive processes like patent drafting. Within\npatents, the description constitutes more than 90% of the document on average.\nYet, its automatic generation remains understudied. When drafting patent\napplications, patent attorneys typically receive invention reports (IRs), which\nare usually confidential, hindering research on LLM-supported patent drafting.\nOften, prepublication research papers serve as IRs. We leverage this duality to\nbuild PAP2PAT, an open and realistic benchmark for patent drafting consisting\nof 1.8k patent-paper pairs describing the same inventions. To address the\ncomplex longdocument patent generation task, we propose chunk-based\noutline-guided generation using the research paper as invention specification.\nOur extensive evaluation using PAP2PAT and a human case study show that LLMs\ncan effectively leverage information from the paper, but still struggle to\nprovide the necessary level of detail. Fine-tuning leads to more patent-style\nlanguage, but also to more hallucination. We release our data and code\nhttps://github.com/boschresearch/Pap2Pat.\n","authors":["Valentin Knappich","Simon Razniewski","Anna Hätty","Annemarie Friedrich"],"pdf_url":"https://arxiv.org/pdf/2410.07009v3.pdf","comment":"ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2506.15522v1","updated":"2025-06-18T14:58:13Z","published":"2025-06-18T14:58:13Z","title":"Lessons from Training Grounded LLMs with Verifiable Rewards","summary":"  Generating grounded and trustworthy responses remains a key challenge for\nlarge language models (LLMs). While retrieval-augmented generation (RAG) with\ncitation-based grounding holds promise, instruction-tuned models frequently\nfail even in straightforward scenarios: missing explicitly stated answers,\nciting incorrectly, or refusing when evidence is available. In this work, we\nexplore how reinforcement learning (RL) and internal reasoning can enhance\ngrounding in LLMs. We use the GRPO (Group Relative Policy Optimization) method\nto train models using verifiable outcome-based rewards targeting answer\ncorrectness, citation sufficiency, and refusal quality, without requiring gold\nreasoning traces or expensive annotations. Through comprehensive experiments\nacross ASQA, QAMPARI, ELI5, and ExpertQA we show that reasoning-augmented\nmodels significantly outperform instruction-only variants, especially in\nhandling unanswerable queries and generating well-cited responses. A two-stage\ntraining setup, first optimizing answer and citation behavior and then refusal,\nfurther improves grounding by stabilizing the learning signal. Additionally, we\nrevisit instruction tuning via GPT-4 distillation and find that combining it\nwith GRPO enhances performance on long-form, generative QA tasks. Overall, our\nfindings highlight the value of reasoning, stage-wise optimization, and\noutcome-driven RL for building more verifiable and reliable LLMs.\n","authors":["Shang Hong Sim","Tej Deep Pala","Vernon Toh","Hai Leong Chieu","Amir Zadeh","Chuan Li","Navonil Majumder","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2506.15522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15621v3","updated":"2025-06-18T14:52:47Z","published":"2024-07-22T13:29:56Z","title":"RadioRAG: Online Retrieval-augmented Generation for Radiology Question\n  Answering","summary":"  Large language models (LLMs) often generate outdated or inaccurate\ninformation based on static training datasets. Retrieval-augmented generation\n(RAG) mitigates this by integrating outside data sources. While previous RAG\nsystems used pre-assembled, fixed databases with limited flexibility, we have\ndeveloped Radiology RAG (RadioRAG), an end-to-end framework that retrieves data\nfrom authoritative radiologic online sources in real-time. We evaluate the\ndiagnostic accuracy of various LLMs when answering radiology-specific questions\nwith and without access to additional online information via RAG. Using 80\nquestions from the RSNA Case Collection across radiologic subspecialties and 24\nadditional expert-curated questions with reference standard answers, LLMs\n(GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B and 70B]) were\nprompted with and without RadioRAG in a zero-shot inference scenario RadioRAG\nretrieved context-specific information from Radiopaedia in real-time. Accuracy\nwas investigated. Statistical analyses were performed using bootstrapping. The\nresults were further compared with human performance. RadioRAG improved\ndiagnostic accuracy across most LLMs, with relative accuracy increases ranging\nup to 54% for different LLMs. It matched or exceeded non-RAG models and the\nhuman radiologist in question answering across radiologic subspecialties,\nparticularly in breast imaging and emergency radiology. However, the degree of\nimprovement varied among models; GPT-3.5-turbo and Mixtral-8x7B-instruct-v0.1\nsaw notable gains, while Mistral-7B-instruct-v0.2 showed no improvement,\nhighlighting variability in RadioRAG's effectiveness. LLMs benefit when\nprovided access to domain-specific data beyond their training data. RadioRAG\nshows potential to improve LLM accuracy and factuality in radiology question\nanswering by integrating real-time domain-specific data.\n","authors":["Soroosh Tayebi Arasteh","Mahshad Lotfinia","Keno Bressem","Robert Siepmann","Lisa Adams","Dyke Ferber","Christiane Kuhl","Jakob Nikolas Kather","Sven Nebelung","Daniel Truhn"],"pdf_url":"https://arxiv.org/pdf/2407.15621v3.pdf","comment":"Published in Radiology: Artificial Intelligence"},{"id":"http://arxiv.org/abs/2506.08343v2","updated":"2025-06-18T14:43:36Z","published":"2025-06-10T01:54:04Z","title":"Wait, We Don't Need to \"Wait\"! Removing Thinking Tokens Improves\n  Reasoning Efficiency","summary":"  Recent advances in large reasoning models have enabled complex, step-by-step\nreasoning but often introduce significant overthinking, resulting in verbose\nand redundant outputs that hinder efficiency. In this study, we examine whether\nexplicit self-reflection, signaled by tokens such as \"Wait\" and \"Hmm\", is\nnecessary for advanced reasoning. We propose NoWait, a simple yet effective\napproach that disables explicit self-reflection by suppressing these tokens\nduring inference. Extensive experiments on ten benchmarks across textual,\nvisual, and video reasoning tasks show that NoWait reduces chain-of-thought\ntrajectory length by up to 27%-51% in five R1-style model series, without\ncompromising model utility. NoWait thus offers a plug-and-play solution for\nefficient and utility-preserving multimodal reasoning.\n","authors":["Chenlong Wang","Yuanning Feng","Dongping Chen","Zhaoyang Chu","Ranjay Krishna","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.08343v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15504v1","updated":"2025-06-18T14:42:34Z","published":"2025-06-18T14:42:34Z","title":"Enhancing Hyperbole and Metaphor Detection with Their Bidirectional\n  Dynamic Interaction and Emotion Knowledge","summary":"  Text-based hyperbole and metaphor detection are of great significance for\nnatural language processing (NLP) tasks. However, due to their semantic\nobscurity and expressive diversity, it is rather challenging to identify them.\nExisting methods mostly focus on superficial text features, ignoring the\nassociations of hyperbole and metaphor as well as the effect of implicit\nemotion on perceiving these rhetorical devices. To implement these hypotheses,\nwe propose an emotion-guided hyperbole and metaphor detection framework based\non bidirectional dynamic interaction (EmoBi). Firstly, the emotion analysis\nmodule deeply mines the emotion connotations behind hyperbole and metaphor.\nNext, the emotion-based domain mapping module identifies the target and source\ndomains to gain a deeper understanding of the implicit meanings of hyperbole\nand metaphor. Finally, the bidirectional dynamic interaction module enables the\nmutual promotion between hyperbole and metaphor. Meanwhile, a verification\nmechanism is designed to ensure detection accuracy and reliability. Experiments\nshow that EmoBi outperforms all baseline methods on four datasets.\nSpecifically, compared to the current SoTA, the F1 score increased by 28.1% for\nhyperbole detection on the TroFi dataset and 23.1% for metaphor detection on\nthe HYPO-L dataset. These results, underpinned by in-depth analyses, underscore\nthe effectiveness and potential of our approach for advancing hyperbole and\nmetaphor detection.\n","authors":["Li Zheng","Sihang Wang","Hao Fei","Zuquan Peng","Fei Li","Jianming Fu","Chong Teng","Donghong Ji"],"pdf_url":"https://arxiv.org/pdf/2506.15504v1.pdf","comment":"Accepted by ACL 2025"},{"id":"http://arxiv.org/abs/2410.17161v3","updated":"2025-06-18T14:42:07Z","published":"2024-10-22T16:34:36Z","title":"Interchangeable Token Embeddings for Extendable Vocabulary and\n  Alpha-Equivalence","summary":"  Language models lack the notion of interchangeable tokens: symbols that are\nsemantically equivalent yet distinct, such as bound variables in formal logic.\nThis limitation prevents generalization to larger vocabularies and hinders the\nmodel's ability to recognize alpha-equivalence, where renaming bound variables\npreserves meaning. We formalize this machine learning problem and introduce\nalpha-covariance, a metric for evaluating robustness to such transformations.\nTo tackle this task, we propose a dual-part token embedding strategy: a shared\ncomponent ensures semantic consistency, while a randomized component maintains\ntoken distinguishability. Compared to a baseline that relies on alpha-renaming\nfor data augmentation, our approach demonstrates improved generalization to\nunseen tokens in linear temporal logic solving, propositional logic assignment\nprediction, and copying with an extendable vocabulary, while introducing a\nfavorable inductive bias for alpha-equivalence. Our findings establish a\nfoundation for designing language models that can learn interchangeable token\nrepresentations, a crucial step toward more flexible and systematic reasoning\nin formal domains. Our code and project page are available at\nhttps://necrashter.github.io/interchangeable-token-embeddings\n","authors":["İlker Işık","Ramazan Gokberk Cinbis","Ebru Aydin Gol"],"pdf_url":"https://arxiv.org/pdf/2410.17161v3.pdf","comment":"ICML 2025 Poster Paper, Camera Ready Version"},{"id":"http://arxiv.org/abs/2506.15498v1","updated":"2025-06-18T14:37:59Z","published":"2025-06-18T14:37:59Z","title":"SPARE: Single-Pass Annotation with Reference-Guided Evaluation for\n  Automatic Process Supervision and Reward Modelling","summary":"  Process or step-wise supervision has played a crucial role in advancing\ncomplex multi-step reasoning capabilities of Large Language Models (LLMs).\nHowever, efficient, high-quality automated process annotation remains a\nsignificant challenge. To address this, we introduce Single-Pass Annotation\nwith Reference-Guided Evaluation (SPARE), a novel structured framework that\nenables single-pass, per-step annotation by aligning each solution step to one\nor multiple steps in a reference solution, accompanied by explicit reasoning\nfor evaluation. We show that reference-guided step-level evaluation effectively\nfacilitates process supervision on four datasets spanning three domains:\nmathematical reasoning, multi-hop compositional question answering, and spatial\nreasoning. We demonstrate that SPARE, when compared to baselines, improves\nreasoning performance when used for: (1) fine-tuning models in an offline RL\nsetup for inference-time greedy-decoding, and (2) training reward models for\nranking/aggregating multiple LLM-generated outputs. Additionally, SPARE\nachieves competitive performance on challenging mathematical datasets while\noffering 2.6 times greater efficiency, requiring only 38% of the runtime,\ncompared to tree search-based automatic annotation. The codebase, along with a\ntrained SPARE-PRM model, is publicly released to facilitate further research\nand reproducibility.\n","authors":["Md Imbesat Hassan Rizvi","Xiaodan Zhu","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2506.15498v1.pdf","comment":"8 pages main content, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2503.08327v2","updated":"2025-06-18T14:23:15Z","published":"2025-03-11T11:40:10Z","title":"Adding Chocolate to Mint: Mitigating Metric Interference in Machine\n  Translation","summary":"  As automatic metrics become increasingly stronger and widely adopted, the\nrisk of unintentionally \"gaming the metric\" during model development rises.\nThis issue is caused by metric interference (MINT), i.e., the use of the same\nor related metrics for both model tuning and evaluation. MINT can misguide\npractitioners into being overoptimistic about the performance of their systems:\nas system outputs become a function of the interfering metric, their estimated\nquality loses correlation with human judgments. In this work, we analyze two\ncommon cases of MINT in machine translation-related tasks: filtering of\ntraining data, and decoding with quality signals. Importantly, we find that\nMINT strongly distorts instance-level metric scores, even when metrics are not\ndirectly optimized for-questioning the common strategy of leveraging a\ndifferent, yet related metric for evaluation that is not used for tuning. To\naddress this problem, we propose MINTADJUST, a method for more reliable\nevaluation under MINT. On the WMT24 MT shared task test set, MINTADJUST ranks\ntranslations and systems more accurately than state-of-the-art metrics across a\nmajority of language pairs, especially for high-quality systems. Furthermore,\nMINTADJUST outperforms AUTORANK, the ensembling method used by the organizers.\n","authors":["José Pombal","Nuno M. Guerreiro","Ricardo Rei","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2503.08327v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15480v1","updated":"2025-06-18T14:13:56Z","published":"2025-06-18T14:13:56Z","title":"Context-Informed Grounding Supervision","summary":"  Large language models (LLMs) are often supplemented with external knowledge\nto provide information not encoded in their parameters or to reduce\nhallucination. In such cases, we expect the model to generate responses by\ngrounding its response in the provided external context. However, prior work\nhas shown that simply appending context at inference time does not ensure\ngrounded generation. To address this, we propose Context-INformed Grounding\nSupervision (CINGS), a post-training supervision in which the model is trained\nwith relevant context prepended to the response, while computing the loss only\nover the response tokens and masking out the context. Our experiments\ndemonstrate that models trained with CINGS exhibit stronger grounding in both\ntextual and visual domains compared to standard instruction-tuned models. In\nthe text domain, CINGS outperforms other training methods across 11\ninformation-seeking datasets and is complementary to inference-time grounding\ntechniques. In the vision-language domain, replacing a vision-language model's\nLLM backbone with a CINGS-trained model reduces hallucinations across four\nbenchmarks and maintains factual consistency throughout the generated response.\nThis improved grounding comes without degradation in general downstream\nperformance. Finally, we analyze the mechanism underlying the enhanced\ngrounding in CINGS and find that it induces a shift in the model's prior\nknowledge and behavior, implicitly encouraging greater reliance on the external\ncontext.\n","authors":["Hyunji Lee","Seunghyun Yoon","Yunjae Won","Hanseok Oh","Geewook Kim","Trung Bui","Franck Dernoncourt","Elias Stengel-Eskin","Mohit Bansal","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2506.15480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10912v2","updated":"2025-06-18T14:00:37Z","published":"2025-06-12T17:25:53Z","title":"Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular\n  Detoxification?","summary":"  Toxicity remains a leading cause of early-stage drug development failure.\nDespite advances in molecular design and property prediction, the task of\nmolecular toxicity repair - generating structurally valid molecular\nalternatives with reduced toxicity - has not yet been systematically defined or\nbenchmarked. To fill this gap, we introduce ToxiMol, the first benchmark task\nfor general-purpose Multimodal Large Language Models (MLLMs) focused on\nmolecular toxicity repair. We construct a standardized dataset covering 11\nprimary tasks and 560 representative toxic molecules spanning diverse\nmechanisms and granularities. We design a prompt annotation pipeline with\nmechanism-aware and task-adaptive capabilities, informed by expert\ntoxicological knowledge. In parallel, we propose an automated evaluation\nframework, ToxiEval, which integrates toxicity endpoint prediction, synthetic\naccessibility, drug-likeness, and structural similarity into a high-throughput\nevaluation chain for repair success. We systematically assess nearly 30\nmainstream general-purpose MLLMs and design multiple ablation studies to\nanalyze key factors such as evaluation criteria, candidate diversity, and\nfailure attribution. Experimental results show that although current MLLMs\nstill face significant challenges on this task, they begin to demonstrate\npromising capabilities in toxicity understanding, semantic constraint\nadherence, and structure-aware molecule editing.\n","authors":["Fei Lin","Ziyang Gong","Cong Wang","Yonglin Tian","Tengchao Zhang","Xue Yang","Gen Luo","Fei-Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2506.10912v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20302v3","updated":"2025-06-18T13:36:39Z","published":"2024-09-30T14:00:04Z","title":"OM4OV: Leveraging Ontology Matching for Ontology Versioning","summary":"  Due to the dynamic nature of the Semantic Web, version control is necessary\nto capture time-varying information, particularly for widely used ontologies.\nDespite the long-standing recognition of ontology versioning (OV) as a crucial\ncomponent for efficient ontology management, the growing size of ontologies and\naccumulating errors caused by manual labour overwhelm current OV approaches. In\nthis paper, we propose yet another approach to performing OV using existing\nontology matching (OM) techniques and systems. We introduce a unified OM4OV\npipeline. From an OM perspective, we reconstruct a new task formulation and\nmeasurement for OV tasks. Building upon the prior alignment(s) from OM, we\npropose a pipeline optimisation method called the cross-reference (CR)\nmechanism to enhance overall OV performance. We experimentally validate the\nOM4OV pipeline and the cross-reference mechanism in the OV tested originating\nfrom the Ontology Alignment Evaluation Initiative (OAEI) datasets. We also\ndiscuss insights into OM used for OV tasks, where some false mappings detected\nby OV systems are not actually untrue.\n","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang"],"pdf_url":"https://arxiv.org/pdf/2409.20302v3.pdf","comment":"15 pages, 8 figures, 1 table"},{"id":"http://arxiv.org/abs/2506.15456v1","updated":"2025-06-18T13:36:34Z","published":"2025-06-18T13:36:34Z","title":"Factorized RVQ-GAN For Disentangled Speech Tokenization","summary":"  We propose Hierarchical Audio Codec (HAC), a unified neural speech codec that\nfactorizes its bottleneck into three linguistic levels-acoustic, phonetic, and\nlexical-within a single model. HAC leverages two knowledge distillation\nobjectives: one from a pre-trained speech encoder (HuBERT) for phoneme-level\nstructure, and another from a text-based encoder (LaBSE) for lexical cues.\nExperiments on English and multilingual data show that HAC's factorized\nbottleneck yields disentangled token sets: one aligns with phonemes, while\nanother captures word-level semantics. Quantitative evaluations confirm that\nHAC tokens preserve naturalness and provide interpretable linguistic\ninformation, outperforming single-level baselines in both disentanglement and\nreconstruction quality. These findings underscore HAC's potential as a unified\ndiscrete speech representation, bridging acoustic detail and lexical meaning\nfor downstream speech generation and understanding tasks.\n","authors":["Sameer Khurana","Dominik Klement","Antoine Laurent","Dominik Bobos","Juraj Novosad","Peter Gazdik","Ellen Zhang","Zili Huang","Amir Hussein","Ricard Marxer","Yoshiki Masuyama","Ryo Aihara","Chiori Hori","Francois G. Germain","Gordon Wichern","Jonathan Le Roux"],"pdf_url":"https://arxiv.org/pdf/2506.15456v1.pdf","comment":"Accepted to Interspeech 2025"},{"id":"http://arxiv.org/abs/2506.15455v1","updated":"2025-06-18T13:35:47Z","published":"2025-06-18T13:35:47Z","title":"RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation","summary":"  Recent Large Language Models (LLMs) have reported high accuracy on reasoning\nbenchmarks. However, it is still unclear whether the observed results arise\nfrom true reasoning or from statistical recall of the training set. Inspired by\nthe ladder of causation (Pearl, 2009) and its three levels (associations,\ninterventions and counterfactuals), this paper introduces RE-IMAGINE, a\nframework to characterize a hierarchy of reasoning ability in LLMs, alongside\nan automated pipeline to generate problem variations at different levels of the\nhierarchy. By altering problems in an intermediate symbolic representation,\nRE-IMAGINE generates arbitrarily many problems that are not solvable using\nmemorization alone. Moreover, the framework is general and can work across\nreasoning domains, including math, code, and logic. We demonstrate our\nframework on four widely-used benchmarks to evaluate several families of LLMs,\nand observe reductions in performance when the models are queried with problem\nvariations. These assessments indicate a degree of reliance on statistical\nrecall for past performance, and open the door to further research targeting\nskills across the reasoning hierarchy.\n","authors":["Xinnuo Xu","Rachel Lawrence","Kshitij Dubey","Atharva Pandey","Risa Ueno","Fabian Falck","Aditya V. Nori","Rahul Sharma","Amit Sharma","Javier Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2506.15455v1.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2506.15451v1","updated":"2025-06-18T13:24:04Z","published":"2025-06-18T13:24:04Z","title":"AgentGroupChat-V2: Divide-and-Conquer Is What LLM-Based Multi-Agent\n  System Need","summary":"  Large language model based multi-agent systems have demonstrated significant\npotential in social simulation and complex task resolution domains. However,\ncurrent frameworks face critical challenges in system architecture design,\ncross-domain generalizability, and performance guarantees, particularly as task\ncomplexity and number of agents increases. We introduces AgentGroupChat-V2, a\nnovel framework addressing these challenges through three core innovations: (1)\na divide-and-conquer fully parallel architecture that decomposes user queries\ninto hierarchical task forest structures enabling dependency management and\ndistributed concurrent processing. (2) an adaptive collaboration engine that\ndynamically selects heterogeneous LLM combinations and interaction modes based\non task characteristics. (3) agent organization optimization strategies\ncombining divide-and-conquer approaches for efficient problem decomposition.\nExtensive experiments demonstrate AgentGroupChat-V2's superior performance\nacross diverse domains, achieving 91.50% accuracy on GSM8K (exceeding the best\nbaseline by 5.6 percentage points), 30.4% accuracy on competition-level AIME\n(nearly doubling other methods), and 79.20% pass@1 on HumanEval. Performance\nadvantages become increasingly pronounced with higher task difficulty,\nparticularly on Level 5 MATH problems where improvements exceed 11 percentage\npoints compared to state-of-the-art baselines. These results confirm that\nAgentGroupChat-V2 provides a comprehensive solution for building efficient,\ngeneral-purpose LLM multi-agent systems with significant advantages in complex\nreasoning scenarios. Code is available at\nhttps://github.com/MikeGu721/AgentGroupChat-V2.\n","authors":["Zhouhong Gu","Xiaoxuan Zhu","Yin Cai","Hao Shen","Xingzhou Chen","Qingyi Wang","Jialin Li","Xiaoran Shi","Haoran Guo","Wenxuan Huang","Hongwei Feng","Yanghua Xiao","Zheyu Ye","Yao Hu","Shaosheng Cao"],"pdf_url":"https://arxiv.org/pdf/2506.15451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14625v2","updated":"2025-06-18T13:21:13Z","published":"2025-06-17T15:22:21Z","title":"Probabilistic Aggregation and Targeted Embedding Optimization for\n  Collective Moral Reasoning in Large Language Models","summary":"  Large Language Models (LLMs) have shown impressive moral reasoning abilities.\nYet they often diverge when confronted with complex, multi-factor moral\ndilemmas. To address these discrepancies, we propose a framework that\nsynthesizes multiple LLMs' moral judgments into a collectively formulated moral\njudgment, realigning models that deviate significantly from this consensus. Our\naggregation mechanism fuses continuous moral acceptability scores (beyond\nbinary labels) into a collective probability, weighting contributions by model\nreliability. For misaligned models, a targeted embedding-optimization procedure\nfine-tunes token embeddings for moral philosophical theories, minimizing JS\ndivergence to the consensus while preserving semantic integrity. Experiments on\na large-scale social moral dilemma dataset show our approach builds robust\nconsensus and improves individual model fidelity. These findings highlight the\nvalue of data-driven moral alignment across multiple models and its potential\nfor safer, more consistent AI systems.\n","authors":["Chenchen Yuan","Zheyu Zhang","Shuo Yang","Bardh Prenkaj","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2506.14625v2.pdf","comment":"Accepted to ACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2506.15425v1","updated":"2025-06-18T12:55:35Z","published":"2025-06-18T12:55:35Z","title":"Understanding GUI Agent Localization Biases through Logit Sharpness","summary":"  Multimodal large language models (MLLMs) have enabled GUI agents to interact\nwith operating systems by grounding language into spatial actions. Despite\ntheir promising performance, these models frequently exhibit\nhallucinations-systematic localization errors that compromise reliability. We\npropose a fine-grained evaluation framework that categorizes model predictions\ninto four distinct types, revealing nuanced failure modes beyond traditional\naccuracy metrics. To better quantify model uncertainty, we introduce the Peak\nSharpness Score (PSS), a metric that evaluates the alignment between semantic\ncontinuity and logits distribution in coordinate prediction. Building on this\ninsight, we further propose Context-Aware Cropping, a training-free technique\nthat improves model performance by adaptively refining input context. Extensive\nexperiments demonstrate that our framework and methods provide actionable\ninsights and enhance the interpretability and robustness of GUI agent behavior.\n","authors":["Xingjian Tao","Yiwei Wang","Yujun Cai","Zhicheng Yang","Jing Tang"],"pdf_url":"https://arxiv.org/pdf/2506.15425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15415v1","updated":"2025-06-18T12:35:53Z","published":"2025-06-18T12:35:53Z","title":"Targeted Lexical Injection: Unlocking Latent Cross-Lingual Alignment in\n  Lugha-Llama via Early-Layer LoRA Fine-Tuning","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir performance in low-resource languages (LRLs), such as Swahili, often lags\ndue to data scarcity and underrepresentation in pre-training. A key challenge\nis achieving robust cross-lingual lexical alignment, crucial for tasks like\ntranslation and cross-lingual information retrieval. This paper introduces\nTargeted Lexical Injection (TLI), a novel and efficient fine-tuning approach.\nWe first demonstrate that Lugha-Llama-8B-wura, a Swahili-centric LLM, exhibits\nstrong, near-perfect lexical alignment for Swahili-English word pairs in its\nearly internal layers (specifically Layer 2, with ~0.99998 average cosine\nsimilarity based on a pilot study), a capability not fully reflected in its\nfinal output representations (baseline ~0.32 similarity on our evaluation set).\nTLI leverages this insight by using Low-Rank Adaptation (LoRA) and a\ncontrastive learning objective to fine-tune the model, specifically targeting\nembeddings from this empirically identified optimal early layer. Our\nexperiments show that TLI significantly improves the output-level lexical\nalignment for 623 trained Swahili-English word pairs, increasing average cosine\nsimilarity from 0.3211 to 0.4113 (+28.08%, p < 1.33 x 10^-240). More\nimportantly, these improvements generalize remarkably well to 63 unseen control\nword pairs, with similarity increasing from 0.3143 to 0.4033 (+28.32%, p < 7.17\nx 10^-27). These findings suggest TLI enhances the model's ability to preserve\nand propagate its inherent early-layer cross-lingual knowledge, offering a\nparameter-efficient and effective strategy for improving lexical alignment in\nLRL-focused LLMs.\n","authors":["Stanley Ngugi"],"pdf_url":"https://arxiv.org/pdf/2506.15415v1.pdf","comment":"11 pages, 3 figures, 2 tables. Research on parameter-efficient\n  fine-tuning (PEFT) for low-resource languages (Swahili). Investigates\n  cross-lingual lexical alignment in Lugha-Llama using LoRA and contrastive\n  learning"},{"id":"http://arxiv.org/abs/2503.01903v2","updated":"2025-06-18T12:24:25Z","published":"2025-02-28T12:17:41Z","title":"PsychBench: A comprehensive and professional benchmark for evaluating\n  the performance of LLM-assisted psychiatric clinical practice","summary":"  The advent of Large Language Models (LLMs) offers potential solutions to\naddress problems such as shortage of medical resources and low diagnostic\nconsistency in psychiatric clinical practice. Despite this potential, a robust\nand comprehensive benchmarking framework to assess the efficacy of LLMs in\nauthentic psychiatric clinical environments is absent. This has impeded the\nadvancement of specialized LLMs tailored to psychiatric applications. In\nresponse to this gap, by incorporating clinical demands in psychiatry and\nclinical data, we proposed a benchmarking system, PsychBench, to evaluate the\npractical performance of LLMs in psychiatric clinical settings. We conducted a\ncomprehensive quantitative evaluation of 16 LLMs using PsychBench, and\ninvestigated the impact of prompt design, chain-of-thought reasoning, input\ntext length, and domain-specific knowledge fine-tuning on model performance.\nThrough detailed error analysis, we identified strengths and potential\nlimitations of the existing models and suggested directions for improvement.\nSubsequently, a clinical reader study involving 60 psychiatrists of varying\nseniority was conducted to further explore the practical benefits of existing\nLLMs as supportive tools for psychiatrists of varying seniority. Through the\nquantitative and reader evaluation, we show that while existing models\ndemonstrate significant potential, they are not yet adequate as decision-making\ntools in psychiatric clinical practice. The reader study further indicates\nthat, as an auxiliary tool, LLM could provide particularly notable support for\njunior psychiatrists, effectively enhancing their work efficiency and overall\nclinical quality. To promote research in this area, we will make the dataset\nand evaluation framework publicly available, with the hope of advancing the\napplication of LLMs in psychiatric clinical settings.\n","authors":["Shuyu Liu","Ruoxi Wang","Ling Zhang","Xuequan Zhu","Rui Yang","Xinzhu Zhou","Fei Wu","Zhi Yang","Cheng Jin","Gang Wang"],"pdf_url":"https://arxiv.org/pdf/2503.01903v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.21342v3","updated":"2025-06-18T12:03:10Z","published":"2025-05-27T15:34:39Z","title":"PEDANTIC: A Dataset for the Automatic Examination of Definiteness in\n  Patent Claims","summary":"  Patent claims define the scope of protection for an invention. If there are\nambiguities in a claim, it is rejected by the patent office. In the US, this is\nreferred to as indefiniteness (35 U.S.C {\\S} 112(b)) and is among the most\nfrequent reasons for patent application rejection. The development of automatic\nmethods for patent definiteness examination has the potential to make patent\ndrafting and examination more efficient, but no annotated dataset has been\npublished to date. We introduce PEDANTIC (Patent Definiteness Examination\nCorpus), a novel dataset of 14k US patent claims from patent applications\nrelating to Natural Language Processing (NLP), annotated with reasons for\nindefiniteness. We construct PEDANTIC using a fully automatic pipeline that\nretrieves office action documents from the USPTO and uses Large Language Models\n(LLMs) to extract the reasons for indefiniteness. A human validation study\nconfirms the pipeline's accuracy in generating high-quality annotations. To\ngain insight beyond binary classification metrics, we implement an LLM-as-Judge\nevaluation that compares the free-form reasoning of every model-cited reason\nwith every examiner-cited reason. We show that LLM agents based on Qwen 2.5 32B\nand 72B struggle to outperform logistic regression baselines on definiteness\nprediction, even though they often correctly identify the underlying reasons.\nPEDANTIC provides a valuable resource for patent AI researchers, enabling the\ndevelopment of advanced examination models. We will publicly release the\ndataset and code.\n","authors":["Valentin Knappich","Annemarie Friedrich","Anna Hätty","Simon Razniewski"],"pdf_url":"https://arxiv.org/pdf/2505.21342v3.pdf","comment":"PatentSemTech@SIGIR2025"},{"id":"http://arxiv.org/abs/2506.15372v1","updated":"2025-06-18T11:38:23Z","published":"2025-06-18T11:38:23Z","title":"COSMMIC: Comment-Sensitive Multimodal Multilingual Indian Corpus for\n  Summarization and Headline Generation","summary":"  Despite progress in comment-aware multimodal and multilingual summarization\nfor English and Chinese, research in Indian languages remains limited. This\nstudy addresses this gap by introducing COSMMIC, a pioneering comment-sensitive\nmultimodal, multilingual dataset featuring nine major Indian languages. COSMMIC\ncomprises 4,959 article-image pairs and 24,484 reader comments, with\nground-truth summaries available in all included languages. Our approach\nenhances summaries by integrating reader insights and feedback. We explore\nsummarization and headline generation across four configurations: (1) using\narticle text alone, (2) incorporating user comments, (3) utilizing images, and\n(4) combining text, comments, and images. To assess the dataset's\neffectiveness, we employ state-of-the-art language models such as LLama3 and\nGPT-4. We conduct a comprehensive study to evaluate different component\ncombinations, including identifying supportive comments, filtering out noise\nusing a dedicated comment classifier using IndicBERT, and extracting valuable\ninsights from images with a multilingual CLIP-based classifier. This helps\ndetermine the most effective configurations for natural language generation\n(NLG) tasks. Unlike many existing datasets that are either text-only or lack\nuser comments in multimodal settings, COSMMIC uniquely integrates text, images,\nand user feedback. This holistic approach bridges gaps in Indian language\nresources, advancing NLP research and fostering inclusivity.\n","authors":["Raghvendra Kumar","S. A. Mohammed Salman","Aryan Sahu","Tridib Nandi","Pragathi Y. P.","Sriparna Saha","Jose G. Moreno"],"pdf_url":"https://arxiv.org/pdf/2506.15372v1.pdf","comment":"ACL 2025 MAINs"},{"id":"http://arxiv.org/abs/2506.15355v1","updated":"2025-06-18T11:19:25Z","published":"2025-06-18T11:19:25Z","title":"SANSKRITI: A Comprehensive Benchmark for Evaluating Language Models'\n  Knowledge of Indian Culture","summary":"  Language Models (LMs) are indispensable tools shaping modern workflows, but\ntheir global effectiveness depends on understanding local socio-cultural\ncontexts. To address this, we introduce SANSKRITI, a benchmark designed to\nevaluate language models' comprehension of India's rich cultural diversity.\nComprising 21,853 meticulously curated question-answer pairs spanning 28 states\nand 8 union territories, SANSKRITI is the largest dataset for testing Indian\ncultural knowledge. It covers sixteen key attributes of Indian culture: rituals\nand ceremonies, history, tourism, cuisine, dance and music, costume, language,\nart, festivals, religion, medicine, transport, sports, nightlife, and\npersonalities, providing a comprehensive representation of India's cultural\ntapestry. We evaluate SANSKRITI on leading Large Language Models (LLMs), Indic\nLanguage Models (ILMs), and Small Language Models (SLMs), revealing significant\ndisparities in their ability to handle culturally nuanced queries, with many\nmodels struggling in region-specific contexts. By offering an extensive,\nculturally rich, and diverse dataset, SANSKRITI sets a new standard for\nassessing and improving the cultural understanding of LMs.\n","authors":["Arijit Maji","Raghvendra Kumar","Akash Ghosh"," Anushka","Sriparna Saha"],"pdf_url":"https://arxiv.org/pdf/2506.15355v1.pdf","comment":"ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2506.15339v1","updated":"2025-06-18T10:42:22Z","published":"2025-06-18T10:42:22Z","title":"DeVisE: Behavioral Testing of Medical Large Language Models","summary":"  Large language models (LLMs) are increasingly used in clinical decision\nsupport, yet current evaluation methods often fail to distinguish genuine\nmedical reasoning from superficial patterns. We introduce DeVisE (Demographics\nand Vital signs Evaluation), a behavioral testing framework for probing\nfine-grained clinical understanding. We construct a dataset of ICU discharge\nnotes from MIMIC-IV, generating both raw (real-world) and template-based\n(synthetic) versions with controlled single-variable counterfactuals targeting\ndemographic (age, gender, ethnicity) and vital sign attributes. We evaluate\nfive LLMs spanning general-purpose and medically fine-tuned variants, under\nboth zero-shot and fine-tuned settings. We assess model behavior via (1)\ninput-level sensitivity - how counterfactuals alter the likelihood of a note;\nand (2) downstream reasoning - how they affect predicted hospital\nlength-of-stay. Our results show that zero-shot models exhibit more coherent\ncounterfactual reasoning patterns, while fine-tuned models tend to be more\nstable yet less responsive to clinically meaningful changes. Notably,\ndemographic factors subtly but consistently influence outputs, emphasizing the\nimportance of fairness-aware evaluation. This work highlights the utility of\nbehavioral testing in exposing the reasoning strategies of clinical LLMs and\ninforming the design of safer, more transparent medical AI systems.\n","authors":["Camila Zurdo Tagliabue","Heloisa Oss Boll","Aykut Erdem","Erkut Erdem","Iacer Calixto"],"pdf_url":"https://arxiv.org/pdf/2506.15339v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17267v2","updated":"2025-06-18T10:12:11Z","published":"2025-05-22T20:24:17Z","title":"GreekBarBench: A Challenging Benchmark for Free-Text Legal Reasoning and\n  Citations","summary":"  We introduce GreekBarBench, a benchmark that evaluates LLMs on legal\nquestions across five different legal areas from the Greek Bar exams, requiring\ncitations to statutory articles and case facts. To tackle the challenges of\nfree-text evaluation, we propose a three-dimensional scoring system combined\nwith an LLM-as-a-judge approach. We also develop a meta-evaluation benchmark to\nassess the correlation between LLM-judges and human expert evaluations,\nrevealing that simple, span-based rubrics improve their alignment. Our\nsystematic evaluation of 13 proprietary and open-weight LLMs shows that even\nthough the best models outperform average expert scores, they fall short of the\n95th percentile of experts.\n","authors":["Odysseas S. Chlapanis","Dimitrios Galanis","Nikolaos Aletras","Ion Androutsopoulos"],"pdf_url":"https://arxiv.org/pdf/2505.17267v2.pdf","comment":"19 pages, 17 figures, submitted to May ARR"},{"id":"http://arxiv.org/abs/2506.15329v1","updated":"2025-06-18T10:01:17Z","published":"2025-06-18T10:01:17Z","title":"When and How Unlabeled Data Provably Improve In-Context Learning","summary":"  Recent research shows that in-context learning (ICL) can be effective even\nwhen demonstrations have missing or incorrect labels. To shed light on this\ncapability, we examine a canonical setting where the demonstrations are drawn\naccording to a binary Gaussian mixture model (GMM) and a certain fraction of\nthe demonstrations have missing labels. We provide a comprehensive theoretical\nstudy to show that: (1) The loss landscape of one-layer linear attention models\nrecover the optimal fully-supervised estimator but completely fail to exploit\nunlabeled data; (2) In contrast, multilayer or looped transformers can\neffectively leverage unlabeled data by implicitly constructing estimators of\nthe form $\\sum_{i\\ge 0} a_i (X^\\top X)^iX^\\top y$ with $X$ and $y$ denoting\nfeatures and partially-observed labels (with missing entries set to zero). We\ncharacterize the class of polynomials that can be expressed as a function of\ndepth and draw connections to Expectation Maximization, an iterative\npseudo-labeling algorithm commonly used in semi-supervised learning.\nImportantly, the leading polynomial power is exponential in depth, so mild\namount of depth/looping suffices. As an application of theory, we propose\nlooping off-the-shelf tabular foundation models to enhance their\nsemi-supervision capabilities. Extensive evaluations on real-world datasets\nshow that our method significantly improves the semisupervised tabular learning\nperformance over the standard single pass inference.\n","authors":["Yingcong Li","Xiangyu Chang","Muti Kara","Xiaofeng Liu","Amit Roy-Chowdhury","Samet Oymak"],"pdf_url":"https://arxiv.org/pdf/2506.15329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14634v2","updated":"2025-06-18T09:56:49Z","published":"2025-06-17T15:28:53Z","title":"AIn't Nothing But a Survey? Using Large Language Models for Coding\n  German Open-Ended Survey Responses on Survey Motivation","summary":"  The recent development and wider accessibility of LLMs have spurred\ndiscussions about how they can be used in survey research, including\nclassifying open-ended survey responses. Due to their linguistic capacities, it\nis possible that LLMs are an efficient alternative to time-consuming manual\ncoding and the pre-training of supervised machine learning models. As most\nexisting research on this topic has focused on English-language responses\nrelating to non-complex topics or on single LLMs, it is unclear whether its\nfindings generalize and how the quality of these classifications compares to\nestablished methods. In this study, we investigate to what extent different\nLLMs can be used to code open-ended survey responses in other contexts, using\nGerman data on reasons for survey participation as an example. We compare\nseveral state-of-the-art LLMs and several prompting approaches, and evaluate\nthe LLMs' performance by using human expert codings. Overall performance\ndiffers greatly between LLMs, and only a fine-tuned LLM achieves satisfactory\nlevels of predictive performance. Performance differences between prompting\napproaches are conditional on the LLM used. Finally, LLMs' unequal\nclassification performance across different categories of reasons for survey\nparticipation results in different categorical distributions when not using\nfine-tuning. We discuss the implications of these findings, both for\nmethodological research on coding open-ended responses and for their\nsubstantive analysis, and for practitioners processing or substantively\nanalyzing such data. Finally, we highlight the many trade-offs researchers need\nto consider when choosing automated methods for open-ended response\nclassification in the age of LLMs. In doing so, our study contributes to the\ngrowing body of research about the conditions under which LLMs can be\nefficiently, accurately, and reliably leveraged in survey research.\n","authors":["Leah von der Heyde","Anna-Carolina Haensch","Bernd Weiß","Jessica Daikeler"],"pdf_url":"https://arxiv.org/pdf/2506.14634v2.pdf","comment":"to appear in Survey Research Methods"},{"id":"http://arxiv.org/abs/2205.02225v4","updated":"2025-06-18T09:49:21Z","published":"2022-05-04T17:56:48Z","title":"HiURE: Hierarchical Exemplar Contrastive Learning for Unsupervised\n  Relation Extraction","summary":"  Unsupervised relation extraction aims to extract the relationship between\nentities from natural language sentences without prior information on\nrelational scope or distribution. Existing works either utilize self-supervised\nschemes to refine relational feature signals by iteratively leveraging adaptive\nclustering and classification that provoke gradual drift problems, or adopt\ninstance-wise contrastive learning which unreasonably pushes apart those\nsentence pairs that are semantically similar. To overcome these defects, we\npropose a novel contrastive learning framework named HiURE, which has the\ncapability to derive hierarchical signals from relational feature space using\ncross hierarchy attention and effectively optimize relation representation of\nsentences under exemplar-wise contrastive learning. Experimental results on two\npublic datasets demonstrate the advanced effectiveness and robustness of HiURE\non unsupervised relation extraction when compared with state-of-the-art models.\n","authors":["Shuliang Liu","Xuming Hu","Chenwei Zhang","Shu`ang Li","Lijie Wen","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2205.02225v4.pdf","comment":"In NAACL 2022 as a long paper. Code and data available at\n  https://github.com/THU-BPM/HiURE"},{"id":"http://arxiv.org/abs/2505.19797v3","updated":"2025-06-18T09:47:20Z","published":"2025-05-26T10:29:42Z","title":"The Avengers: A Simple Recipe for Uniting Smaller Language Models to\n  Challenge Proprietary Giants","summary":"  Proprietary giants are increasingly dominating the race for ever-larger\nlanguage models. Can open-source, smaller models remain competitive across a\nbroad range of tasks? In this paper, we present the Avengers -- a simple recipe\nthat leverages the collective intelligence of these smaller models. The\nAvengers builds upon four lightweight operations: (i) embedding: encode queries\nusing a text embedding model; (ii) clustering: group queries based on their\nsemantic similarity; (iii) scoring: scores each model's performance within each\ncluster; and (iv) voting: improve outputs via repeated sampling and voting. At\ninference time, each query is embedded and assigned to its nearest cluster. The\ntop-performing model(s) within that cluster are selected to generate the\nresponse with repeated sampling. Remarkably, with 10 open-source models (~7B\nparameters each), the Avengers surpasses GPT-4o, 4.1, and 4.5 in average\nperformance across 15 diverse datasets spanning mathematics, coding, logical\nreasoning, general knowledge, and affective tasks. In particular, it surpasses\nGPT-4.1 on mathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore,\nthe Avengers delivers superior out-of-distribution generalization, and remains\nrobust across various embedding models, clustering algorithms, ensemble\nstrategies, and values of its sole parameter -- the number of clusters.\n","authors":["Yiqun Zhang","Hao Li","Chenxu Wang","Linyao Chen","Qiaosheng Zhang","Peng Ye","Shi Feng","Daling Wang","Zhen Wang","Xinrun Wang","Jia Xu","Lei Bai","Wanli Ouyang","Shuyue Hu"],"pdf_url":"https://arxiv.org/pdf/2505.19797v3.pdf","comment":"9 pages, 4 figures, 6 tables, supplementary material (appendix)\n  included separately"},{"id":"http://arxiv.org/abs/2506.15304v1","updated":"2025-06-18T09:35:33Z","published":"2025-06-18T09:35:33Z","title":"ConLID: Supervised Contrastive Learning for Low-Resource Language\n  Identification","summary":"  Language identification (LID) is a critical step in curating multilingual LLM\npretraining corpora from web crawls. While many studies on LID model training\nfocus on collecting diverse training data to improve performance, low-resource\nlanguages -- often limited to single-domain data, such as the Bible -- continue\nto perform poorly. To resolve these class imbalance and bias issues, we propose\na novel supervised contrastive learning (SCL) approach to learn\ndomain-invariant representations for low-resource languages. Through an\nextensive analysis, we show that our approach improves LID performance on\nout-of-domain data for low-resource languages by 3.2%, demonstrating its\neffectiveness in enhancing LID models.\n","authors":["Negar Foroutan","Jakhongir Saydaliev","Ye Eun Kim","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2506.15304v1.pdf","comment":"Submitted to EMNLP"},{"id":"http://arxiv.org/abs/2506.15301v1","updated":"2025-06-18T09:32:16Z","published":"2025-06-18T09:32:16Z","title":"Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment","summary":"  Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet,\ntheir adoption in critical domains, such as clinical trial recruitment, remains\nlimited. As trials are designed in natural language and patient data is\nrepresented as both structured and unstructured text, the task of matching\ntrials and patients benefits from knowledge aggregation and reasoning abilities\nof LLMs. Classical approaches are trial-specific and LLMs with their ability to\nconsolidate distributed knowledge hold the potential to build a more general\nsolution. Yet recent applications of LLM-assisted methods rely on proprietary\nmodels and weak evaluation benchmarks. In this survey, we are the first to\nanalyze the task of trial-patient matching and contextualize emerging LLM-based\napproaches in clinical trial recruitment. We critically examine existing\nbenchmarks, approaches and evaluation frameworks, the challenges to adopting\nLLM technologies in clinical research and exciting future directions.\n","authors":["Shrestha Ghosh","Moritz Schneider","Carina Reinicke","Carsten Eickhoff"],"pdf_url":"https://arxiv.org/pdf/2506.15301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13534v2","updated":"2025-06-18T08:59:03Z","published":"2024-02-21T05:04:29Z","title":"An Effective Incorporating Heterogeneous Knowledge Curriculum Learning\n  for Sequence Labeling","summary":"  Sequence labeling models often benefit from incorporating external knowledge.\nHowever, this practice introduces data heterogeneity and complicates the model\nwith additional modules, leading to increased expenses for training a\nhigh-performing model. To address this challenge, we propose a two-stage\ncurriculum learning (TCL) framework specifically designed for sequence labeling\ntasks. The TCL framework enhances training by gradually introducing data\ninstances from easy to hard, aiming to improve both performance and training\nspeed. Furthermore, we explore different metrics for assessing the difficulty\nlevels of sequence labeling tasks. Through extensive experimentation on six\nChinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we\ndemonstrate the effectiveness of our model in enhancing the performance of\nsequence labeling models. Additionally, our analysis indicates that TCL\naccelerates training and alleviates the slow training problem associated with\ncomplex models.\n","authors":["Xuemei Tang","Jun Wang","Qi Su","Chu-ren Huang","Jinghang Gu"],"pdf_url":"https://arxiv.org/pdf/2402.13534v2.pdf","comment":"10 pages, 9 tables, 3 figures, Accepted by ACL 2025 (short paper)"},{"id":"http://arxiv.org/abs/2506.15266v1","updated":"2025-06-18T08:41:28Z","published":"2025-06-18T08:41:28Z","title":"Thunder-DeID: Accurate and Efficient De-identification Framework for\n  Korean Court Judgments","summary":"  To ensure a balance between open access to justice and personal data\nprotection, the South Korean judiciary mandates the de-identification of court\njudgments before they can be publicly disclosed. However, the current\nde-identification process is inadequate for handling court judgments at scale\nwhile adhering to strict legal requirements. Additionally, the legal\ndefinitions and categorizations of personal identifiers are vague and not\nwell-suited for technical solutions. To tackle these challenges, we propose a\nde-identification framework called Thunder-DeID, which aligns with relevant\nlaws and practices. Specifically, we (i) construct and release the first Korean\nlegal dataset containing annotated judgments along with corresponding lists of\nentity mentions, (ii) introduce a systematic categorization of Personally\nIdentifiable Information (PII), and (iii) develop an end-to-end deep neural\nnetwork (DNN)-based de-identification pipeline. Our experimental results\ndemonstrate that our model achieves state-of-the-art performance in the\nde-identification of court judgments.\n","authors":["Sungen Hahm","Heejin Kim","Gyuseong Lee","Hyunji Park","Jaejin Lee"],"pdf_url":"https://arxiv.org/pdf/2506.15266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13612v4","updated":"2025-06-18T08:37:18Z","published":"2024-12-18T08:42:25Z","title":"Large Language Models for Automated Literature Review: An Evaluation of\n  Reference Generation, Abstract Writing, and Review Composition","summary":"  Large language models (LLMs) have emerged as a potential solution to automate\nthe complex processes involved in writing literature reviews, such as\nliterature collection, organization, and summarization. However, it is yet\nunclear how good LLMs are at automating comprehensive and reliable literature\nreviews. This study introduces a framework to automatically evaluate the\nperformance of LLMs in three key tasks of literature writing: reference\ngeneration, literature summary, and literature review composition. We introduce\nmultidimensional evaluation metrics that assess the hallucination rates in\ngenerated references and measure the semantic coverage and factual consistency\nof the literature summaries and compositions against human-written\ncounterparts. The experimental results reveal that even the most advanced\nmodels still generate hallucinated references, despite recent progress.\nMoreover, we observe that the performance of different models varies across\ndisciplines when it comes to writing literature reviews. These findings\nhighlight the need for further research and development to improve the\nreliability of LLMs in automating academic literature reviews.\n","authors":["Xuemei Tang","Xufeng Duan","Zhenguang G. Cai"],"pdf_url":"https://arxiv.org/pdf/2412.13612v4.pdf","comment":"12 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2506.15246v1","updated":"2025-06-18T08:24:27Z","published":"2025-06-18T08:24:27Z","title":"TopClustRAG at SIGIR 2025 LiveRAG Challenge","summary":"  We present TopClustRAG, a retrieval-augmented generation (RAG) system\ndeveloped for the LiveRAG Challenge, which evaluates end-to-end question\nanswering over large-scale web corpora. Our system employs a hybrid retrieval\nstrategy combining sparse and dense indices, followed by K-Means clustering to\ngroup semantically similar passages. Representative passages from each cluster\nare used to construct cluster-specific prompts for a large language model\n(LLM), generating intermediate answers that are filtered, reranked, and finally\nsynthesized into a single, comprehensive response. This multi-stage pipeline\nenhances answer diversity, relevance, and faithfulness to retrieved evidence.\nEvaluated on the FineWeb Sample-10BT dataset, TopClustRAG ranked 2nd in\nfaithfulness and 7th in correctness on the official leaderboard, demonstrating\nthe effectiveness of clustering-based context filtering and prompt aggregation\nin large-scale RAG systems.\n","authors":["Juli Bakagianni","John Pavlopoulos","Aristidis Likas"],"pdf_url":"https://arxiv.org/pdf/2506.15246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18043v2","updated":"2025-06-18T08:22:47Z","published":"2024-12-23T23:39:05Z","title":"Aligning AI Research with the Needs of Clinical Coding Workflows: Eight\n  Recommendations Based on US Data Analysis and Critical Review","summary":"  Clinical coding is crucial for healthcare billing and data analysis. Manual\nclinical coding is labour-intensive and error-prone, which has motivated\nresearch towards full automation of the process. However, our analysis, based\non US English electronic health records and automated coding research using\nthese records, shows that widely used evaluation methods are not aligned with\nreal clinical contexts. For example, evaluations that focus on the top 50 most\ncommon codes are an oversimplification, as there are thousands of codes used in\npractice. This position paper aims to align AI coding research more closely\nwith practical challenges of clinical coding. Based on our analysis, we offer\neight specific recommendations, suggesting ways to improve current evaluation\nmethods. Additionally, we propose new AI-based methods beyond automated coding,\nsuggesting alternative approaches to assist clinical coders in their workflows.\n","authors":["Yidong Gan","Maciej Rybinski","Ben Hachey","Jonathan K. Kummerfeld"],"pdf_url":"https://arxiv.org/pdf/2412.18043v2.pdf","comment":"Accepted to the ACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2506.15241v1","updated":"2025-06-18T08:20:29Z","published":"2025-06-18T08:20:29Z","title":"Research on Graph-Retrieval Augmented Generation Based on Historical\n  Text Knowledge Graphs","summary":"  This article addresses domain knowledge gaps in general large language models\nfor historical text analysis in the context of computational humanities and\nAIGC technology. We propose the Graph RAG framework, combining chain-of-thought\nprompting, self-instruction generation, and process supervision to create a The\nFirst Four Histories character relationship dataset with minimal manual\nannotation. This dataset supports automated historical knowledge extraction,\nreducing labor costs. In the graph-augmented generation phase, we introduce a\ncollaborative mechanism between knowledge graphs and retrieval-augmented\ngeneration, improving the alignment of general models with historical\nknowledge. Experiments show that the domain-specific model Xunzi-Qwen1.5-14B,\nwith Simplified Chinese input and chain-of-thought prompting, achieves optimal\nperformance in relation extraction (F1 = 0.68). The DeepSeek model integrated\nwith GraphRAG improves F1 by 11% (0.08-0.19) on the open-domain C-CLUE relation\nextraction dataset, surpassing the F1 value of Xunzi-Qwen1.5-14B (0.12),\neffectively alleviating hallucinations phenomenon, and improving\ninterpretability. This framework offers a low-resource solution for classical\ntext knowledge extraction, advancing historical knowledge services and\nhumanities research.\n","authors":["Yang Fan","Zhang Qi","Xing Wenqian","Liu Chang","Liu Liu"],"pdf_url":"https://arxiv.org/pdf/2506.15241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15239v1","updated":"2025-06-18T08:20:19Z","published":"2025-06-18T08:20:19Z","title":"Lost in Variation? Evaluating NLI Performance in Basque and Spanish\n  Geographical Variants","summary":"  In this paper, we evaluate the capacity of current language technologies to\nunderstand Basque and Spanish language varieties. We use Natural Language\nInference (NLI) as a pivot task and introduce a novel, manually-curated\nparallel dataset in Basque and Spanish, along with their respective variants.\nOur empirical analysis of crosslingual and in-context learning experiments\nusing encoder-only and decoder-based Large Language Models (LLMs) shows a\nperformance drop when handling linguistic variation, especially in Basque.\nError analysis suggests that this decline is not due to lexical overlap, but\nrather to the linguistic variation itself. Further ablation experiments\nindicate that encoder-only models particularly struggle with Western Basque,\nwhich aligns with linguistic theory that identifies peripheral dialects (e.g.,\nWestern) as more distant from the standard. All data and code are publicly\navailable.\n","authors":["Jaione Bengoetxea","Itziar Gonzalez-Dios","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2506.15239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13180v2","updated":"2025-06-18T08:02:29Z","published":"2025-06-16T07:47:34Z","title":"Dynamic Acoustic Model Architecture Optimization in Training for ASR","summary":"  Architecture design is inherently complex. Existing approaches rely on either\nhandcrafted rules, which demand extensive empirical expertise, or automated\nmethods like neural architecture search, which are computationally intensive.\nIn this paper, we introduce DMAO, an architecture optimization framework that\nemploys a grow-and-drop strategy to automatically reallocate parameters during\ntraining. This reallocation shifts resources from less-utilized areas to those\nparts of the model where they are most beneficial. Notably, DMAO only\nintroduces negligible training overhead at a given model complexity. We\nevaluate DMAO through experiments with CTC on LibriSpeech, TED-LIUM-v2 and\nSwitchboard datasets. The results show that, using the same amount of training\nresources, our proposed DMAO consistently improves WER by up to 6% relatively\nacross various architectures, model sizes, and datasets. Furthermore, we\nanalyze the pattern of parameter redistribution and uncover insightful\nfindings.\n","authors":["Jingjing Xu","Zijian Yang","Albert Zeyer","Eugen Beck","Ralf Schlueter","Hermann Ney"],"pdf_url":"https://arxiv.org/pdf/2506.13180v2.pdf","comment":"Accepted by Interspeech 2025"},{"id":"http://arxiv.org/abs/2407.11770v2","updated":"2025-06-18T08:01:35Z","published":"2024-07-16T14:28:56Z","title":"Robust Utility-Preserving Text Anonymization Based on Large Language\n  Models","summary":"  Anonymizing text that contains sensitive information is crucial for a wide\nrange of applications. Existing techniques face the emerging challenges of the\nre-identification ability of large language models (LLMs), which have shown\nadvanced capability in memorizing detailed information and reasoning over\ndispersed pieces of patterns to draw conclusions. When defending against\nLLM-based re-identification, anonymization could jeopardize the utility of the\nresulting anonymized data in downstream tasks. In general, the interaction\nbetween anonymization and data utility requires a deeper understanding within\nthe context of LLMs. In this paper, we propose a framework composed of three\nkey LLM-based components: a privacy evaluator, a utility evaluator, and an\noptimization component, which work collaboratively to perform anonymization.\nExtensive experiments demonstrate that the proposed model outperforms existing\nbaselines, showing robustness in reducing the risk of re-identification while\npreserving greater data utility in downstream tasks. We provide detailed\nstudies on these core modules. To consider large-scale and real-time\napplications, we investigate the distillation of the anonymization capabilities\ninto lightweight models. All of our code and datasets will be made publicly\navailable at https://github.com/UKPLab/acl2025-rupta.\n","authors":["Tianyu Yang","Xiaodan Zhu","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2407.11770v2.pdf","comment":"Accepted by ACL'2025 Main Conference"},{"id":"http://arxiv.org/abs/2506.15220v1","updated":"2025-06-18T07:58:41Z","published":"2025-06-18T07:58:41Z","title":"video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models","summary":"  Videos contain a wealth of information, and generating detailed and accurate\ndescriptions in natural language is a key aspect of video understanding. In\nthis paper, we present video-SALMONN 2, an advanced audio-visual large language\nmodel (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with\npaired audio) captioning through directed preference optimisation (DPO). We\npropose new metrics to evaluate the completeness and accuracy of video\ndescriptions, which are optimised using DPO. To further improve training, we\npropose a novel multi-round DPO (MrDPO) approach, which involves periodically\nupdating the DPO reference model, merging and re-initialising the LoRA module\nas a proxy for parameter updates after each training round (1,000 steps), and\nincorporating guidance from ground-truth video captions to stabilise the\nprocess. Experimental results show that MrDPO significantly enhances\nvideo-SALMONN 2's captioning accuracy, reducing the captioning error rates by\n28\\%. The final video-SALMONN 2 model, with just 7 billion parameters,\nsurpasses leading models such as GPT-4o and Gemini-1.5-Pro in video captioning\ntasks, while maintaining highly competitive performance to the state-of-the-art\non widely used video question-answering benchmarks among models of similar\nsize. Codes are available at\n\\href{https://github.com/bytedance/video-SALMONN-2}{https://github.com/bytedance/video-SALMONN-2}.\n","authors":["Changli Tang","Yixuan Li","Yudong Yang","Jimin Zhuang","Guangzhi Sun","Wei Li","Zejun Ma","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.15220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07890v4","updated":"2025-06-18T07:55:43Z","published":"2025-05-11T14:30:56Z","title":"TSLFormer: A Lightweight Transformer Model for Turkish Sign Language\n  Recognition Using Skeletal Landmarks","summary":"  This study presents TSLFormer, a light and robust word-level Turkish Sign\nLanguage (TSL) recognition model that treats sign gestures as ordered,\nstring-like language. Instead of using raw RGB or depth videos, our method only\nworks with 3D joint positions - articulation points - extracted using Google's\nMediapipe library, which focuses on the hand and torso skeletal locations. This\ncreates efficient input dimensionality reduction while preserving important\nsemantic gesture information. Our approach revisits sign language recognition\nas sequence-to-sequence translation, inspired by the linguistic nature of sign\nlanguages and the success of transformers in natural language processing. Since\nTSLFormer uses the self-attention mechanism, it effectively captures temporal\nco-occurrence within gesture sequences and highlights meaningful motion\npatterns as words unfold. Evaluated on the AUTSL dataset with over 36,000\nsamples and 227 different words, TSLFormer achieves competitive performance\nwith minimal computational cost. These results show that joint-based input is\nsufficient for enabling real-time, mobile, and assistive communication systems\nfor hearing-impaired individuals.\n","authors":["Kutay Ertürk","Furkan Altınışık","İrem Sarıaltın","Ömer Nezih Gerek"],"pdf_url":"https://arxiv.org/pdf/2505.07890v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15215v1","updated":"2025-06-18T07:49:13Z","published":"2025-06-18T07:49:13Z","title":"MinosEval: Distinguishing Factoid and Non-Factoid for Tailored\n  Open-Ended QA Evaluation with LLMs","summary":"  Open-ended question answering (QA) is a key task for evaluating the\ncapabilities of large language models (LLMs). Compared to closed-ended QA, it\ndemands longer answer statements, more nuanced reasoning processes, and diverse\nexpressions, making refined and interpretable automatic evaluation both crucial\nand challenging. Traditional metrics like ROUGE and BERTScore struggle to\ncapture semantic similarities due to different patterns between model responses\nand reference answers. Current LLM-based evaluation approaches, such as\npairwise or listwise comparisons of candidate answers, lack intuitive\ninterpretability. While pointwise scoring of each response provides some\ndescriptions, it fails to adapt across different question contents. Most\nnotably, existing methods overlook the distinction between factoid and\nnon-factoid questions. To address these challenges, we propose\n\\textbf{MinosEval}, a novel evaluation method that first distinguishes\nopen-ended questions and then ranks candidate answers using different\nevaluation strategies. For factoid questions, it applies an adaptive key-point\nscoring strategy, while for non-factoid questions, it uses an instance-aware\nlistwise ranking strategy. Experiments on multiple open-ended QA datasets,\nincluding self-built ones with more candidate responses to complement community\nresources, show that MinosEval better aligns with human annotations and offers\nmore interpretable results.\n","authors":["Yongqi Fan","Yating Wang","Guandong Wang","Jie Zhai","Jingping Liu","Qi Ye","Tong Ruan"],"pdf_url":"https://arxiv.org/pdf/2506.15215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15211v1","updated":"2025-06-18T07:44:09Z","published":"2025-06-18T07:44:09Z","title":"ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning\n  in LLMs","summary":"  Recent advances in Large Reasoning Models (LRMs) trained with Long\nChain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domain\ngeneralization capabilities. However, the underlying mechanisms supporting such\ntransfer remain poorly understood. We hypothesize that cross-domain\ngeneralization arises from shared abstract reasoning prototypes -- fundamental\nreasoning patterns that capture the essence of problems across domains. These\nprototypes minimize the nuances of the representation, revealing that seemingly\ndiverse tasks are grounded in shared reasoning structures.Based on this\nhypothesis, we propose ProtoReasoning, a framework that enhances the reasoning\nability of LLMs by leveraging scalable and verifiable prototypical\nrepresentations (Prolog for logical reasoning, PDDL for\nplanning).ProtoReasoning features: (1) an automated prototype construction\npipeline that transforms problems into corresponding prototype representations;\n(2) a comprehensive verification system providing reliable feedback through\nProlog/PDDL interpreters; (3) the scalability to synthesize problems\narbitrarily within prototype space while ensuring correctness. Extensive\nexperiments show that ProtoReasoning achieves 4.7% improvement over baseline\nmodels on logical reasoning (Enigmata-Eval), 6.3% improvement on planning\ntasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics\n(AIME24). Significantly, our ablation studies confirm that learning in\nprototype space also demonstrates enhanced generalization to structurally\nsimilar problems compared to training solely on natural language\nrepresentations, validating our hypothesis that reasoning prototypes serve as\nthe foundation for generalizable reasoning in large language models.\n","authors":["Feng He","Zijun Chen","Xinnian Liang","Tingting Ma","Yunqi Qiu","Shuangzhi Wu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2506.15211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15208v1","updated":"2025-06-18T07:42:32Z","published":"2025-06-18T07:42:32Z","title":"A Comparative Study of Task Adaptation Techniques of Large Language\n  Models for Identifying Sustainable Development Goals","summary":"  In 2012, the United Nations introduced 17 Sustainable Development Goals\n(SDGs) aimed at creating a more sustainable and improved future by 2030.\nHowever, tracking progress toward these goals is difficult because of the\nextensive scale and complexity of the data involved. Text classification models\nhave become vital tools in this area, automating the analysis of vast amounts\nof text from a variety of sources. Additionally, large language models (LLMs)\nhave recently proven indispensable for many natural language processing tasks,\nincluding text classification, thanks to their ability to recognize complex\nlinguistic patterns and semantics. This study analyzes various proprietary and\nopen-source LLMs for a single-label, multi-class text classification task\nfocused on the SDGs. Then, it also evaluates the effectiveness of task\nadaptation techniques (i.e., in-context learning approaches), namely Zero-Shot\nand Few-Shot Learning, as well as Fine-Tuning within this domain. The results\nreveal that smaller models, when optimized through prompt engineering, can\nperform on par with larger models like OpenAI's GPT (Generative Pre-trained\nTransformer).\n","authors":["Andrea Cadeddu","Alessandro Chessa","Vincenzo De Leo","Gianni Fenu","Enrico Motta","Francesco Osborne","Diego Reforgiato Recupero","Angelo Salatino","Luca Secchi"],"pdf_url":"https://arxiv.org/pdf/2506.15208v1.pdf","comment":"Submitted to IEEE Access"},{"id":"http://arxiv.org/abs/2506.06955v2","updated":"2025-06-18T07:39:43Z","published":"2025-06-08T00:38:18Z","title":"BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for\n  Belief-Inconsistent Syllogistic Reasoning","summary":"  We present BIS Reasoning 1.0, the first large-scale Japanese dataset of\nsyllogistic reasoning problems explicitly designed to evaluate\nbelief-inconsistent reasoning in large language models (LLMs). Unlike prior\ndatasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned\nreasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent\nsyllogisms to uncover reasoning biases in LLMs trained on human-aligned\ncorpora. We benchmark state-of-the-art models - including GPT models, Claude\nmodels, and leading Japanese LLMs - revealing significant variance in\nperformance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies\ncritical weaknesses in current LLMs when handling logically valid but\nbelief-conflicting inputs. These findings have important implications for\ndeploying LLMs in high-stakes domains such as law, healthcare, and scientific\nliterature, where truth must override intuitive belief to ensure integrity and\nsafety.\n","authors":["Ha-Thanh Nguyen","Chaoran Liu","Koichi Takeda","Yusuke Miyao","Pontus Stenetorp","Qianying Liu","Su Myat Noe","Hideyuki Tachibana","Sadao Kurohashi"],"pdf_url":"https://arxiv.org/pdf/2506.06955v2.pdf","comment":"This version includes an updated literature review, added\n  acknowledgements, and a revised author list"},{"id":"http://arxiv.org/abs/2407.09861v4","updated":"2025-06-18T07:16:37Z","published":"2024-07-13T12:01:52Z","title":"A Systematic Survey of Natural Language Processing for the Greek\n  Language","summary":"  Comprehensive monolingual Natural Language Processing (NLP) surveys are\nessential for assessing language-specific challenges, resource availability,\nand research gaps. However, existing surveys often lack standardized\nmethodologies, leading to selection bias and fragmented coverage of NLP tasks\nand resources. This study introduces a generalizable framework for systematic\nmonolingual NLP surveys. Our approach integrates a structured search protocol\nto minimize bias, an NLP task taxonomy for classification, and language\nresource taxonomies to identify potential benchmarks and highlight\nopportunities for improving resource availability. We apply this framework to\nGreek NLP (2012-2023), providing an in-depth analysis of its current state,\ntask-specific progress, and resource gaps. The survey results are publicly\navailable (https://doi.org/10.5281/zenodo.15314882) and are regularly updated\nto provide an evergreen resource. This systematic survey of Greek NLP serves as\na case study, demonstrating the effectiveness of our framework and its\npotential for broader application to other not so well-resourced languages as\nregards NLP.\n","authors":["Juli Bakagianni","Kanella Pouli","Maria Gavriilidou","John Pavlopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.09861v4.pdf","comment":"This version matches the paper published in Patterns (Cell Press).\n  The title has been updated to reflect the published version"},{"id":"http://arxiv.org/abs/2506.13300v3","updated":"2025-06-18T06:57:58Z","published":"2025-06-16T09:42:05Z","title":"Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning\n  Language Models","summary":"  This paper presents Seewo's systems for both tracks of the Multilingual\nConversational Speech Language Model Challenge (MLC-SLM), addressing automatic\nspeech recognition (ASR) and speaker diarization with ASR (SD-ASR). We\nintroduce a multi-stage training pipeline that explicitly enhances reasoning\nand self-correction in speech language models for ASR. Our approach combines\ncurriculum learning for progressive capability acquisition, Chain-of-Thought\ndata augmentation to foster intermediate reflection, and Reinforcement Learning\nwith Verifiable Rewards (RLVR) to further refine self-correction through\nreward-driven optimization. This approach achieves substantial improvements\nover the official challenge baselines. On the evaluation set, our best system\nattains a WER/CER of 11.57% for Track 1 and a tcpWER/tcpCER of 17.67% for Track\n2. Comprehensive ablation studies demonstrate the effectiveness of each\ncomponent under challenge constraints.\n","authors":["Bo Li","Chengben Xu","Wufeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.13300v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11171v5","updated":"2025-06-18T06:29:57Z","published":"2024-11-17T20:44:34Z","title":"LLäMmlein: Transparent, Compact and Competitive German-Only Language\n  Models from Scratch","summary":"  We create two German-only decoder models, LL\\\"aMmlein 120M and 1B,\ntransparently from scratch and publish them, along with the training data, for\nthe German NLP research community to use. The model training involved several\nkey steps, including extensive data preprocessing, the creation of a custom\nGerman tokenizer, the training itself, as well as the evaluation of the final\nmodels on various benchmarks. Throughout the training process, multiple\ncheckpoints were saved and analyzed using the SuperGLEBer benchmark to monitor\nthe models' learning dynamics. Compared to state-of-the-art models on the\nSuperGLEBer benchmark, both LL\\\"aMmlein models performed competitively,\nconsistently matching or surpassing models with similar parameter sizes. The\nresults show that the models' quality scales with size as expected, but\nperformance improvements on some tasks plateaued early, offering valuable\ninsights into resource allocation for future model development.\n","authors":["Jan Pfister","Julia Wunderle","Andreas Hotho"],"pdf_url":"https://arxiv.org/pdf/2411.11171v5.pdf","comment":"camera ready @ACL25;\n  https://www.informatik.uni-wuerzburg.de/datascience/projects/nlp/llammlein/"},{"id":"http://arxiv.org/abs/2506.13366v3","updated":"2025-06-18T06:17:35Z","published":"2025-06-16T11:15:21Z","title":"Enhancing Goal-oriented Proactive Dialogue Systems via Consistency\n  Reflection and Correction","summary":"  Goal-oriented proactive dialogue systems are designed to guide user\nconversations seamlessly towards specific objectives by planning a\ngoal-oriented path. However, previous research has focused predominantly on\noptimizing these paths while neglecting the inconsistencies that may arise\nbetween generated responses and dialogue contexts, including user profiles,\ndialogue history, domain knowledge, and subgoals. To address this issue, we\nintroduce a model-agnostic two-stage Consistency Reflection and Correction\n(CRC) framework. Specifically, in the consistency reflection stage, the model\nis prompted to reflect on the discrepancies between generated responses and\ndialogue contexts, identifying inconsistencies and suggesting possible\ncorrections. In the consistency correction stage, the model generates responses\nthat are more consistent with the dialogue context based on these reflection\nresults. We conducted experiments on various model architectures with different\nparameter sizes, including encoder-decoder models (BART, T5) and decoder-only\nmodels (GPT-2, DialoGPT, Phi3, Mistral and LLaMA3), and the experimental\nresults on three datasets demonstrate that our CRC framework significantly\nimproves the consistency between generated responses and dialogue contexts.\n","authors":["Didi Zhang","Yaxin Fan","Peifeng Li","Qiaoming Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.13366v3.pdf","comment":"Accepted by ACL'25 (main conference)"},{"id":"http://arxiv.org/abs/2505.18440v2","updated":"2025-06-18T06:11:08Z","published":"2025-05-24T00:22:52Z","title":"Efficient Long CoT Reasoning in Small Language Models","summary":"  Recent large reasoning models such as DeepSeek-R1 exhibit strong complex\nproblems solving abilities by generating long chain-of-thought (CoT) reasoning\nsteps. It is challenging to directly train small language models (SLMs) to\nemerge long CoT. Thus, distillation becomes a practical method to enable SLMs\nfor such reasoning ability. However, the long CoT often contains a lot of\nredundant contents (e.g., overthinking steps) which may make SLMs hard to learn\nconsidering their relatively poor capacity and generalization. To address this\nissue, we propose a simple-yet-effective method to prune unnecessary steps in\nlong CoT, and then employ an on-policy method for the SLM itself to curate\nvalid and useful long CoT training data. In this way, SLMs can effectively\nlearn efficient long CoT reasoning and preserve competitive performance at the\nsame time. Experimental results across a series of mathematical reasoning\nbenchmarks demonstrate the effectiveness of the proposed method in distilling\nlong CoT reasoning ability into SLMs which maintains the competitive\nperformance but significantly reduces generating redundant reasoning steps.\n","authors":["Zhaoyang Wang","Jinqi Jiang","Tian Qiu","Hui Liu","Xianfeng Tang","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2505.18440v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15156v1","updated":"2025-06-18T06:02:02Z","published":"2025-06-18T06:02:02Z","title":"Emergence of Primacy and Recency Effect in Mamba: A Mechanistic Point of\n  View","summary":"  We study memory in state-space language models using primacy and recency\neffects as behavioral tools to uncover how information is retained and\nforgotten over time. Applying structured recall tasks to the Mamba\narchitecture, we observe a consistent U-shaped accuracy profile, indicating\nstrong performance at the beginning and end of input sequences. We identify\nthree mechanisms that give rise to this pattern. First, long-term memory is\nsupported by a sparse subset of channels within the model's selective state\nspace block, which persistently encode early input tokens and are causally\nlinked to primacy effects. Second, short-term memory is governed by\ndelta-modulated recurrence: recent inputs receive more weight due to\nexponential decay, but this recency advantage collapses when distractor items\nare introduced, revealing a clear limit to memory depth. Third, we find that\nmemory allocation is dynamically modulated by semantic regularity: repeated\nrelations in the input sequence shift the delta gating behavior, increasing the\ntendency to forget intermediate items. We validate these findings via targeted\nablations and input perturbations on two large-scale Mamba-based language\nmodels: one with 1.4B and another with 7B parameters.\n","authors":["Muhammad Cendekia Airlangga","Hilal AlQuabeh","Munachiso S Nwadike","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2506.15156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18799v4","updated":"2025-06-18T05:56:01Z","published":"2025-05-24T17:19:34Z","title":"ALPS: Attention Localization and Pruning Strategy for Efficient\n  Alignment of Large Language Models","summary":"  Aligning general-purpose large language models (LLMs) to downstream tasks\noften incurs significant training adjustment costs. Prior research has explored\nvarious avenues to enhance alignment efficiency, primarily through minimal-data\ntraining or data-driven activations to identify key attention heads. However,\nthese approaches inherently introduce data dependency, which hinders\ngeneralization and reusability. To address this issue and enhance model\nalignment efficiency, we propose the Attention Localization and Pruning\nStrategy (ALPS), an efficient algorithm that localizes the most task-sensitive\nattention heads and prunes by restricting attention training updates to these\nheads, thereby reducing alignment costs. Experimental results demonstrate that\nour method activates only 10% of attention parameters during fine-tuning while\nachieving a 2% performance improvement over baselines on three tasks. Moreover,\nthe identified task-specific heads are transferable across datasets and\nmitigate knowledge forgetting. Our work and findings provide a novel\nperspective on efficient LLM alignment. The code is available at\nhttps://github.com/VoiceBeer/ALPS.\n","authors":["Hao Chen","Haoze Li","Zhiqing Xiao","Lirong Gao","Qi Zhang","Xiaomeng Hu","Ningtao Wang","Xing Fu","Junbo Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.18799v4.pdf","comment":"Accepted@ACL25-findings, 17 pages, 8 figures, 14 tables"},{"id":"http://arxiv.org/abs/2506.15154v1","updated":"2025-06-18T05:51:36Z","published":"2025-06-18T05:51:36Z","title":"SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning","summary":"  Detailed captions that accurately reflect the characteristics of a music\npiece can enrich music databases and drive forward research in music AI. This\npaper introduces a multi-task music captioning model, SonicVerse, that\nintegrates caption generation with auxiliary music feature detection tasks such\nas key detection, vocals detection, and more, so as to directly capture both\nlow-level acoustic details as well as high-level musical attributes. The key\ncontribution is a projection-based architecture that transforms audio input\ninto language tokens, while simultaneously detecting music features through\ndedicated auxiliary heads. The outputs of these heads are also projected into\nlanguage tokens, to enhance the captioning input. This framework not only\nproduces rich, descriptive captions for short music fragments but also directly\nenables the generation of detailed time-informed descriptions for longer music\npieces, by chaining the outputs using a large-language model. To train the\nmodel, we extended the MusicBench dataset by annotating it with music features\nusing MIRFLEX, a modular music feature extractor, resulting in paired audio,\ncaptions and music feature data. Experimental results show that incorporating\nfeatures in this way improves the quality and detail of the generated captions.\n","authors":["Anuradha Chopra","Abhinaba Roy","Dorien Herremans"],"pdf_url":"https://arxiv.org/pdf/2506.15154v1.pdf","comment":"14 pages, 2 figures, Accepted to AIMC 2025"},{"id":"http://arxiv.org/abs/2506.09507v3","updated":"2025-06-18T05:38:44Z","published":"2025-06-11T08:26:51Z","title":"TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary\n  Position Embedding","summary":"  Transformers exhibit proficiency in capturing long-range dependencies,\nwhereas State Space Models (SSMs) facilitate linear-time sequence modeling.\nNotwithstanding their synergistic potential, the integration of these\narchitectures presents a significant challenge, primarily attributable to a\nfundamental incongr inuity their respective positional encoding mechanisms:\nTransformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs\nleverage implicit positional representations via convolutions. This divergence\noften precipitates discontinuities and suboptimal performance.To address this\nimpediment, we propose a unified rotary position embedding (Unified RoPE)\nmethodology, thereby establishing a consistent positional encoding framework\nfor both self-attention and state-space components. Using this Unified RoPE, we\nintroduce TransXSSM, a hybrid architecture that coherently integrates the\nTransformer and SSM layers under this unified positional encoding scheme. At a\n4 sequenceK length, TransXSSM exhibits training and inference speeds that are\n42.3% and 29.5% faster, respectively, relative to standard Transformer models.\nIt also delivers higher accuracy: under comparable settings, it surpasses a\nTransformer baseline by over 4% on language modeling benchmarks.TransXSSM\nfurthermore scales more effectively: TransXSSM-1.3B gains 7.22% in average\naccuracy over its 320M version (versus about 6% gains for equivalent\nTransformers or SSMs). Our results show that unified positional encoding\nresolves positional incompatibility in hybrid models, enabling efficient,\nhigh-performance long-context modeling.\n","authors":["Bingheng Wu","Jingze Shi","Yifan Wu","Nan Tang","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2506.09507v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06619v2","updated":"2025-06-18T04:49:11Z","published":"2025-06-07T01:33:44Z","title":"BriefMe: A Legal NLP Benchmark for Assisting with Legal Briefs","summary":"  A core part of legal work that has been under-explored in Legal NLP is the\nwriting and editing of legal briefs. This requires not only a thorough\nunderstanding of the law of a jurisdiction, from judgments to statutes, but\nalso the ability to make new arguments to try to expand the law in a new\ndirection and make novel and creative arguments that are persuasive to judges.\nTo capture and evaluate these legal skills in language models, we introduce\nBRIEFME, a new dataset focused on legal briefs. It contains three tasks for\nlanguage models to assist legal professionals in writing briefs: argument\nsummarization, argument completion, and case retrieval. In this work, we\ndescribe the creation of these tasks, analyze them, and show how current models\nperform. We see that today's large language models (LLMs) are already quite\ngood at the summarization and guided completion tasks, even beating\nhuman-generated headings. Yet, they perform poorly on other tasks in our\nbenchmark: realistic argument completion and retrieving relevant legal cases.\nWe hope this dataset encourages more development in Legal NLP in ways that will\nspecifically aid people in performing legal work.\n","authors":["Jesse Woo","Fateme Hashemi Chaleshtori","Ana Marasović","Kenneth Marino"],"pdf_url":"https://arxiv.org/pdf/2506.06619v2.pdf","comment":"ACL Findings 2025; 10 pages main, 5 pages references, 37 pages\n  appendix"},{"id":"http://arxiv.org/abs/2506.15138v1","updated":"2025-06-18T04:40:44Z","published":"2025-06-18T04:40:44Z","title":"Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for\n  Generative Language Models","summary":"  This paper introduces Thunder-Tok, a new Korean tokenizer designed to reduce\ntoken fertility without compromising model performance. Our approach uses a\nrule-based pre-tokenization method that aligns with the linguistic structure of\nthe Korean language. We also create a seed vocabulary containing tokens that\nresemble linguistic units and employ a branching entropy-based selection\nalgorithm. These techniques increase the average token length, thus lowering\nfertility while preserving linguistic information. Experimental results\nindicate that Thunder-Tok reduces fertility by approximately 10% (i.e., reduces\nthe number of tokens by 10%, improving the inference speed by 10%) compared to\nBPE without compromising performance across various downstream tasks. These\nfindings demonstrate that our linguistically informed approach is effective and\npractical for designing efficient tokenizers for language models.\n","authors":["Gyeongje Cho","Yeonkyoun So","Chanwoo Park","Sangmin Lee","Sungmok Jung","Jaejin Lee"],"pdf_url":"https://arxiv.org/pdf/2506.15138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14175v2","updated":"2025-06-18T04:31:51Z","published":"2025-06-17T04:34:27Z","title":"GRAM: A Generative Foundation Reward Model for Reward Generalization","summary":"  In aligning large language models (LLMs), reward models have played an\nimportant role, but are standardly trained as discriminative models and rely\nonly on labeled human preference data. In this paper, we explore methods that\ntrain reward models using both unlabeled and labeled data. Building on the\ngenerative models in LLMs, we develop a generative reward model that is first\ntrained via large-scale unsupervised learning and then fine-tuned via\nsupervised learning. We also show that by using label smoothing, we are in fact\noptimizing a regularized pairwise ranking loss. This result, in turn, provides\na new view of training reward models, which links generative models and\ndiscriminative models under the same class of training objectives. The outcome\nof these techniques is a foundation reward model, which can be applied to a\nwide range of tasks with little or no further fine-tuning effort. Extensive\nexperiments show that this model generalizes well across several tasks,\nincluding response ranking, reinforcement learning from human feedback, and\ntask adaptation with fine-tuning, achieving significant performance\nimprovements over several strong baseline models.\n","authors":["Chenglong Wang","Yang Gan","Yifu Huo","Yongyu Mu","Qiaozhi He","Murun Yang","Bei Li","Tong Xiao","Chunliang Zhang","Tongran Liu","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.14175v2.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2506.15131v1","updated":"2025-06-18T04:19:33Z","published":"2025-06-18T04:19:33Z","title":"Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs","summary":"  Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby\nmultiple appropriate responses exist for a single dialogue context. Despite\nprior research showing that modeling this property boosts response diversity,\nmost modern LLM-based dialogue agents do not explicitly do so. In this work, we\nmodel the o2m property of OD in LLMs by decomposing OD generation into two key\ntasks: Multi-Response Generation (MRG) and Preference-based Selection (PS),\nwhich entail generating a set of n semantically and lexically diverse\nhigh-quality responses for a given dialogue context, followed by selecting a\nsingle response based on human preference, respectively. To facilitate MRG and\nPS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the\no2m property by featuring multiple plausible responses for each context.\nLeveraging o2mDial, we propose new in-context learning and instruction-tuning\nstrategies, as well as novel evaluation metrics for MRG, alongside a\nmodel-based approach for PS. Empirical results demonstrate that applying the\nproposed two-stage framework to smaller LLMs for OD generation enhances overall\nresponse diversity while maintaining contextual coherence, improving response\nquality by up to 90%, bringing them closer to the performance of larger models.\n","authors":["Jing Yang Lee","Kong-Aik Lee","Woon-Seng Gan"],"pdf_url":"https://arxiv.org/pdf/2506.15131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03092v2","updated":"2025-06-18T04:06:03Z","published":"2024-12-04T07:44:35Z","title":"REVOLVE: Optimizing AI Systems by Tracking Response Evolution in Textual\n  Optimization","summary":"  Recent advancements in large language models (LLMs) have significantly\nenhanced the ability of LLM-based systems to perform complex tasks through\nnatural language processing and tool interaction. However, optimizing these\nLLM-based systems for specific tasks remains challenging, often requiring\nmanual interventions like prompt engineering and hyperparameter tuning.\nExisting automatic optimization methods, such as textual feedback-based\ntechniques (e.g., TextGrad), tend to focus on immediate feedback, analogous to\nusing immediate derivatives in traditional numerical gradient descent. However,\nrelying solely on such feedback can be limited when the adjustments made in\nresponse to this feedback are either too small or fluctuate irregularly,\npotentially slowing down or even stalling the optimization process. To overcome\nthese challenges, more adaptive methods are needed, especially in situations\nwhere the system's response is evolving slowly or unpredictably. In this paper,\nwe introduce REVOLVE, an optimization method that tracks how \"R\"esponses\n\"EVOLVE\" across iterations in LLM systems. By focusing on the evolution of\nresponses over time, REVOLVE enables more stable and effective optimization by\nmaking thoughtful, progressive adjustments at each step. Experimental results\ndemonstrate that REVOLVE outperforms competitive baselines, achieving a 7.8%\nimprovement in prompt optimization, a 20.72% gain in solution refinement, and a\n29.17% increase in code optimization. Additionally, REVOLVE converges in fewer\niterations, resulting in significant computational savings. Beyond its\npractical contributions, REVOLVE highlights a promising direction, where the\nrich knowledge from established optimization principles can be leveraged to\nenhance LLM systems, which paves the way for further advancements in this\nhybrid domain.\n","authors":["Peiyan Zhang","Haibo Jin","Leyang Hu","Xinnuo Li","Liying Kang","Man Luo","Yangqiu Song","Haohan Wang"],"pdf_url":"https://arxiv.org/pdf/2412.03092v2.pdf","comment":"20 pages, 2 figures, accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2502.19941v3","updated":"2025-06-18T04:05:18Z","published":"2025-02-27T10:11:53Z","title":"Alleviating Distribution Shift in Synthetic Data for Machine Translation\n  Quality Estimation","summary":"  Quality Estimation (QE) models evaluate the quality of machine translations\nwithout reference translations, serving as the reward models for the\ntranslation task. Due to the data scarcity, synthetic data generation has\nemerged as a promising solution. However, synthetic QE data often suffers from\ndistribution shift, which can manifest as discrepancies between pseudo and real\ntranslations, or in pseudo labels that do not align with human preferences. To\ntackle this issue, we introduce DCSQE, a novel framework for alleviating\ndistribution shift in synthetic QE data. To reduce the difference between\npseudo and real translations, we employ the constrained beam search algorithm\nand enhance translation diversity through the use of distinct generation\nmodels. DCSQE uses references, i.e., translation supervision signals, to guide\nboth the generation and annotation processes, enhancing the quality of\ntoken-level labels. DCSQE further identifies the shortest phrase covering\nconsecutive error tokens, mimicking human annotation behavior, to assign the\nfinal phrase-level labels. Specially, we underscore that the translation model\ncan not annotate translations of itself accurately. Extensive experiments\ndemonstrate that DCSQE outperforms SOTA baselines like CometKiwi in both\nsupervised and unsupervised settings. Further analysis offers insights into\nsynthetic data generation that could benefit reward models for other tasks. The\ncode is available at https://github.com/NJUNLP/njuqe.\n","authors":["Xiang Geng","Zhejian Lai","Jiajun Chen","Hao Yang","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2502.19941v3.pdf","comment":"ACL2025 Main"},{"id":"http://arxiv.org/abs/2505.11810v3","updated":"2025-06-18T03:57:51Z","published":"2025-05-17T03:43:16Z","title":"Efficiently Building a Domain-Specific Large Language Model from\n  Scratch: A Case Study of a Classical Chinese Large Language Model","summary":"  General-purpose large language models demonstrate notable capabilities in\nlanguage comprehension and generation, achieving results that are comparable\nto, or even surpass, human performance in many natural language processing\ntasks. Nevertheless, when general models are applied to some specific domains,\ne.g., Classical Chinese texts, their effectiveness is often unsatisfactory, and\nfine-tuning open-source foundational models similarly struggles to adequately\nincorporate domain-specific knowledge. To address this challenge, this study\ndeveloped a large language model, AI Taiyan, specifically designed for\nunderstanding and generating Classical Chinese. Experiments show that with a\nreasonable model design, data processing, foundational training, and\nfine-tuning, satisfactory results can be achieved with only 1.8 billion\nparameters. In key tasks related to language processing of Classical Chinese\nsuch as punctuation, identification of allusions, explanation of word meanings,\nand translation between ancient and modern Chinese, this model exhibits a clear\nadvantage over both general-purpose large models and domain-specific\ntraditional models, achieving levels close to or surpassing human baselines.\nThis research provides a reference for the efficient construction of\nspecialized domain-specific large language models. Furthermore, the paper\ndiscusses the application of this model in fields such as the collation of\nancient texts, dictionary editing, and language research, combined with case\nstudies.\n","authors":["Shen Li","Renfen Hu","Lijun Wang"],"pdf_url":"https://arxiv.org/pdf/2505.11810v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16645v2","updated":"2025-06-18T03:54:24Z","published":"2025-02-23T16:46:18Z","title":"CODESYNC: Synchronizing Large Language Models with Dynamic Code\n  Evolution at Scale","summary":"  Large Language Models (LLMs) have exhibited exceptional performance in\nsoftware engineering yet face challenges in adapting to continually evolving\ncode knowledge, particularly regarding the frequent updates of third-party\nlibrary APIs. This limitation, stemming from static pre-training datasets,\noften results in non-executable code or implementations with suboptimal safety\nand efficiency. To this end, this paper introduces CODESYNC, a data engine for\nidentifying outdated code patterns and collecting real-time code knowledge\nupdates from Python third-party libraries. Building upon CODESYNC, we develop\nCODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay\nsynchronized with code evolution, which covers real-world updates for 220 APIs\nfrom six Python libraries. Our benchmark offers 3,300 test cases across three\nevaluation tasks and an update-aware instruction tuning dataset consisting of\n2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs\nreveal that they struggle with dynamic code evolution, even with the support of\nadvanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe\nthat our benchmark can offer a strong foundation for the development of more\neffective methods for real-time code knowledge updating in the future. The\nexperimental code and dataset are publicly available at:\nhttps://github.com/Lucky-voyage/Code-Sync.\n","authors":["Chenlong Wang","Zhaoyang Chu","Zhengxiang Cheng","Xuyi Yang","Kaiyue Qiu","Yao Wan","Zhou Zhao","Xuanhua Shi","Dongping Chen"],"pdf_url":"https://arxiv.org/pdf/2502.16645v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13040v6","updated":"2025-06-18T03:38:12Z","published":"2023-05-22T13:47:51Z","title":"SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented\n  Dialogue Agents","summary":"  Task-oriented dialogue (TOD) models have made significant progress in recent\nyears. However, previous studies primarily focus on datasets written by\nannotators, which has resulted in a gap between academic research and\nreal-world spoken conversation scenarios. While several small-scale spoken TOD\ndatasets are proposed to address robustness issues such as ASR errors, they\nignore the unique challenges in spoken conversation. To tackle the limitations,\nwe introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD,\ncontaining 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from\nhuman-to-human spoken conversations. SpokenWOZ further incorporates common\nspoken characteristics such as word-by-word processing and reasoning in spoken\nlanguage. Based on these characteristics, we present cross-turn slot and\nreasoning slot detection as new challenges. We conduct experiments on various\nbaselines, including text-modal models, newly proposed dual-modal models, and\nLLMs, e.g., ChatGPT. The results show that the current models still have\nsubstantial room for improvement in spoken conversation, where the most\nadvanced dialogue state tracker only achieves 25.65% in joint goal accuracy and\nthe SOTA end-to-end model only correctly completes the user request in 52.1% of\ndialogues. The dataset, code, and leaderboard are available:\nhttps://spokenwoz.github.io/.\n","authors":["Shuzheng Si","Wentao Ma","Haoyu Gao","Yuchuan Wu","Ting-En Lin","Yinpei Dai","Hangyu Li","Rui Yan","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2305.13040v6.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2506.15118v1","updated":"2025-06-18T03:35:24Z","published":"2025-06-18T03:35:24Z","title":"CKD-EHR:Clinical Knowledge Distillation for Electronic Health Records","summary":"  Electronic Health Records (EHR)-based disease prediction models have\ndemonstrated significant clinical value in promoting precision medicine and\nenabling early intervention. However, existing large language models face two\nmajor challenges: insufficient representation of medical knowledge and low\nefficiency in clinical deployment. To address these challenges, this study\nproposes the CKD-EHR (Clinical Knowledge Distillation for EHR) framework, which\nachieves efficient and accurate disease risk prediction through knowledge\ndistillation techniques. Specifically, the large language model Qwen2.5-7B is\nfirst fine-tuned on medical knowledge-enhanced data to serve as the teacher\nmodel.It then generates interpretable soft labels through a multi-granularity\nattention distillation mechanism. Finally, the distilled knowledge is\ntransferred to a lightweight BERT student model. Experimental results show that\non the MIMIC-III dataset, CKD-EHR significantly outperforms the baseline\nmodel:diagnostic accuracy is increased by 9%, F1-score is improved by 27%, and\na 22.2 times inference speedup is achieved. This innovative solution not only\ngreatly improves resource utilization efficiency but also significantly\nenhances the accuracy and timeliness of diagnosis, providing a practical\ntechnical approach for resource optimization in clinical settings. The code and\ndata for this research are available athttps://github.com/209506702/CKD_EHR.\n","authors":["Junke Wang","Hongshun Ling","Li Zhang","Longqian Zhang","Fang Wang","Yuan Gao","Zhi Li"],"pdf_url":"https://arxiv.org/pdf/2506.15118v1.pdf","comment":"20 pages,5 figures"},{"id":"http://arxiv.org/abs/2501.09265v2","updated":"2025-06-18T03:29:11Z","published":"2025-01-16T03:30:47Z","title":"Perspective Transition of Large Language Models for Solving Subjective\n  Tasks","summary":"  Large language models (LLMs) have revolutionized the field of natural\nlanguage processing, enabling remarkable progress in various tasks. Different\nfrom objective tasks such as commonsense reasoning and arithmetic\nquestion-answering, the performance of LLMs on subjective tasks is still\nlimited, where the perspective on the specific problem plays crucial roles for\nbetter interpreting the context and giving proper response. For example, in\ncertain scenarios, LLMs may perform better when answering from an expert role\nperspective, potentially eliciting their relevant domain knowledge. In\ncontrast, in some scenarios, LLMs may provide more accurate responses when\nanswering from a third-person standpoint, enabling a more comprehensive\nunderstanding of the problem and potentially mitigating inherent biases. In\nthis paper, we propose Reasoning through Perspective Transition (RPT), a method\nbased on in-context learning that enables LLMs to dynamically select among\ndirect, role, and third-person perspectives for the best way to solve\ncorresponding subjective problem. Through extensive experiments on totally 12\nsubjective tasks by using both closed-source and open-source LLMs including\nGPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single\nfixed perspective based methods such as chain-of-thought prompting and expert\nprompting, highlights the intricate ways that LLMs can adapt their perspectives\nto provide nuanced and contextually appropriate responses for different\nproblems.\n","authors":["Xiaolong Wang","Yuanchi Zhang","Ziyue Wang","Yuzhuang Xu","Fuwen Luo","Yile Wang","Peng Li","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2501.09265v2.pdf","comment":"ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2206.13155v2","updated":"2025-06-18T03:26:43Z","published":"2022-06-27T09:58:34Z","title":"Bi-VLDoc: Bidirectional Vision-Language Modeling for Visually-Rich\n  Document Understanding","summary":"  Multi-modal document pre-trained models have proven to be very effective in a\nvariety of visually-rich document understanding (VrDU) tasks. Though existing\ndocument pre-trained models have achieved excellent performance on standard\nbenchmarks for VrDU, the way they model and exploit the interactions between\nvision and language on documents has hindered them from better generalization\nability and higher accuracy. In this work, we investigate the problem of\nvision-language joint representation learning for VrDU mainly from the\nperspective of supervisory signals. Specifically, a pre-training paradigm\ncalled Bi-VLDoc is proposed, in which a bidirectional vision-language\nsupervision strategy and a vision-language hybrid-attention mechanism are\ndevised to fully explore and utilize the interactions between these two\nmodalities, to learn stronger cross-modal document representations with richer\nsemantics. Benefiting from the learned informative cross-modal document\nrepresentations, Bi-VLDoc significantly advances the state-of-the-art\nperformance on three widely-used document understanding benchmarks, including\nForm Understanding (from 85.14% to 93.44%), Receipt Information Extraction\n(from 96.01% to 97.84%), and Document Classification (from 96.08% to 97.12%).\nOn Document Visual QA, Bi-VLDoc achieves the state-of-the-art performance\ncompared to previous single model methods.\n","authors":["Chuwei Luo","Guozhi Tang","Qi Zheng","Cong Yao","Lianwen Jin","Chenliang Li","Yang Xue","Luo Si"],"pdf_url":"https://arxiv.org/pdf/2206.13155v2.pdf","comment":"IJDAR 2025"},{"id":"http://arxiv.org/abs/2502.14693v3","updated":"2025-06-18T03:15:07Z","published":"2025-02-20T16:19:09Z","title":"I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree\n  Search","summary":"  Recent advancements in large language models (LLMs) have shown remarkable\npotential in automating machine learning tasks. However, existing LLM-based\nagents often struggle with low-diversity and suboptimal code generation. While\nrecent work has introduced Monte Carlo Tree Search (MCTS) to address these\nissues, limitations persist in the quality and diversity of thoughts generated,\nas well as in the scalar value feedback mechanisms used for node selection. In\nthis study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a\nnovel approach that iteratively expands tree nodes through an introspective\nprocess that meticulously analyzes solutions and results from parent and\nsibling nodes. This facilitates a continuous refinement of the node in the\nsearch tree, thereby enhancing the overall decision-making process.\nFurthermore, we integrate a Large Language Model (LLM)-based value model to\nfacilitate direct evaluation of each node's solution prior to conducting\ncomprehensive computational rollouts. A hybrid rewarding mechanism is\nimplemented to seamlessly transition the Q-value from LLM-estimated scores to\nactual performance scores. This allows higher-quality nodes to be traversed\nearlier. Applied to the various ML tasks, our approach demonstrates a 6%\nabsolute improvement in performance compared to the strong open-source AutoML\nagents, showcasing its effectiveness in enhancing agentic AutoML systems.\nResource available at https://github.com/jokieleung/I-MCTS\n","authors":["Zujie Liang","Feng Wei","Wujiang Xu","Lin Chen","Yuxi Qian","Xinhui Wu"],"pdf_url":"https://arxiv.org/pdf/2502.14693v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.21569v2","updated":"2025-06-18T03:05:54Z","published":"2025-05-27T06:22:57Z","title":"ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools","summary":"  Large Language Model (LLM)-based agents have demonstrated the ability to\nimprove performance in chemistry-related tasks by selecting appropriate tools.\nHowever, their effectiveness remains limited by the inherent prediction errors\nof chemistry tools. In this paper, we take a step further by exploring how\nLLMbased agents can, in turn, be leveraged to reduce prediction errors of the\ntools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),\na simple yet effective method that enhances chemistry tools through optimizing\nagent-stacking structures from limited data. ChemHAS achieves state-of-the-art\nperformance across four fundamental chemistry tasks, demonstrating that our\nmethod can effectively compensate for prediction errors of the tools.\nFurthermore, we identify and characterize four distinct agent-stacking\nbehaviors, potentially improving interpretability and revealing new\npossibilities for AI agent applications in scientific research. Our code and\ndataset are publicly available at https:\n//anonymous.4open.science/r/ChemHAS-01E4/README.md.\n","authors":["Zhucong Li","Bowei Zhang","Jin Xiao","Zhijian Zhou","Fenglei Cao","Jiaqing Liang","Yuan Qi"],"pdf_url":"https://arxiv.org/pdf/2505.21569v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2506.14731v2","updated":"2025-06-18T02:53:14Z","published":"2025-06-17T17:12:34Z","title":"Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\n  for LLMs","summary":"  We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model\noptimized via reinforcement learning (RL) to achieve efficient and robust\nreasoning capabilities. Built upon the publicly available Ling-lite model, a\n16.8 billion parameter model with 2.75 billion activated parameters, our\napproach matches the performance of state-of-the-art (SOTA) small-scale\nreasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,\nGPQA-Diamond) while activating only one-third of the parameters required by\ncomparable models. To accomplish this, we introduce a joint training pipeline\nintegrating distillation with RL, revealing undocumented challenges in MoE RL\ntraining. First, we identify optimization instability during RL training, and\nwe propose Constrained Contextual Computation Policy Optimization(C3PO), a\nnovel approach that enhances training stability and improves computational\nthroughput via algorithm-system co-design methodology. Second, we empirically\ndemonstrate that selecting distillation checkpoints based on entropy loss for\nRL training, rather than validation metrics, yields superior\nperformance-efficiency trade-offs in subsequent RL training. Finally, we\ndevelop a two-stage training paradigm to harmonize multi-domain data\nintegration, addressing domain conflicts that arise in training with mixed\ndataset. We will release the model, dataset, and code.\n","authors":[" Ling Team","Bin Hu","Cai Chen","Deng Zhao","Ding Liu","Dingnan Jin","Feng Zhu","Hao Dai","Hongzhi Luan","Jia Guo","Jiaming Liu","Jiewei Wu","Jun Mei","Jun Zhou","Junbo Zhao","Junwu Xiong","Kaihong Zhang","Kuan Xu","Lei Liang","Liang Jiang","Liangcheng Fu","Longfei Zheng","Qiang Gao","Qing Cui","Quan Wan","Shaomian Zheng","Shuaicheng Li","Tongkai Yang","Wang Ren","Xiaodong Yan","Xiaopei Wan","Xiaoyun Feng","Xin Zhao","Xinxing Yang","Xinyu Kong","Xuemin Yang","Yang Li","Yingting Wu","Yongkang Liu","Zhankai Xu","Zhenduo Zhang","Zhenglei Zhou","Zhenyu Huang","Zhiqiang Zhang","Zihao Wang","Zujie Wen"],"pdf_url":"https://arxiv.org/pdf/2506.14731v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2410.06809v3","updated":"2025-06-18T02:50:22Z","published":"2024-10-09T12:09:30Z","title":"Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level","summary":"  Large language models (LLMs) have demonstrated immense utility across various\nindustries. However, as LLMs advance, the risk of harmful outputs increases due\nto incorrect or malicious instruction prompts. While current methods\neffectively address jailbreak risks, they share common limitations: 1) Judging\nharmful responses from the prefill-level lacks utilization of the model's\ndecoding outputs, leading to relatively lower effectiveness and robustness. 2)\nRejecting potentially harmful responses based on a single evaluation can\nsignificantly impair the model's helpfulness.This paper examines the LLMs'\ncapability to recognize harmful outputs, revealing and quantifying their\nproficiency in assessing the danger of previous tokens. Motivated by pilot\nexperiment results, we design a robust defense mechanism at the decoding level.\nOur novel decoder-oriented, step-by-step defense architecture corrects harmful\nqueries directly rather than rejecting them outright. We introduce speculative\ndecoding to enhance usability and facilitate deployment to boost secure\ndecoding speed. Extensive experiments demonstrate that our approach improves\nmodel security without compromising reasoning speed. Notably, our method\nleverages the model's ability to discern hazardous information, maintaining its\nhelpfulness compared to existing methods.\n","authors":["Xinyi Zeng","Yuying Shang","Jiawei Chen","Jingyuan Zhang","Yu Tian"],"pdf_url":"https://arxiv.org/pdf/2410.06809v3.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2506.15081v1","updated":"2025-06-18T02:47:14Z","published":"2025-06-18T02:47:14Z","title":"Improving Dialogue Discourse Parsing through Discourse-aware Utterance\n  Clarification","summary":"  Dialogue discourse parsing aims to identify and analyze discourse relations\nbetween the utterances within dialogues. However, linguistic features in\ndialogues, such as omission and idiom, frequently introduce ambiguities that\nobscure the intended discourse relations, posing significant challenges for\nparsers. To address this issue, we propose a Discourse-aware Clarification\nModule (DCM) to enhance the performance of the dialogue discourse parser. DCM\nemploys two distinct reasoning processes: clarification type reasoning and\ndiscourse goal reasoning. The former analyzes linguistic features, while the\nlatter distinguishes the intended relation from the ambiguous one. Furthermore,\nwe introduce Contribution-aware Preference Optimization (CPO) to mitigate the\nrisk of erroneous clarifications, thereby reducing cascading errors. CPO\nenables the parser to assess the contributions of the clarifications from DCM\nand provide feedback to optimize the DCM, enhancing its adaptability and\nalignment with the parser's requirements. Extensive experiments on the STAC and\nMolweni datasets demonstrate that our approach effectively resolves ambiguities\nand significantly outperforms the state-of-the-art (SOTA) baselines.\n","authors":["Yaxin Fan","Peifeng Li","Qiaoming Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.15081v1.pdf","comment":"Accepted by ACL2025(main conference)"},{"id":"http://arxiv.org/abs/2506.15076v1","updated":"2025-06-18T02:42:02Z","published":"2025-06-18T02:42:02Z","title":"Learning-Time Encoding Shapes Unlearning in LLMs","summary":"  As large language models (LLMs) are increasingly deployed in the real world,\nthe ability to ``unlearn'', or remove specific pieces of knowledge post hoc,\nhas become essential for a variety of reasons ranging from privacy regulations\nto correcting outdated or harmful content. Prior work has proposed unlearning\nbenchmarks and algorithms, and has typically assumed that the training process\nand the target model are fixed. In this work, we empirically investigate how\nlearning-time choices in knowledge encoding impact the effectiveness of\nunlearning factual knowledge. Our experiments reveal two key findings: (1)\nlearning with paraphrased descriptions improves unlearning performance and (2)\nunlearning individual piece of knowledge from a chunk of text is challenging.\nOur results suggest that learning-time knowledge encoding may play a central\nrole in enabling reliable post-hoc unlearning.\n","authors":["Ruihan Wu","Konstantin Garov","Kamalika Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2506.15076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16205v6","updated":"2025-06-18T02:41:56Z","published":"2024-07-23T06:14:41Z","title":"LLMs can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on\n  Large Language Models","summary":"  The rapid development of Large Language Models (LLMs) has brought impressive\nadvancements across various tasks. However, despite these achievements, LLMs\nstill pose inherent safety risks, especially in the context of jailbreak\nattacks. Most existing jailbreak methods follow an input-level manipulation\nparadigm to bypass safety mechanisms. Yet, as alignment techniques improve,\nsuch attacks are becoming increasingly detectable. In this work, we identify an\nunderexplored threat vector: the model's internal reasoning process, which can\nbe manipulated to elicit harmful outputs in a more stealthy way. To explore\nthis overlooked attack surface, we propose a novel black-box jailbreak attack\nmethod, Analyzing-based Jailbreak (ABJ). ABJ comprises two independent attack\npaths: textual and visual reasoning attacks, which exploit the model's\nmultimodal reasoning capabilities to bypass safety mechanisms, comprehensively\nexposing vulnerabilities in its reasoning chain. We conduct extensive\nexperiments on ABJ across various open-source and closed-source LLMs, VLMs, and\nRLMs. In particular, ABJ achieves high attack success rate (ASR) (82.1% on\nGPT-4o-2024-11-20) with exceptional attack efficiency (AE) among all target\nmodels, showcasing its remarkable attack effectiveness, transferability, and\nefficiency. Our work reveals a new type of safety risk and highlights the\nurgent need to mitigate implicit vulnerabilities in the model's reasoning\nprocess.\n","authors":["Shi Lin","Hongming Yang","Rongchang Li","Xun Wang","Changting Lin","Wenpeng Xing","Meng Han"],"pdf_url":"https://arxiv.org/pdf/2407.16205v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15068v1","updated":"2025-06-18T02:16:53Z","published":"2025-06-18T02:16:53Z","title":"Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form\n  Generation","summary":"  Evaluating open-ended long-form generation is challenging because it is hard\nto define what clearly separates good from bad outputs. Existing methods often\nmiss key aspects like coherence, style, or relevance, or are biased by\npretraining data, making open-ended long-form evaluation an underexplored\nproblem. To address this gap, we propose PrefBERT, a scoring model for\nevaluating open-ended long-form generation in GRPO and guiding its training\nwith distinct rewards for good and bad outputs. Trained on two response\nevaluation datasets with diverse long-form styles and Likert-rated quality,\nPrefBERT effectively supports GRPO by offering better semantic reward feedback\nthan traditional metrics ROUGE-L and BERTScore do. Through comprehensive\nevaluations, including LLM-as-a-judge, human ratings, and qualitative analysis,\nwe show that PrefBERT, trained on multi-sentence and paragraph-length\nresponses, remains reliable across varied long passages and aligns well with\nthe verifiable rewards GRPO needs. Human evaluations confirm that using\nPrefBERT as the reward signal to train policy models yields responses better\naligned with human preferences than those trained with traditional metrics. Our\ncode is available at https://github.com/zli12321/long_form_rl.\n","authors":["Zongxia Li","Yapei Chang","Yuhang Zhou","Xiyang Wu","Zichao Liang","Yoo Yeon Sung","Jordan Lee Boyd-Graber"],"pdf_url":"https://arxiv.org/pdf/2506.15068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16930v4","updated":"2025-06-18T02:05:18Z","published":"2024-10-22T12:00:58Z","title":"Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities\n  Using Only Forward Passes","summary":"  Math reasoning is an active area of Large Language Model (LLM) research\nbecause it is a hallmark of artificial intelligence and has implications in\nseveral domains, including math education. However, few works have explored how\nmath reasoning is encoded within LLM parameters and if it is a skill that can\nbe isolated within models. Doing so could allow targeted intervention to\nimprove math performance without altering non-math behavior and foster\nunderstanding of how models encode math reasoning. We introduce Math\nNeurosurgery (MathNeuro), a computationally efficient method we use to isolate\nmath-specific parameters in LLMs using only forward passes. MathNeuro builds on\nexisting work by using weights and activations to calculate parameter\nimportance, but isolates math-specific parameters by filtering out those\nimportant for general language tasks. Through pruning parameters MathNeuro\nidentifies, we delete a LLM's math reasoning ability without significantly\nimpacting its general language ability. Scaling the identified parameters by a\nsmall constant improves a pretrained or instruction-tuned LLM's performance by\n4-17% on GSM8K and 5-35% on MATH while leaving non-math behavior unaltered.\nMathNeuro is also data efficient: most of its effectiveness holds when\nidentifying math-specific parameters using a single sample. MathNeuro\nhighlights the potential for future work to intervene on math-specific\nparameters.\n","authors":["Bryan R. Christ","Zack Gottesman","Jonathan Kropko","Thomas Hartvigsen"],"pdf_url":"https://arxiv.org/pdf/2410.16930v4.pdf","comment":"38 pages, 54 figures, Accepted to ACL 2025 (Main)"},{"id":"http://arxiv.org/abs/2506.02803v2","updated":"2025-06-18T01:50:39Z","published":"2025-06-03T12:33:47Z","title":"SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via\n  Visual Global Thinking","summary":"  Vision-language models (VLMs) excel in semantic tasks but falter at a core\nhuman capability: detecting hidden content in optical illusions or AI-generated\nimages through perceptual adjustments like zooming. We introduce HC-Bench, a\nbenchmark of 112 images with hidden text, objects, and illusions, revealing\nthat leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit\nprompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to\nan overreliance on high-level semantics. Strikingly, we propose SemVink\n(Semantic Visual Thinking) by simply scaling images to low resolutions (32-128\npixels), which unlocks >99% accuracy by eliminating redundant visual noise.\nThis exposes a critical architectural flaw: VLMs prioritize abstract reasoning\nover low-level visual operations crucial for real-world robustness. Our work\nurges a shift toward hybrid models integrating multi-scale processing, bridging\nthe gap between computational vision and human cognition for applications in\nmedical imaging, security, and beyond.\n","authors":["Sifan Li","Yujun Cai","Yiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2506.02803v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14397v2","updated":"2025-06-18T01:18:11Z","published":"2025-06-17T10:51:39Z","title":"Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation\n  Understanding","summary":"  Negation is a fundamental linguistic phenomenon that poses persistent\nchallenges for Large Language Models (LLMs), particularly in tasks requiring\ndeep semantic understanding. Existing benchmarks often treat negation as a side\ncase within broader tasks like natural language inference, resulting in a lack\nof benchmarks that exclusively target negation understanding. In this work, we\nintroduce Thunder-NUBench, a novel benchmark explicitly designed to assess\nsentence-level negation understanding in LLMs. Thunder-NUBench goes beyond\nsurface-level cue detection by contrasting standard negation with structurally\ndiverse alternatives such as local negation, contradiction, and paraphrase. The\nbenchmark consists of manually curated sentence-negation pairs and a\nmultiple-choice dataset that enables in-depth evaluation of models' negation\nunderstanding.\n","authors":["Yeonkyoung So","Gyuseong Lee","Sungmok Jung","Joonhak Lee","JiA Kang","Sangho Kim","Jaejin Lee"],"pdf_url":"https://arxiv.org/pdf/2506.14397v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15041v1","updated":"2025-06-18T01:00:59Z","published":"2025-06-18T01:00:59Z","title":"Identifying economic narratives in large text corpora -- An integrated\n  approach using Large Language Models","summary":"  As interest in economic narratives has grown in recent years, so has the\nnumber of pipelines dedicated to extracting such narratives from texts.\nPipelines often employ a mix of state-of-the-art natural language processing\ntechniques, such as BERT, to tackle this task. While effective on foundational\nlinguistic operations essential for narrative extraction, such models lack the\ndeeper semantic understanding required to distinguish extracting economic\nnarratives from merely conducting classic tasks like Semantic Role Labeling.\nInstead of relying on complex model pipelines, we evaluate the benefits of\nLarge Language Models (LLMs) by analyzing a corpus of Wall Street Journal and\nNew York Times newspaper articles about inflation. We apply a rigorous\nnarrative definition and compare GPT-4o outputs to gold-standard narratives\nproduced by expert annotators. Our results suggests that GPT-4o is capable of\nextracting valid economic narratives in a structured format, but still falls\nshort of expert-level performance when handling complex documents and\nnarratives. Given the novelty of LLMs in economic research, we also provide\nguidance for future work in economics and the social sciences that employs LLMs\nto pursue similar objectives.\n","authors":["Tobias Schmidt","Kai-Robin Lange","Matthias Reccius","Henrik Müller","Michael Roos","Carsten Jentsch"],"pdf_url":"https://arxiv.org/pdf/2506.15041v1.pdf","comment":"53 pages, 5 figures"},{"id":"http://arxiv.org/abs/2506.15030v1","updated":"2025-06-18T00:13:42Z","published":"2025-06-18T00:13:42Z","title":"Identifying social isolation themes in NVDRS text narratives using topic\n  modeling and text-classification methods","summary":"  Social isolation and loneliness, which have been increasing in recent years\nstrongly contribute toward suicide rates. Although social isolation and\nloneliness are not currently recorded within the US National Violent Death\nReporting System's (NVDRS) structured variables, natural language processing\n(NLP) techniques can be used to identify these constructs in law enforcement\nand coroner medical examiner narratives. Using topic modeling to generate\nlexicon development and supervised learning classifiers, we developed\nhigh-quality classifiers (average F1: .86, accuracy: .82). Evaluating over\n300,000 suicides from 2002 to 2020, we identified 1,198 mentioning chronic\nsocial isolation. Decedents had higher odds of chronic social isolation\nclassification if they were men (OR = 1.44; CI: 1.24, 1.69, p<.0001), gay (OR =\n3.68; 1.97, 6.33, p<.0001), or were divorced (OR = 3.34; 2.68, 4.19, p<.0001).\nWe found significant predictors for other social isolation topics of recent or\nimpending divorce, child custody loss, eviction or recent move, and break-up.\nOur methods can improve surveillance and prevention of social isolation and\nloneliness in the United States.\n","authors":["Drew Walker","Swati Rajwal","Sudeshna Das","Snigdha Peddireddy","Abeed Sarker"],"pdf_url":"https://arxiv.org/pdf/2506.15030v1.pdf","comment":"22 pages, 2 figures, 5 tables"},{"id":"http://arxiv.org/abs/2506.15029v1","updated":"2025-06-18T00:11:06Z","published":"2025-06-18T00:11:06Z","title":"An accurate and revised version of optical character recognition-based\n  speech synthesis using LabVIEW","summary":"  Knowledge extraction through sound is a distinctive property. Visually\nimpaired individuals often rely solely on Braille books and audio recordings\nprovided by NGOs. Due to limitations in these approaches, blind individuals\noften cannot access books of their choice. Speech is a more effective mode of\ncommunication than text for blind and visually impaired persons, as they can\neasily respond to sounds. This paper presents the development of an accurate,\nreliable, cost-effective, and user-friendly optical character recognition\n(OCR)-based speech synthesis system. The OCR-based system has been implemented\nusing Laboratory Virtual Instrument Engineering Workbench (LabVIEW).\n","authors":["Prateek Mehta","Anasuya Patil"],"pdf_url":"https://arxiv.org/pdf/2506.15029v1.pdf","comment":"9 pages, 9 figures"}]}}