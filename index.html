<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-18T00:00:00Z">2025-06-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">108</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PhantomHunter: Detecting Unseen Privately-Tuned LLM-Generated Text via
  Family-Aware Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhui Shi, Yehan Yang, Qiang Sheng, Hao Mi, Beizhe Hu, Chaoxi Xu, Juan Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the popularity of large language models (LLMs), undesirable societal
problems like misinformation production and academic misconduct have been more
severe, making LLM-generated text detection now of unprecedented importance.
Although existing methods have made remarkable progress, a new challenge posed
by text from privately tuned LLMs remains underexplored. Users could easily
possess private LLMs by fine-tuning an open-source one with private corpora,
resulting in a significant performance drop of existing detectors in practice.
To address this issue, we propose PhantomHunter, an LLM-generated text detector
specialized for detecting text from unseen, privately-tuned LLMs. Its
family-aware learning framework captures family-level traits shared across the
base models and their derivatives, instead of memorizing individual
characteristics. Experiments on data from LLaMA, Gemma, and Mistral families
show its superiority over 7 baselines and 3 industrial services, with F1 scores
of over 96%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 3 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenRecal: Generation after Recalibration from Large to Small
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byung-Kwan Lee, Ryo Hachiuma, Yong Man Ro, Yu-Chiang Frank Wang, Yueh-Hua Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in vision-language models (VLMs) have leveraged large
language models (LLMs) to achieve performance on par with closed-source systems
like GPT-4V. However, deploying these models in real-world scenarios,
particularly on resource-constrained devices, remains challenging due to their
substantial computational demands. This has spurred interest in distilling
knowledge from large VLMs into smaller, more efficient counterparts. A key
challenge arises here from the diversity of VLM architectures, which are built
on different LLMs and employ varying token types-differing in vocabulary size,
token splits, and token index ordering. To address this challenge of limitation
to a specific VLM type, we present Generation after Recalibration (GenRecal), a
novel, general-purpose distillation framework for VLMs. GenRecal incorporates a
Recalibrator that aligns and adapts feature representations between
heterogeneous VLMs, enabling effective knowledge transfer across different
types of VLMs. Through extensive experiments on multiple challenging
benchmarks, we demonstrate that GenRecal significantly improves baseline
performances, eventually outperforming large-scale open- and closed-source
VLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://byungkwanlee.github.io/GenRecal-page/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dense SAE Latents Are Features, Not Bugs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoqing Sun, Alessandro Stolfo, Joshua Engels, Ben Wu, Senthooran Rajamanoharan, Mrinmaya Sachan, Max Tegmark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse autoencoders (SAEs) are designed to extract interpretable features
from language models by enforcing a sparsity constraint. Ideally, training an
SAE would yield latents that are both sparse and semantically meaningful.
However, many SAE latents activate frequently (i.e., are \emph{dense}), raising
concerns that they may be undesirable artifacts of the training procedure. In
this work, we systematically investigate the geometry, function, and origin of
dense latents and show that they are not only persistent but often reflect
meaningful model representations. We first demonstrate that dense latents tend
to form antipodal pairs that reconstruct specific directions in the residual
stream, and that ablating their subspace suppresses the emergence of new dense
features in retrained SAEs -- suggesting that high density features are an
intrinsic property of the residual space. We then introduce a taxonomy of dense
latents, identifying classes tied to position tracking, context binding,
entropy regulation, letter-specific output signals, part-of-speech, and
principal component reconstruction. Finally, we analyze how these features
evolve across layers, revealing a shift from structural features in early
layers, to semantic features in mid layers, and finally to output-oriented
signals in the last layers of the model. Our findings indicate that dense
latents serve functional roles in language model computation and should not be
dismissed as training noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embodied Web <span class="highlight-title">Agent</span>s: Bridging Physical-Digital Realms for Integrated
  <span class="highlight-title">Agent</span> Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yining Hong, Rui Sun, Bingxuan Li, Xingcheng Yao, Maxine Wu, Alexander Chien, Da Yin, Ying Nian Wu, Zhecan James Wang, Kai-Wei Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI agents today are mostly siloed - they either retrieve and reason over vast
amount of digital information and knowledge obtained online; or interact with
the physical world through embodied perception, planning and action - but
rarely both. This separation limits their ability to solve tasks that require
integrated physical and digital intelligence, such as cooking from online
recipes, navigating with dynamic map data, or interpreting real-world landmarks
using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI
agents that fluidly bridge embodiment and web-scale reasoning. To
operationalize this concept, we first develop the Embodied Web Agents task
environments, a unified simulation platform that tightly integrates realistic
3D indoor and outdoor environments with functional web interfaces. Building
upon this platform, we construct and release the Embodied Web Agents Benchmark,
which encompasses a diverse suite of tasks including cooking, navigation,
shopping, tourism, and geolocation - all requiring coordinated reasoning across
physical and digital realms for systematic assessment of cross-domain
intelligence. Experimental results reveal significant performance gaps between
state-of-the-art AI systems and human capabilities, establishing both
challenges and opportunities at the intersection of embodied cognition and
web-scale knowledge access. All datasets, codes and websites are publicly
available at our project page https://embodied-web-agent.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gender-Neutral Machine Translation Strategies in Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hillary Dawkins, Isar Nejadgholi, Chi-kiu Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gender-inclusive machine translation (MT) should preserve gender ambiguity in
the source to avoid misgendering and representational harms. While gender
ambiguity often occurs naturally in notional gender languages such as English,
maintaining that gender neutrality in grammatical gender languages is a
challenge. Here we assess the sensitivity of 21 MT systems to the need for
gender neutrality in response to gender ambiguity in three translation
directions of varying difficulty. The specific gender-neutral strategies that
are observed in practice are categorized and discussed. Additionally, we
examine the effect of binary gender stereotypes on the use of gender-neutral
translation. In general, we report a disappointing absence of gender-neutral
translations in response to gender ambiguity. However, we observe a small
handful of MT systems that switch to gender neutral translation using specific
strategies, depending on the target language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear at GITT 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tommaso Green, Martin Gubri, Haritz Puerto, Sangdoo Yun, Seong Joon Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study privacy leakage in the reasoning traces of large reasoning models
used as personal agents. Unlike final outputs, reasoning traces are often
assumed to be internal and safe. We challenge this assumption by showing that
reasoning traces frequently contain sensitive user data, which can be extracted
via prompt injections or accidentally leak into outputs. Through probing and
agentic evaluations, we demonstrate that test-time compute approaches,
particularly increased reasoning steps, amplify such leakage. While increasing
the budget of those test-time compute approaches makes models more cautious in
their final answers, it also leads them to reason more verbosely and leak more
in their own thinking. This reveals a core tension: reasoning improves utility
but enlarges the privacy attack surface. We argue that safety efforts must
extend to the model's internal thinking, not just its outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CC-LEARN: Cohort-based Consistency Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Ye, Shaswat Shrivastava, Zhaonan Li, Jacob Dineen, Shijie Lu, Avneet Ahuja, Ming Shen, Zhikun Xu, Ben Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models excel at many tasks but still struggle with consistent,
robust reasoning. We introduce Cohort-based Consistency Learning (CC-Learn), a
reinforcement learning framework that improves the reliability of LLM reasoning
by training on cohorts of similar questions derived from shared programmatic
abstractions. To enforce cohort-level consistency, we define a composite
objective combining cohort accuracy, a retrieval bonus for effective problem
decomposition, and a rejection penalty for trivial or invalid lookups that
reinforcement learning can directly optimize, unlike supervised fine-tuning.
Optimizing this reward guides the model to adopt uniform reasoning patterns
across all cohort members. Experiments on challenging reasoning benchmarks
(including ARC-Challenge and StrategyQA) show that CC-Learn boosts both
accuracy and reasoning stability over pretrained and SFT baselines. These
results demonstrate that cohort-level RL effectively enhances reasoning
consistency in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards
  Improve Preference Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tevin Wang, Chenyan Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rule-based rewards offer a promising strategy for improving reinforcement
learning from human feedback (RLHF), but current approaches often rely on
manual rule engineering. We present AutoRule, a fully automated method for
extracting rules from preference feedback and formulating them into rule-based
rewards. AutoRule extraction operates in three stages: it leverages a reasoning
model to interpret user preferences, identifies candidate rules from the
reasoning chain of these interpretations, and synthesizes them into a unified
rule set. Leveraging the finalized rule set, we employ language-model verifiers
to compute the fraction of rules satisfied by each output, using this metric as
an auxiliary reward alongside the learned reward model during policy
optimization. Training a Llama-3-8B model with AutoRule results in a 28.6\%
relative improvement in length-controlled win rate on AlpacaEval2.0, and a
6.1\% relative gain in second-turn performance on a held-out MT-Bench subset,
compared to a GRPO baseline trained with the same learned reward model but
without the rule-based auxiliary reward. Our analysis confirms that the
extracted rules exhibit good agreement with dataset preference. We find that
AutoRule demonstrates reduced reward hacking compared to a learned reward model
when run over two episodes. Finally, our case study suggests that the extracted
rules capture unique qualities valued in different datasets. The extracted
rules are provided in the appendix, and the code is open-sourced at
https://github.com/cxcscmu/AutoRule.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Oldies but Goldies: The Potential of Character N-grams for Romanian
  Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dana Lupsa, Sanda-Maria Avram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the problem of authorship attribution for Romanian texts
using the ROST corpus, a standard benchmark in the field. We systematically
evaluate six machine learning techniques: Support Vector Machine (SVM),
Logistic Regression (LR), k-Nearest Neighbors (k-NN), Decision Trees (DT),
Random Forests (RF), and Artificial Neural Networks (ANN), employing character
n-gram features for classification. Among these, the ANN model achieved the
highest performance, including perfect classification in four out of fifteen
runs when using 5-gram features. These results demonstrate that lightweight,
interpretable character n-gram approaches can deliver state-of-the-art accuracy
for Romanian authorship attribution, rivaling more complex methods. Our
findings highlight the potential of simple stylometric features in resource,
constrained or under-studied language settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Compositional Generalization Capability of Large Language
  Models Considering Instruction Following Ability <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In generative commonsense reasoning tasks such as CommonGen, generative large
language models (LLMs) compose sentences that include all given concepts.
However, when focusing on instruction-following capabilities, if a prompt
specifies a concept order, LLMs must generate sentences that adhere to the
specified order. To address this, we propose Ordered CommonGen, a benchmark
designed to evaluate the compositional generalization and instruction-following
abilities of LLMs. This benchmark measures ordered coverage to assess whether
concepts are generated in the specified order, enabling a simultaneous
evaluation of both abilities. We conducted a comprehensive analysis using 36
LLMs and found that, while LLMs generally understand the intent of
instructions, biases toward specific concept order patterns often lead to
low-diversity outputs or identical results even when the concept order is
altered. Moreover, even the most instruction-compliant LLM achieved only about
75% ordered coverage, highlighting the need for improvements in both
instruction-following and compositional generalization capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minding the Politeness Gap in Cross-cultural Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuka Machino, Matthias Hofer, Max Siegel, Joshua B. Tenenbaum, Robert D. Hawkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Misunderstandings in cross-cultural communication often arise from subtle
differences in interpretation, but it is unclear whether these differences
arise from the literal meanings assigned to words or from more general
pragmatic factors such as norms around politeness and brevity. In this paper,
we report three experiments examining how speakers of British and American
English interpret intensifiers like "quite" and "very." To better understand
these cross-cultural differences, we developed a computational cognitive model
where listeners recursively reason about speakers who balance informativity,
politeness, and utterance cost. Our model comparisons suggested that
cross-cultural differences in intensifier interpretation stem from a
combination of (1) different literal meanings, (2) different weights on
utterance cost. These findings challenge accounts based purely on semantic
variation or politeness norms, demonstrating that cross-cultural differences in
interpretation emerge from an intricate interplay between the two.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Compositional Architecture of Regret in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangxiang Cui, Shu Yang, Tianjin Huang, Wanyu Lin, Lijie Hu, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regret in Large Language Models refers to their explicit regret expression
when presented with evidence contradicting their previously generated
misinformation. Studying the regret mechanism is crucial for enhancing model
reliability and helps in revealing how cognition is coded in neural networks.
To understand this mechanism, we need to first identify regret expressions in
model outputs, then analyze their internal representation. This analysis
requires examining the model's hidden states, where information processing
occurs at the neuron level. However, this faces three key challenges: (1) the
absence of specialized datasets capturing regret expressions, (2) the lack of
metrics to find the optimal regret representation layer, and (3) the lack of
metrics for identifying and analyzing regret neurons. Addressing these
limitations, we propose: (1) a workflow for constructing a comprehensive regret
dataset through strategically designed prompting scenarios, (2) the Supervised
Compression-Decoupling Index (S-CDI) metric to identify optimal regret
representation layers, and (3) the Regret Dominance Score (RDS) metric to
identify regret neurons and the Group Impact Coefficient (GIC) to analyze
activation patterns. Our experimental results successfully identified the
optimal regret representation layer using the S-CDI metric, which significantly
enhanced performance in probe classification experiments. Additionally, we
discovered an M-shaped decoupling pattern across model layers, revealing how
information processing alternates between coupling and decoupling phases.
Through the RDS metric, we categorized neurons into three distinct functional
groups: regret neurons, non-regret neurons, and dual neurons.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabrel J. Perin, Runjin Chen, Xuxi Chen, Nina S. T. Hirata, Zhangyang Wang, Junyuan Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have become indispensable in real-world
applications. However, their widespread adoption raises significant safety
concerns, particularly in responding to socially harmful questions. Despite
substantial efforts to improve model safety through alignment, aligned models
can still have their safety protections undermined by subsequent fine-tuning -
even when the additional training data appears benign. In this paper, we
empirically demonstrate that this vulnerability stems from the sensitivity of
safety-critical low-rank subspaces in LLM parameters to fine-tuning. Building
on this insight, we propose a novel training-free method, termed Low-Rank
Extrapolation (LoX), to enhance safety robustness by extrapolating the safety
subspace of an aligned LLM. Our experimental results confirm the effectiveness
of LoX, demonstrating significant improvements in robustness against both
benign and malicious fine-tuning attacks while preserving the model's
adaptability to new tasks. For instance, LoX leads to 11% to 54% absolute
reductions in attack success rates (ASR) facing benign or malicious fine-tuning
attacks. By investigating the ASR landscape of parameters, we attribute the
success of LoX to that the extrapolation moves LLM parameters to a flatter
zone, thereby less sensitive to perturbations. The code is available at
github.com/VITA-Group/LoX.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Model to Classroom: Evaluating Generated MCQs for Portuguese with
  Narrative and Difficulty Concerns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernardo Leite, Henrique Lopes Cardoso, Pedro Pinto, Abel Ferreira, Luís Abreu, Isabel Rangel, Sandra Monteiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While MCQs are valuable for learning and evaluation, manually creating them
with varying difficulty levels and targeted reading skills remains a
time-consuming and costly task. Recent advances in generative AI provide an
opportunity to automate MCQ generation efficiently. However, assessing the
actual quality and reliability of generated MCQs has received limited attention
-- particularly regarding cases where generation fails. This aspect becomes
particularly important when the generated MCQs are meant to be applied in
real-world settings. Additionally, most MCQ generation studies focus on
English, leaving other languages underexplored. This paper investigates the
capabilities of current generative models in producing MCQs for reading
comprehension in Portuguese, a morphologically rich language. Our study focuses
on generating MCQs that align with curriculum-relevant narrative elements and
span different difficulty levels. We evaluate these MCQs through expert review
and by analyzing the psychometric properties extracted from student responses
to assess their suitability for elementary school students. Our results show
that current models can generate MCQs of comparable quality to human-authored
ones. However, we identify issues related to semantic clarity and
answerability. Also, challenges remain in generating distractors that engage
students and meet established criteria for high-quality MCQ option design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a preprint version of the manuscript currently under review
  at an international journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and
  Charts <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Negar Foroutan, Angelika Romanou, Matin Ansaripour, Julian Martin Eisenschlos, Karl Aberer, Rémi Lebret
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Documents are fundamental to preserving and disseminating information, often
incorporating complex layouts, tables, and charts that pose significant
challenges for automatic document understanding (DU). While vision-language
large models (VLLMs) have demonstrated improvements across various tasks, their
effectiveness in processing long-context vision inputs remains unclear. This
paper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice
questions (MCQs) designed to evaluate cross-modal reasoning over tables and
charts extracted from 4,000 Wikipedia pages spanning seven distinct topics.
Unlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring
models to synthesize information from multiple modalities. We evaluate 12
state-of-the-art vision-language models, revealing that while proprietary
models achieve ~70% accuracy when provided with direct context, their
performance deteriorates significantly when retrieval from long documents is
required. Among these, GPT-4-o is the only model exceeding 50% accuracy in this
setting, whereas open-source models perform considerably worse, with a maximum
accuracy of 27%. These findings underscore the challenges of long-context,
multi-modal reasoning and establish WikiMixQA as a crucial benchmark for
advancing document understanding research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiscoSG: Towards Dis<span class="highlight-title">course</span>-Level Text Scene Graph Parsing through
  Iterative Graph Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoqing Lin, Chong Teng, Fei Li, Donghong Ji, Lizhen Qu, Zhuang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) now generate discourse-level, multi-sentence
visual descriptions, challenging text scene graph parsers originally designed
for single-sentence caption-to-graph mapping. Current approaches typically
merge sentence-level parsing outputs for discourse input, often missing
phenomena like cross-sentence coreference, resulting in fragmented graphs and
degraded downstream VLM task performance. To address this, we introduce a new
task, Discourse-level text Scene Graph parsing (DiscoSG), supported by our
dataset DiscoSG-DS, which comprises 400 expert-annotated and 8,430 synthesised
multi-sentence caption-graph pairs for images. Each caption averages 9
sentences, and each graph contains at least 3 times more triples than those in
existing datasets. While fine-tuning large PLMs (i.e., GPT-4) on DiscoSG-DS
improves SPICE by approximately 48% over the best sentence-merging baseline,
high inference cost and restrictive licensing hinder its open-source use, and
smaller fine-tuned PLMs struggle with complex graphs. We propose
DiscoSG-Refiner, which drafts a base graph using one small PLM, then employs a
second PLM to iteratively propose graph edits, reducing full-graph generation
overhead. Using two Flan-T5-Base models, DiscoSG-Refiner still improves SPICE
by approximately 30% over the best baseline while achieving 86 times faster
inference than GPT-4. It also consistently improves downstream VLM tasks like
discourse-level caption evaluation and hallucination detection. Code and data
are available at: https://github.com/ShaoqLin/DiscoSG
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SciVer: Evaluating Foundation Models for Multimodal Scientific Claim
  Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengye Wang, Yifei Shen, Zexi Kuang, Arman Cohan, Yilun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SciVer, the first benchmark specifically designed to evaluate
the ability of foundation models to verify claims within a multimodal
scientific context. SciVer consists of 3,000 expert-annotated examples over
1,113 scientific papers, covering four subsets, each representing a common
reasoning type in multimodal scientific claim verification. To enable
fine-grained evaluation, each example includes expert-annotated supporting
evidence. We assess the performance of 21 state-of-the-art multimodal
foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and
Qwen2.5-VL. Our experiment reveals a substantial performance gap between these
models and human experts on SciVer. Through an in-depth analysis of
retrieval-augmented generation (RAG), and human-conducted error evaluations, we
identify critical limitations in current open-source models, offering key
insights to advance models' comprehension and reasoning in multimodal
scientific literature tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gender Inclusivity Fairness Index (GIFI): A Multilevel Framework for
  Evaluating Gender Diversity in Large Language Models <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyang Shan, Emily Ruth Diana, Jiawei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a comprehensive evaluation of gender fairness in large language
models (LLMs), focusing on their ability to handle both binary and non-binary
genders. While previous studies primarily focus on binary gender distinctions,
we introduce the Gender Inclusivity Fairness Index (GIFI), a novel and
comprehensive metric that quantifies the diverse gender inclusivity of LLMs.
GIFI consists of a wide range of evaluations at different levels, from simply
probing the model with respect to provided gender pronouns to testing various
aspects of model generation and cognitive behaviors under different gender
assumptions, revealing biases associated with varying gender identifiers. We
conduct extensive evaluations with GIFI on 22 prominent open-source and
proprietary LLMs of varying sizes and capabilities, discovering significant
variations in LLMs' gender inclusivity. Our study highlights the importance of
improving LLMs' inclusivity, providing a critical benchmark for future
advancements in gender fairness in generative models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PredGen: Accelerated Inference of Large Language Models through
  Input-Time Speculation for Real-Time Speech Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shufan Li, Aditya Grover
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are widely used in real-time voice chat
applications, typically in combination with text-to-speech (TTS) systems to
generate audio responses. However, their large size often leads to noticeable
latency between the end of user input and the start of audio output, resulting
in suboptimal user experiences. This latency is particularly evident when LLMs
are deployed as single-user voice assistants on consumer-grade hardware with
limited computing capacity. We discovered that this latency is primarily
dominated by the time it takes for the LLMs to generate the first sentence,
which is required as input by the TTS systems that synthesize audio responses
on a sentence-by-sentence basis. To address this bottleneck, we propose
Predictive Generation (PredGen), a novel framework that mitigates-or even
eliminates-this delay through speculative decoding at input time. PredGen
generates candidate responses while the user is still speaking, enabling the
system to begin TTS processing with minimal delay. Simulated experiments on the
Lmsys and MT-Bench datasets show that the proposed method can effectively
reduce the latency by around 2x across a wide range of use cases, while
incurring only minimal additional computation cost at input time-computation
that would otherwise go unused.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages,4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximating Language Model Training Data from Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John X. Morris, Junjie Oscar Yin, Woojeong Kim, Vitaly Shmatikov, Alexander M. Rush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern language models often have open weights but closed training data. We
formalize the problem of data approximation from model weights and propose
several baselines and metrics. We develop a gradient-based approach that
selects the highest-matching data from a large public text corpus and show its
effectiveness at recovering useful data given only weights of the original and
finetuned models. Even when none of the true training data is known, our method
is able to locate a small subset of public Web documents can be used to train a
model to close to the original model performance given models trained for both
classification and supervised-finetuning. On the AG News classification task,
our method improves performance from 65% (using randomly selected data) to 80%,
approaching the expert benchmark of 88%. When applied to a model trained with
SFT on MSMARCO web documents, our method reduces perplexity from 3.3 to 2.3,
compared to an expert LLAMA model's perplexity of 2.0.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RATTENTION: Towards the Minimal Sliding Window Size in Local-Global
  Attention Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bailin Wang, Chang Lan, Chong Wang, Ruoming Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Local-global attention models have recently emerged as compelling
alternatives to standard Transformers, promising improvements in both training
and inference efficiency. However, the crucial choice of window size presents a
Pareto tradeoff: larger windows maintain performance akin to full attention but
offer minimal efficiency gains in short-context scenarios, while smaller
windows can lead to performance degradation. Current models, such as Gemma2 and
Mistral, adopt conservative window sizes (e.g., 4096 out of an 8192 pretraining
length) to preserve performance. This work investigates strategies to shift
this Pareto frontier, enabling local-global models to achieve efficiency gains
even in short-context regimes. Our core motivation is to address the intrinsic
limitation of local attention -- its complete disregard for tokens outside the
defined window. We explore RATTENTION, a variant of local attention integrated
with a specialized linear attention mechanism designed to capture information
from these out-of-window tokens. Pretraining experiments at the 3B and 12B
scales demonstrate that RATTENTION achieves a superior Pareto tradeoff between
performance and efficiency. As a sweet spot, RATTENTION with a window size of
just 512 consistently matches the performance of full-attention models across
diverse settings. Furthermore, the recurrent nature inherent in the linear
attention component of RATTENTION contributes to enhanced long-context
performance, as validated on the RULER benchmark. Crucially, these improvements
do not compromise training efficiency; thanks to a specialized kernel
implementation and the reduced window size, RATTENTION maintains training
speeds comparable to existing state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Capturing Polysemanticity with PRISM: A Multi-Concept Feature
  Description Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Kopf, Nils Feldhus, Kirill Bykov, Philine Lou Bommer, Anna Hedström, Marina M. -C. Höhne, Oliver Eberle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated interpretability research aims to identify concepts encoded in
neural network features to enhance human understanding of model behavior.
Current feature description methods face two critical challenges: limited
robustness and the flawed assumption that each neuron encodes only a single
concept (monosemanticity), despite growing evidence that neurons are often
polysemantic. This assumption restricts the expressiveness of feature
descriptions and limits their ability to capture the full range of behaviors
encoded in model internals. To address this, we introduce Polysemantic FeatuRe
Identification and Scoring Method (PRISM), a novel framework that captures the
inherent complexity of neural network features. Unlike prior approaches that
assign a single description per feature, PRISM provides more nuanced
descriptions for both polysemantic and monosemantic features. We apply PRISM to
language models and, through extensive benchmarking against existing methods,
demonstrate that our approach produces more accurate and faithful feature
descriptions, improving both overall description quality (via a description
score) and the ability to capture distinct concepts when polysemanticity is
present (via a polysemanticity score).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lessons from Training Grounded LLMs with Verifiable Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shang Hong Sim, Tej Deep Pala, Vernon Toh, Hai Leong Chieu, Amir Zadeh, Chuan Li, Navonil Majumder, Soujanya Poria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating grounded and trustworthy responses remains a key challenge for
large language models (LLMs). While retrieval-augmented generation (RAG) with
citation-based grounding holds promise, instruction-tuned models frequently
fail even in straightforward scenarios: missing explicitly stated answers,
citing incorrectly, or refusing when evidence is available. In this work, we
explore how reinforcement learning (RL) and internal reasoning can enhance
grounding in LLMs. We use the GRPO (Group Relative Policy Optimization) method
to train models using verifiable outcome-based rewards targeting answer
correctness, citation sufficiency, and refusal quality, without requiring gold
reasoning traces or expensive annotations. Through comprehensive experiments
across ASQA, QAMPARI, ELI5, and ExpertQA we show that reasoning-augmented
models significantly outperform instruction-only variants, especially in
handling unanswerable queries and generating well-cited responses. A two-stage
training setup, first optimizing answer and citation behavior and then refusal,
further improves grounding by stabilizing the learning signal. Additionally, we
revisit instruction tuning via GPT-4 distillation and find that combining it
with GRPO enhances performance on long-form, generative QA tasks. Overall, our
findings highlight the value of reasoning, stage-wise optimization, and
outcome-driven RL for building more verifiable and reliable LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Hyperbole and Metaphor Detection with Their Bidirectional
  Dynamic Interaction and Emotion Knowledge <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zheng, Sihang Wang, Hao Fei, Zuquan Peng, Fei Li, Jianming Fu, Chong Teng, Donghong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-based hyperbole and metaphor detection are of great significance for
natural language processing (NLP) tasks. However, due to their semantic
obscurity and expressive diversity, it is rather challenging to identify them.
Existing methods mostly focus on superficial text features, ignoring the
associations of hyperbole and metaphor as well as the effect of implicit
emotion on perceiving these rhetorical devices. To implement these hypotheses,
we propose an emotion-guided hyperbole and metaphor detection framework based
on bidirectional dynamic interaction (EmoBi). Firstly, the emotion analysis
module deeply mines the emotion connotations behind hyperbole and metaphor.
Next, the emotion-based domain mapping module identifies the target and source
domains to gain a deeper understanding of the implicit meanings of hyperbole
and metaphor. Finally, the bidirectional dynamic interaction module enables the
mutual promotion between hyperbole and metaphor. Meanwhile, a verification
mechanism is designed to ensure detection accuracy and reliability. Experiments
show that EmoBi outperforms all baseline methods on four datasets.
Specifically, compared to the current SoTA, the F1 score increased by 28.1% for
hyperbole detection on the TroFi dataset and 23.1% for metaphor detection on
the HYPO-L dataset. These results, underpinned by in-depth analyses, underscore
the effectiveness and potential of our approach for advancing hyperbole and
metaphor detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPARE: Single-Pass Annotation with Reference-Guided Evaluation for
  Automatic Process Supervision and Reward Modelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Imbesat Hassan Rizvi, Xiaodan Zhu, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process or step-wise supervision has played a crucial role in advancing
complex multi-step reasoning capabilities of Large Language Models (LLMs).
However, efficient, high-quality automated process annotation remains a
significant challenge. To address this, we introduce Single-Pass Annotation
with Reference-Guided Evaluation (SPARE), a novel structured framework that
enables single-pass, per-step annotation by aligning each solution step to one
or multiple steps in a reference solution, accompanied by explicit reasoning
for evaluation. We show that reference-guided step-level evaluation effectively
facilitates process supervision on four datasets spanning three domains:
mathematical reasoning, multi-hop compositional question answering, and spatial
reasoning. We demonstrate that SPARE, when compared to baselines, improves
reasoning performance when used for: (1) fine-tuning models in an offline RL
setup for inference-time greedy-decoding, and (2) training reward models for
ranking/aggregating multiple LLM-generated outputs. Additionally, SPARE
achieves competitive performance on challenging mathematical datasets while
offering 2.6 times greater efficiency, requiring only 38% of the runtime,
compared to tree search-based automatic annotation. The codebase, along with a
trained SPARE-PRM model, is publicly released to facilitate further research
and reproducibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages main content, 4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-Informed Grounding Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunji Lee, Seunghyun Yoon, Yunjae Won, Hanseok Oh, Geewook Kim, Trung Bui, Franck Dernoncourt, Elias Stengel-Eskin, Mohit Bansal, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are often supplemented with external knowledge
to provide information not encoded in their parameters or to reduce
hallucination. In such cases, we expect the model to generate responses by
grounding its response in the provided external context. However, prior work
has shown that simply appending context at inference time does not ensure
grounded generation. To address this, we propose Context-INformed Grounding
Supervision (CINGS), a post-training supervision in which the model is trained
with relevant context prepended to the response, while computing the loss only
over the response tokens and masking out the context. Our experiments
demonstrate that models trained with CINGS exhibit stronger grounding in both
textual and visual domains compared to standard instruction-tuned models. In
the text domain, CINGS outperforms other training methods across 11
information-seeking datasets and is complementary to inference-time grounding
techniques. In the vision-language domain, replacing a vision-language model's
LLM backbone with a CINGS-trained model reduces hallucinations across four
benchmarks and maintains factual consistency throughout the generated response.
This improved grounding comes without degradation in general downstream
performance. Finally, we analyze the mechanism underlying the enhanced
grounding in CINGS and find that it induces a shift in the model's prior
knowledge and behavior, implicitly encouraging greater reliance on the external
context.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Factorized RVQ-GAN For Disentangled Speech Tokenization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sameer Khurana, Dominik Klement, Antoine Laurent, Dominik Bobos, Juraj Novosad, Peter Gazdik, Ellen Zhang, Zili Huang, Amir Hussein, Ricard Marxer, Yoshiki Masuyama, Ryo Aihara, Chiori Hori, Francois G. Germain, Gordon Wichern, Jonathan Le Roux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Hierarchical Audio Codec (HAC), a unified neural speech codec that
factorizes its bottleneck into three linguistic levels-acoustic, phonetic, and
lexical-within a single model. HAC leverages two knowledge distillation
objectives: one from a pre-trained speech encoder (HuBERT) for phoneme-level
structure, and another from a text-based encoder (LaBSE) for lexical cues.
Experiments on English and multilingual data show that HAC's factorized
bottleneck yields disentangled token sets: one aligns with phonemes, while
another captures word-level semantics. Quantitative evaluations confirm that
HAC tokens preserve naturalness and provide interpretable linguistic
information, outperforming single-level baselines in both disentanglement and
reconstruction quality. These findings underscore HAC's potential as a unified
discrete speech representation, bridging acoustic detail and lexical meaning
for downstream speech generation and understanding tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinnuo Xu, Rachel Lawrence, Kshitij Dubey, Atharva Pandey, Risa Ueno, Fabian Falck, Aditya V. Nori, Rahul Sharma, Amit Sharma, Javier Gonzalez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Large Language Models (LLMs) have reported high accuracy on reasoning
benchmarks. However, it is still unclear whether the observed results arise
from true reasoning or from statistical recall of the training set. Inspired by
the ladder of causation (Pearl, 2009) and its three levels (associations,
interventions and counterfactuals), this paper introduces RE-IMAGINE, a
framework to characterize a hierarchy of reasoning ability in LLMs, alongside
an automated pipeline to generate problem variations at different levels of the
hierarchy. By altering problems in an intermediate symbolic representation,
RE-IMAGINE generates arbitrarily many problems that are not solvable using
memorization alone. Moreover, the framework is general and can work across
reasoning domains, including math, code, and logic. We demonstrate our
framework on four widely-used benchmarks to evaluate several families of LLMs,
and observe reductions in performance when the models are queried with problem
variations. These assessments indicate a degree of reliance on statistical
recall for past performance, and open the door to further research targeting
skills across the reasoning hierarchy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Agent</span>GroupChat-V2: Divide-and-Conquer Is What LLM-Based <span class="highlight-title">Multi-Agent</span>
  System Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouhong Gu, Xiaoxuan Zhu, Yin Cai, Hao Shen, Xingzhou Chen, Qingyi Wang, Jialin Li, Xiaoran Shi, Haoran Guo, Wenxuan Huang, Hongwei Feng, Yanghua Xiao, Zheyu Ye, Yao Hu, Shaosheng Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model based multi-agent systems have demonstrated significant
potential in social simulation and complex task resolution domains. However,
current frameworks face critical challenges in system architecture design,
cross-domain generalizability, and performance guarantees, particularly as task
complexity and number of agents increases. We introduces AgentGroupChat-V2, a
novel framework addressing these challenges through three core innovations: (1)
a divide-and-conquer fully parallel architecture that decomposes user queries
into hierarchical task forest structures enabling dependency management and
distributed concurrent processing. (2) an adaptive collaboration engine that
dynamically selects heterogeneous LLM combinations and interaction modes based
on task characteristics. (3) agent organization optimization strategies
combining divide-and-conquer approaches for efficient problem decomposition.
Extensive experiments demonstrate AgentGroupChat-V2's superior performance
across diverse domains, achieving 91.50% accuracy on GSM8K (exceeding the best
baseline by 5.6 percentage points), 30.4% accuracy on competition-level AIME
(nearly doubling other methods), and 79.20% pass@1 on HumanEval. Performance
advantages become increasingly pronounced with higher task difficulty,
particularly on Level 5 MATH problems where improvements exceed 11 percentage
points compared to state-of-the-art baselines. These results confirm that
AgentGroupChat-V2 provides a comprehensive solution for building efficient,
general-purpose LLM multi-agent systems with significant advantages in complex
reasoning scenarios. Code is available at
https://github.com/MikeGu721/AgentGroupChat-V2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding GUI <span class="highlight-title">Agent</span> Localization Biases through Logit Sharpness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Tao, Yiwei Wang, Yujun Cai, Zhicheng Yang, Jing Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have enabled GUI agents to interact
with operating systems by grounding language into spatial actions. Despite
their promising performance, these models frequently exhibit
hallucinations-systematic localization errors that compromise reliability. We
propose a fine-grained evaluation framework that categorizes model predictions
into four distinct types, revealing nuanced failure modes beyond traditional
accuracy metrics. To better quantify model uncertainty, we introduce the Peak
Sharpness Score (PSS), a metric that evaluates the alignment between semantic
continuity and logits distribution in coordinate prediction. Building on this
insight, we further propose Context-Aware Cropping, a training-free technique
that improves model performance by adaptively refining input context. Extensive
experiments demonstrate that our framework and methods provide actionable
insights and enhance the interpretability and robustness of GUI agent behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Targeted Lexical Injection: Unlocking Latent Cross-Lingual Alignment in
  Lugha-Llama via Early-Layer LoRA Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stanley Ngugi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities, yet
their performance in low-resource languages (LRLs), such as Swahili, often lags
due to data scarcity and underrepresentation in pre-training. A key challenge
is achieving robust cross-lingual lexical alignment, crucial for tasks like
translation and cross-lingual information retrieval. This paper introduces
Targeted Lexical Injection (TLI), a novel and efficient fine-tuning approach.
We first demonstrate that Lugha-Llama-8B-wura, a Swahili-centric LLM, exhibits
strong, near-perfect lexical alignment for Swahili-English word pairs in its
early internal layers (specifically Layer 2, with ~0.99998 average cosine
similarity based on a pilot study), a capability not fully reflected in its
final output representations (baseline ~0.32 similarity on our evaluation set).
TLI leverages this insight by using Low-Rank Adaptation (LoRA) and a
contrastive learning objective to fine-tune the model, specifically targeting
embeddings from this empirically identified optimal early layer. Our
experiments show that TLI significantly improves the output-level lexical
alignment for 623 trained Swahili-English word pairs, increasing average cosine
similarity from 0.3211 to 0.4113 (+28.08%, p < 1.33 x 10^-240). More
importantly, these improvements generalize remarkably well to 63 unseen control
word pairs, with similarity increasing from 0.3143 to 0.4033 (+28.32%, p < 7.17
x 10^-27). These findings suggest TLI enhances the model's ability to preserve
and propagate its inherent early-layer cross-lingual knowledge, offering a
parameter-efficient and effective strategy for improving lexical alignment in
LRL-focused LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures, 2 tables. Research on parameter-efficient
  fine-tuning (PEFT) for low-resource languages (Swahili). Investigates
  cross-lingual lexical alignment in Lugha-Llama using LoRA and contrastive
  learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COSMMIC: Comment-Sensitive Multimodal Multilingual Indian Corpus for
  Summarization and Headline Generation <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghvendra Kumar, S. A. Mohammed Salman, Aryan Sahu, Tridib Nandi, Pragathi Y. P., Sriparna Saha, Jose G. Moreno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite progress in comment-aware multimodal and multilingual summarization
for English and Chinese, research in Indian languages remains limited. This
study addresses this gap by introducing COSMMIC, a pioneering comment-sensitive
multimodal, multilingual dataset featuring nine major Indian languages. COSMMIC
comprises 4,959 article-image pairs and 24,484 reader comments, with
ground-truth summaries available in all included languages. Our approach
enhances summaries by integrating reader insights and feedback. We explore
summarization and headline generation across four configurations: (1) using
article text alone, (2) incorporating user comments, (3) utilizing images, and
(4) combining text, comments, and images. To assess the dataset's
effectiveness, we employ state-of-the-art language models such as LLama3 and
GPT-4. We conduct a comprehensive study to evaluate different component
combinations, including identifying supportive comments, filtering out noise
using a dedicated comment classifier using IndicBERT, and extracting valuable
insights from images with a multilingual CLIP-based classifier. This helps
determine the most effective configurations for natural language generation
(NLG) tasks. Unlike many existing datasets that are either text-only or lack
user comments in multimodal settings, COSMMIC uniquely integrates text, images,
and user feedback. This holistic approach bridges gaps in Indian language
resources, advancing NLP research and fostering inclusivity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 MAINs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SANSKRITI: A Comprehensive Benchmark for Evaluating Language Models'
  Knowledge of Indian Culture <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arijit Maji, Raghvendra Kumar, Akash Ghosh,  Anushka, Sriparna Saha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LMs) are indispensable tools shaping modern workflows, but
their global effectiveness depends on understanding local socio-cultural
contexts. To address this, we introduce SANSKRITI, a benchmark designed to
evaluate language models' comprehension of India's rich cultural diversity.
Comprising 21,853 meticulously curated question-answer pairs spanning 28 states
and 8 union territories, SANSKRITI is the largest dataset for testing Indian
cultural knowledge. It covers sixteen key attributes of Indian culture: rituals
and ceremonies, history, tourism, cuisine, dance and music, costume, language,
art, festivals, religion, medicine, transport, sports, nightlife, and
personalities, providing a comprehensive representation of India's cultural
tapestry. We evaluate SANSKRITI on leading Large Language Models (LLMs), Indic
Language Models (ILMs), and Small Language Models (SLMs), revealing significant
disparities in their ability to handle culturally nuanced queries, with many
models struggling in region-specific contexts. By offering an extensive,
culturally rich, and diverse dataset, SANSKRITI sets a new standard for
assessing and improving the cultural understanding of LMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeVisE: Behavioral Testing of Medical Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camila Zurdo Tagliabue, Heloisa Oss Boll, Aykut Erdem, Erkut Erdem, Iacer Calixto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly used in clinical decision
support, yet current evaluation methods often fail to distinguish genuine
medical reasoning from superficial patterns. We introduce DeVisE (Demographics
and Vital signs Evaluation), a behavioral testing framework for probing
fine-grained clinical understanding. We construct a dataset of ICU discharge
notes from MIMIC-IV, generating both raw (real-world) and template-based
(synthetic) versions with controlled single-variable counterfactuals targeting
demographic (age, gender, ethnicity) and vital sign attributes. We evaluate
five LLMs spanning general-purpose and medically fine-tuned variants, under
both zero-shot and fine-tuned settings. We assess model behavior via (1)
input-level sensitivity - how counterfactuals alter the likelihood of a note;
and (2) downstream reasoning - how they affect predicted hospital
length-of-stay. Our results show that zero-shot models exhibit more coherent
counterfactual reasoning patterns, while fine-tuned models tend to be more
stable yet less responsive to clinically meaningful changes. Notably,
demographic factors subtly but consistently influence outputs, emphasizing the
importance of fairness-aware evaluation. This work highlights the utility of
behavioral testing in exposing the reasoning strategies of clinical LLMs and
informing the design of safer, more transparent medical AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When and How Unlabeled Data Provably Improve In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingcong Li, Xiangyu Chang, Muti Kara, Xiaofeng Liu, Amit Roy-Chowdhury, Samet Oymak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research shows that in-context learning (ICL) can be effective even
when demonstrations have missing or incorrect labels. To shed light on this
capability, we examine a canonical setting where the demonstrations are drawn
according to a binary Gaussian mixture model (GMM) and a certain fraction of
the demonstrations have missing labels. We provide a comprehensive theoretical
study to show that: (1) The loss landscape of one-layer linear attention models
recover the optimal fully-supervised estimator but completely fail to exploit
unlabeled data; (2) In contrast, multilayer or looped transformers can
effectively leverage unlabeled data by implicitly constructing estimators of
the form $\sum_{i\ge 0} a_i (X^\top X)^iX^\top y$ with $X$ and $y$ denoting
features and partially-observed labels (with missing entries set to zero). We
characterize the class of polynomials that can be expressed as a function of
depth and draw connections to Expectation Maximization, an iterative
pseudo-labeling algorithm commonly used in semi-supervised learning.
Importantly, the leading polynomial power is exponential in depth, so mild
amount of depth/looping suffices. As an application of theory, we propose
looping off-the-shelf tabular foundation models to enhance their
semi-supervision capabilities. Extensive evaluations on real-world datasets
show that our method significantly improves the semisupervised tabular learning
performance over the standard single pass inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConLID: Supervised Contrastive Learning for Low-Resource Language
  Identification <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Negar Foroutan, Jakhongir Saydaliev, Ye Eun Kim, Antoine Bosselut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language identification (LID) is a critical step in curating multilingual LLM
pretraining corpora from web crawls. While many studies on LID model training
focus on collecting diverse training data to improve performance, low-resource
languages -- often limited to single-domain data, such as the Bible -- continue
to perform poorly. To resolve these class imbalance and bias issues, we propose
a novel supervised contrastive learning (SCL) approach to learn
domain-invariant representations for low-resource languages. Through an
extensive analysis, we show that our approach improves LID performance on
out-of-domain data for low-resource languages by 3.2%, demonstrating its
effectiveness in enhancing LID models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EMNLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrestha Ghosh, Moritz Schneider, Carina Reinicke, Carsten Eickhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet,
their adoption in critical domains, such as clinical trial recruitment, remains
limited. As trials are designed in natural language and patient data is
represented as both structured and unstructured text, the task of matching
trials and patients benefits from knowledge aggregation and reasoning abilities
of LLMs. Classical approaches are trial-specific and LLMs with their ability to
consolidate distributed knowledge hold the potential to build a more general
solution. Yet recent applications of LLM-assisted methods rely on proprietary
models and weak evaluation benchmarks. In this survey, we are the first to
analyze the task of trial-patient matching and contextualize emerging LLM-based
approaches in clinical trial recruitment. We critically examine existing
benchmarks, approaches and evaluation frameworks, the challenges to adopting
LLM technologies in clinical research and exciting future directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Thunder-DeID: Accurate and Efficient De-identification Framework for
  Korean Court Judgments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungen Hahm, Heejin Kim, Gyuseong Lee, Hyunji Park, Jaejin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To ensure a balance between open access to justice and personal data
protection, the South Korean judiciary mandates the de-identification of court
judgments before they can be publicly disclosed. However, the current
de-identification process is inadequate for handling court judgments at scale
while adhering to strict legal requirements. Additionally, the legal
definitions and categorizations of personal identifiers are vague and not
well-suited for technical solutions. To tackle these challenges, we propose a
de-identification framework called Thunder-DeID, which aligns with relevant
laws and practices. Specifically, we (i) construct and release the first Korean
legal dataset containing annotated judgments along with corresponding lists of
entity mentions, (ii) introduce a systematic categorization of Personally
Identifiable Information (PII), and (iii) develop an end-to-end deep neural
network (DNN)-based de-identification pipeline. Our experimental results
demonstrate that our model achieves state-of-the-art performance in the
de-identification of court judgments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TopClustRAG at SIGIR 2025 LiveRAG Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juli Bakagianni, John Pavlopoulos, Aristidis Likas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present TopClustRAG, a retrieval-augmented generation (RAG) system
developed for the LiveRAG Challenge, which evaluates end-to-end question
answering over large-scale web corpora. Our system employs a hybrid retrieval
strategy combining sparse and dense indices, followed by K-Means clustering to
group semantically similar passages. Representative passages from each cluster
are used to construct cluster-specific prompts for a large language model
(LLM), generating intermediate answers that are filtered, reranked, and finally
synthesized into a single, comprehensive response. This multi-stage pipeline
enhances answer diversity, relevance, and faithfulness to retrieved evidence.
Evaluated on the FineWeb Sample-10BT dataset, TopClustRAG ranked 2nd in
faithfulness and 7th in correctness on the official leaderboard, demonstrating
the effectiveness of clustering-based context filtering and prompt aggregation
in large-scale RAG systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Research on Graph-Retrieval Augmented Generation Based on Historical
  Text Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Fan, Zhang Qi, Xing Wenqian, Liu Chang, Liu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article addresses domain knowledge gaps in general large language models
for historical text analysis in the context of computational humanities and
AIGC technology. We propose the Graph RAG framework, combining chain-of-thought
prompting, self-instruction generation, and process supervision to create a The
First Four Histories character relationship dataset with minimal manual
annotation. This dataset supports automated historical knowledge extraction,
reducing labor costs. In the graph-augmented generation phase, we introduce a
collaborative mechanism between knowledge graphs and retrieval-augmented
generation, improving the alignment of general models with historical
knowledge. Experiments show that the domain-specific model Xunzi-Qwen1.5-14B,
with Simplified Chinese input and chain-of-thought prompting, achieves optimal
performance in relation extraction (F1 = 0.68). The DeepSeek model integrated
with GraphRAG improves F1 by 11% (0.08-0.19) on the open-domain C-CLUE relation
extraction dataset, surpassing the F1 value of Xunzi-Qwen1.5-14B (0.12),
effectively alleviating hallucinations phenomenon, and improving
interpretability. This framework offers a low-resource solution for classical
text knowledge extraction, advancing historical knowledge services and
humanities research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lost in Variation? Evaluating NLI Performance in Basque and Spanish
  Geographical Variants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaione Bengoetxea, Itziar Gonzalez-Dios, Rodrigo Agerri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we evaluate the capacity of current language technologies to
understand Basque and Spanish language varieties. We use Natural Language
Inference (NLI) as a pivot task and introduce a novel, manually-curated
parallel dataset in Basque and Spanish, along with their respective variants.
Our empirical analysis of crosslingual and in-context learning experiments
using encoder-only and decoder-based Large Language Models (LLMs) shows a
performance drop when handling linguistic variation, especially in Basque.
Error analysis suggests that this decline is not due to lexical overlap, but
rather to the linguistic variation itself. Further ablation experiments
indicate that encoder-only models particularly struggle with Western Basque,
which aligns with linguistic theory that identifies peripheral dialects (e.g.,
Western) as more distant from the standard. All data and code are publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changli Tang, Yixuan Li, Yudong Yang, Jimin Zhuang, Guangzhi Sun, Wei Li, Zejun Ma, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Videos contain a wealth of information, and generating detailed and accurate
descriptions in natural language is a key aspect of video understanding. In
this paper, we present video-SALMONN 2, an advanced audio-visual large language
model (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with
paired audio) captioning through directed preference optimisation (DPO). We
propose new metrics to evaluate the completeness and accuracy of video
descriptions, which are optimised using DPO. To further improve training, we
propose a novel multi-round DPO (MrDPO) approach, which involves periodically
updating the DPO reference model, merging and re-initialising the LoRA module
as a proxy for parameter updates after each training round (1,000 steps), and
incorporating guidance from ground-truth video captions to stabilise the
process. Experimental results show that MrDPO significantly enhances
video-SALMONN 2's captioning accuracy, reducing the captioning error rates by
28\%. The final video-SALMONN 2 model, with just 7 billion parameters,
surpasses leading models such as GPT-4o and Gemini-1.5-Pro in video captioning
tasks, while maintaining highly competitive performance to the state-of-the-art
on widely used video question-answering benchmarks among models of similar
size. Codes are available at
\href{https://github.com/bytedance/video-SALMONN-2}{https://github.com/bytedance/video-SALMONN-2}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MinosEval: Distinguishing Factoid and Non-Factoid for Tailored
  Open-Ended QA Evaluation with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqi Fan, Yating Wang, Guandong Wang, Jie Zhai, Jingping Liu, Qi Ye, Tong Ruan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-ended question answering (QA) is a key task for evaluating the
capabilities of large language models (LLMs). Compared to closed-ended QA, it
demands longer answer statements, more nuanced reasoning processes, and diverse
expressions, making refined and interpretable automatic evaluation both crucial
and challenging. Traditional metrics like ROUGE and BERTScore struggle to
capture semantic similarities due to different patterns between model responses
and reference answers. Current LLM-based evaluation approaches, such as
pairwise or listwise comparisons of candidate answers, lack intuitive
interpretability. While pointwise scoring of each response provides some
descriptions, it fails to adapt across different question contents. Most
notably, existing methods overlook the distinction between factoid and
non-factoid questions. To address these challenges, we propose
\textbf{MinosEval}, a novel evaluation method that first distinguishes
open-ended questions and then ranks candidate answers using different
evaluation strategies. For factoid questions, it applies an adaptive key-point
scoring strategy, while for non-factoid questions, it uses an instance-aware
listwise ranking strategy. Experiments on multiple open-ended QA datasets,
including self-built ones with more candidate responses to complement community
resources, show that MinosEval better aligns with human annotations and offers
more interpretable results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning
  in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng He, Zijun Chen, Xinnian Liang, Tingting Ma, Yunqi Qiu, Shuangzhi Wu, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Reasoning Models (LRMs) trained with Long
Chain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domain
generalization capabilities. However, the underlying mechanisms supporting such
transfer remain poorly understood. We hypothesize that cross-domain
generalization arises from shared abstract reasoning prototypes -- fundamental
reasoning patterns that capture the essence of problems across domains. These
prototypes minimize the nuances of the representation, revealing that seemingly
diverse tasks are grounded in shared reasoning structures.Based on this
hypothesis, we propose ProtoReasoning, a framework that enhances the reasoning
ability of LLMs by leveraging scalable and verifiable prototypical
representations (Prolog for logical reasoning, PDDL for
planning).ProtoReasoning features: (1) an automated prototype construction
pipeline that transforms problems into corresponding prototype representations;
(2) a comprehensive verification system providing reliable feedback through
Prolog/PDDL interpreters; (3) the scalability to synthesize problems
arbitrarily within prototype space while ensuring correctness. Extensive
experiments show that ProtoReasoning achieves 4.7% improvement over baseline
models on logical reasoning (Enigmata-Eval), 6.3% improvement on planning
tasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics
(AIME24). Significantly, our ablation studies confirm that learning in
prototype space also demonstrates enhanced generalization to structurally
similar problems compared to training solely on natural language
representations, validating our hypothesis that reasoning prototypes serve as
the foundation for generalizable reasoning in large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Study of Task Adaptation Techniques of Large Language
  Models for Identifying Sustainable Development Goals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Cadeddu, Alessandro Chessa, Vincenzo De Leo, Gianni Fenu, Enrico Motta, Francesco Osborne, Diego Reforgiato Recupero, Angelo Salatino, Luca Secchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In 2012, the United Nations introduced 17 Sustainable Development Goals
(SDGs) aimed at creating a more sustainable and improved future by 2030.
However, tracking progress toward these goals is difficult because of the
extensive scale and complexity of the data involved. Text classification models
have become vital tools in this area, automating the analysis of vast amounts
of text from a variety of sources. Additionally, large language models (LLMs)
have recently proven indispensable for many natural language processing tasks,
including text classification, thanks to their ability to recognize complex
linguistic patterns and semantics. This study analyzes various proprietary and
open-source LLMs for a single-label, multi-class text classification task
focused on the SDGs. Then, it also evaluates the effectiveness of task
adaptation techniques (i.e., in-context learning approaches), namely Zero-Shot
and Few-Shot Learning, as well as Fine-Tuning within this domain. The results
reveal that smaller models, when optimized through prompt engineering, can
perform on par with larger models like OpenAI's GPT (Generative Pre-trained
Transformer).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emergence of Primacy and Recency Effect in Mamba: A Mechanistic Point of
  View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Cendekia Airlangga, Hilal AlQuabeh, Munachiso S Nwadike, Kentaro Inui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study memory in state-space language models using primacy and recency
effects as behavioral tools to uncover how information is retained and
forgotten over time. Applying structured recall tasks to the Mamba
architecture, we observe a consistent U-shaped accuracy profile, indicating
strong performance at the beginning and end of input sequences. We identify
three mechanisms that give rise to this pattern. First, long-term memory is
supported by a sparse subset of channels within the model's selective state
space block, which persistently encode early input tokens and are causally
linked to primacy effects. Second, short-term memory is governed by
delta-modulated recurrence: recent inputs receive more weight due to
exponential decay, but this recency advantage collapses when distractor items
are introduced, revealing a clear limit to memory depth. Third, we find that
memory allocation is dynamically modulated by semantic regularity: repeated
relations in the input sequence shift the delta gating behavior, increasing the
tendency to forget intermediate items. We validate these findings via targeted
ablations and input perturbations on two large-scale Mamba-based language
models: one with 1.4B and another with 7B parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anuradha Chopra, Abhinaba Roy, Dorien Herremans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detailed captions that accurately reflect the characteristics of a music
piece can enrich music databases and drive forward research in music AI. This
paper introduces a multi-task music captioning model, SonicVerse, that
integrates caption generation with auxiliary music feature detection tasks such
as key detection, vocals detection, and more, so as to directly capture both
low-level acoustic details as well as high-level musical attributes. The key
contribution is a projection-based architecture that transforms audio input
into language tokens, while simultaneously detecting music features through
dedicated auxiliary heads. The outputs of these heads are also projected into
language tokens, to enhance the captioning input. This framework not only
produces rich, descriptive captions for short music fragments but also directly
enables the generation of detailed time-informed descriptions for longer music
pieces, by chaining the outputs using a large-language model. To train the
model, we extended the MusicBench dataset by annotating it with music features
using MIRFLEX, a modular music feature extractor, resulting in paired audio,
captions and music feature data. Experimental results show that incorporating
features in this way improves the quality and detail of the generated captions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 2 figures, Accepted to AIMC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for
  Generative Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyeongje Cho, Yeonkyoun So, Chanwoo Park, Sangmin Lee, Sungmok Jung, Jaejin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Thunder-Tok, a new Korean tokenizer designed to reduce
token fertility without compromising model performance. Our approach uses a
rule-based pre-tokenization method that aligns with the linguistic structure of
the Korean language. We also create a seed vocabulary containing tokens that
resemble linguistic units and employ a branching entropy-based selection
algorithm. These techniques increase the average token length, thus lowering
fertility while preserving linguistic information. Experimental results
indicate that Thunder-Tok reduces fertility by approximately 10% (i.e., reduces
the number of tokens by 10%, improving the inference speed by 10%) compared to
BPE without compromising performance across various downstream tasks. These
findings demonstrate that our linguistically informed approach is effective and
practical for designing efficient tokenizers for language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling the One-to-Many Property in Open-Domain <span class="highlight-title">Dialogue</span> with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Yang Lee, Kong-Aik Lee, Woon-Seng Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby
multiple appropriate responses exist for a single dialogue context. Despite
prior research showing that modeling this property boosts response diversity,
most modern LLM-based dialogue agents do not explicitly do so. In this work, we
model the o2m property of OD in LLMs by decomposing OD generation into two key
tasks: Multi-Response Generation (MRG) and Preference-based Selection (PS),
which entail generating a set of n semantically and lexically diverse
high-quality responses for a given dialogue context, followed by selecting a
single response based on human preference, respectively. To facilitate MRG and
PS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the
o2m property by featuring multiple plausible responses for each context.
Leveraging o2mDial, we propose new in-context learning and instruction-tuning
strategies, as well as novel evaluation metrics for MRG, alongside a
model-based approach for PS. Empirical results demonstrate that applying the
proposed two-stage framework to smaller LLMs for OD generation enhances overall
response diversity while maintaining contextual coherence, improving response
quality by up to 90%, bringing them closer to the performance of larger models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CKD-EHR:Clinical Knowledge Distillation for Electronic Health Records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junke Wang, Hongshun Ling, Li Zhang, Longqian Zhang, Fang Wang, Yuan Gao, Zhi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Records (EHR)-based disease prediction models have
demonstrated significant clinical value in promoting precision medicine and
enabling early intervention. However, existing large language models face two
major challenges: insufficient representation of medical knowledge and low
efficiency in clinical deployment. To address these challenges, this study
proposes the CKD-EHR (Clinical Knowledge Distillation for EHR) framework, which
achieves efficient and accurate disease risk prediction through knowledge
distillation techniques. Specifically, the large language model Qwen2.5-7B is
first fine-tuned on medical knowledge-enhanced data to serve as the teacher
model.It then generates interpretable soft labels through a multi-granularity
attention distillation mechanism. Finally, the distilled knowledge is
transferred to a lightweight BERT student model. Experimental results show that
on the MIMIC-III dataset, CKD-EHR significantly outperforms the baseline
model:diagnostic accuracy is increased by 9%, F1-score is improved by 27%, and
a 22.2 times inference speedup is achieved. This innovative solution not only
greatly improves resource utilization efficiency but also significantly
enhances the accuracy and timeliness of diagnosis, providing a practical
technical approach for resource optimization in clinical settings. The code and
data for this research are available athttps://github.com/209506702/CKD_EHR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving <span class="highlight-title">Dialogue</span> Dis<span class="highlight-title">course</span> Parsing through Dis<span class="highlight-title">course</span>-aware Utterance
  Clarification <span class="chip">ACL2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaxin Fan, Peifeng Li, Qiaoming Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue discourse parsing aims to identify and analyze discourse relations
between the utterances within dialogues. However, linguistic features in
dialogues, such as omission and idiom, frequently introduce ambiguities that
obscure the intended discourse relations, posing significant challenges for
parsers. To address this issue, we propose a Discourse-aware Clarification
Module (DCM) to enhance the performance of the dialogue discourse parser. DCM
employs two distinct reasoning processes: clarification type reasoning and
discourse goal reasoning. The former analyzes linguistic features, while the
latter distinguishes the intended relation from the ambiguous one. Furthermore,
we introduce Contribution-aware Preference Optimization (CPO) to mitigate the
risk of erroneous clarifications, thereby reducing cascading errors. CPO
enables the parser to assess the contributions of the clarifications from DCM
and provide feedback to optimize the DCM, enhancing its adaptability and
alignment with the parser's requirements. Extensive experiments on the STAC and
Molweni datasets demonstrate that our approach effectively resolves ambiguities
and significantly outperforms the state-of-the-art (SOTA) baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL2025(main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning-Time Encoding Shapes Unlearning in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihan Wu, Konstantin Garov, Kamalika Chaudhuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) are increasingly deployed in the real world,
the ability to ``unlearn'', or remove specific pieces of knowledge post hoc,
has become essential for a variety of reasons ranging from privacy regulations
to correcting outdated or harmful content. Prior work has proposed unlearning
benchmarks and algorithms, and has typically assumed that the training process
and the target model are fixed. In this work, we empirically investigate how
learning-time choices in knowledge encoding impact the effectiveness of
unlearning factual knowledge. Our experiments reveal two key findings: (1)
learning with paraphrased descriptions improves unlearning performance and (2)
unlearning individual piece of knowledge from a chunk of text is challenging.
Our results suggest that learning-time knowledge encoding may play a central
role in enabling reliable post-hoc unlearning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongxia Li, Yapei Chang, Yuhang Zhou, Xiyang Wu, Zichao Liang, Yoo Yeon Sung, Jordan Lee Boyd-Graber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating open-ended long-form generation is challenging because it is hard
to define what clearly separates good from bad outputs. Existing methods often
miss key aspects like coherence, style, or relevance, or are biased by
pretraining data, making open-ended long-form evaluation an underexplored
problem. To address this gap, we propose PrefBERT, a scoring model for
evaluating open-ended long-form generation in GRPO and guiding its training
with distinct rewards for good and bad outputs. Trained on two response
evaluation datasets with diverse long-form styles and Likert-rated quality,
PrefBERT effectively supports GRPO by offering better semantic reward feedback
than traditional metrics ROUGE-L and BERTScore do. Through comprehensive
evaluations, including LLM-as-a-judge, human ratings, and qualitative analysis,
we show that PrefBERT, trained on multi-sentence and paragraph-length
responses, remains reliable across varied long passages and aligns well with
the verifiable rewards GRPO needs. Human evaluations confirm that using
PrefBERT as the reward signal to train policy models yields responses better
aligned with human preferences than those trained with traditional metrics. Our
code is available at https://github.com/zli12321/long_form_rl.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying economic narratives in large text corpora -- An integrated
  approach using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Schmidt, Kai-Robin Lange, Matthias Reccius, Henrik Müller, Michael Roos, Carsten Jentsch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As interest in economic narratives has grown in recent years, so has the
number of pipelines dedicated to extracting such narratives from texts.
Pipelines often employ a mix of state-of-the-art natural language processing
techniques, such as BERT, to tackle this task. While effective on foundational
linguistic operations essential for narrative extraction, such models lack the
deeper semantic understanding required to distinguish extracting economic
narratives from merely conducting classic tasks like Semantic Role Labeling.
Instead of relying on complex model pipelines, we evaluate the benefits of
Large Language Models (LLMs) by analyzing a corpus of Wall Street Journal and
New York Times newspaper articles about inflation. We apply a rigorous
narrative definition and compare GPT-4o outputs to gold-standard narratives
produced by expert annotators. Our results suggests that GPT-4o is capable of
extracting valid economic narratives in a structured format, but still falls
short of expert-level performance when handling complex documents and
narratives. Given the novelty of LLMs in economic research, we also provide
guidance for future work in economics and the social sciences that employs LLMs
to pursue similar objectives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>53 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying social isolation themes in NVDRS text narratives using topic
  modeling and text-classification methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Drew Walker, Swati Rajwal, Sudeshna Das, Snigdha Peddireddy, Abeed Sarker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social isolation and loneliness, which have been increasing in recent years
strongly contribute toward suicide rates. Although social isolation and
loneliness are not currently recorded within the US National Violent Death
Reporting System's (NVDRS) structured variables, natural language processing
(NLP) techniques can be used to identify these constructs in law enforcement
and coroner medical examiner narratives. Using topic modeling to generate
lexicon development and supervised learning classifiers, we developed
high-quality classifiers (average F1: .86, accuracy: .82). Evaluating over
300,000 suicides from 2002 to 2020, we identified 1,198 mentioning chronic
social isolation. Decedents had higher odds of chronic social isolation
classification if they were men (OR = 1.44; CI: 1.24, 1.69, p<.0001), gay (OR =
3.68; 1.97, 6.33, p<.0001), or were divorced (OR = 3.34; 2.68, 4.19, p<.0001).
We found significant predictors for other social isolation topics of recent or
impending divorce, child custody loss, eviction or recent move, and break-up.
Our methods can improve surveillance and prevention of social isolation and
loneliness in the United States.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 2 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An accurate and revised version of optical character recognition-based
  speech synthesis using LabVIEW 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15029v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15029v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prateek Mehta, Anasuya Patil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge extraction through sound is a distinctive property. Visually
impaired individuals often rely solely on Braille books and audio recordings
provided by NGOs. Due to limitations in these approaches, blind individuals
often cannot access books of their choice. Speech is a more effective mode of
communication than text for blind and visually impaired persons, as they can
easily respond to sounds. This paper presents the development of an accurate,
reliable, cost-effective, and user-friendly optical character recognition
(OCR)-based speech synthesis system. The OCR-based system has been implemented
using Laboratory Virtual Instrument Engineering Workbench (LabVIEW).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated
  Synthetic Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.16065v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.16065v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruijie Xi, He Ba, Hao Yuan, Rishu Agrawal, Yuxin Tian, Ruoyan Long, Arul Prakash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding-Based Retrieval (EBR) is an important technique in modern search
engines, enabling semantic match between search queries and relevant results.
However, search logging data on platforms like Facebook Marketplace lacks the
diversity and details needed for effective EBR model training, limiting the
models' ability to capture nuanced search patterns. To address this challenge,
we propose Aug2Search, an EBR-based framework leveraging synthetic data
generated by Generative AI (GenAI) models, in a multimodal and multitask
approach to optimize query-product relevance. This paper investigates the
capabilities of GenAI, particularly Large Language Models (LLMs), in generating
high-quality synthetic data, and analyzing its impact on enhancing EBR models.
We conducted experiments using eight Llama models and 100 million data points
from Facebook Marketplace logs. Our synthetic data generation follows three
strategies: (1) generate queries, (2) enhance product listings, and (3)
generate queries from enhanced listings. We train EBR models on three different
datasets: sampled engagement data or original data ((e.g., "Click" and "Listing
Interactions")), synthetic data, and a mixture of both engagement and synthetic
data to assess their performance across various training sets. Our findings
underscore the robustness of Llama models in producing synthetic queries and
listings with high coherence, relevance, and diversity, while maintaining low
levels of hallucination. Aug2Search achieves an improvement of up to 4% in
ROC_AUC with 100 million synthetic data samples, demonstrating the
effectiveness of our approach. Moreover, our experiments reveal that with the
same volume of training data, models trained exclusively on synthetic data
often outperform those trained on original data only or a mixture of original
and synthetic data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ J4R: Learning to Judge with Equivalent Initial State Group Relative
  Policy Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13346v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13346v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Austin Xu, Yilun Zhou, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To keep pace with the increasing pace of large language models (LLM)
development, model output evaluation has transitioned away from time-consuming
human evaluation to automatic evaluation, where LLMs themselves are tasked with
assessing and critiquing other model outputs. LLM-as-judge models are a class
of generative evaluators that excel in evaluating relatively simple domains,
like chat quality, but struggle in reasoning intensive domains where model
responses contain more substantive and challenging content. To remedy existing
judge shortcomings, we explore training judges with reinforcement learning
(RL). We make three key contributions: (1) We propose the Equivalent Initial
State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us
to train our judge to be robust to positional biases that arise in more complex
evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that
evaluates judges in diverse reasoning settings not covered by prior work. (3)
We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that
outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or
exceeding the performance of larger GRPO-trained judges on both JudgeBench and
ReasoningJudgeBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 4 figures, 6 tables. Updated with code and benchmark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Guide to Misinformation Detection Data and Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05060v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05060v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camille Thibault, Jacob-Junqi Tian, Gabrielle Peloquin-Skulski, Taylor Lynn Curtis, James Zhou, Florence Laflamme, Yuxiang Guan, Reihaneh Rabbany, Jean-François Godbout, Kellin Pelrine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Misinformation is a complex societal issue, and mitigating solutions are
difficult to create due to data deficiencies. To address this, we have curated
the largest collection of (mis)information datasets in the literature, totaling
75. From these, we evaluated the quality of 36 datasets that consist of
statements or claims, as well as the 9 datasets that consist of data in purely
paragraph form. We assess these datasets to identify those with solid
foundations for empirical work and those with flaws that could result in
misleading and non-generalizable results, such as spurious correlations, or
examples that are ambiguous or otherwise impossible to assess for veracity. We
find the latter issue is particularly severe and affects most datasets in the
literature. We further provide state-of-the-art baselines on all these
datasets, but show that regardless of label quality, categorical labels may no
longer give an accurate evaluation of detection model performance. Finally, we
propose and highlight Evaluation Quality Assurance (EQA) as a tool to guide the
field toward systemic solutions rather than inadvertently propagating issues in
evaluation. Overall, this guide aims to provide a roadmap for higher quality
data and better grounded evaluations, ultimately improving research in
misinformation detection. All datasets and other artifacts are available at
misinfo-datasets.complexdatalab.com.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09033v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09033v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhen Zhang, Tao Feng, Jiaxuan You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid emergence of diverse large language models (LLMs) has spurred the
development of LLM routers that assign user queries to the most suitable model.
However, existing LLM routers typically perform a single-round, one-to-one
mapping (\textit{i.e.}, assigning each query to a single model in isolation),
which limits their capability to tackle complex tasks that demand the
complementary strengths of multiple LLMs. In this paper, we present
\textbf{Router-R1}, a reinforcement learning (RL)-based framework that
formulates multi-LLM routing and aggregation as a sequential decision process.
Router-R1 instantiates the router itself as a capable LLM, leveraging its
reasoning ability to interleave "think" actions (internal deliberation) with
"route" actions (dynamic model invocation), and integrates each response into
its evolving context. To facilitate learning, we employ a lightweight
rule-based reward comprising format rewards, final outcome rewards, and a novel
cost reward for optimizing the balance between performance and cost, opening a
pathway toward enhancing performance-cost trade-offs via RL. Router-R1 also
conditions only on simple model descriptors such as pricing, latency, and
example performance, enabling strong generalization to unseen model selection.
Experiments on seven general and multi-hop QA benchmarks show that Router-R1
outperforms several strong baselines, achieving superior performance while
maintaining robust generalization and cost management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/ulab-uiuc/Router-R1. Models
  and Datasets are available at
  https://huggingface.co/collections/ulab-ai/router-r1-6851bbe099c7a56914b5db03</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lean Workbook: A large-scale Lean problem set formalized from natural
  language math problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03847v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03847v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huaiyuan Ying, Zijian Wu, Yihan Geng, Zheng Yuan, Dahua Lin, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have demonstrated impressive capabilities across
various natural language processing tasks, especially in solving mathematical
problems. However, large language models are not good at math theorem proving
using formal languages like Lean. A significant challenge in this area is the
scarcity of training data available in these formal languages. To address this
issue, we propose a novel pipeline that iteratively generates and filters
synthetic data to translate natural language mathematical problems into Lean 4
statements, and vice versa. Our results indicate that the synthetic data
pipeline can provide useful training data and improve the performance of LLMs
in translating and understanding complex mathematical problems and proofs. Our
final dataset contains about 57K formal-informal question pairs along with
searched proof from the math contest forum and 21 new IMO questions. We
open-source our code at https://github.com/InternLM/InternLM-Math and our data
at https://huggingface.co/datasets/InternLM/Lean-Workbook.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fractured Chain-of-Thought Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12992v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12992v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baohao Liao, Hanze Dong, Yuhui Xu, Doyen Sahoo, Christof Monz, Junnan Li, Caiming Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inference-time scaling techniques have significantly bolstered the reasoning
capabilities of large language models (LLMs) by harnessing additional
computational effort at inference without retraining. Similarly,
Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy
by generating rich intermediate reasoning trajectories, but these approaches
incur substantial token costs that impede their deployment in latency-sensitive
settings. In this work, we first show that truncated CoT, which stops reasoning
before completion and directly generates the final answer, often matches full
CoT sampling while using dramatically fewer tokens. Building on this insight,
we introduce Fractured Sampling, a unified inference-time strategy that
interpolates between full CoT and solution-only sampling along three orthogonal
axes: (1) the number of reasoning trajectories, (2) the number of final
solutions per trajectory, and (3) the depth at which reasoning traces are
truncated. Through extensive experiments on five diverse reasoning benchmarks
and several model scales, we demonstrate that Fractured Sampling consistently
achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling
gains in Pass@k versus token budget. Our analysis reveals how to allocate
computation across these dimensions to maximize performance, paving the way for
more efficient and scalable LLM reasoning. Code is available at
https://github.com/BaohaoLiao/frac-cot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How much do language models memorize? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24832v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24832v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John X. Morris, Chawin Sitawarin, Chuan Guo, Narine Kokhlikyan, G. Edward Suh, Alexander M. Rush, Kamalika Chaudhuri, Saeed Mahloujifar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new method for estimating how much a model knows about a
datapoint and use it to measure the capacity of modern language models. Prior
studies of language model memorization have struggled to disentangle
memorization from generalization. We formally separate memorization into two
components: unintended memorization, the information a model contains about a
specific dataset, and generalization, the information a model contains about
the true data-generation process. When we completely eliminate generalization,
we can compute the total memorization, which provides an estimate of model
capacity: our measurements estimate that GPT-style models have a capacity of
approximately 3.6 bits per parameter. We train language models on datasets of
increasing size and observe that models memorize until their capacity fills, at
which point "grokking" begins, and unintended memorization decreases as models
begin to generalize. We train hundreds of transformer language models ranging
from $500K$ to $1.5B$ parameters and produce a series of scaling laws relating
model capacity and data size to membership inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pap2Pat: Benchmarking Outline-Guided Long-Text Patent Generation with
  Patent-Paper Pairs <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07009v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07009v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Knappich, Simon Razniewski, Anna Hätty, Annemarie Friedrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dealing with long and highly complex technical text is a challenge for Large
Language Models (LLMs), which still have to unfold their potential in
supporting expensive and timeintensive processes like patent drafting. Within
patents, the description constitutes more than 90% of the document on average.
Yet, its automatic generation remains understudied. When drafting patent
applications, patent attorneys typically receive invention reports (IRs), which
are usually confidential, hindering research on LLM-supported patent drafting.
Often, prepublication research papers serve as IRs. We leverage this duality to
build PAP2PAT, an open and realistic benchmark for patent drafting consisting
of 1.8k patent-paper pairs describing the same inventions. To address the
complex longdocument patent generation task, we propose chunk-based
outline-guided generation using the research paper as invention specification.
Our extensive evaluation using PAP2PAT and a human case study show that LLMs
can effectively leverage information from the paper, but still struggle to
provide the necessary level of detail. Fine-tuning leads to more patent-style
language, but also to more hallucination. We release our data and code
https://github.com/boschresearch/Pap2Pat.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RadioRAG: Online Retrieval-augmented Generation for Radiology Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15621v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15621v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroosh Tayebi Arasteh, Mahshad Lotfinia, Keno Bressem, Robert Siepmann, Lisa Adams, Dyke Ferber, Christiane Kuhl, Jakob Nikolas Kather, Sven Nebelung, Daniel Truhn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often generate outdated or inaccurate
information based on static training datasets. Retrieval-augmented generation
(RAG) mitigates this by integrating outside data sources. While previous RAG
systems used pre-assembled, fixed databases with limited flexibility, we have
developed Radiology RAG (RadioRAG), an end-to-end framework that retrieves data
from authoritative radiologic online sources in real-time. We evaluate the
diagnostic accuracy of various LLMs when answering radiology-specific questions
with and without access to additional online information via RAG. Using 80
questions from the RSNA Case Collection across radiologic subspecialties and 24
additional expert-curated questions with reference standard answers, LLMs
(GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B and 70B]) were
prompted with and without RadioRAG in a zero-shot inference scenario RadioRAG
retrieved context-specific information from Radiopaedia in real-time. Accuracy
was investigated. Statistical analyses were performed using bootstrapping. The
results were further compared with human performance. RadioRAG improved
diagnostic accuracy across most LLMs, with relative accuracy increases ranging
up to 54% for different LLMs. It matched or exceeded non-RAG models and the
human radiologist in question answering across radiologic subspecialties,
particularly in breast imaging and emergency radiology. However, the degree of
improvement varied among models; GPT-3.5-turbo and Mixtral-8x7B-instruct-v0.1
saw notable gains, while Mistral-7B-instruct-v0.2 showed no improvement,
highlighting variability in RadioRAG's effectiveness. LLMs benefit when
provided access to domain-specific data beyond their training data. RadioRAG
shows potential to improve LLM accuracy and factuality in radiology question
answering by integrating real-time domain-specific data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Radiology: Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves
  Reasoning Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenlong Wang, Yuanning Feng, Dongping Chen, Zhaoyang Chu, Ranjay Krishna, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large reasoning models have enabled complex, step-by-step
reasoning but often introduce significant overthinking, resulting in verbose
and redundant outputs that hinder efficiency. In this study, we examine whether
explicit self-reflection, signaled by tokens such as "Wait" and "Hmm", is
necessary for advanced reasoning. We propose NoWait, a simple yet effective
approach that disables explicit self-reflection by suppressing these tokens
during inference. Extensive experiments on ten benchmarks across textual,
visual, and video reasoning tasks show that NoWait reduces chain-of-thought
trajectory length by up to 27%-51% in five R1-style model series, without
compromising model utility. NoWait thus offers a plug-and-play solution for
efficient and utility-preserving multimodal reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interchangeable Token Embeddings for Extendable Vocabulary and
  Alpha-Equivalence <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17161v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17161v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        İlker Işık, Ramazan Gokberk Cinbis, Ebru Aydin Gol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models lack the notion of interchangeable tokens: symbols that are
semantically equivalent yet distinct, such as bound variables in formal logic.
This limitation prevents generalization to larger vocabularies and hinders the
model's ability to recognize alpha-equivalence, where renaming bound variables
preserves meaning. We formalize this machine learning problem and introduce
alpha-covariance, a metric for evaluating robustness to such transformations.
To tackle this task, we propose a dual-part token embedding strategy: a shared
component ensures semantic consistency, while a randomized component maintains
token distinguishability. Compared to a baseline that relies on alpha-renaming
for data augmentation, our approach demonstrates improved generalization to
unseen tokens in linear temporal logic solving, propositional logic assignment
prediction, and copying with an extendable vocabulary, while introducing a
favorable inductive bias for alpha-equivalence. Our findings establish a
foundation for designing language models that can learn interchangeable token
representations, a crucial step toward more flexible and systematic reasoning
in formal domains. Our code and project page are available at
https://necrashter.github.io/interchangeable-token-embeddings
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025 Poster Paper, Camera Ready Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adding Chocolate to Mint: Mitigating Metric Interference in Machine
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.08327v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.08327v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        José Pombal, Nuno M. Guerreiro, Ricardo Rei, André F. T. Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As automatic metrics become increasingly stronger and widely adopted, the
risk of unintentionally "gaming the metric" during model development rises.
This issue is caused by metric interference (MINT), i.e., the use of the same
or related metrics for both model tuning and evaluation. MINT can misguide
practitioners into being overoptimistic about the performance of their systems:
as system outputs become a function of the interfering metric, their estimated
quality loses correlation with human judgments. In this work, we analyze two
common cases of MINT in machine translation-related tasks: filtering of
training data, and decoding with quality signals. Importantly, we find that
MINT strongly distorts instance-level metric scores, even when metrics are not
directly optimized for-questioning the common strategy of leveraging a
different, yet related metric for evaluation that is not used for tuning. To
address this problem, we propose MINTADJUST, a method for more reliable
evaluation under MINT. On the WMT24 MT shared task test set, MINTADJUST ranks
translations and systems more accurately than state-of-the-art metrics across a
majority of language pairs, especially for high-quality systems. Furthermore,
MINTADJUST outperforms AUTORANK, the ensembling method used by the organizers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular
  Detoxification? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10912v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10912v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Lin, Ziyang Gong, Cong Wang, Yonglin Tian, Tengchao Zhang, Xue Yang, Gen Luo, Fei-Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Toxicity remains a leading cause of early-stage drug development failure.
Despite advances in molecular design and property prediction, the task of
molecular toxicity repair - generating structurally valid molecular
alternatives with reduced toxicity - has not yet been systematically defined or
benchmarked. To fill this gap, we introduce ToxiMol, the first benchmark task
for general-purpose Multimodal Large Language Models (MLLMs) focused on
molecular toxicity repair. We construct a standardized dataset covering 11
primary tasks and 560 representative toxic molecules spanning diverse
mechanisms and granularities. We design a prompt annotation pipeline with
mechanism-aware and task-adaptive capabilities, informed by expert
toxicological knowledge. In parallel, we propose an automated evaluation
framework, ToxiEval, which integrates toxicity endpoint prediction, synthetic
accessibility, drug-likeness, and structural similarity into a high-throughput
evaluation chain for repair success. We systematically assess nearly 30
mainstream general-purpose MLLMs and design multiple ablation studies to
analyze key factors such as evaluation criteria, candidate diversity, and
failure attribution. Experimental results show that although current MLLMs
still face significant challenges on this task, they begin to demonstrate
promising capabilities in toxicity understanding, semantic constraint
adherence, and structure-aware molecule editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OM4OV: Leveraging Ontology Matching for Ontology Versioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20302v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20302v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang, Kerry Taylor, Weiqing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the dynamic nature of the Semantic Web, version control is necessary
to capture time-varying information, particularly for widely used ontologies.
Despite the long-standing recognition of ontology versioning (OV) as a crucial
component for efficient ontology management, the growing size of ontologies and
accumulating errors caused by manual labour overwhelm current OV approaches. In
this paper, we propose yet another approach to performing OV using existing
ontology matching (OM) techniques and systems. We introduce a unified OM4OV
pipeline. From an OM perspective, we reconstruct a new task formulation and
measurement for OV tasks. Building upon the prior alignment(s) from OM, we
propose a pipeline optimisation method called the cross-reference (CR)
mechanism to enhance overall OV performance. We experimentally validate the
OM4OV pipeline and the cross-reference mechanism in the OV tested originating
from the Ontology Alignment Evaluation Initiative (OAEI) datasets. We also
discuss insights into OM used for OV tasks, where some false mappings detected
by OV systems are not actually untrue.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistic Aggregation and Targeted Embedding Optimization for
  Collective Moral Reasoning in Large Language Models <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenchen Yuan, Zheyu Zhang, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown impressive moral reasoning abilities.
Yet they often diverge when confronted with complex, multi-factor moral
dilemmas. To address these discrepancies, we propose a framework that
synthesizes multiple LLMs' moral judgments into a collectively formulated moral
judgment, realigning models that deviate significantly from this consensus. Our
aggregation mechanism fuses continuous moral acceptability scores (beyond
binary labels) into a collective probability, weighting contributions by model
reliability. For misaligned models, a targeted embedding-optimization procedure
fine-tunes token embeddings for moral philosophical theories, minimizing JS
divergence to the consensus while preserving semantic integrity. Experiments on
a large-scale social moral dilemma dataset show our approach builds robust
consensus and improves individual model fidelity. These findings highlight the
value of data-driven moral alignment across multiple models and its potential
for safer, more consistent AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PsychBench: A comprehensive and professional benchmark for evaluating
  the performance of LLM-assisted psychiatric clinical practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01903v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01903v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyu Liu, Ruoxi Wang, Ling Zhang, Xuequan Zhu, Rui Yang, Xinzhu Zhou, Fei Wu, Zhi Yang, Cheng Jin, Gang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models (LLMs) offers potential solutions to
address problems such as shortage of medical resources and low diagnostic
consistency in psychiatric clinical practice. Despite this potential, a robust
and comprehensive benchmarking framework to assess the efficacy of LLMs in
authentic psychiatric clinical environments is absent. This has impeded the
advancement of specialized LLMs tailored to psychiatric applications. In
response to this gap, by incorporating clinical demands in psychiatry and
clinical data, we proposed a benchmarking system, PsychBench, to evaluate the
practical performance of LLMs in psychiatric clinical settings. We conducted a
comprehensive quantitative evaluation of 16 LLMs using PsychBench, and
investigated the impact of prompt design, chain-of-thought reasoning, input
text length, and domain-specific knowledge fine-tuning on model performance.
Through detailed error analysis, we identified strengths and potential
limitations of the existing models and suggested directions for improvement.
Subsequently, a clinical reader study involving 60 psychiatrists of varying
seniority was conducted to further explore the practical benefits of existing
LLMs as supportive tools for psychiatrists of varying seniority. Through the
quantitative and reader evaluation, we show that while existing models
demonstrate significant potential, they are not yet adequate as decision-making
tools in psychiatric clinical practice. The reader study further indicates
that, as an auxiliary tool, LLM could provide particularly notable support for
junior psychiatrists, effectively enhancing their work efficiency and overall
clinical quality. To promote research in this area, we will make the dataset
and evaluation framework publicly available, with the hope of advancing the
application of LLMs in psychiatric clinical settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PEDANTIC: A Dataset for the Automatic Examination of Definiteness in
  Patent Claims <span class="chip">SIGIR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.21342v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.21342v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Knappich, Annemarie Friedrich, Anna Hätty, Simon Razniewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Patent claims define the scope of protection for an invention. If there are
ambiguities in a claim, it is rejected by the patent office. In the US, this is
referred to as indefiniteness (35 U.S.C {\S} 112(b)) and is among the most
frequent reasons for patent application rejection. The development of automatic
methods for patent definiteness examination has the potential to make patent
drafting and examination more efficient, but no annotated dataset has been
published to date. We introduce PEDANTIC (Patent Definiteness Examination
Corpus), a novel dataset of 14k US patent claims from patent applications
relating to Natural Language Processing (NLP), annotated with reasons for
indefiniteness. We construct PEDANTIC using a fully automatic pipeline that
retrieves office action documents from the USPTO and uses Large Language Models
(LLMs) to extract the reasons for indefiniteness. A human validation study
confirms the pipeline's accuracy in generating high-quality annotations. To
gain insight beyond binary classification metrics, we implement an LLM-as-Judge
evaluation that compares the free-form reasoning of every model-cited reason
with every examiner-cited reason. We show that LLM agents based on Qwen 2.5 32B
and 72B struggle to outperform logistic regression baselines on definiteness
prediction, even though they often correctly identify the underlying reasons.
PEDANTIC provides a valuable resource for patent AI researchers, enabling the
development of advanced examination models. We will publicly release the
dataset and code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PatentSemTech@SIGIR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GreekBarBench: A Challenging Benchmark for Free-Text Legal Reasoning and
  Citations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.17267v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.17267v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Odysseas S. Chlapanis, Dimitrios Galanis, Nikolaos Aletras, Ion Androutsopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce GreekBarBench, a benchmark that evaluates LLMs on legal
questions across five different legal areas from the Greek Bar exams, requiring
citations to statutory articles and case facts. To tackle the challenges of
free-text evaluation, we propose a three-dimensional scoring system combined
with an LLM-as-a-judge approach. We also develop a meta-evaluation benchmark to
assess the correlation between LLM-judges and human expert evaluations,
revealing that simple, span-based rubrics improve their alignment. Our
systematic evaluation of 13 proprietary and open-weight LLMs shows that even
though the best models outperform average expert scores, they fall short of the
95th percentile of experts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 17 figures, submitted to May ARR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AIn't Nothing But a Survey? Using Large Language Models for Coding
  German Open-Ended Survey Responses on Survey Motivation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leah von der Heyde, Anna-Carolina Haensch, Bernd Weiß, Jessica Daikeler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent development and wider accessibility of LLMs have spurred
discussions about how they can be used in survey research, including
classifying open-ended survey responses. Due to their linguistic capacities, it
is possible that LLMs are an efficient alternative to time-consuming manual
coding and the pre-training of supervised machine learning models. As most
existing research on this topic has focused on English-language responses
relating to non-complex topics or on single LLMs, it is unclear whether its
findings generalize and how the quality of these classifications compares to
established methods. In this study, we investigate to what extent different
LLMs can be used to code open-ended survey responses in other contexts, using
German data on reasons for survey participation as an example. We compare
several state-of-the-art LLMs and several prompting approaches, and evaluate
the LLMs' performance by using human expert codings. Overall performance
differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory
levels of predictive performance. Performance differences between prompting
approaches are conditional on the LLM used. Finally, LLMs' unequal
classification performance across different categories of reasons for survey
participation results in different categorical distributions when not using
fine-tuning. We discuss the implications of these findings, both for
methodological research on coding open-ended responses and for their
substantive analysis, and for practitioners processing or substantively
analyzing such data. Finally, we highlight the many trade-offs researchers need
to consider when choosing automated methods for open-ended response
classification in the age of LLMs. In doing so, our study contributes to the
growing body of research about the conditions under which LLMs can be
efficiently, accurately, and reliably leveraged in survey research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in Survey Research Methods</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HiURE: Hierarchical Exemplar Contrastive Learning for Unsupervised
  Relation Extraction <span class="chip">NAACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.02225v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.02225v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuliang Liu, Xuming Hu, Chenwei Zhang, Shu`ang Li, Lijie Wen, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised relation extraction aims to extract the relationship between
entities from natural language sentences without prior information on
relational scope or distribution. Existing works either utilize self-supervised
schemes to refine relational feature signals by iteratively leveraging adaptive
clustering and classification that provoke gradual drift problems, or adopt
instance-wise contrastive learning which unreasonably pushes apart those
sentence pairs that are semantically similar. To overcome these defects, we
propose a novel contrastive learning framework named HiURE, which has the
capability to derive hierarchical signals from relational feature space using
cross hierarchy attention and effectively optimize relation representation of
sentences under exemplar-wise contrastive learning. Experimental results on two
public datasets demonstrate the advanced effectiveness and robustness of HiURE
on unsupervised relation extraction when compared with state-of-the-art models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In NAACL 2022 as a long paper. Code and data available at
  https://github.com/THU-BPM/HiURE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Avengers: A Simple Recipe for Uniting Smaller Language Models to
  Challenge Proprietary Giants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19797v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19797v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqun Zhang, Hao Li, Chenxu Wang, Linyao Chen, Qiaosheng Zhang, Peng Ye, Shi Feng, Daling Wang, Zhen Wang, Xinrun Wang, Jia Xu, Lei Bai, Wanli Ouyang, Shuyue Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proprietary giants are increasingly dominating the race for ever-larger
language models. Can open-source, smaller models remain competitive across a
broad range of tasks? In this paper, we present the Avengers -- a simple recipe
that leverages the collective intelligence of these smaller models. The
Avengers builds upon four lightweight operations: (i) embedding: encode queries
using a text embedding model; (ii) clustering: group queries based on their
semantic similarity; (iii) scoring: scores each model's performance within each
cluster; and (iv) voting: improve outputs via repeated sampling and voting. At
inference time, each query is embedded and assigned to its nearest cluster. The
top-performing model(s) within that cluster are selected to generate the
response with repeated sampling. Remarkably, with 10 open-source models (~7B
parameters each), the Avengers surpasses GPT-4o, 4.1, and 4.5 in average
performance across 15 diverse datasets spanning mathematics, coding, logical
reasoning, general knowledge, and affective tasks. In particular, it surpasses
GPT-4.1 on mathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore,
the Avengers delivers superior out-of-distribution generalization, and remains
robust across various embedding models, clustering algorithms, ensemble
strategies, and values of its sole parameter -- the number of clusters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, 6 tables, supplementary material (appendix)
  included separately</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Effective Incorporating Heterogeneous Knowledge Curriculum Learning
  for Sequence Labeling <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13534v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13534v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuemei Tang, Jun Wang, Qi Su, Chu-ren Huang, Jinghang Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequence labeling models often benefit from incorporating external knowledge.
However, this practice introduces data heterogeneity and complicates the model
with additional modules, leading to increased expenses for training a
high-performing model. To address this challenge, we propose a two-stage
curriculum learning (TCL) framework specifically designed for sequence labeling
tasks. The TCL framework enhances training by gradually introducing data
instances from easy to hard, aiming to improve both performance and training
speed. Furthermore, we explore different metrics for assessing the difficulty
levels of sequence labeling tasks. Through extensive experimentation on six
Chinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we
demonstrate the effectiveness of our model in enhancing the performance of
sequence labeling models. Additionally, our analysis indicates that TCL
accelerates training and alleviates the slow training problem associated with
complex models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 tables, 3 figures, Accepted by ACL 2025 (short paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models for Automated Literature Review: An Evaluation of
  Reference Generation, Abstract Writing, and Review Composition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13612v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13612v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuemei Tang, Xufeng Duan, Zhenguang G. Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have emerged as a potential solution to automate
the complex processes involved in writing literature reviews, such as
literature collection, organization, and summarization. However, it is yet
unclear how good LLMs are at automating comprehensive and reliable literature
reviews. This study introduces a framework to automatically evaluate the
performance of LLMs in three key tasks of literature writing: reference
generation, literature summary, and literature review composition. We introduce
multidimensional evaluation metrics that assess the hallucination rates in
generated references and measure the semantic coverage and factual consistency
of the literature summaries and compositions against human-written
counterparts. The experimental results reveal that even the most advanced
models still generate hallucinated references, despite recent progress.
Moreover, we observe that the performance of different models varies across
disciplines when it comes to writing literature reviews. These findings
highlight the need for further research and development to improve the
reliability of LLMs in automating academic literature reviews.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning AI Research with the Needs of Clinical Coding Workflows: Eight
  Recommendations Based on US Data Analysis and Critical Review <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18043v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18043v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yidong Gan, Maciej Rybinski, Ben Hachey, Jonathan K. Kummerfeld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical coding is crucial for healthcare billing and data analysis. Manual
clinical coding is labour-intensive and error-prone, which has motivated
research towards full automation of the process. However, our analysis, based
on US English electronic health records and automated coding research using
these records, shows that widely used evaluation methods are not aligned with
real clinical contexts. For example, evaluations that focus on the top 50 most
common codes are an oversimplification, as there are thousands of codes used in
practice. This position paper aims to align AI coding research more closely
with practical challenges of clinical coding. Based on our analysis, we offer
eight specific recommendations, suggesting ways to improve current evaluation
methods. Additionally, we propose new AI-based methods beyond automated coding,
suggesting alternative approaches to assist clinical coders in their workflows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the ACL 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Acoustic Model Architecture Optimization in Training for ASR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingjing Xu, Zijian Yang, Albert Zeyer, Eugen Beck, Ralf Schlueter, Hermann Ney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Architecture design is inherently complex. Existing approaches rely on either
handcrafted rules, which demand extensive empirical expertise, or automated
methods like neural architecture search, which are computationally intensive.
In this paper, we introduce DMAO, an architecture optimization framework that
employs a grow-and-drop strategy to automatically reallocate parameters during
training. This reallocation shifts resources from less-utilized areas to those
parts of the model where they are most beneficial. Notably, DMAO only
introduces negligible training overhead at a given model complexity. We
evaluate DMAO through experiments with CTC on LibriSpeech, TED-LIUM-v2 and
Switchboard datasets. The results show that, using the same amount of training
resources, our proposed DMAO consistently improves WER by up to 6% relatively
across various architectures, model sizes, and datasets. Furthermore, we
analyze the pattern of parameter redistribution and uncover insightful
findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Interspeech 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Utility-Preserving Text Anonymization Based on Large Language
  Models <span class="chip">ACL'2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Yang, Xiaodan Zhu, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anonymizing text that contains sensitive information is crucial for a wide
range of applications. Existing techniques face the emerging challenges of the
re-identification ability of large language models (LLMs), which have shown
advanced capability in memorizing detailed information and reasoning over
dispersed pieces of patterns to draw conclusions. When defending against
LLM-based re-identification, anonymization could jeopardize the utility of the
resulting anonymized data in downstream tasks. In general, the interaction
between anonymization and data utility requires a deeper understanding within
the context of LLMs. In this paper, we propose a framework composed of three
key LLM-based components: a privacy evaluator, a utility evaluator, and an
optimization component, which work collaboratively to perform anonymization.
Extensive experiments demonstrate that the proposed model outperforms existing
baselines, showing robustness in reducing the risk of re-identification while
preserving greater data utility in downstream tasks. We provide detailed
studies on these core modules. To consider large-scale and real-time
applications, we investigate the distillation of the anonymization capabilities
into lightweight models. All of our code and datasets will be made publicly
available at https://github.com/UKPLab/acl2025-rupta.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL'2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TSLFormer: A Lightweight Transformer Model for Turkish Sign Language
  Recognition Using Skeletal Landmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07890v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07890v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kutay Ertürk, Furkan Altınışık, İrem Sarıaltın, Ömer Nezih Gerek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents TSLFormer, a light and robust word-level Turkish Sign
Language (TSL) recognition model that treats sign gestures as ordered,
string-like language. Instead of using raw RGB or depth videos, our method only
works with 3D joint positions - articulation points - extracted using Google's
Mediapipe library, which focuses on the hand and torso skeletal locations. This
creates efficient input dimensionality reduction while preserving important
semantic gesture information. Our approach revisits sign language recognition
as sequence-to-sequence translation, inspired by the linguistic nature of sign
languages and the success of transformers in natural language processing. Since
TSLFormer uses the self-attention mechanism, it effectively captures temporal
co-occurrence within gesture sequences and highlights meaningful motion
patterns as words unfold. Evaluated on the AUTSL dataset with over 36,000
samples and 227 different words, TSLFormer achieves competitive performance
with minimal computational cost. These results show that joint-based input is
sufficient for enabling real-time, mobile, and assistive communication systems
for hearing-impaired individuals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for
  Belief-Inconsistent Syllogistic Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06955v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06955v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ha-Thanh Nguyen, Chaoran Liu, Koichi Takeda, Yusuke Miyao, Pontus Stenetorp, Qianying Liu, Su Myat Noe, Hideyuki Tachibana, Sadao Kurohashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present BIS Reasoning 1.0, the first large-scale Japanese dataset of
syllogistic reasoning problems explicitly designed to evaluate
belief-inconsistent reasoning in large language models (LLMs). Unlike prior
datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned
reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent
syllogisms to uncover reasoning biases in LLMs trained on human-aligned
corpora. We benchmark state-of-the-art models - including GPT models, Claude
models, and leading Japanese LLMs - revealing significant variance in
performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies
critical weaknesses in current LLMs when handling logically valid but
belief-conflicting inputs. These findings have important implications for
deploying LLMs in high-stakes domains such as law, healthcare, and scientific
literature, where truth must override intuitive belief to ensure integrity and
safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version includes an updated literature review, added
  acknowledgements, and a revised author list</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Systematic Survey of Natural Language Processing for the Greek
  Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09861v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09861v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juli Bakagianni, Kanella Pouli, Maria Gavriilidou, John Pavlopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Comprehensive monolingual Natural Language Processing (NLP) surveys are
essential for assessing language-specific challenges, resource availability,
and research gaps. However, existing surveys often lack standardized
methodologies, leading to selection bias and fragmented coverage of NLP tasks
and resources. This study introduces a generalizable framework for systematic
monolingual NLP surveys. Our approach integrates a structured search protocol
to minimize bias, an NLP task taxonomy for classification, and language
resource taxonomies to identify potential benchmarks and highlight
opportunities for improving resource availability. We apply this framework to
Greek NLP (2012-2023), providing an in-depth analysis of its current state,
task-specific progress, and resource gaps. The survey results are publicly
available (https://doi.org/10.5281/zenodo.15314882) and are regularly updated
to provide an evergreen resource. This systematic survey of Greek NLP serves as
a case study, demonstrating the effectiveness of our framework and its
potential for broader application to other not so well-resourced languages as
regards NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version matches the paper published in Patterns (Cell Press).
  The title has been updated to reflect the published version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13300v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13300v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Li, Chengben Xu, Wufeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Seewo's systems for both tracks of the Multilingual
Conversational Speech Language Model Challenge (MLC-SLM), addressing automatic
speech recognition (ASR) and speaker diarization with ASR (SD-ASR). We
introduce a multi-stage training pipeline that explicitly enhances reasoning
and self-correction in speech language models for ASR. Our approach combines
curriculum learning for progressive capability acquisition, Chain-of-Thought
data augmentation to foster intermediate reflection, and Reinforcement Learning
with Verifiable Rewards (RLVR) to further refine self-correction through
reward-driven optimization. This approach achieves substantial improvements
over the official challenge baselines. On the evaluation set, our best system
attains a WER/CER of 11.57% for Track 1 and a tcpWER/tcpCER of 17.67% for Track
2. Comprehensive ablation studies demonstrate the effectiveness of each
component under challenge constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLäMmlein: Transparent, Compact and Competitive German-Only Language
  Models from Scratch <span class="chip">ACL25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11171v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11171v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Pfister, Julia Wunderle, Andreas Hotho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We create two German-only decoder models, LL\"aMmlein 120M and 1B,
transparently from scratch and publish them, along with the training data, for
the German NLP research community to use. The model training involved several
key steps, including extensive data preprocessing, the creation of a custom
German tokenizer, the training itself, as well as the evaluation of the final
models on various benchmarks. Throughout the training process, multiple
checkpoints were saved and analyzed using the SuperGLEBer benchmark to monitor
the models' learning dynamics. Compared to state-of-the-art models on the
SuperGLEBer benchmark, both LL\"aMmlein models performed competitively,
consistently matching or surpassing models with similar parameter sizes. The
results show that the models' quality scales with size as expected, but
performance improvements on some tasks plateaued early, offering valuable
insights into resource allocation for future model development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>camera ready @ACL25;
  https://www.informatik.uni-wuerzburg.de/datascience/projects/nlp/llammlein/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Goal-oriented Proactive <span class="highlight-title">Dialogue</span> Systems via Consistency
  Reflection and Correction <span class="chip">ACL'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13366v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13366v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Didi Zhang, Yaxin Fan, Peifeng Li, Qiaoming Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Goal-oriented proactive dialogue systems are designed to guide user
conversations seamlessly towards specific objectives by planning a
goal-oriented path. However, previous research has focused predominantly on
optimizing these paths while neglecting the inconsistencies that may arise
between generated responses and dialogue contexts, including user profiles,
dialogue history, domain knowledge, and subgoals. To address this issue, we
introduce a model-agnostic two-stage Consistency Reflection and Correction
(CRC) framework. Specifically, in the consistency reflection stage, the model
is prompted to reflect on the discrepancies between generated responses and
dialogue contexts, identifying inconsistencies and suggesting possible
corrections. In the consistency correction stage, the model generates responses
that are more consistent with the dialogue context based on these reflection
results. We conducted experiments on various model architectures with different
parameter sizes, including encoder-decoder models (BART, T5) and decoder-only
models (GPT-2, DialoGPT, Phi3, Mistral and LLaMA3), and the experimental
results on three datasets demonstrate that our CRC framework significantly
improves the consistency between generated responses and dialogue contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL'25 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Long CoT Reasoning in Small Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.18440v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.18440v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Wang, Jinqi Jiang, Tian Qiu, Hui Liu, Xianfeng Tang, Huaxiu Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large reasoning models such as DeepSeek-R1 exhibit strong complex
problems solving abilities by generating long chain-of-thought (CoT) reasoning
steps. It is challenging to directly train small language models (SLMs) to
emerge long CoT. Thus, distillation becomes a practical method to enable SLMs
for such reasoning ability. However, the long CoT often contains a lot of
redundant contents (e.g., overthinking steps) which may make SLMs hard to learn
considering their relatively poor capacity and generalization. To address this
issue, we propose a simple-yet-effective method to prune unnecessary steps in
long CoT, and then employ an on-policy method for the SLM itself to curate
valid and useful long CoT training data. In this way, SLMs can effectively
learn efficient long CoT reasoning and preserve competitive performance at the
same time. Experimental results across a series of mathematical reasoning
benchmarks demonstrate the effectiveness of the proposed method in distilling
long CoT reasoning ability into SLMs which maintains the competitive
performance but significantly reduces generating redundant reasoning steps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ALPS: Attention Localization and Pruning Strategy for Efficient
  Alignment of Large Language Models <span class="chip">ACL25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.18799v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.18799v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Chen, Haoze Li, Zhiqing Xiao, Lirong Gao, Qi Zhang, Xiaomeng Hu, Ningtao Wang, Xing Fu, Junbo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning general-purpose large language models (LLMs) to downstream tasks
often incurs significant training adjustment costs. Prior research has explored
various avenues to enhance alignment efficiency, primarily through minimal-data
training or data-driven activations to identify key attention heads. However,
these approaches inherently introduce data dependency, which hinders
generalization and reusability. To address this issue and enhance model
alignment efficiency, we propose the Attention Localization and Pruning
Strategy (ALPS), an efficient algorithm that localizes the most task-sensitive
attention heads and prunes by restricting attention training updates to these
heads, thereby reducing alignment costs. Experimental results demonstrate that
our method activates only 10% of attention parameters during fine-tuning while
achieving a 2% performance improvement over baselines on three tasks. Moreover,
the identified task-specific heads are transferable across datasets and
mitigate knowledge forgetting. Our work and findings provide a novel
perspective on efficient LLM alignment. The code is available at
https://github.com/VoiceBeer/ALPS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted@ACL25-findings, 17 pages, 8 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary
  Position Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09507v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09507v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingheng Wu, Jingze Shi, Yifan Wu, Nan Tang, Yuyu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers exhibit proficiency in capturing long-range dependencies,
whereas State Space Models (SSMs) facilitate linear-time sequence modeling.
Notwithstanding their synergistic potential, the integration of these
architectures presents a significant challenge, primarily attributable to a
fundamental incongr inuity their respective positional encoding mechanisms:
Transformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs
leverage implicit positional representations via convolutions. This divergence
often precipitates discontinuities and suboptimal performance.To address this
impediment, we propose a unified rotary position embedding (Unified RoPE)
methodology, thereby establishing a consistent positional encoding framework
for both self-attention and state-space components. Using this Unified RoPE, we
introduce TransXSSM, a hybrid architecture that coherently integrates the
Transformer and SSM layers under this unified positional encoding scheme. At a
4 sequenceK length, TransXSSM exhibits training and inference speeds that are
42.3% and 29.5% faster, respectively, relative to standard Transformer models.
It also delivers higher accuracy: under comparable settings, it surpasses a
Transformer baseline by over 4% on language modeling benchmarks.TransXSSM
furthermore scales more effectively: TransXSSM-1.3B gains 7.22% in average
accuracy over its 320M version (versus about 6% gains for equivalent
Transformers or SSMs). Our results show that unified positional encoding
resolves positional incompatibility in hybrid models, enabling efficient,
high-performance long-context modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BriefMe: A Legal NLP Benchmark for Assisting with Legal Briefs <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06619v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06619v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesse Woo, Fateme Hashemi Chaleshtori, Ana Marasović, Kenneth Marino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A core part of legal work that has been under-explored in Legal NLP is the
writing and editing of legal briefs. This requires not only a thorough
understanding of the law of a jurisdiction, from judgments to statutes, but
also the ability to make new arguments to try to expand the law in a new
direction and make novel and creative arguments that are persuasive to judges.
To capture and evaluate these legal skills in language models, we introduce
BRIEFME, a new dataset focused on legal briefs. It contains three tasks for
language models to assist legal professionals in writing briefs: argument
summarization, argument completion, and case retrieval. In this work, we
describe the creation of these tasks, analyze them, and show how current models
perform. We see that today's large language models (LLMs) are already quite
good at the summarization and guided completion tasks, even beating
human-generated headings. Yet, they perform poorly on other tasks in our
benchmark: realistic argument completion and retrieving relevant legal cases.
We hope this dataset encourages more development in Legal NLP in ways that will
specifically aid people in performing legal work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL Findings 2025; 10 pages main, 5 pages references, 37 pages
  appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRAM: A Generative Foundation Reward Model for Reward Generalization <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenglong Wang, Yang Gan, Yifu Huo, Yongyu Mu, Qiaozhi He, Murun Yang, Bei Li, Tong Xiao, Chunliang Zhang, Tongran Liu, Jingbo Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In aligning large language models (LLMs), reward models have played an
important role, but are standardly trained as discriminative models and rely
only on labeled human preference data. In this paper, we explore methods that
train reward models using both unlabeled and labeled data. Building on the
generative models in LLMs, we develop a generative reward model that is first
trained via large-scale unsupervised learning and then fine-tuned via
supervised learning. We also show that by using label smoothing, we are in fact
optimizing a regularized pairwise ranking loss. This result, in turn, provides
a new view of training reward models, which links generative models and
discriminative models under the same class of training objectives. The outcome
of these techniques is a foundation reward model, which can be applied to a
wide range of tasks with little or no further fine-tuning effort. Extensive
experiments show that this model generalizes well across several tasks,
including response ranking, reinforcement learning from human feedback, and
task adaptation with fine-tuning, achieving significant performance
improvements over several strong baseline models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ REVOLVE: Optimizing AI Systems by Tracking Response Evolution in Textual
  Optimization <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03092v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03092v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyan Zhang, Haibo Jin, Leyang Hu, Xinnuo Li, Liying Kang, Man Luo, Yangqiu Song, Haohan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have significantly
enhanced the ability of LLM-based systems to perform complex tasks through
natural language processing and tool interaction. However, optimizing these
LLM-based systems for specific tasks remains challenging, often requiring
manual interventions like prompt engineering and hyperparameter tuning.
Existing automatic optimization methods, such as textual feedback-based
techniques (e.g., TextGrad), tend to focus on immediate feedback, analogous to
using immediate derivatives in traditional numerical gradient descent. However,
relying solely on such feedback can be limited when the adjustments made in
response to this feedback are either too small or fluctuate irregularly,
potentially slowing down or even stalling the optimization process. To overcome
these challenges, more adaptive methods are needed, especially in situations
where the system's response is evolving slowly or unpredictably. In this paper,
we introduce REVOLVE, an optimization method that tracks how "R"esponses
"EVOLVE" across iterations in LLM systems. By focusing on the evolution of
responses over time, REVOLVE enables more stable and effective optimization by
making thoughtful, progressive adjustments at each step. Experimental results
demonstrate that REVOLVE outperforms competitive baselines, achieving a 7.8%
improvement in prompt optimization, a 20.72% gain in solution refinement, and a
29.17% increase in code optimization. Additionally, REVOLVE converges in fewer
iterations, resulting in significant computational savings. Beyond its
practical contributions, REVOLVE highlights a promising direction, where the
rich knowledge from established optimization principles can be leveraged to
enhance LLM systems, which paves the way for further advancements in this
hybrid domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 2 figures, accepted by ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Alleviating Distribution Shift in Synthetic Data for Machine Translation
  Quality Estimation <span class="chip">ACL2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19941v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19941v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Geng, Zhejian Lai, Jiajun Chen, Hao Yang, Shujian Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quality Estimation (QE) models evaluate the quality of machine translations
without reference translations, serving as the reward models for the
translation task. Due to the data scarcity, synthetic data generation has
emerged as a promising solution. However, synthetic QE data often suffers from
distribution shift, which can manifest as discrepancies between pseudo and real
translations, or in pseudo labels that do not align with human preferences. To
tackle this issue, we introduce DCSQE, a novel framework for alleviating
distribution shift in synthetic QE data. To reduce the difference between
pseudo and real translations, we employ the constrained beam search algorithm
and enhance translation diversity through the use of distinct generation
models. DCSQE uses references, i.e., translation supervision signals, to guide
both the generation and annotation processes, enhancing the quality of
token-level labels. DCSQE further identifies the shortest phrase covering
consecutive error tokens, mimicking human annotation behavior, to assign the
final phrase-level labels. Specially, we underscore that the translation model
can not annotate translations of itself accurately. Extensive experiments
demonstrate that DCSQE outperforms SOTA baselines like CometKiwi in both
supervised and unsupervised settings. Further analysis offers insights into
synthetic data generation that could benefit reward models for other tasks. The
code is available at https://github.com/NJUNLP/njuqe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL2025 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficiently Building a Domain-Specific Large Language Model from
  Scratch: A Case Study of a Classical Chinese Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11810v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11810v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shen Li, Renfen Hu, Lijun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General-purpose large language models demonstrate notable capabilities in
language comprehension and generation, achieving results that are comparable
to, or even surpass, human performance in many natural language processing
tasks. Nevertheless, when general models are applied to some specific domains,
e.g., Classical Chinese texts, their effectiveness is often unsatisfactory, and
fine-tuning open-source foundational models similarly struggles to adequately
incorporate domain-specific knowledge. To address this challenge, this study
developed a large language model, AI Taiyan, specifically designed for
understanding and generating Classical Chinese. Experiments show that with a
reasonable model design, data processing, foundational training, and
fine-tuning, satisfactory results can be achieved with only 1.8 billion
parameters. In key tasks related to language processing of Classical Chinese
such as punctuation, identification of allusions, explanation of word meanings,
and translation between ancient and modern Chinese, this model exhibits a clear
advantage over both general-purpose large models and domain-specific
traditional models, achieving levels close to or surpassing human baselines.
This research provides a reference for the efficient construction of
specialized domain-specific large language models. Furthermore, the paper
discusses the application of this model in fields such as the collation of
ancient texts, dictionary editing, and language research, combined with case
studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CODESYNC: Synchronizing Large Language Models with Dynamic Code
  Evolution at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16645v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16645v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenlong Wang, Zhaoyang Chu, Zhengxiang Cheng, Xuyi Yang, Kaiyue Qiu, Yao Wan, Zhou Zhao, Xuanhua Shi, Dongping Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have exhibited exceptional performance in
software engineering yet face challenges in adapting to continually evolving
code knowledge, particularly regarding the frequent updates of third-party
library APIs. This limitation, stemming from static pre-training datasets,
often results in non-executable code or implementations with suboptimal safety
and efficiency. To this end, this paper introduces CODESYNC, a data engine for
identifying outdated code patterns and collecting real-time code knowledge
updates from Python third-party libraries. Building upon CODESYNC, we develop
CODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay
synchronized with code evolution, which covers real-world updates for 220 APIs
from six Python libraries. Our benchmark offers 3,300 test cases across three
evaluation tasks and an update-aware instruction tuning dataset consisting of
2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs
reveal that they struggle with dynamic code evolution, even with the support of
advanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe
that our benchmark can offer a strong foundation for the development of more
effective methods for real-time code knowledge updating in the future. The
experimental code and dataset are publicly available at:
https://github.com/Lucky-voyage/Code-Sync.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented
  <span class="highlight-title">Dialogue</span> <span class="highlight-title">Agent</span>s <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13040v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13040v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuzheng Si, Wentao Ma, Haoyu Gao, Yuchuan Wu, Ting-En Lin, Yinpei Dai, Hangyu Li, Rui Yan, Fei Huang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented dialogue (TOD) models have made significant progress in recent
years. However, previous studies primarily focus on datasets written by
annotators, which has resulted in a gap between academic research and
real-world spoken conversation scenarios. While several small-scale spoken TOD
datasets are proposed to address robustness issues such as ASR errors, they
ignore the unique challenges in spoken conversation. To tackle the limitations,
we introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD,
containing 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from
human-to-human spoken conversations. SpokenWOZ further incorporates common
spoken characteristics such as word-by-word processing and reasoning in spoken
language. Based on these characteristics, we present cross-turn slot and
reasoning slot detection as new challenges. We conduct experiments on various
baselines, including text-modal models, newly proposed dual-modal models, and
LLMs, e.g., ChatGPT. The results show that the current models still have
substantial room for improvement in spoken conversation, where the most
advanced dialogue state tracker only achieves 25.65% in joint goal accuracy and
the SOTA end-to-end model only correctly completes the user request in 52.1% of
dialogues. The dataset, code, and leaderboard are available:
https://spokenwoz.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perspective Transition of Large Language Models for Solving Subjective
  Tasks <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09265v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09265v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaolong Wang, Yuanchi Zhang, Ziyue Wang, Yuzhuang Xu, Fuwen Luo, Yile Wang, Peng Li, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have revolutionized the field of natural
language processing, enabling remarkable progress in various tasks. Different
from objective tasks such as commonsense reasoning and arithmetic
question-answering, the performance of LLMs on subjective tasks is still
limited, where the perspective on the specific problem plays crucial roles for
better interpreting the context and giving proper response. For example, in
certain scenarios, LLMs may perform better when answering from an expert role
perspective, potentially eliciting their relevant domain knowledge. In
contrast, in some scenarios, LLMs may provide more accurate responses when
answering from a third-person standpoint, enabling a more comprehensive
understanding of the problem and potentially mitigating inherent biases. In
this paper, we propose Reasoning through Perspective Transition (RPT), a method
based on in-context learning that enables LLMs to dynamically select among
direct, role, and third-person perspectives for the best way to solve
corresponding subjective problem. Through extensive experiments on totally 12
subjective tasks by using both closed-source and open-source LLMs including
GPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single
fixed perspective based methods such as chain-of-thought prompting and expert
prompting, highlights the intricate ways that LLMs can adapt their perspectives
to provide nuanced and contextually appropriate responses for different
problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bi-VLDoc: Bidirectional Vision-Language Modeling for Visually-Rich
  Document Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.13155v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.13155v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuwei Luo, Guozhi Tang, Qi Zheng, Cong Yao, Lianwen Jin, Chenliang Li, Yang Xue, Luo Si
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal document pre-trained models have proven to be very effective in a
variety of visually-rich document understanding (VrDU) tasks. Though existing
document pre-trained models have achieved excellent performance on standard
benchmarks for VrDU, the way they model and exploit the interactions between
vision and language on documents has hindered them from better generalization
ability and higher accuracy. In this work, we investigate the problem of
vision-language joint representation learning for VrDU mainly from the
perspective of supervisory signals. Specifically, a pre-training paradigm
called Bi-VLDoc is proposed, in which a bidirectional vision-language
supervision strategy and a vision-language hybrid-attention mechanism are
devised to fully explore and utilize the interactions between these two
modalities, to learn stronger cross-modal document representations with richer
semantics. Benefiting from the learned informative cross-modal document
representations, Bi-VLDoc significantly advances the state-of-the-art
performance on three widely-used document understanding benchmarks, including
Form Understanding (from 85.14% to 93.44%), Receipt Information Extraction
(from 96.01% to 97.84%), and Document Classification (from 96.08% to 97.12%).
On Document Visual QA, Bi-VLDoc achieves the state-of-the-art performance
compared to previous single model methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJDAR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I-MCTS: Enhancing <span class="highlight-title">Agent</span>ic AutoML via Introspective Monte Carlo Tree
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14693v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14693v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zujie Liang, Feng Wei, Wujiang Xu, Lin Chen, Yuxi Qian, Xinhui Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have shown remarkable
potential in automating machine learning tasks. However, existing LLM-based
agents often struggle with low-diversity and suboptimal code generation. While
recent work has introduced Monte Carlo Tree Search (MCTS) to address these
issues, limitations persist in the quality and diversity of thoughts generated,
as well as in the scalar value feedback mechanisms used for node selection. In
this study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a
novel approach that iteratively expands tree nodes through an introspective
process that meticulously analyzes solutions and results from parent and
sibling nodes. This facilitates a continuous refinement of the node in the
search tree, thereby enhancing the overall decision-making process.
Furthermore, we integrate a Large Language Model (LLM)-based value model to
facilitate direct evaluation of each node's solution prior to conducting
comprehensive computational rollouts. A hybrid rewarding mechanism is
implemented to seamlessly transition the Q-value from LLM-estimated scores to
actual performance scores. This allows higher-quality nodes to be traversed
earlier. Applied to the various ML tasks, our approach demonstrates a 6%
absolute improvement in performance compared to the strong open-source AutoML
agents, showcasing its effectiveness in enhancing agentic AutoML systems.
Resource available at https://github.com/jokieleung/I-MCTS
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChemHAS: Hierarchical <span class="highlight-title">Agent</span> Stacking for Enhancing Chemistry <span class="highlight-title">Tool</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.21569v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.21569v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhucong Li, Bowei Zhang, Jin Xiao, Zhijian Zhou, Fenglei Cao, Jiaqing Liang, Yuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM)-based agents have demonstrated the ability to
improve performance in chemistry-related tasks by selecting appropriate tools.
However, their effectiveness remains limited by the inherent prediction errors
of chemistry tools. In this paper, we take a step further by exploring how
LLMbased agents can, in turn, be leveraged to reduce prediction errors of the
tools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),
a simple yet effective method that enhances chemistry tools through optimizing
agent-stacking structures from limited data. ChemHAS achieves state-of-the-art
performance across four fundamental chemistry tasks, demonstrating that our
method can effectively compensate for prediction errors of the tools.
Furthermore, we identify and characterize four distinct agent-stacking
behaviors, potentially improving interpretability and revealing new
possibilities for AI agent applications in scientific research. Our code and
dataset are publicly available at https:
//anonymous.4open.science/r/ChemHAS-01E4/README.md.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning
  for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14731v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14731v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Ling Team, Bin Hu, Cai Chen, Deng Zhao, Ding Liu, Dingnan Jin, Feng Zhu, Hao Dai, Hongzhi Luan, Jia Guo, Jiaming Liu, Jiewei Wu, Jun Mei, Jun Zhou, Junbo Zhao, Junwu Xiong, Kaihong Zhang, Kuan Xu, Lei Liang, Liang Jiang, Liangcheng Fu, Longfei Zheng, Qiang Gao, Qing Cui, Quan Wan, Shaomian Zheng, Shuaicheng Li, Tongkai Yang, Wang Ren, Xiaodong Yan, Xiaopei Wan, Xiaoyun Feng, Xin Zhao, Xinxing Yang, Xinyu Kong, Xuemin Yang, Yang Li, Yingting Wu, Yongkang Liu, Zhankai Xu, Zhenduo Zhang, Zhenglei Zhou, Zhenyu Huang, Zhiqiang Zhang, Zihao Wang, Zujie Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model
optimized via reinforcement learning (RL) to achieve efficient and robust
reasoning capabilities. Built upon the publicly available Ling-lite model, a
16.8 billion parameter model with 2.75 billion activated parameters, our
approach matches the performance of state-of-the-art (SOTA) small-scale
reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,
GPQA-Diamond) while activating only one-third of the parameters required by
comparable models. To accomplish this, we introduce a joint training pipeline
integrating distillation with RL, revealing undocumented challenges in MoE RL
training. First, we identify optimization instability during RL training, and
we propose Constrained Contextual Computation Policy Optimization(C3PO), a
novel approach that enhances training stability and improves computational
throughput via algorithm-system co-design methodology. Second, we empirically
demonstrate that selecting distillation checkpoints based on entropy loss for
RL training, rather than validation metrics, yields superior
performance-efficiency trade-offs in subsequent RL training. Finally, we
develop a two-stage training paradigm to harmonize multi-domain data
integration, addressing domain conflicts that arise in training with mixed
dataset. We will release the model, dataset, and code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06809v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06809v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Zeng, Yuying Shang, Jiawei Chen, Jingyuan Zhang, Yu Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated immense utility across various
industries. However, as LLMs advance, the risk of harmful outputs increases due
to incorrect or malicious instruction prompts. While current methods
effectively address jailbreak risks, they share common limitations: 1) Judging
harmful responses from the prefill-level lacks utilization of the model's
decoding outputs, leading to relatively lower effectiveness and robustness. 2)
Rejecting potentially harmful responses based on a single evaluation can
significantly impair the model's helpfulness.This paper examines the LLMs'
capability to recognize harmful outputs, revealing and quantifying their
proficiency in assessing the danger of previous tokens. Motivated by pilot
experiment results, we design a robust defense mechanism at the decoding level.
Our novel decoder-oriented, step-by-step defense architecture corrects harmful
queries directly rather than rejecting them outright. We introduce speculative
decoding to enhance usability and facilitate deployment to boost secure
decoding speed. Extensive experiments demonstrate that our approach improves
model security without compromising reasoning speed. Notably, our method
leverages the model's ability to discern hazardous information, maintaining its
helpfulness compared to existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16205v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16205v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Lin, Hongming Yang, Rongchang Li, Xun Wang, Changting Lin, Wenpeng Xing, Meng Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of Large Language Models (LLMs) has brought impressive
advancements across various tasks. However, despite these achievements, LLMs
still pose inherent safety risks, especially in the context of jailbreak
attacks. Most existing jailbreak methods follow an input-level manipulation
paradigm to bypass safety mechanisms. Yet, as alignment techniques improve,
such attacks are becoming increasingly detectable. In this work, we identify an
underexplored threat vector: the model's internal reasoning process, which can
be manipulated to elicit harmful outputs in a more stealthy way. To explore
this overlooked attack surface, we propose a novel black-box jailbreak attack
method, Analyzing-based Jailbreak (ABJ). ABJ comprises two independent attack
paths: textual and visual reasoning attacks, which exploit the model's
multimodal reasoning capabilities to bypass safety mechanisms, comprehensively
exposing vulnerabilities in its reasoning chain. We conduct extensive
experiments on ABJ across various open-source and closed-source LLMs, VLMs, and
RLMs. In particular, ABJ achieves high attack success rate (ASR) (82.1% on
GPT-4o-2024-11-20) with exceptional attack efficiency (AE) among all target
models, showcasing its remarkable attack effectiveness, transferability, and
efficiency. Our work reveals a new type of safety risk and highlights the
urgent need to mitigate implicit vulnerabilities in the model's reasoning
process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities
  Using Only Forward Passes <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16930v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16930v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bryan R. Christ, Zack Gottesman, Jonathan Kropko, Thomas Hartvigsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Math reasoning is an active area of Large Language Model (LLM) research
because it is a hallmark of artificial intelligence and has implications in
several domains, including math education. However, few works have explored how
math reasoning is encoded within LLM parameters and if it is a skill that can
be isolated within models. Doing so could allow targeted intervention to
improve math performance without altering non-math behavior and foster
understanding of how models encode math reasoning. We introduce Math
Neurosurgery (MathNeuro), a computationally efficient method we use to isolate
math-specific parameters in LLMs using only forward passes. MathNeuro builds on
existing work by using weights and activations to calculate parameter
importance, but isolates math-specific parameters by filtering out those
important for general language tasks. Through pruning parameters MathNeuro
identifies, we delete a LLM's math reasoning ability without significantly
impacting its general language ability. Scaling the identified parameters by a
small constant improves a pretrained or instruction-tuned LLM's performance by
4-17% on GSM8K and 5-35% on MATH while leaving non-math behavior unaltered.
MathNeuro is also data efficient: most of its effectiveness holds when
identifying math-specific parameters using a single sample. MathNeuro
highlights the potential for future work to intervene on math-specific
parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 54 figures, Accepted to ACL 2025 (Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via
  Visual Global Thinking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sifan Li, Yujun Cai, Yiwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) excel in semantic tasks but falter at a core
human capability: detecting hidden content in optical illusions or AI-generated
images through perceptual adjustments like zooming. We introduce HC-Bench, a
benchmark of 112 images with hidden text, objects, and illusions, revealing
that leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit
prompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to
an overreliance on high-level semantics. Strikingly, we propose SemVink
(Semantic Visual Thinking) by simply scaling images to low resolutions (32-128
pixels), which unlocks >99% accuracy by eliminating redundant visual noise.
This exposes a critical architectural flaw: VLMs prioritize abstract reasoning
over low-level visual operations crucial for real-world robustness. Our work
urges a shift toward hybrid models integrating multi-scale processing, bridging
the gap between computational vision and human cognition for applications in
medical imaging, security, and beyond.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14397v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14397v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeonkyoung So, Gyuseong Lee, Sungmok Jung, Joonhak Lee, JiA Kang, Sangho Kim, Jaejin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Negation is a fundamental linguistic phenomenon that poses persistent
challenges for Large Language Models (LLMs), particularly in tasks requiring
deep semantic understanding. Existing benchmarks often treat negation as a side
case within broader tasks like natural language inference, resulting in a lack
of benchmarks that exclusively target negation understanding. In this work, we
introduce Thunder-NUBench, a novel benchmark explicitly designed to assess
sentence-level negation understanding in LLMs. Thunder-NUBench goes beyond
surface-level cue detection by contrasting standard negation with structurally
diverse alternatives such as local negation, contradiction, and paraphrase. The
benchmark consists of manually curated sentence-negation pairs and a
multiple-choice dataset that enables in-depth evaluation of models' negation
understanding.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-17T00:00:00Z">2025-06-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">145</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Variational Framework for Improving Naturalness in Generative Spoken
  Language Models <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li-Wei Chen, Takuya Higuchi, Zakaria Aldeneh, Ahmed Hussen Abdelaziz, Alexander Rudnicky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of large language models in text processing has inspired their
adaptation to speech modeling. However, since speech is continuous and complex,
it is often discretized for autoregressive modeling. Speech tokens derived from
self-supervised models (known as semantic tokens) typically focus on the
linguistic aspects of speech but neglect prosodic information. As a result,
models trained on these tokens can generate speech with reduced naturalness.
Existing approaches try to fix this by adding pitch features to the semantic
tokens. However, pitch alone cannot fully represent the range of paralinguistic
attributes, and selecting the right features requires careful hand-engineering.
To overcome this, we propose an end-to-end variational approach that
automatically learns to encode these continuous speech attributes to enhance
the semantic tokens. Our approach eliminates the need for manual extraction and
selection of paralinguistic features. Moreover, it produces preferred speech
continuations according to human raters. Code, samples and models are available
at https://github.com/b04901014/vae-gslm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Machine Learning (ICML) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASCD: Attention-Steerable Contrastive Decoding for Reducing
  Hallucination in MLLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujun Wang, Jinhe Bi, Yunpu Ma, Soeren Pirk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Model (MLLM) often suffer from hallucinations. They
over-rely on partial cues and generate incorrect responses. Recently, methods
like Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding
(ICD) have been proposed to mitigate hallucinations by contrasting predictions
from perturbed or negatively prefixed inputs against original outputs. In this
work, we uncover that methods like VCD and ICD fundamentally influence internal
attention dynamics of the model. This observation suggests that their
effectiveness may not stem merely from surface-level modifications to logits
but from deeper shifts in attention distribution. Inspired by this insight, we
propose an attention-steerable contrastive decoding framework that directly
intervenes in attention mechanisms of the model to offer a more principled
approach to mitigating hallucinations. Our experiments across multiple MLLM
architectures and diverse decoding methods demonstrate that our approach
significantly reduces hallucinations and improves the performance on benchmarks
such as POPE, CHAIR, and MMHal-Bench, while simultaneously enhancing
performance on standard VQA benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Bytes to Ideas: Language Modeling with Autoregressive U-Nets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathurin Videau, Badr Youbi Idrissi, Alessandro Leite, Marc Schoenauer, Olivier Teytaud, David Lopez-Paz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tokenization imposes a fixed granularity on the input text, freezing how a
language model operates on data and how far in the future it predicts. Byte
Pair Encoding (BPE) and similar schemes split text once, build a static
vocabulary, and leave the model stuck with that choice. We relax this rigidity
by introducing an autoregressive U-Net that learns to embed its own tokens as
it trains. The network reads raw bytes, pools them into words, then pairs of
words, then up to 4 words, giving it a multi-scale view of the sequence. At
deeper stages, the model must predict further into the future -- anticipating
the next few words rather than the next byte -- so deeper stages focus on
broader semantic patterns while earlier stages handle fine details. When
carefully tuning and controlling pretraining compute, shallow hierarchies tie
strong BPE baselines, and deeper hierarchies have a promising trend. Because
tokenization now lives inside the model, the same system can handle
character-level tasks and carry knowledge across low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning with Exploration: An Entropy Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Balancing exploration and exploitation is a central goal in reinforcement
learning (RL). Despite recent advances in enhancing language model (LM)
reasoning, most methods lean toward exploitation, and increasingly encounter
performance plateaus. In this work, we revisit entropy -- a signal of
exploration in RL -- and examine its relationship to exploratory reasoning in
LMs. Through empirical analysis, we uncover strong positive correlations
between high-entropy regions and three types of exploratory reasoning actions:
(1) pivotal tokens that determine or connect logical steps, (2) reflective
actions such as self-verification and correction, and (3) rare behaviors
under-explored by the base LMs. Motivated by this, we introduce a minimal
modification to standard RL with only one line of code: augmenting the
advantage function with an entropy-based term. Unlike traditional
maximum-entropy methods which encourage exploration by promoting uncertainty,
we encourage exploration by promoting longer and deeper reasoning chains.
Notably, our method achieves significant gains on the Pass@K metric -- an
upper-bound estimator of LM reasoning capabilities -- even when evaluated with
extremely large K values, pushing the boundaries of LM reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Length Compression in Large Reasoning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxiang Cheng, Dongping Chen, Mingyang Fu, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Reasoning Models (LRMs) have achieved remarkable success, yet they
often suffer from producing unnecessary and verbose reasoning chains. We
identify a core aspect of this issue as "invalid thinking" -- models tend to
repeatedly double-check their work after having derived the correct answer. To
address this specific inefficiency, we move beyond the general principles of
Efficacy and Efficiency to propose two new, fine-grained principles: Brevity,
which advocates for eliminating redundancy, and Sufficiency, which ensures
critical reasoning steps are preserved. Guided by these principles, we
introduce LC-R1, a post-training method based on Group Relative Policy
Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for
overall conciseness and a Compress Reward that is specifically designed to
remove the invalid portion of the thinking process. Extensive experiments on
multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant
reduction in sequence length (~50%) with only a marginal (~2%) drop in
accuracy, achieving a favorable trade-off point on the Pareto frontier that
prioritizes high compression. Our analysis further validates the robustness of
LC-R1 and provides valuable insights for developing more powerful yet
computationally efficient LRMs. Our code is released at
https://github.com/zxiangx/LC-R1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning
  for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Ring Team, Bin Hu, Cai Chen, Deng Zhao, Ding Liu, Dingnan Jin, Feng Zhu, Hao Dai, Hongzhi Luan, Jia Guo, Jiaming Liu, Jiewei Wu, Jun Mei, Jun Zhou, Junbo Zhao, Junwu Xiong, Kaihong Zhang, Kuan Xu, Lei Liang, Liang Jiang, Liangcheng Fu, Longfei Zheng, Qiang Gao, Qing Cui, Quan Wan, Shaomian Zheng, Shuaicheng Li, Tongkai Yang, Wang Ren, Xiaodong Yan, Xiaopei Wan, Xiaoyun Feng, Xin Zhao, Xinxing Yang, Xinyu Kong, Xuemin Yang, Yang Li, Yingting Wu, Yongkang Liu, Zhankai Xu, Zhenduo Zhang, Zhenglei Zhou, Zhenyu Huang, Zhiqiang Zhang, Zihao Wang, Zujie Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model
optimized via reinforcement learning (RL) to achieve efficient and robust
reasoning capabilities. Built upon the publicly available Ling-lite model, a
16.8 billion parameter model with 2.75 billion activated parameters, our
approach matches the performance of state-of-the-art (SOTA) small-scale
reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,
GPQA-Diamond) while activating only one-third of the parameters required by
comparable models. To accomplish this, we introduce a joint training pipeline
integrating distillation with RL, revealing undocumented challenges in MoE RL
training. First, we identify optimization instability during RL training, and
we propose Constrained Contextual Computation Policy Optimization(C3PO), a
novel approach that enhances training stability and improves computational
throughput via algorithm-system co-design methodology. Second, we empirically
demonstrate that selecting distillation checkpoints based on entropy loss for
RL training, rather than validation metrics, yields superior
performance-efficiency trade-offs in subsequent RL training. Finally, we
develop a two-stage training paradigm to harmonize multi-domain data
integration, addressing domain conflicts that arise in training with mixed
dataset. We will release the model, dataset, and code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Capacity Matters: a Proof-of-Concept for Transformer Memorization on
  Real-World Data <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Changalidis, Aki Härmä
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies how the model architecture and data configurations
influence the empirical memorization capacity of generative transformers. The
models are trained using synthetic text datasets derived from the Systematized
Nomenclature of Medicine (SNOMED) knowledge graph: triplets, representing
static connections, and sequences, simulating complex relation patterns. The
results show that embedding size is the primary determinant of learning speed
and capacity, while additional layers provide limited benefits and may hinder
performance on simpler datasets. Activation functions play a crucial role, and
Softmax demonstrates greater stability and capacity. Furthermore, increasing
the complexity of the data set seems to improve the final memorization. These
insights improve our understanding of transformer memory mechanisms and provide
a framework for optimizing model design with structured real-world data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted for publication at the First Workshop on
  Large Language Model Memorization (L2M2) at ACL 2025, Vienna, Austria</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time
  Markers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel D'souza, Julia Kreutzer, Adrien Morisot, Ahmet Üstün, Sara Hooker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most profound challenges of modern machine learning is performing
well on the long-tail of rare and underrepresented features. Large
general-purpose models are trained for many tasks, but work best on
high-frequency use cases. After training, it is hard to adapt a model to
perform well on specific use cases underrepresented in the training corpus.
Relying on prompt engineering or few-shot examples to maximize the output
quality on a particular test case can be frustrating, as models can be highly
sensitive to small changes, react in unpredicted ways or rely on a fixed system
prompt for maintaining performance. In this work, we ask: "Can we optimize our
training protocols to both improve controllability and performance on
underrepresented use cases at inference time?" We revisit the divide between
training and inference techniques to improve long-tail performance while
providing users with a set of control levers the model is trained to be
responsive to. We create a detailed taxonomy of data characteristics and task
provenance to explicitly control generation attributes and implicitly condition
generations at inference time. We fine-tune a base model to infer these markers
automatically, which makes them optional at inference time. This principled and
flexible approach yields pronounced improvements in performance, especially on
examples from the long tail of the training distribution. While we observe an
average lift of 5.7% win rates in open-ended generation quality with our
markers, we see over 9.1% gains in underrepresented domains. We also observe
relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and
absolute improvements of 35.3% on length instruction following evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and
  Training Factors Shape LLM Alignment Quality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuto Harada, Yusuke Yamauchi, Yusuke Oda, Yohei Oseki, Yusuke Miyao, Yu Takagi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised fine-tuning (SFT) is a critical step in aligning large language
models (LLMs) with human instructions and values, yet many aspects of SFT
remain poorly understood. We trained a wide range of base models on a variety
of datasets including code generation, mathematical reasoning, and
general-domain tasks, resulting in 1,000+ SFT models under controlled
conditions. We then identified the dataset properties that matter most and
examined the layer-wise modifications introduced by SFT. Our findings reveal
that some training-task synergies persist across all models while others vary
substantially, emphasizing the importance of model-specific strategies.
Moreover, we demonstrate that perplexity consistently predicts SFT
effectiveness--often surpassing superficial similarity between trained data and
benchmark--and that mid-layer weight changes correlate most strongly with
performance gains. We will release these 1,000+ SFT models and benchmark
results to accelerate further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel
  Optimization with GuidedSelection Vectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengyuan Zhang, Xinrong Chen, Yingmin Qiu, Xiao Liang, Ziyue Li, Guanyu Wang, Weiping Li, Tong Mo, Wenyue Li, Hayden Kwok-Hay So, Ngai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) methods, particularly Low-Rank
Adaptation (LoRA), offer an efficient way to adapt large language models with
reduced computational costs. However, their performance is limited by the small
number of trainable parameters. Recent work combines LoRA with the
Mixture-of-Experts (MoE), i.e., LoRA-MoE, to enhance capacity, but two
limitations remain in hindering the full exploitation of its potential: 1) the
influence of downstream tasks when assigning expert numbers, and 2) the uniform
rank assignment across all LoRA experts, which restricts representational
diversity. To mitigate these gaps, we propose GuiLoMo, a fine-grained
layer-wise expert numbers and ranks allocation strategy with GuidedSelection
Vectors (GSVs). GSVs are learned via a prior bilevel optimization process to
capture both model- and task-specific needs, and are then used to allocate
optimal expert numbers and ranks. Experiments on three backbone models across
diverse benchmarks show that GuiLoMo consistently achieves superior or
comparable performance to all baselines. Further analysis offers key insights
into how expert numbers and ranks vary across layers and tasks, highlighting
the benefits of adaptive expert configuration. Our code is available at
https://github.com/Liar406/Gui-LoMo.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Passing the Turing Test in Political Dis<span class="highlight-title">course</span>: Fine-Tuning LLMs to
  Mimic Polarized Social Media Comments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        . Pazzaglia, V. Vendetti, L. D. Comencini, F. Deriu, V. Modugno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing sophistication of large language models (LLMs) has sparked
growing concerns regarding their potential role in exacerbating ideological
polarization through the automated generation of persuasive and biased content.
This study explores the extent to which fine-tuned LLMs can replicate and
amplify polarizing discourse within online environments. Using a curated
dataset of politically charged discussions extracted from Reddit, we fine-tune
an open-source LLM to produce context-aware and ideologically aligned
responses. The model's outputs are evaluated through linguistic analysis,
sentiment scoring, and human annotation, with particular attention to
credibility and rhetorical alignment with the original discourse. The results
indicate that, when trained on partisan data, LLMs are capable of producing
highly plausible and provocative comments, often indistinguishable from those
written by humans. These findings raise significant ethical questions about the
use of AI in political discourse, disinformation, and manipulation campaigns.
The paper concludes with a discussion of the broader implications for AI
governance, platform regulation, and the development of detection tools to
mitigate adversarial fine-tuning risks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than
  Few-shot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Cheng, Chengyan Pan, Minjun Zhao, Deyang Li, Fangchao Liu, Xinyu Zhang, Xiao Zhang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-Context Learning (ICL) is an essential emergent ability of Large Language
Models (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars
of ICL to enhance the reasoning capability, especially in mathematics tasks.
However, given the continuous advancement of model capabilities, it remains
unclear whether CoT exemplars still benefit recent, stronger models in such
tasks. Through systematic experiments, we find that for recent strong models
such as the Qwen2.5 series, adding traditional CoT exemplars does not improve
reasoning performance compared to Zero-Shot CoT. Instead, their primary
function is to align the output format with human expectations. We further
investigate the effectiveness of enhanced CoT exemplars, constructed using
answers from advanced models such as \texttt{Qwen2.5-Max} and
\texttt{DeepSeek-R1}. Experimental results indicate that these enhanced
exemplars still fail to improve the model's reasoning performance. Further
analysis reveals that models tend to ignore the exemplars and focus primarily
on the instructions, leading to no observable gain in reasoning ability.
Overall, our findings highlight the limitations of the current ICL+CoT
framework in mathematical reasoning, calling for a re-examination of the ICL
paradigm and the definition of exemplars.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages,22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIn't Nothing But a Survey? Using Large Language Models for Coding
  German Open-Ended Survey Responses on Survey Motivation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leah von der Heyde, Anna-Carolina Haensch, Bernd Weiß, Jessika Daikeler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent development and wider accessibility of LLMs have spurred
discussions about how they can be used in survey research, including
classifying open-ended survey responses. Due to their linguistic capacities, it
is possible that LLMs are an efficient alternative to time-consuming manual
coding and the pre-training of supervised machine learning models. As most
existing research on this topic has focused on English-language responses
relating to non-complex topics or on single LLMs, it is unclear whether its
findings generalize and how the quality of these classifications compares to
established methods. In this study, we investigate to what extent different
LLMs can be used to code open-ended survey responses in other contexts, using
German data on reasons for survey participation as an example. We compare
several state-of-the-art LLMs and several prompting approaches, and evaluate
the LLMs' performance by using human expert codings. Overall performance
differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory
levels of predictive performance. Performance differences between prompting
approaches are conditional on the LLM used. Finally, LLMs' unequal
classification performance across different categories of reasons for survey
participation results in different categorical distributions when not using
fine-tuning. We discuss the implications of these findings, both for
methodological research on coding open-ended responses and for their
substantive analysis, and for practitioners processing or substantively
analyzing such data. Finally, we highlight the many trade-offs researchers need
to consider when choosing automated methods for open-ended response
classification in the age of LLMs. In doing so, our study contributes to the
growing body of research about the conditions under which LLMs can be
efficiently, accurately, and reliably leveraged in survey research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in Survey Research Methods</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based
  Mosquito Breeding Site Detection and Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Adnanul Islam, Md. Faiyaz Abdullah Sayeedi, Md. Asaduzzaman Shuvo, Muhammad Ziaur Rahman, Shahanur Rahman Bappy, Raiyan Rahman, Swakkhar Shatabda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mosquito-borne diseases pose a major global health risk, requiring early
detection and proactive control of breeding sites to prevent outbreaks. In this
paper, we present VisText-Mosquito, a multimodal dataset that integrates visual
and textual data to support automated detection, segmentation, and reasoning
for mosquito breeding site analysis. The dataset includes 1,828 annotated
images for object detection, 142 images for water surface segmentation, and
natural language reasoning texts linked to each image. The YOLOv9s model
achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object
detection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and
mAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves
a final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and
ROUGE-L of 0.87. This dataset and model framework emphasize the theme
"Prevention is Better than Cure", showcasing how AI-based detection can
proactively address mosquito-borne disease risks. The dataset and
implementation code are publicly available at GitHub:
https://github.com/adnanul-islam-jisun/VisText-Mosquito
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probabilistic Aggregation and Targeted Embedding Optimization for
  Collective Moral Reasoning in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenchen Yuan, Zheyu Zhang, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown impressive moral reasoning abilities.
Yet they often diverge when confronted with complex, multi-factor moral
dilemmas. To address these discrepancies, we propose a framework that
synthesizes multiple LLMs' moral judgments into a collectively formulated moral
judgment, realigning models that deviate significantly from this consensus. Our
aggregation mechanism fuses continuous moral acceptability scores (beyond
binary labels) into a collective probability, weighting contributions by model
reliability. For misaligned models, a targeted embedding-optimization procedure
fine-tunes token embeddings for moral philosophical theories, minimizing JS
divergence to the consensus while preserving semantic integrity. Experiments on
a large-scale social moral dilemma dataset show our approach builds robust
consensus and improves individual model fidelity. These findings highlight the
value of data-driven moral alignment across multiple models and its potential
for safer, more consistent AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Does Meaning Backfire? Investigating the <span class="highlight-title">Role</span> of AMRs in NLI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junghyun Min, Xiulin Yang, Shira Wein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Inference (NLI) relies heavily on adequately parsing the
semantic content of the premise and hypothesis. In this work, we investigate
whether adding semantic information in the form of an Abstract Meaning
Representation (AMR) helps pretrained language models better generalize in NLI.
Our experiments integrating AMR into NLI in both fine-tuning and prompting
settings show that the presence of AMR in fine-tuning hinders model
generalization while prompting with AMR leads to slight gains in
\texttt{GPT-4o}. However, an ablation study reveals that the improvement comes
from amplifying surface-level differences rather than aiding semantic
reasoning. This amplification can mislead models to predict non-entailment even
when the core meaning is preserved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC
  Transpilation with Testing Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Heakl, Sarim Hashmi, Chaimaa Abi, Celine Lee, Abdulrahman Mahmoud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The hardware ecosystem is rapidly evolving, with increasing interest in
translating low-level programs across different instruction set architectures
(ISAs) in a quick, flexible, and correct way to enhance the portability and
longevity of existing code. A particularly challenging class of this
transpilation problem is translating between complex- (CISC) and reduced-
(RISC) hardware architectures, due to fundamental differences in instruction
complexity, memory models, and execution paradigms. In this work, we introduce
GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the
translation power of pre-trained large language models (LLMs) with the rigor of
established software testing constructs. Our method generates candidate
translations using an LLM from one ISA to another, and embeds such translations
within a software-testing framework to build quantifiable confidence in the
translation. We evaluate our GG approach over two diverse datasets, enforce
high code coverage (>98%) across unit tests, and achieve functional/semantic
correctness of 99% on HumanEval programs and 49% on BringupBench programs,
respectively. Further, we compare our approach to the state-of-the-art Rosetta
2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,
1.47x better energy efficiency, and 2.41x better memory usage for our
transpiled code, demonstrating the effectiveness of GG for real-world
CISC-to-RISC translation tasks. We will open-source our codes, data, models,
and benchmarks to establish a common foundation for ISA-level code translation
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ahmedheakl.github.io/Guaranteed-Guess/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computational Studies in Influencer Marketing: A Systematic Literature
  Review 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyang Gui, Thales Bertaglia, Catalina Goanta, Gerasimos Spanakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Influencer marketing has become a crucial feature of digital marketing
strategies. Despite its rapid growth and algorithmic relevance, the field of
computational studies in influencer marketing remains fragmented, especially
with limited systematic reviews covering the computational methodologies
employed. This makes overarching scientific measurements in the influencer
economy very scarce, to the detriment of interested stakeholders outside of
platforms themselves, such as regulators, but also researchers from other
fields. This paper aims to provide an overview of the state of the art of
computational studies in influencer marketing by conducting a systematic
literature review (SLR) based on the PRISMA model. The paper analyses 69
studies to identify key research themes, methodologies, and future directions
in this research field. The review identifies four major research themes:
Influencer identification and characterisation, Advertising strategies and
engagement, Sponsored content analysis and discovery, and Fairness.
Methodologically, the studies are categorised into machine learning-based
techniques (e.g., classification, clustering) and non-machine-learning-based
techniques (e.g., statistical analysis, network analysis). Key findings reveal
a strong focus on optimising commercial outcomes, with limited attention to
regulatory compliance and ethical considerations. The review highlights the
need for more nuanced computational research that incorporates contextual
factors such as language, platform, and industry type, as well as improved
model explainability and dataset reproducibility. The paper concludes by
proposing a multidisciplinary research agenda that emphasises the need for
further links to regulation and compliance technology, finer granularity in
analysis, and the development of standardised datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>journal submission, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenerationPrograms: Fine-grained Attribution with Executable Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Wan, Eran Hirsch, Elias Stengel-Eskin, Ido Dagan, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large language models (LLMs) achieve impressive performance in
source-conditioned text generation but often fail to correctly provide
fine-grained attributions for their outputs, undermining verifiability and
trust. Moreover, existing attribution methods do not explain how and why models
leverage the provided source documents to generate their final responses,
limiting interpretability. To overcome these challenges, we introduce a modular
generation framework, GenerationPrograms, inspired by recent advancements in
executable "code agent" architectures. Unlike conventional generation methods
that simultaneously generate outputs and attributions or rely on post-hoc
attribution, GenerationPrograms decomposes the process into two distinct
stages: first, creating an executable program plan composed of modular text
operations (such as paraphrasing, compression, and fusion) explicitly tailored
to the query, and second, executing these operations following the program's
specified instructions to produce the final response. Empirical evaluations
demonstrate that GenerationPrograms significantly improves attribution quality
at both the document level and sentence level across two long-form
question-answering tasks and a multi-document summarization task. We further
demonstrate that GenerationPrograms can effectively function as a post-hoc
attribution method, outperforming traditional techniques in recovering accurate
attributions. In addition, the interpretable programs generated by
GenerationPrograms enable localized refinement through modular-level
improvements that further enhance overall attribution quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 Pages. Code: https://github.com/meetdavidwan/generationprograms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct
  Preference Optimization <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingkang Zhu, Xi Chen, Zhongdao Wang, Bei Yu, Hengshuang Zhao, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in reinforcement learning from human feedback have shown
that utilizing fine-grained token-level reward models can substantially enhance
the performance of Proximal Policy Optimization (PPO) in aligning large
language models. However, it is challenging to leverage such token-level reward
as guidance for Direct Preference Optimization (DPO), since DPO is formulated
as a sequence-level bandit problem. To address this challenge, this work
decomposes the sequence-level PPO into a sequence of token-level proximal
policy optimization problems and then frames the problem of token-level PPO
with token-level reward guidance, from which closed-form optimal token-level
policy and the corresponding token-level reward can be derived. Using the
obtained reward and Bradley-Terry model, this work establishes a framework of
computable loss functions with token-level reward guidance for DPO, and
proposes a practical reward guidance based on the induced DPO reward. This
formulation enables different tokens to exhibit varying degrees of deviation
from reference policy based on their respective rewards. Experiment results
demonstrate that our method achieves substantial performance improvements over
DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on
AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at
https://github.com/dvlab-research/TGDPO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di He, Ajay Jaiswal, Songjun Tu, Li Shen, Ganzhao Yuan, Shiwei Liu, Lu Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weight decay is a standard regularization technique for training large
language models (LLMs). While it is common to assign a uniform decay rate to
every layer, this approach overlooks the structural diversity of LLMs and the
varying spectral properties across modules. In this paper, we introduce
AlphaDecay, a simple yet effective method that adaptively assigns different
weight decay strengths to each module of an LLM. Our approach is guided by
Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical
spectral density (ESD) of weight correlation matrices to quantify
"heavy-tailedness." Modules exhibiting more pronounced heavy-tailed ESDs,
reflecting stronger feature learning, are assigned weaker decay, while modules
with lighter-tailed spectra receive stronger decay. Our method leverages
tailored weight decay assignments to balance the module-wise differences in
spectral properties, leading to improved performance. Extensive pre-training
tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay
achieves better perplexity and generalization than conventional uniform decay
and other adaptive decay baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Zheng, Jiguang He, Chung G. Kang, Guofa Cai, Zitong Yu, Merouane Debbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel neural network framework called M2BeamLLM for
beam prediction in millimeter-wave (mmWave) massive multi-input multi-output
(mMIMO) communication systems. M2BeamLLM integrates multi-modal sensor data,
including images, radar, LiDAR, and GPS, leveraging the powerful reasoning
capabilities of large language models (LLMs) such as GPT-2 for beam prediction.
By combining sensing data encoding, multimodal alignment and fusion, and
supervised fine-tuning (SFT), M2BeamLLM achieves significantly higher beam
prediction accuracy and robustness, demonstrably outperforming traditional deep
learning (DL) models in both standard and few-shot scenarios. Furthermore, its
prediction performance consistently improves with increased diversity in
sensing modalities. Our study provides an efficient and intelligent beam
prediction solution for vehicle-to-infrastructure (V2I) mmWave communication
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LingoLoop Attack: Trapping MLLMs via Linguistic Context and State
  Entrapment into Endless Loops 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyuan Fu, Kaixun Jiang, Lingyi Hong, Jinglun Li, Haijing Guo, Dingkang Yang, Zhaoyu Chen, Wenqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have shown great promise but require
substantial computational resources during inference. Attackers can exploit
this by inducing excessive output, leading to resource exhaustion and service
degradation. Prior energy-latency attacks aim to increase generation time by
broadly shifting the output token distribution away from the EOS token, but
they neglect the influence of token-level Part-of-Speech (POS) characteristics
on EOS and sentence-level structural patterns on output counts, limiting their
efficacy. To address this, we propose LingoLoop, an attack designed to induce
MLLMs to generate excessively verbose and repetitive sequences. First, we find
that the POS tag of a token strongly affects the likelihood of generating an
EOS token. Based on this insight, we propose a POS-Aware Delay Mechanism to
postpone EOS token generation by adjusting attention weights guided by POS
information. Second, we identify that constraining output diversity to induce
repetitive loops is effective for sustained generation. We introduce a
Generative Path Pruning Mechanism that limits the magnitude of hidden states,
encouraging the model to produce persistent loops. Extensive experiments
demonstrate LingoLoop can increase generated tokens by up to 30 times and
energy consumption by a comparable factor on models like Qwen2.5-VL-3B,
consistently driving MLLMs towards their maximum generation limits. These
findings expose significant MLLMs' vulnerabilities, posing challenges for their
reliable deployment. The code will be released publicly following the paper's
acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LexiMark: Robust Watermarking via Lexical Substitutions to Enhance
  Membership Verification of an LLM's Textual Training Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eyal German, Sagiv Antebi, Edan Habler, Asaf Shabtai, Yuval Elovici
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can be trained or fine-tuned on data obtained
without the owner's consent. Verifying whether a specific LLM was trained on
particular data instances or an entire dataset is extremely challenging.
Dataset watermarking addresses this by embedding identifiable modifications in
training data to detect unauthorized use. However, existing methods often lack
stealth, making them relatively easy to detect and remove. In light of these
limitations, we propose LexiMark, a novel watermarking technique designed for
text and documents, which embeds synonym substitutions for carefully selected
high-entropy words. Our method aims to enhance an LLM's memorization
capabilities on the watermarked text without altering the semantic integrity of
the text. As a result, the watermark is difficult to detect, blending
seamlessly into the text with no visible markers, and is resistant to removal
due to its subtle, contextually appropriate substitutions that evade automated
and manual detection. We evaluated our method using baseline datasets from
recent studies and seven open-source models: LLaMA-1 7B, LLaMA-3 8B, Mistral
7B, Pythia 6.9B, as well as three smaller variants from the Pythia family
(160M, 410M, and 1B). Our evaluation spans multiple training settings,
including continued pretraining and fine-tuning scenarios. The results
demonstrate significant improvements in AUROC scores compared to existing
methods, underscoring our method's effectiveness in reliably verifying whether
unauthorized watermarked data was used in LLM training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Far Can LLMs Improve from Experience? Measuring Test-Time Learning
  Ability in LLMs with <span class="highlight-title">Human</span> Comparison 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayin Wang, Zhiquang Guo, Weizhi Ma, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As evaluation designs of large language models may shape our trajectory
toward artificial general intelligence, comprehensive and forward-looking
assessment is essential. Existing benchmarks primarily assess static knowledge,
while intelligence also entails the ability to rapidly learn from experience.
To this end, we advocate for the evaluation of Test-time Learning, the capacity
to improve performance in experience-based, reasoning-intensive tasks during
test time. In this work, we propose semantic games as effective testbeds for
evaluating test-time learning, due to their resistance to saturation and
inherent demand for strategic reasoning. We introduce an objective evaluation
framework that compares model performance under both limited and cumulative
experience settings, and contains four forms of experience representation. To
provide a comparative baseline, we recruit eight human participants to complete
the same task. Results show that LLMs exhibit measurable test-time learning
capabilities; however, their improvements are less stable under cumulative
experience and progress more slowly than those observed in humans. These
findings underscore the potential of LLMs as general-purpose learning machines,
while also revealing a substantial intellectual gap between models and humans,
irrespective of how well LLMs perform on static benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoran Liu, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Diffusion Models, or diffusion LLMs, have emerged as a
significant focus in NLP research, with substantial effort directed toward
understanding their scalability and downstream task performance. However, their
long-context capabilities remain unexplored, lacking systematic analysis or
methods for context extension. In this work, we present the first systematic
investigation comparing the long-context performance of diffusion LLMs and
traditional auto-regressive LLMs. We first identify a unique characteristic of
diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably
\textbf{\textit{stable perplexity}} during direct context extrapolation.
Furthermore, where auto-regressive models fail outright during the
Needle-In-A-Haystack task with context exceeding their pretrained length, we
discover diffusion LLMs exhibit a distinct \textbf{\textit{local perception}}
phenomenon, enabling successful retrieval from recent context segments. We
explain both phenomena through the lens of Rotary Position Embedding (RoPE)
scaling theory. Building on these observations, we propose LongLLaDA, a
training-free method that integrates LLaDA with the NTK-based RoPE
extrapolation. Our results validate that established extrapolation scaling laws
remain effective for extending the context windows of diffusion LLMs.
Furthermore, we identify long-context tasks where diffusion LLMs outperform
auto-regressive LLMs and others where they fall short. Consequently, this study
establishes the first context extrapolation method for diffusion LLMs while
providing essential theoretical insights and empirical benchmarks critical for
advancing future research on long-context diffusion LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 12 figures, work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeinab Sadat Taghavi, Ali Modarressi, Yunpu Ma, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval systems are central to many NLP pipelines, but often rely on
surface-level cues such as keyword overlap and lexical semantic similarity. To
evaluate retrieval beyond these shallow signals, recent benchmarks introduce
reasoning-heavy queries; however, they primarily shift the burden to query-side
processing techniques -- like prompting or multi-hop retrieval -- that can help
resolve complexity. In contrast, we present ImpliRet, a benchmark that shifts
the reasoning challenge to document-side processing: The queries are simple,
but relevance depends on facts stated implicitly in documents through temporal
(e.g., resolving "two days ago"), arithmetic, and world knowledge
relationships. We evaluate a range of sparse and dense retrievers, all of which
struggle in this setting: the best nDCG@10 is only 15.07%. We also test whether
long-context models can overcome this limitation. But even with a short context
of only ten documents, including the positive document, GPT-4.1 scores only
35.06%, showing that document-side reasoning remains a challenge. Our codes are
available at github.com/ZeinabTaghavi/IMPLIRET.Contribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeonkyoung So, Gyuseong Lee, Sungmok Jung, Joonhak Lee, JiA Kang, Sangho Kim, Jaejin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Negation is a fundamental linguistic phenomenon that poses persistent
challenges for Large Language Models (LLMs), particularly in tasks requiring
deep semantic understanding. Existing benchmarks often treat negation as a side
case within broader tasks like natural language inference, resulting in a lack
of benchmarks that exclusively target negation understanding. In this work, we
introduce \textbf{Thunder-NUBench}, a novel benchmark explicitly designed to
assess sentence-level negation understanding in LLMs. Thunder-NUBench goes
beyond surface-level cue detection by contrasting standard negation with
structurally diverse alternatives such as local negation, contradiction, and
paraphrase. The benchmark consists of manually curated sentence-negation pairs
and a multiple-choice dataset that enables in-depth evaluation of models'
negation understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions
  shared task: LLM-based question generation and selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucile Favero, Daniel Frases, Juan Antonio Pérez-Ortiz, Tanja Käser, Nuria Oliver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of chat interfaces based on Large Language Models
(LLMs) raises concerns about promoting superficial learning and undermining the
development of critical thinking skills. Instead of relying on LLMs purely for
retrieving factual information, this work explores their potential to foster
deeper reasoning by generating critical questions that challenge unsupported or
vague claims in debate interventions. This study is part of a shared task of
the 12th Workshop on Argument Mining, co-located with ACL 2025, focused on
automatic critical question generation. We propose a two-step framework
involving two small-scale open source language models: a Questioner that
generates multiple candidate questions and a Judge that selects the most
relevant ones. Our system ranked first in the shared task competition,
demonstrating the potential of the proposed LLM-based approach to encourage
critical engagement with argumentative texts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 12th Workshop on Argument Mining</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital Gatekeepers: Google's <span class="highlight-title">Role</span> in Curating Hashtags and Subreddits <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amrit Poudel, Yifan Ding, Jurgen Pfeffer, Tim Weninger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Search engines play a crucial role as digital gatekeepers, shaping the
visibility of Web and social media content through algorithmic curation. This
study investigates how search engines like Google selectively promotes or
suppresses certain hashtags and subreddits, impacting the information users
encounter. By comparing search engine results with nonsampled data from Reddit
and Twitter/X, we reveal systematic biases in content visibility. Google's
algorithms tend to suppress subreddits and hashtags related to sexually
explicit material, conspiracy theories, advertisements, and cryptocurrencies,
while promoting content associated with higher engagement. These findings
suggest that Google's gatekeeping practices influence public discourse by
curating the social media narratives available to users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive,
  Transparent, and Reproducible Geo-Temporal Information Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Martins, Piotr Szymański, Piotr Gramacki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of Large Language Models (LLMs) has transformed information
access, with current LLMs also powering deep research systems that can generate
comprehensive report-style answers, through planned iterative search,
retrieval, and reasoning. Still, current deep research systems lack the
geo-temporal capabilities that are essential for answering context-rich
questions involving geographic and/or temporal constraints, frequently
occurring in domains like public health, environmental science, or
socio-economic analysis. This paper reports our vision towards next generation
systems, identifying important technical, infrastructural, and evaluative
challenges in integrating geo-temporal reasoning into deep research pipelines.
We argue for augmenting retrieval and synthesis processes with the ability to
handle geo-temporal constraints, supported by open and reproducible
infrastructures and rigorous evaluation protocols. Our vision outlines a path
towards more advanced and geo-temporally aware deep research systems, of
potential impact to the future of AI-driven information access.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation Should Not Ignore Variation: On the Impact of Reference Set
  Choice on Summarization Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silvia Casola, Yang Janet Liu, Siyao Peng, Oliver Kraus, Albert Gatt, Barbara Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human language production exhibits remarkable richness and variation,
reflecting diverse communication styles and intents. However, this variation is
often overlooked in summarization evaluation. While having multiple reference
summaries is known to improve correlation with human judgments, the impact of
using different reference sets on reference-based metrics has not been
systematically investigated. This work examines the sensitivity of widely used
reference-based metrics in relation to the choice of reference sets, analyzing
three diverse multi-reference summarization datasets: SummEval, GUMSum, and
DUC2004. We demonstrate that many popular metrics exhibit significant
instability. This instability is particularly concerning for n-gram-based
metrics like ROUGE, where model rankings vary depending on the reference sets,
undermining the reliability of model comparisons. We also collect human
judgments on LLM outputs for genre-diverse data and examine their correlation
with metrics to supplement existing findings beyond newswire summaries, finding
weak-to-no correlation. Taken together, we recommend incorporating reference
set variation into summarization evaluation to enhance consistency alongside
correlation with human judgments, especially when evaluating LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expectation Confirmation Preference Optimization for Multi-Turn
  <span class="highlight-title">Conversation</span>al Recommendation <span class="highlight-title">Agent</span> <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueyang Feng, Jingsen Zhang, Jiakai Tang, Wei Li, Guohao Cai, Xu Chen, Quanyu Dai, Yue Zhu, Zhenhua Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have significantly
propelled the development of Conversational Recommendation Agents (CRAs).
However, these agents often generate short-sighted responses that fail to
sustain user guidance and meet expectations. Although preference optimization
has proven effective in aligning LLMs with user expectations, it remains costly
and performs poorly in multi-turn dialogue. To address this challenge, we
introduce a novel multi-turn preference optimization (MTPO) paradigm ECPO,
which leverages Expectation Confirmation Theory to explicitly model the
evolution of user satisfaction throughout multi-turn dialogues, uncovering the
underlying causes of dissatisfaction. These causes can be utilized to support
targeted optimization of unsatisfactory responses, thereby achieving turn-level
preference optimization. ECPO ingeniously eliminates the significant sampling
overhead of existing MTPO methods while ensuring the optimization process
drives meaningful improvements. To support ECPO, we introduce an LLM-based user
simulator, AILO, to simulate user feedback and perform expectation confirmation
during conversational recommendations. Experimental results show that ECPO
significantly enhances CRA's interaction capabilities, delivering notable
improvements in both efficiency and effectiveness over existing MTPO methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Findings of ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From What to Respond to When to Respond: Timely Response Generation for
  Open-domain <span class="highlight-title">Dialogue</span> <span class="highlight-title">Agent</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongbo Jang, Minjin Jeon, Jaehoon Lee, Seonghyeon Lee, Dongha Lee, Hwanjo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While research on dialogue response generation has primarily focused on
generating coherent responses conditioning on textual context, the critical
question of when to respond grounded on the temporal context remains
underexplored. To bridge this gap, we propose a novel task called timely
dialogue response generation and introduce the TimelyChat benchmark, which
evaluates the capabilities of language models to predict appropriate time
intervals and generate time-conditioned responses. Additionally, we construct a
large-scale training dataset by leveraging unlabeled event knowledge from a
temporal commonsense knowledge graph and employing a large language model (LLM)
to synthesize 55K event-driven dialogues. We then train Timer, a dialogue agent
designed to proactively predict time intervals and generate timely responses
that align with those intervals. Experimental results show that Timer
outperforms prompting-based LLMs and other fine-tuned baselines in both
turn-level and dialogue-level evaluations. We publicly release our data, model,
and code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving LoRA with Variational Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bai Cong, Nico Daheim, Yuesong Shen, Rio Yokota, Mohammad Emtiyaz Khan, Thomas Möllenhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian methods have recently been used to improve LoRA finetuning and,
although they improve calibration, their effect on other metrics (such as
accuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian
methods also increase computational overheads and require additional tricks for
them to work well. Here, we fix these issues by using a recently proposed
variational algorithm called IVON. We show that IVON is easy to implement and
has similar costs to AdamW, and yet it can also drastically improve many
metrics by using a simple posterior pruning technique. We present extensive
results on billion-scale LLMs (Llama and Qwen series) going way beyond the
scale of existing applications of IVON. For example, we finetune a Llama-3.2-3B
model on a set of commonsense reasoning tasks and improve accuracy over AdamW
by 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian
methods like Laplace-LoRA and BLoB. Overall, our results show that variational
learning with IVON can effectively improve LoRA finetuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Re-Initialization Token Learning for <span class="highlight-title">Tool</span>-Augmented Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenghao Li, Liu Liu, Baosheng Yu, Jiayan Qiu, Yibing Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have demonstrated exceptional performance, yet struggle
with complex tasks such as numerical reasoning, plan generation. Integrating
external tools, such as calculators and databases, into large language models
(LLMs) is crucial for enhancing problem-solving capabilities. Current methods
assign a unique token to each tool, enabling LLMs to call tools through token
prediction-similar to word generation. However, this approach fails to account
for the relationship between tool and word tokens, limiting adaptability within
pre-trained LLMs. To address this issue, we propose a novel token learning
method that aligns tool tokens with the existing word embedding space from the
perspective of initialization, thereby enhancing model performance. We begin by
constructing prior token embeddings for each tool based on the tool's name or
description, which are used to initialize and regularize the learnable tool
token embeddings. This ensures the learned embeddings are well-aligned with the
word token space, improving tool call accuracy. We evaluate the method on tasks
such as numerical reasoning, knowledge-based question answering, and embodied
plan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The
results demonstrate clear improvements over recent baselines, including CoT,
REACT, ICL, and ToolkenGPT, indicating that our approach effectively augments
LLMs with tools through relevant tokens across diverse domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes
  Correct Reasoning in Base LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xumeng Wen, Zihan Liu, Shun Zheng, Zhijian Xu, Shengyu Ye, Zhirong Wu, Xiao Liang, Yang Wang, Junjie Li, Ziming Miao, Jiang Bian, Mao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
promising paradigm for advancing the reasoning capabilities of Large Language
Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned
models often underperform their base models on the $Pass@K$ metric for
solution-finding, leading to the hypothesis that RLVR merely re-weights
existing reasoning paths at the cost of reasoning diversity. In this work, we
resolve this contradiction by identifying the source of the problem: the
$Pass@K$ metric itself is a flawed measure of reasoning, as it credits correct
final answers that probably arise from inaccurate or incomplete chains of
thought (CoTs). To address this, we introduce a more precise evaluation metric,
$CoT$-$Pass@K$, which mandates that both the reasoning path and the final
answer be correct. We provide a new theoretical foundation that formalizes how
RLVR, unlike traditional RL, is uniquely structured to incentivize logical
integrity. Our empirical results are supportive: using $CoT$-$Pass@K$, we
observe that RLVR can incentivize the generalization of correct reasoning for
all values of $K$. Furthermore, by analyzing the training dynamics, we find
that this enhanced reasoning capability emerges early in the training process
and smoothly generalizes. Our work provides a clear perspective on the role of
RLVR, offers a more reliable method for its evaluation, and confirms its
potential to genuinely advance machine reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling
  Historical Patterns in Temporal Knowledge Graphs <span class="chip">ACL25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimin Deng, Yuxia Wu, Yejing Wang, Guoshuai Zhao, Li Zhu, Qidong Liu, Derong Xu, Zichuan Fu, Xian Wu, Yefeng Zheng, Xiangyu Zhao, Xueming Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal knowledge graph reasoning aims to predict future events with
knowledge of existing facts and plays a key role in various downstream tasks.
Previous methods focused on either graph structure learning or semantic
reasoning, failing to integrate dual reasoning perspectives to handle different
prediction scenarios. Moreover, they lack the capability to capture the
inherent differences between historical and non-historical events, which limits
their generalization across different temporal contexts. To this end, we
propose a Multi-Expert Structural-Semantic Hybrid (MESH) framework that employs
three kinds of expert modules to integrate both structural and semantic
information, guiding the reasoning process for different events. Extensive
experiments on three datasets demonstrate the effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL25 findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Xolver: <span class="highlight-title">Multi-Agent</span> Reasoning with Holistic Experience Learning Just
  Like an Olympiad Team 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Tanzib Hosain, Salman Rahman, Md Kishor Morol, Md Rizwan Parvez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite impressive progress on complex reasoning, current large language
models (LLMs) typically operate in isolation - treating each problem as an
independent attempt, without accumulating or integrating experiential
knowledge. In contrast, expert problem solvers - such as Olympiad or
programming contest teams - leverage a rich tapestry of experiences: absorbing
mentorship from coaches, developing intuition from past problems, leveraging
knowledge of tool usage and library functionality, adapting strategies based on
the expertise and experiences of peers, continuously refining their reasoning
through trial and error, and learning from other related problems even during
competition. We introduce Xolver, a training-free multi-agent reasoning
framework that equips a black-box LLM with a persistent, evolving memory of
holistic experience. Xolver integrates diverse experience modalities, including
external and self-retrieval, tool use, collaborative interactions, agent-driven
evaluation, and iterative refinement. By learning from relevant strategies,
code fragments, and abstract reasoning patterns at inference time, Xolver
avoids generating solutions from scratch - marking a transition from isolated
inference toward experience-aware language agents. Built on both open-weight
and proprietary models, Xolver consistently outperforms specialized reasoning
agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses
advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high.
With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24
(94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) -
highlighting holistic experience learning as a key step toward generalist
agents capable of expert-level reasoning. Code and data are available at
https://kagnlp.github.io/xolver.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fretting-Transformer: Encoder-Decoder Model for MIDI to Tablature
  Transcription 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Hamberger, Sebastian Murgul, Jochen Schmidt, Michael Heizmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music transcription plays a pivotal role in Music Information Retrieval
(MIR), particularly for stringed instruments like the guitar, where symbolic
music notations such as MIDI lack crucial playability information. This
contribution introduces the Fretting-Transformer, an encoderdecoder model that
utilizes a T5 transformer architecture to automate the transcription of MIDI
sequences into guitar tablature. By framing the task as a symbolic translation
problem, the model addresses key challenges, including string-fret ambiguity
and physical playability. The proposed system leverages diverse datasets,
including DadaGP, GuitarToday, and Leduc, with novel data pre-processing and
tokenization strategies. We have developed metrics for tablature accuracy and
playability to quantitatively evaluate the performance. The experimental
results demonstrate that the Fretting-Transformer surpasses baseline methods
like A* and commercial applications like Guitar Pro. The integration of
context-sensitive processing and tuning/capo conditioning further enhances the
model's performance, laying a robust foundation for future developments in
automated guitar transcription.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 50th International Computer Music Conference (ICMC),
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chaining Event Spans for Temporal Relation Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongho Kim, Dohyeon Lee, Minsoo Kim, Seung-won Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately understanding temporal relations between events is a critical
building block of diverse tasks, such as temporal reading comprehension (TRC)
and relation extraction (TRE). For example in TRC, we need to understand the
temporal semantic differences between the following two questions that are
lexically near-identical: "What finished right before the decision?" or "What
finished right after the decision?". To discern the two questions, existing
solutions have relied on answer overlaps as a proxy label to contrast similar
and dissimilar questions. However, we claim that answer overlap can lead to
unreliable results, due to spurious overlaps of two dissimilar questions with
coincidentally identical answers. To address the issue, we propose a novel
approach that elicits proper reasoning behaviors through a module for
predicting time spans of events. We introduce the Timeline Reasoning Network
(TRN) operating in a two-step inductive reasoning process: In the first step
model initially answers each question with semantic and syntactic information.
The next step chains multiple questions on the same event to predict a
timeline, which is then used to ground the answers. Results on the TORQUE and
TB-dense, TRC and TRE tasks respectively, demonstrate that TRN outperforms
previous methods by effectively resolving the spurious overlaps using the
predicted timeline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 18th Conference of the European Chapter of the
  Association for Computational Linguistics (Volume 1: Long Papers), pages
  1689-1700</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Detection of Implicit Influential Patterns in <span class="highlight-title">Conversation</span>s
  via Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sina Abdidizaji, Md Kowsher, Niloofar Yousefi, Ivan Garibay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of digitalization, as individuals increasingly rely on digital
platforms for communication and news consumption, various actors employ
linguistic strategies to influence public perception. While models have become
proficient at detecting explicit patterns, which typically appear in texts as
single remarks referred to as utterances, such as social media posts, malicious
actors have shifted toward utilizing implicit influential verbal patterns
embedded within conversations. These verbal patterns aim to mentally penetrate
the victim's mind in order to influence them, enabling the actor to obtain the
desired information through implicit means. This paper presents an improved
approach for detecting such implicit influential patterns. Furthermore, the
proposed model is capable of identifying the specific locations of these
influential elements within a conversation. To achieve this, the existing
dataset was augmented using the reasoning capabilities of state-of-the-art
language models. Our designed framework resulted in a 6% improvement in the
detection of implicit influential patterns in conversations. Moreover, this
approach improved the multi-label classification tasks related to both the
techniques used for influence and the vulnerability of victims by 33% and 43%,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the HCI International conference 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Chen Zhang, Zheng Zhou, Yu-Jie Xiong, Chun-Ming Xia, Fei Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training data has been proven to be one of the most critical components in
training generative AI. However, obtaining high-quality data remains
challenging, with data privacy issues presenting a significant hurdle. To
address the need for high-quality data. Synthesize data has emerged as a
mainstream solution, demonstrating impressive performance in areas such as
images, audio, and video. Generating mixed-type data, especially high-quality
tabular data, still faces significant challenges. These primarily include its
inherent heterogeneous data types, complex inter-variable relationships, and
intricate column-wise distributions. In this paper, we introduce CausalDiffTab,
a diffusion model-based generative model specifically designed to handle mixed
tabular data containing both numerical and categorical features, while being
more flexible in capturing complex interactions among variables. We further
propose a hybrid adaptive causal regularization method based on the principle
of Hierarchical Prior Fusion. This approach adaptively controls the weight of
causal regularization, enhancing the model's performance without compromising
its generative capabilities. Comprehensive experiments conducted on seven
datasets demonstrate that CausalDiffTab outperforms baseline methods across all
metrics. Our code is publicly available at:
https://github.com/Godz-z/CausalDiffTab.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Agent</span>Synth: Scalable Task Generation for Generalist Computer-Use <span class="highlight-title">Agent</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingxu Xie, Dylan Xu, Xuandong Zhao, Dawn Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce AgentSynth, a scalable and cost-efficient pipeline for
automatically synthesizing high-quality tasks and trajectory datasets for
generalist computer-use agents. Leveraging information asymmetry, AgentSynth
constructs subtasks that are simple during generation but significantly more
challenging when composed into long-horizon tasks, enabling the creation of
over 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based
task proposer guided by a persona, followed by an execution agent that
completes the task and logs the trajectory. This process is repeated
iteratively to form a sequence of subtasks, which are then summarized by a
separate agent into a composite task of controllable difficulty. A key strength
of AgentSynth is its ability to precisely modulate task complexity by varying
the number of subtasks. Empirical evaluations show that state-of-the-art LLM
agents suffer a steep performance drop, from 18% success at difficulty level 1
to just 4% at level 6, highlighting the benchmark's difficulty and
discriminative power. Moreover, our pipeline achieves a low average cost of
\$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our
code and data are publicly available at
https://github.com/sunblaze-ucb/AgentSynth
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Practical Aspects of End-to-End Multi-Talker Speech
  Recognition for Online and Offline Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aswin Shanmugam Subramanian, Amit Das, Naoyuki Kanda, Jinyu Li, Xiaofei Wang, Yifan Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We extend the frameworks of Serialized Output Training (SOT) to address
practical needs of both streaming and offline automatic speech recognition
(ASR) applications. Our approach focuses on balancing latency and accuracy,
catering to real-time captioning and summarization requirements. We propose
several key improvements: (1) Leveraging Continuous Speech Separation (CSS)
single-channel front-end with end-to-end (E2E) systems for highly overlapping
scenarios, challenging the conventional wisdom of E2E versus cascaded setups.
The CSS framework improves the accuracy of the ASR system by separating
overlapped speech from multiple speakers. (2) Implementing dual models --
Conformer Transducer for streaming and Sequence-to-Sequence for offline -- or
alternatively, a two-pass model based on cascaded encoders. (3) Exploring
segment-based SOT (segSOT) which is better suited for offline scenarios while
also enhancing readability of multi-talker transcriptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intended Target Identification for Anomia Patients with Gradient-based
  Selective Augmentation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongho Kim, Romain Storaï, Seung-won Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we investigate the potential of language models (LMs) in
aiding patients experiencing anomia, a difficulty identifying the names of
items. Identifying the intended target item from patient's circumlocution
involves the two challenges of term failure and error: (1) The terms relevant
to identifying the item remain unseen. (2) What makes the challenge unique is
inherent perturbed terms by semantic paraphasia, which are not exactly related
to the target item, hindering the identification process. To address each, we
propose robustifying the model from semantically paraphasic errors and
enhancing the model with unseen terms with gradient-based selective
augmentation. Specifically, the gradient value controls augmented data quality
amid semantic errors, while the gradient variance guides the inclusion of
unseen but relevant terms. Due to limited domain-specific datasets, we evaluate
the model on the Tip-of-the-Tongue dataset as an intermediary task and then
apply our findings to real patient data from AphasiaBank. Our results
demonstrate strong performance against baselines, aiding anomia patients by
addressing the outlined challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings (long)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELI-Why: Evaluating the Pedagogical Utility of Language Model
  Explanations <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brihi Joshi, Keyu He, Sahana Ramnath, Sadra Sabouri, Kaitlyn Zhou, Souti Chattopadhyay, Swabha Swayamdipta, Xiang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models today are widely used in education, yet their ability to
tailor responses for learners with varied informational needs and knowledge
backgrounds remains under-explored. To this end, we introduce ELI-Why, a
benchmark of 13.4K "Why" questions to evaluate the pedagogical capabilities of
language models. We then conduct two extensive human studies to assess the
utility of language model-generated explanatory answers (explanations) on our
benchmark, tailored to three distinct educational grades: elementary,
high-school and graduate school. In our first study, human raters assume the
role of an "educator" to assess model explanations' fit to different
educational grades. We find that GPT-4-generated explanations match their
intended educational background only 50% of the time, compared to 79% for lay
human-curated explanations. In our second study, human raters assume the role
of a learner to assess if an explanation fits their own informational needs.
Across all educational backgrounds, users deemed GPT-4-generated explanations
20% less suited on average to their informational needs, when compared to
explanations curated by lay people. Additionally, automated evaluation metrics
reveal that explanations generated across different language model families for
different informational needs remain indistinguishable in their grade-level,
limiting their pedagogical effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAS-LitEval : <span class="highlight-title">Multi-Agent</span> System for Literary Translation Quality
  Assessment <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junghwan Kim, Kieun Park, Sohee Park, Hyunggug Kim, Bongwon Suh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Literary translation requires preserving cultural nuances and stylistic
elements, which traditional metrics like BLEU and METEOR fail to assess due to
their focus on lexical overlap. This oversight neglects the narrative
consistency and stylistic fidelity that are crucial for literary works. To
address this, we propose MAS-LitEval, a multi-agent system using Large Language
Models (LLMs) to evaluate translations based on terminology, narrative, and
style. We tested MAS-LitEval on translations of The Little Prince and A
Connecticut Yankee in King Arthur's Court, generated by various LLMs, and
compared it to traditional metrics. \textbf{MAS-LitEval} outperformed these
metrics, with top models scoring up to 0.890 in capturing literary nuances.
This work introduces a scalable, nuanced framework for Translation Quality
Assessment (TQA), offering a practical tool for translators and researchers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 Pages, 2 tables, EMNLP submitted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14190v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14190v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan Nguyen, Huy-Dat Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing code-switched ASR systems is challenging due to language ambiguity
and limited exposure to multilingual, code-switched data, while collecting such
speech is costly. Prior work generates synthetic audio from text, but these
methods are computationally intensive and hard to scale. We introduce
AsyncSwitch, a novel asynchronous adaptation framework that leverages
large-scale, text-rich web data to pre-expose ASR models to diverse
code-switched domains before fine-tuning on paired speech-text corpora. Our
three-stage process (1) trains decoder self-attention and feedforward layers on
code-switched text, (2) aligns decoder and encoder via cross-attention using
limited speech-text data, and (3) fully fine-tunes the entire model.
Experiments with Whisper on Malay-English code-switching demonstrate a 9.02%
relative WER reduction, while improving monolingual performance in Singlish,
Malay, and other English variants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  This paper is a preprint version submitted to the 2025 IEEE Automatic Speech
  Recognition and Understanding Workshop (ASRU 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can we train ASR systems on Code-switch without real code-switch data?
  Case study for Singapore's languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan Nguyen, Huy-Dat Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code-switching (CS), common in multilingual settings, presents challenges for
ASR due to scarce and costly transcribed data caused by linguistic complexity.
This study investigates building CS-ASR using synthetic CS data. We propose a
phrase-level mixing method to generate synthetic CS data that mimics natural
patterns. Utilizing monolingual augmented with synthetic phrase-mixed CS data
to fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T). This
paper focuses on three under-resourced Southeast Asian language pairs:
Malay-English (BM-EN), Mandarin-Malay (ZH-BM), and Tamil-English (TA-EN),
establishing a new comprehensive benchmark for CS-ASR to evaluate the
performance of leading ASR models. Experimental results show that the proposed
training strategy enhances ASR performance on monolingual and CS tests, with
BM-EN showing highest gains, then TA-EN and ZH-BM. This finding offers a
cost-effective approach for CS-ASR development, benefiting research and
industry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Interspeech 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRAM: A Generative Foundation Reward Model for Reward Generalization <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenglong Wang, Yang Gan, Yifu Huo, Yongyu Mu, Qiaozhi He, Murun Yang, Bei Li, Tong Xiao, Chunliang Zhang, Tongran Liu, Jingbo Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In aligning large language models (LLMs), reward models have played an
important role, but are standardly trained as discriminative models and rely
only on labeled human preference data. In this paper, we explore methods that
train reward models using both unlabeled and labeled data. Building on the
generative models in LLMs, we develop a generative reward model that is first
trained via large-scale unsupervised learning and then fine-tuned via
supervised learning. We also show that by using label smoothing, we are in fact
optimizing a regularized pairwise ranking loss. This result, in turn, provides
a new view of training reward models, which links generative models and
discriminative models under the same class of training objectives. The outcome
of these techniques is a foundation reward model, which can be applied to a
wide range of tasks with little or no further fine-tuning effort. Extensive
experiments show that this model generalizes well across several tasks,
including response ranking, reinforcement learning from human feedback, and
task adaptation with fine-tuning, achieving significant performance
improvements over several strong baseline models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation
  of LLMs via Theory of Mind 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanlin Li, Hao Liu, Huimin Liu, Yinwei Wei, Yupeng Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Theory of Mind (ToM) in Large Language Models (LLMs) refers to their capacity
for reasoning about mental states, yet failures in this capacity often manifest
as systematic implicit bias. Evaluating this bias is challenging, as
conventional direct-query methods are susceptible to social desirability
effects and fail to capture its subtle, multi-dimensional nature. To this end,
we propose an evaluation framework that leverages the Stereotype Content Model
(SCM) to reconceptualize bias as a multi-dimensional failure in ToM across
Competence, Sociability, and Morality. The framework introduces two indirect
tasks: the Word Association Bias Test (WABT) to assess implicit lexical
associations and the Affective Attribution Test (AAT) to measure covert
affective leanings, both designed to probe latent stereotypes without
triggering model avoidance. Extensive experiments on 8 State-of-the-Art LLMs
demonstrate our framework's capacity to reveal complex bias structures,
including pervasive sociability bias, multi-dimensional divergence, and
asymmetric stereotype amplification, thereby providing a more robust
methodology for identifying the structural nature of implicit bias.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for
  Efficient Inference of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao He, Guang Huang, Yu Yang, Tianshi Xu, Sicheng Zhao, Guiguang Ding, Pengyang Wang, Feng Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit remarkable reasoning capabilities across
diverse downstream tasks. However, their autoregressive nature leads to
substantial inference latency, posing challenges for real-time applications.
Speculative sampling mitigates this issue by introducing a drafting phase
followed by a parallel validation phase, enabling faster token generation and
verification. Existing approaches, however, overlook the inherent coherence in
text generation, limiting their efficiency. To address this gap, we propose a
Speculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework,
which extends speculative sampling by leveraging multi-head drafting for rapid
token generation and a continuous verification tree for efficient candidate
validation and feature reuse. Experimental results demonstrate that S$^4$C
surpasses baseline methods across mainstream tasks, offering enhanced
efficiency, parallelism, and the ability to generate more valid tokens with
fewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an
acceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DCRM: A Heuristic to Measure Response Pair Quality in Preference
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyu Huang, Tanya Goyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has attempted to associate preference optimization (PO)
performance with the underlying preference datasets. In this work, our
observation is that the differences between the preferred response $y^+$ and
dispreferred response $y^-$ influence what LLMs can learn, which may not match
the desirable differences to learn. Therefore, we use distance and reward
margin to quantify these differences, and combine them to get Distance
Calibrated Reward Margin (DCRM), a metric that measures the quality of a
response pair for PO. Intuitively, DCRM encourages minimal noisy differences
and maximal desired differences. With this, we study 3 types of commonly used
preference datasets, classified along two axes: the source of the responses and
the preference labeling function. We establish a general correlation between
higher DCRM of the training set and better learning outcome. Inspired by this,
we propose a best-of-$N^2$ pairing method that selects response pairs with the
highest DCRM. Empirically, in various settings, our method produces training
datasets that can further improve models' performance on AlpacaEval, MT-Bench,
and Arena-Hard over the existing training sets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pushing the Performance of Synthetic Speech Detection with
  Kolmogorov-Arnold Networks and Self-Supervised Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan Dat Phuong, Long-Vu Hoang, Huy Dat Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in speech synthesis technologies have led to increasingly
advanced spoofing attacks, posing significant challenges for automatic speaker
verification systems. While systems based on self-supervised learning (SSL)
models, particularly the XLSR-Conformer model, have demonstrated remarkable
performance in synthetic speech detection, there remains room for architectural
improvements. In this paper, we propose a novel approach that replaces the
traditional Multi-Layer Perceptron in the XLSR-Conformer model with a
Kolmogorov-Arnold Network (KAN), a novel architecture based on the
Kolmogorov-Arnold representation theorem. Our results on ASVspoof2021
demonstrate that integrating KAN into the SSL-based models can improve the
performance by 60.55% relatively on LA and DF sets, further achieving 0.70% EER
on the 21LA set. These findings suggest that incorporating KAN into SSL-based
models is a promising direction for advances in synthetic speech detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Acoustic scattering AI for non-invasive object classifications: A case
  study on hair assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long-Vu Hoang, Tuan Nguyen, Tran Huy Dat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel non-invasive object classification approach using
acoustic scattering, demonstrated through a case study on hair assessment. When
an incident wave interacts with an object, it generates a scattered acoustic
field encoding structural and material properties. By emitting acoustic stimuli
and capturing the scattered signals from head-with-hair-sample objects, we
classify hair type and moisture using AI-driven, deep-learning-based sound
classification. We benchmark comprehensive methods, including (i) fully
supervised deep learning, (ii) embedding-based classification, (iii) supervised
foundation model fine-tuning, and (iv) self-supervised model fine-tuning. Our
best strategy achieves nearly 90% classification accuracy by fine-tuning all
parameters of a self-supervised model. These results highlight acoustic
scattering as a privacy-preserving, non-contact alternative to visual
classification, opening huge potential for applications in various industries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RadFabric: <span class="highlight-title">Agent</span>ic AI System with Reasoning Capability for Radiology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenting Chen, Yi Dong, Zhaojun Ding, Yucheng Shi, Yifan Zhou, Fang Zeng, Yijun Luo, Tianyu Lin, Yihang Su, Yichen Wu, Kai Zhang, Zhen Xiang, Tianming Liu, Ninghao Liu, Lichao Sun, Yixuan Yuan, Xiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic
conditions, but current automated systems face limitations in pathology
coverage, diagnostic accuracy, and integration of visual and textual reasoning.
To address these gaps, we propose RadFabric, a multi agent, multimodal
reasoning framework that unifies visual and textual analysis for comprehensive
CXR interpretation. RadFabric is built on the Model Context Protocol (MCP),
enabling modularity, interoperability, and scalability for seamless integration
of new diagnostic agents. The system employs specialized CXR agents for
pathology detection, an Anatomical Interpretation Agent to map visual findings
to precise anatomical structures, and a Reasoning Agent powered by large
multimodal reasoning models to synthesize visual, anatomical, and clinical data
into transparent and evidence based diagnoses. RadFabric achieves significant
performance improvements, with near-perfect detection of challenging
pathologies like fractures (1.000 accuracy) and superior overall diagnostic
accuracy (0.799) compared to traditional systems (0.229 to 0.527). By
integrating cross modal feature alignment and preference-driven reasoning,
RadFabric advances AI-driven radiology toward transparent, anatomically
precise, and clinically actionable CXR analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sampling from Your Language Model One Byte at a Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Hayase, Alisa Liu, Noah A. Smith, Sewoong Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tokenization is used almost universally by modern language models, enabling
efficient text representation using multi-byte or multi-character tokens.
However, prior work has shown that tokenization can introduce distortion into
the model's generations. For example, users are often advised not to end their
prompts with a space because it prevents the model from including the space as
part of the next token. This Prompt Boundary Problem (PBP) also arises in
languages such as Chinese and in code generation, where tokens often do not
line up with syntactic boundaries. Additionally mismatching tokenizers often
hinder model composition and interoperability. For example, it is not possible
to directly ensemble models with different tokenizers due to their mismatching
vocabularies. To address these issues, we present an inference-time method to
convert any autoregressive LM with a BPE tokenizer into a character-level or
byte-level LM, without changing its generative distribution at the text level.
Our method efficient solves the PBP and is also able to unify the vocabularies
of language models with different tokenizers, allowing one to ensemble LMs with
different tokenizers at inference time as well as transfer the post-training
from one model to another using proxy-tuning. We demonstrate in experiments
that the ensemble and proxy-tuned models outperform their constituents on
downstream evals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Essential-Web v1.0: 24T tokens of organized web data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Essential AI,  :, Andrew Hojel, Michael Pust, Tim Romanski, Yash Vanjani, Ritvik Kapila, Mohit Parmar, Adarsh Chaluvaraju, Alok Tripathy, Anil Thomas, Ashish Tanwer, Darsh J Shah, Ishaan Shah, Karl Stratos, Khoi Nguyen, Kurt Smith, Michael Callahan, Peter Rushton, Philip Monk, Platon Mazarakis, Saad Jamal, Saurabh Srivastava, Somanshu Singla, Ashish Vaswani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data plays the most prominent role in how language models acquire skills and
knowledge. The lack of massive, well-organized pre-training datasets results in
costly and inaccessible data pipelines. We present Essential-Web v1.0, a
24-trillion-token dataset in which every document is annotated with a
twelve-category taxonomy covering topic, format, content complexity, and
quality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned
0.5b-parameter model that achieves an annotator agreement within 3% of
Qwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain
competitive web-curated datasets in math (-8.0% relative to SOTA), web code
(+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on
HuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Innovating China's Intangible Cultural Heritage with DeepSeek +
  MidJourney: The Case of Yangliuqing theme Woodblock Prints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        RuiKun Yang, ZhongLiang Wei, Longdi Xian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Yangliuqing woodblock prints, a cornerstone of China's intangible cultural
heritage, are celebrated for their intricate designs and vibrant colors.
However, preserving these traditional art forms while fostering innovation
presents significant challenges. This study explores the DeepSeek + MidJourney
approach to generating creative, themed Yangliuqing woodblock prints focused on
the fight against COVID-19 and depicting joyous winners. Using Fr\'echet
Inception Distance (FID) scores for evaluation, the method that combined
DeepSeek-generated thematic prompts, MidJourney-generated thematic images,
original Yangliuqing prints, and DeepSeek-generated key prompts in
MidJourney-generated outputs achieved the lowest mean FID score (150.2) with
minimal variability ({\sigma} = 4.9). Additionally, feedback from 62
participants, collected via questionnaires, confirmed that this hybrid approach
produced the most representative results. Moreover, the questionnaire data
revealed that participants demonstrated the highest willingness to promote
traditional culture and the strongest interest in consuming the AI-generated
images produced through this method. These findings underscore the
effectiveness of an innovative approach that seamlessly blends traditional
artistic elements with modern AI-driven creativity, ensuring both cultural
preservation and contemporary relevance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Abstract Meaning Representation for Hospital Discharge Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Landes, Sitara Rao, Aaron Jeremy Chaise, Barbara Di Eugenio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Achilles heel of Large Language Models (LLMs) is hallucination, which has
drastic consequences for the clinical domain. This is particularly important
with regards to automatically generating discharge summaries (a lengthy medical
document that summarizes a hospital in-patient visit). Automatically generating
these summaries would free physicians to care for patients and reduce
documentation burden. The goal of this work is to discover new methods that
combine language-based graphs and deep learning models to address provenance of
content and trustworthiness in automatic summarization. Our method shows
impressive reliability results on the publicly available Medical Information
Mart for Intensive III (MIMIC-III) corpus and clinical notes written by
physicians at Anonymous Hospital. rovide our method, generated discharge ary
output examples, source code and trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InsertRank: LLMs can reason over BM25 scores to Improve Listwise
  Reranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Seetharaman, Kaustubh D. Dhole, Aman Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated significant strides across
various information retrieval tasks, particularly as rerankers, owing to their
strong generalization and knowledge-transfer capabilities acquired from
extensive pretraining. In parallel, the rise of LLM-based chat interfaces has
raised user expectations, encouraging users to pose more complex queries that
necessitate retrieval by ``reasoning'' over documents rather than through
simple keyword matching or semantic similarity. While some recent efforts have
exploited reasoning abilities of LLMs for reranking such queries, considerable
potential for improvement remains. In that regards, we introduce InsertRank, an
LLM-based reranker that leverages lexical signals like BM25 scores during
reranking to further improve retrieval performance. InsertRank demonstrates
improved retrieval effectiveness on -- BRIGHT, a reasoning benchmark spanning
12 diverse domains, and R2MED, a specialized medical reasoning retrieval
benchmark spanning 8 different tasks. We conduct an exhaustive evaluation and
several ablation studies and demonstrate that InsertRank consistently improves
retrieval effectiveness across multiple families of LLMs, including GPT,
Gemini, and Deepseek models. %In addition, we also conduct ablation studies on
normalization by varying the scale of the BM25 scores, and positional bias by
shuffling the order of the documents. With Deepseek-R1, InsertRank achieves a
score of 37.5 on the BRIGHT benchmark. and 51.1 on the R2MED benchmark,
surpassing previous methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soufiane Hayou, Liyuan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretraining large language models is a costly process. To make this process
more efficient, several methods have been proposed to optimize model
architecture/parametrization and hardware use. On the parametrization side,
$\mu P$ (Maximal Update Parametrization) parametrizes model weights and
learning rate (LR) in a way that makes hyperparameters (HPs) transferable with
width (embedding dimension): HPs can be tuned for a small model and used for
larger models without additional tuning. While $\mu$P showed impressive results
in practice, recent empirical studies have reported conflicting observations
when applied to LLMs. One limitation of the theory behind $\mu$P is the fact
that input dimension (vocabulary size in LLMs) is considered fixed when taking
the width to infinity. This is unrealistic since vocabulary size is generally
much larger than width in practice. In this work, we provide a theoretical
analysis of the effect of vocabulary size on training dynamics, and
subsequently show that as vocabulary size increases, the training dynamics
\emph{interpolate between the $\mu$P regime and another regime that we call
Large Vocab (LV) Regime}, where optimal scaling rules are different from those
predicted by $\mu$P. Our analysis reveals that in the LV regime, the optimal
embedding LR to hidden LR ratio should roughly scale as $\Theta(\sqrt{width})$,
surprisingly close to the empirical findings previously reported in the
literature, and different from the $\Theta(width)$ ratio predicted by $\mu$P.
We conduct several experiments to validate our theory, and pretrain a 1B model
from scratch to show the benefit of our suggested scaling rule for the
embedding LR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TD,LR: How to set the learning rate for emebdding layer in LLMs?</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Memory</span> Tokens: Large Language Models Can Generate Reversible Sentence
  Embeddings <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ignacio Sastre, Aiala Rosá
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we observe an interesting phenomenon: it is possible to
generate reversible sentence embeddings that allow an LLM to reconstruct the
original text exactly, without modifying the model's weights. This is achieved
by introducing a special memory token, whose embedding is optimized through
training on a fixed sequence. When prompted with this embedding, the model
reconstructs the fixed sequence exactly. We evaluate this phenomenon across
English and Spanish datasets, sequences of up to approximately 240 tokens, and
model scales ranging from 100M to 8B parameters. Notably, Llama 3.1 8B
successfully reconstructs all tested sequences. Our findings highlight an
interesting capability of LLMs and suggest potential applications in
memory-based retrieval, compression, and controlled text generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper will be presented at The First Workshop on Large Language
  Model Memorization (L2M2) at ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hypothesis Testing for Quantifying LLM-<span class="highlight-title">Human</span> Misalignment in Multiple
  Choice Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harbin Hong, Sebastian Caldas, Liu Leqi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) increasingly appear in social science
research (e.g., economics and marketing), it becomes crucial to assess how well
these models replicate human behavior. In this work, using hypothesis testing,
we present a quantitative framework to assess the misalignment between
LLM-simulated and actual human behaviors in multiple-choice survey settings.
This framework allows us to determine in a principled way whether a specific
language model can effectively simulate human opinions, decision-making, and
general behaviors represented through multiple-choice options. We applied this
framework to a popular language model for simulating people's opinions in
various public surveys and found that this model is ill-suited for simulating
the tested sub-populations (e.g., across different races, ages, and incomes)
for contentious questions. This raises questions about the alignment of this
language model with the tested populations, highlighting the need for new
practices in using LLMs for social science studies beyond naive simulations of
human subjects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhoujun Cheng, Shibo Hao, Tianyang Liu, Fan Zhou, Yutao Xie, Feng Yao, Yuexin Bian, Yonghao Zhuang, Nilabjo Dey, Yuheng Zha, Yi Gu, Kun Zhou, Yuqi Wang, Yuan Li, Richard Fan, Jianshu She, Chengqian Gao, Abulhair Saparov, Haonan Li, Taylor W. Killian, Mikhail Yurochkin, Zhengzhong Liu, Eric P. Xing, Zhiting Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has emerged as a promising approach to improve
large language model (LLM) reasoning, yet most open efforts focus narrowly on
math and code, limiting our understanding of its broader applicability to
general reasoning. A key challenge lies in the lack of reliable, scalable RL
reward signals across diverse reasoning domains. We introduce Guru, a curated
RL reasoning corpus of 92K verifiable examples spanning six reasoning
domains--Math, Code, Science, Logic, Simulation, and Tabular--each built
through domain-specific reward design, deduplication, and filtering to ensure
reliability and effectiveness for RL training. Based on Guru, we systematically
revisit established findings in RL for LLM reasoning and observe significant
variation across domains. For example, while prior work suggests that RL
primarily elicits existing knowledge from pretrained models, our results reveal
a more nuanced pattern: domains frequently seen during pretraining (Math, Code,
Science) easily benefit from cross-domain RL training, while domains with
limited pretraining exposure (Logic, Simulation, and Tabular) require in-domain
training to achieve meaningful performance gains, suggesting that RL is likely
to facilitate genuine skill acquisition. Finally, we present Guru-7B and
Guru-32B, two models that achieve state-of-the-art performance among open
models RL-trained with publicly available data, outperforming best baselines by
7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We
also show that our models effectively improve the Pass@k performance of their
base models, particularly on complex tasks less likely to appear in pretraining
data. We release data, models, training and evaluation code to facilitate
general-purpose reasoning at: https://github.com/LLM360/Reasoning360
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 9 figures. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Chat to Checkup: Can Large Language Models Assist in Diabetes
  Prediction? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shadman Sakib, Oishy Fatema Akhand, Ajwad Abrar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Machine Learning (ML) and Deep Learning (DL) models have been widely
used for diabetes prediction, the use of Large Language Models (LLMs) for
structured numerical data is still not well explored. In this study, we test
the effectiveness of LLMs in predicting diabetes using zero-shot, one-shot, and
three-shot prompting methods. We conduct an empirical analysis using the Pima
Indian Diabetes Database (PIDD). We evaluate six LLMs, including four
open-source models: Gemma-2-27B, Mistral-7B, Llama-3.1-8B, and Llama-3.2-2B. We
also test two proprietary models: GPT-4o and Gemini Flash 2.0. In addition, we
compare their performance with three traditional machine learning models:
Random Forest, Logistic Regression, and Support Vector Machine (SVM). We use
accuracy, precision, recall, and F1-score as evaluation metrics. Our results
show that proprietary LLMs perform better than open-source ones, with GPT-4o
and Gemma-2-27B achieving the highest accuracy in few-shot settings. Notably,
Gemma-2-27B also outperforms the traditional ML models in terms of F1-score.
However, there are still issues such as performance variation across prompting
strategies and the need for domain-specific fine-tuning. This study shows that
LLMs can be useful for medical prediction tasks and encourages future work on
prompt engineering and hybrid approaches to improve healthcare predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in 1st IEEE QPAIN 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with
  Knowledge Guidance <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph J. Peper, Wenzhao Qiu, Ali Payani, Lu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language processing evaluation has made significant progress, largely
driven by the proliferation of powerful large language mod-els (LLMs). New
evaluation benchmarks are of increasing priority as the reasoning capabilities
of LLMs are expanding at a rapid pace. In particular, while multi-document (MD)
reasoning is an area of extreme relevance given LLM capabilities in handling
longer-context inputs, few benchmarks exist to rigorously examine model
behavior in this setting. Moreover, the multi-document setting is historically
challenging for benchmark creation due to the expensive cost of annotating long
inputs. In this work, we introduce MDBench, a new dataset for evaluating LLMs
on the task of multi-document reasoning. Notably, MDBench is created through a
novel synthetic generation process, allowing us to controllably and efficiently
generate challenging document sets and the corresponding question-answer (QA)
examples. Our novel technique operates on condensed structured seed knowledge,
modifying it through LLM-assisted edits to induce MD-specific reasoning
challenges. We then convert this structured knowledge into a natural text
surface form, generating a document set and corresponding QA example. We
analyze the behavior of popular LLMs and prompting techniques, finding that
MDBENCH poses significant challenges for all methods, even with relatively
short document sets. We also see our knowledge-guided generation technique (1)
allows us to readily perform targeted analysis of MD-specific reasoning
capabilities and (2) can be adapted quickly to account for new challenges and
future modeling improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrEst: Credibility Estimation for Contexts in LLMs via Weak Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dyah Adila, Shuai Zhang, Boran Han, Bonan Min, Yuyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of contextual information has significantly enhanced the
performance of large language models (LLMs) on knowledge-intensive tasks.
However, existing methods often overlook a critical challenge: the credibility
of context documents can vary widely, potentially leading to the propagation of
unreliable information. In this paper, we introduce CrEst, a novel weakly
supervised framework for assessing the credibility of context documents during
LLM inference--without requiring manual annotations. Our approach is grounded
in the insight that credible documents tend to exhibit higher semantic
coherence with other credible documents, enabling automated credibility
estimation through inter-document agreement. To incorporate credibility into
LLM inference, we propose two integration strategies: a black-box approach for
models without access to internal weights or activations, and a white-box
method that directly modifies attention mechanisms. Extensive experiments
across three model architectures and five datasets demonstrate that CrEst
consistently outperforms strong baselines, achieving up to a 26.86% improvement
in accuracy and a 3.49% increase in F1 score. Further analysis shows that CrEst
maintains robust performance even under high-noise conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining Constrained and Unconstrained Decoding via Boosting: BoostCD
  and Its Application to Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marija Šakota, Robert West
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent approaches to structured NLP tasks use an autoregressive language
model $M$ to map unstructured input text $x$ to output text $y$ representing
structured objects (such as tuples, lists, trees, code, etc.), where the
desired output structure is enforced via constrained decoding. During training,
these approaches do not require the model to be aware of the constraints, which
are merely implicit in the training outputs $y$. This is advantageous as it
allows for dynamic constraints without requiring retraining, but can lead to
low-quality output during constrained decoding at test time. We overcome this
problem with Boosted Constrained Decoding (BoostCD), which combines constrained
and unconstrained decoding in two phases: Phase 1 decodes from the base model
$M$ twice, in constrained and unconstrained mode, obtaining two weak
predictions. In phase 2, a learned autoregressive boosted model combines the
two weak predictions into one final prediction. The mistakes made by the base
model with vs. without constraints tend to be complementary, which the boosted
model learns to exploit for improved performance. We demonstrate the power of
BoostCD by applying it to closed information extraction. Our model, BoostIE,
outperforms prior approaches both in and out of distribution, addressing
several common errors identified in those approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adverse Event Extraction from Discharge Summaries: A New Dataset,
  Annotation Scheme, and Initial Findings <span class="chip">ACL2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imane Guellil, Salomé Andres, Atul Anand, Bruce Guthrie, Huayu Zhang, Abul Hasan, Honghan Wu, Beatrice Alex
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a manually annotated corpus for Adverse Event (AE)
extraction from discharge summaries of elderly patients, a population often
underrepresented in clinical NLP resources. The dataset includes 14 clinically
significant AEs-such as falls, delirium, and intracranial haemorrhage, along
with contextual attributes like negation, diagnosis type, and in-hospital
occurrence. Uniquely, the annotation schema supports both discontinuous and
overlapping entities, addressing challenges rarely tackled in prior work. We
evaluate multiple models using FlairNLP across three annotation granularities:
fine-grained, coarse-grained, and coarse-grained with negation. While
transformer-based models (e.g., BERT-cased) achieve strong performance on
document-level coarse-grained extraction (F1 = 0.943), performance drops
notably for fine-grained entity-level tasks (e.g., F1 = 0.675), particularly
for rare events and complex attributes. These results demonstrate that despite
high-level scores, significant challenges remain in detecting underrepresented
AEs and capturing nuanced clinical language. Developed within a Trusted
Research Environment (TRE), the dataset is available upon request via DataLoch
and serves as a robust benchmark for evaluating AE extraction methods and
supporting future cross-dataset generalisation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and will be published at ACL2025 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chain-of-Thought Reasoning In The Wild Is Not Always Faithful <span class="chip">ICLR 25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.08679v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.08679v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iván Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran Rajamanoharan, Neel Nanda, Arthur Conmy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art
AI capabilities. However, recent studies have shown that CoT reasoning is not
always faithful when models face an explicit bias in their prompts, i.e., the
CoT can give an incorrect picture of how models arrive at conclusions. We go
further and show that unfaithful CoT can also occur on realistic prompts with
no artificial bias. We find that when separately presented with the questions
"Is X bigger than Y?" and "Is Y bigger than X?", models sometimes produce
superficially coherent arguments to justify systematically answering Yes to
both questions or No to both questions, despite such responses being logically
contradictory. We show preliminary evidence that this is due to models'
implicit biases towards Yes or No, thus labeling this unfaithfulness as
Implicit Post-Hoc Rationalization. Our results reveal that several production
models exhibit surprisingly high rates of post-hoc rationalization in our
settings: GPT-4o-mini (13%) and Haiku 3.5 (7%). While frontier models are more
faithful, especially thinking ones, none are entirely faithful: Gemini 2.5
Flash (2.17%), ChatGPT-4o (0.49%), DeepSeek R1 (0.37%), Gemini 2.5 Pro (0.14%),
and Sonnet 3.7 with thinking (0.04%). We also investigate Unfaithful Illogical
Shortcuts, where models use subtly illogical reasoning to try to make a
speculative answer to hard maths problems seem rigorously proven. Our findings
raise challenges for strategies for detecting undesired behavior in LLMs via
the chain of thought.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Reasoning and Planning for LLMs Workshop (ICLR 25),
  10 main paper pages, 39 appendix pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controllable and Reliable Knowledge-Intensive Task-Oriented
  <span class="highlight-title">Conversation</span>al <span class="highlight-title">Agent</span>s with Declarative Genie Worksheets <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05674v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05674v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harshit Joshi, Shicheng Liu, James Chen, Robert Weigle, Monica S. Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models can carry out human-like conversations in diverse
settings, responding to user requests for tasks and knowledge. However,
existing conversational agents implemented with LLMs often struggle with
hallucination, following instructions with conditional logic, and integrating
knowledge from different sources. These shortcomings compromise the agents'
effectiveness, rendering them unsuitable for deployment. To address these
challenges, we introduce Genie, a programmable framework for creating
knowledge-intensive task-oriented conversational agents. Genie can handle
involved interactions and answer complex queries. Unlike LLMs, it delivers
reliable, grounded responses through advanced dialogue state management and
supports controllable agent policies via its declarative specification -- Genie
Worksheet. This is achieved through an algorithmic runtime system that
implements the developer-supplied policy, limiting LLMs to (1) parse user input
using a succinct conversational history, and (2) generate responses according
to supplied context. Agents built with Genie outperform SOTA methods on complex
logic dialogue datasets. We conducted a user study with 62 participants on
three real-life applications: restaurant reservations with Yelp, as well as
ticket submission and course enrollment for university students. Genie agents
with GPT-4 Turbo outperformed the GPT-4 Turbo agents with function calling,
improving goal completion rates from 21.8% to 82.8% across three real-world
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SOPBench: Evaluating Language <span class="highlight-title">Agent</span>s at Following Standard Operating
  Procedures and Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.08669v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.08669v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Li, Shinda Huang, Jiangtian Wang, Nathan Zhang, Antonis Antoniades, Wenyue Hua, Kaijie Zhu, Sirui Zeng, Chi Wang, William Yang Wang, Xifeng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As language agents increasingly automate critical tasks, their ability to
follow domain-specific standard operating procedures (SOPs), policies, and
constraints when taking actions and making tool calls becomes essential yet
remains underexplored. To address this gap, we develop an automated evaluation
pipeline SOPBench with: (1) executable environments containing 167
tools/functions across seven customer service domains with service-specific
SOPs and rule-based verifiers, (2) an automated test generation framework
producing over 900 verified test cases, and (3) an automated evaluation
framework to rigorously assess agent adherence from multiple dimensions. Our
approach transforms each service-specific SOP code program into a directed
graph of executable functions and requires agents to call these functions based
on natural language SOP descriptions. The original code serves as oracle
rule-based verifiers to assess compliance, reducing reliance on manual
annotations and LLM-based evaluations. We evaluate 18 leading models, and
results show the task is challenging even for top-tier models (like GPT-4o,
Claude-3.7-Sonnet), with variances across domains. Reasoning models like
o4-mini-high show superiority while other powerful models perform less
effectively (pass rates of 30%-50%), and small models (7B, 8B) perform
significantly worse. Additionally, language agents can be easily jailbroken to
overlook SOPs and constraints. Code, data, and over 24k agent trajectories are
released at https://github.com/Leezekun/SOPBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code, data, and over 24k agent trajectories are released at
  https://github.com/Leezekun/SOPBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Better Open-Ended Text Generation: A Multicriteria Evaluation
  Framework <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18653v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18653v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Esteban Garces Arias, Hannah Blocher, Julian Rodemann, Meimingwei Li, Christian Heumann, Matthias Aßenmacher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-ended text generation has become a prominent task in natural language
processing due to the rise of powerful (large) language models. However,
evaluating the quality of these models and the employed decoding strategies
remains challenging due to trade-offs among widely used metrics such as
coherence, diversity, and perplexity. This paper addresses the specific problem
of multicriteria evaluation for open-ended text generation, proposing novel
methods for both relative and absolute rankings of decoding methods.
Specifically, we employ benchmarking approaches based on partial orderings and
present a new summary metric to balance existing automatic indicators,
providing a more holistic evaluation of text generation quality. Our
experiments demonstrate that the proposed approaches offer a robust way to
compare decoding strategies and serve as valuable tools to guide model
selection for open-ended text generation tasks. We suggest future directions
for improving evaluation methodologies in text generation and make our code,
datasets, and models publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the $GEM^2$ Workshop (co-located with ACL 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Large Language Models to Measure Gender Representation Bias
  in Gendered Language Corpora <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13677v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13677v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Derner, Sara Sansalvador de la Fuente, Yoan Gutiérrez, Paloma Moreda, Nuria Oliver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often inherit and amplify social biases embedded
in their training data. A prominent social bias is gender bias. In this regard,
prior work has mainly focused on gender stereotyping bias - the association of
specific roles or traits with a particular gender - in English and on
evaluating gender bias in model embeddings or generated outputs. In contrast,
gender representation bias - the unequal frequency of references to individuals
of different genders - in the training corpora has received less attention. Yet
such imbalances in the training data constitute an upstream source of bias that
can propagate and intensify throughout the entire model lifecycle. To fill this
gap, we propose a novel LLM-based method to detect and quantify gender
representation bias in LLM training data in gendered languages, where
grammatical gender challenges the applicability of methods developed for
English. By leveraging the LLMs' contextual understanding, our approach
automatically identifies and classifies person-referencing words in gendered
language corpora. Applied to four Spanish-English benchmarks and five Valencian
corpora, our method reveals substantial male-dominant imbalances. We show that
such biases in training data affect model outputs, but can surprisingly be
mitigated leveraging small-scale training on datasets that are biased towards
the opposite gender. Our findings highlight the need for corpus-level gender
bias analysis in multilingual NLP. We make our code and data publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at the 6th Workshop on Gender Bias in
  Natural Language Processing (GeBNLP) at ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing the Reasoning Capabilities of LLMs in the context of
  Evidence-based Claim Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10735v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10735v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Dougrez-Lewis, Mahmud Elahi Akhter, Federico Ruggeri, Sebastian Löbbers, Yulan He, Maria Liakata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although LLMs have shown great performance on Mathematics and Coding related
reasoning tasks, the reasoning capabilities of LLMs regarding other forms of
reasoning are still an open problem. Here, we examine the issue of reasoning
from the perspective of claim verification. We propose a framework designed to
break down any claim paired with evidence into atomic reasoning types that are
necessary for verification. We use this framework to create RECV, the first
claim verification benchmark, incorporating real-world claims, to assess the
deductive and abductive reasoning capabilities of LLMs. The benchmark comprises
of three datasets, covering reasoning problems of increasing complexity. We
evaluate three state-of-the-art proprietary LLMs under multiple prompt
settings. Our results show that while LLMs can address deductive reasoning
problems, they consistently fail in cases of abductive reasoning. Moreover, we
observe that enhancing LLMs with rationale generation is not always beneficial.
Nonetheless, we find that generated rationales are semantically similar to
those provided by humans, especially in deductive reasoning cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally to this work. 25 pages, 3
  figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reparameterized LLM Training via Orthogonal Equivalence Transformation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08001v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08001v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeju Qiu, Simon Buchholz, Tim Z. Xiao, Maximilian Dax, Bernhard Schölkopf, Weiyang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) are driving the rapid advancement of
artificial intelligence, effectively and reliably training these large models
remains one of the field's most significant challenges. To address this
challenge, we propose POET, a novel reParameterized training algorithm that
uses Orthogonal Equivalence Transformation to optimize neurons. Specifically,
POET reparameterizes each neuron with two learnable orthogonal matrices and a
fixed random weight matrix. Because of its provable preservation of spectral
properties of weight matrices, POET can stably optimize the objective function
with improved generalization. We further develop efficient approximations that
make POET flexible and scalable for training large-scale neural networks.
Extensive experiments validate the effectiveness and scalability of POET in
training LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report v3 (38 pages, 26 figures, project page:
  https://spherelab.ai/poet/, v3: added singular spectrum and energy analyses
  in Section 4)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging Social Media and Search Engines: Dredge Words and the Detection
  of Unreliable Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11423v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11423v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evan M. Williams, Peter Carragher, Kathleen M. Carley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proactive content moderation requires platforms to rapidly and continuously
evaluate the credibility of websites. Leveraging the direct and indirect paths
users follow to unreliable websites, we develop a website credibility
classification and discovery system that integrates both webgraph and
large-scale social media contexts. We additionally introduce the concept of
dredge words, terms or phrases for which unreliable domains rank highly on
search engines, and provide the first exploration of their usage on social
media. Our graph neural networks that combine webgraph and social media
contexts generate to state-of-the-art results in website credibility
classification and significantly improves the top-k identification of
unreliable domains. Additionally, we release a novel dataset of dredge words,
highlighting their strong connections to both social media and online commerce
platforms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically
  Justify Replacing <span class="highlight-title">Human</span> Annotators with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10970v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10970v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nitay Calderon, Roi Reichart, Rotem Dror
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The "LLM-as-an-annotator" and "LLM-as-a-judge" paradigms employ Large
Language Models (LLMs) as annotators, judges, and evaluators in tasks
traditionally performed by humans. LLM annotations are widely used, not only in
NLP research but also in fields like medicine, psychology, and social science.
Despite their role in shaping study results and insights, there is no standard
or rigorous procedure to determine whether LLMs can replace human annotators.
In this paper, we propose a novel statistical procedure, the Alternative
Annotator Test (alt-test), that requires only a modest subset of annotated
examples to justify using LLM annotations. Additionally, we introduce a
versatile and interpretable measure for comparing LLM annotators and judges. To
demonstrate our procedure, we curated a diverse collection of ten datasets,
consisting of language and vision-language tasks, and conducted experiments
with six LLMs and four prompting techniques. Our results show that LLMs can
sometimes replace humans with closed-source LLMs (such as GPT-4o),
outperforming the open-source LLMs we examine, and that prompting techniques
yield judges of varying quality. We hope this study encourages more rigorous
and reliable practices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language and Planning in Robotic Navigation: A Multilingual Evaluation
  of State-of-the-Art Models <span class="chip">AAAI'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malak Mansour, Ahmed Aly, Bahey Tharwat, Sarim Hashmi, Dong An, Ian Reid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) such as GPT-4, trained on huge amount of
datasets spanning multiple domains, exhibit significant reasoning,
understanding, and planning capabilities across various tasks. This study
presents the first-ever work in Arabic language integration within the
Vision-and-Language Navigation (VLN) domain in robotics, an area that has been
notably underexplored in existing research. We perform a comprehensive
evaluation of state-of-the-art multi-lingual Small Language Models (SLMs),
including GPT-4o mini, Llama 3 8B, and Phi-3 medium 14B, alongside the
Arabic-centric LLM, Jais. Our approach utilizes the NavGPT framework, a pure
LLM-based instruction-following navigation agent, to assess the impact of
language on navigation reasoning through zero-shot sequential action prediction
using the R2R dataset. Through comprehensive experiments, we demonstrate that
our framework is capable of high-level planning for navigation tasks when
provided with instructions in both English and Arabic. However, certain models
struggled with reasoning and planning in the Arabic language due to inherent
limitations in their capabilities, sub-optimal performance, and parsing issues.
These findings highlight the importance of enhancing planning and reasoning
capabilities in language models for effective navigation, emphasizing this as a
key area for further development while also unlocking the potential of
Arabic-language models for impactful real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted for presentation at LM4Plan@AAAI'25. For
  more details, please check: https://llmforplanning.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Agent</span> Laboratory: Using LLM <span class="highlight-title">Agent</span>s as Research Assistants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04227v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04227v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Michael Moor, Zicheng Liu, Emad Barsoum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Historically, scientific discovery has been a lengthy and costly process,
demanding substantial time and resources from initial conception to final
results. To accelerate scientific discovery, reduce research costs, and improve
research quality, we introduce Agent Laboratory, an autonomous LLM-based
framework capable of completing the entire research process. This framework
accepts a human-provided research idea and progresses through three
stages--literature review, experimentation, and report writing to produce
comprehensive research outputs, including a code repository and a research
report, while enabling users to provide feedback and guidance at each stage. We
deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple
researchers to assess its quality by participating in a survey, providing human
feedback to guide the research process, and then evaluate the final paper. We
found that: (1) Agent Laboratory driven by o1-preview generates the best
research outcomes; (2) The generated machine learning code is able to achieve
state-of-the-art performance compared to existing methods; (3) Human
involvement, providing feedback at each stage, significantly improves the
overall quality of research; (4) Agent Laboratory significantly reduces
research expenses, achieving an 84% decrease compared to previous autonomous
research methods. We hope Agent Laboratory enables researchers to allocate more
effort toward creative ideation rather than low-level coding and writing,
ultimately accelerating scientific discovery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with
  <span class="highlight-title">Human</span> Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.10867v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.10867v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashish Singh, Ashutosh Singh, Prateek Agarwal, Zixuan Huang, Arpita Singh, Tong Yu, Sungchul Kim, Victor Bursztyn, Nesreen K. Ahmed, Puneet Mathur, Erik Learned-Miller, Franck Dernoncourt, Ryan A. Rossi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Captions are crucial for understanding scientific visualizations and
documents. Existing captioning methods for scientific figures rely on
figure-caption pairs extracted from documents for training, many of which fall
short with respect to metrics like helpfulness, explainability, and
visual-descriptiveness [15] leading to generated captions being misaligned with
reader preferences. To enable the generation of high-quality figure captions,
we introduce FigCaps-HF a new framework for figure-caption generation that can
incorporate domain expert feedback in generating captions optimized for reader
preferences. Our framework comprises of 1) an automatic method for evaluating
quality of figure-caption pairs, 2) a novel reinforcement learning with human
feedback (RLHF) method to optimize a generative figure-to-caption model for
reader preferences. We demonstrate the effectiveness of our simple learning
framework by improving performance over standard fine-tuning across different
types of models. In particular, when using BLIP as the base model, our RLHF
framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and
Meteor, respectively. Finally, we release a large-scale benchmark dataset with
human feedback on figure-caption pairs to enable further evaluation and
development of RLHF techniques for this problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures. Benchmark Documentation:
  https://figcapshf.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hybrid <span class="highlight-title">Multi-Agent</span> Prompting Approach for Simplifying Complex
  Sentences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11681v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11681v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pratibha Zunjare, Michael Hsiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of transforming complex sentences into
sequences of logical, simplified sentences while preserving semantic and
logical integrity with the help of Large Language Models. We propose a hybrid
approach that combines advanced prompting with multi-agent architectures to
enhance the sentence simplification process. Experimental results show that our
approach was able to successfully simplify 70% of the complex sentences written
for video game design application. In comparison, a single-agent approach
attained a 48% success rate on the same task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended
  Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06745v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06745v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adhiraj Ghosh, Sebastian Dziadzio, Ameya Prabhu, Vishaal Udandarao, Samuel Albanie, Matthias Bethge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional fixed test sets fall short in evaluating open-ended capabilities
of foundation models. To address this, we propose ONEBench(OpeN-Ended
Benchmarking), a new testing paradigm that consolidates individual evaluation
datasets into a unified, ever-expanding sample pool. ONEBench allows users to
generate custom, open-ended evaluation benchmarks from this pool, corresponding
to specific capabilities of interest. By aggregating samples across test sets,
ONEBench enables the assessment of diverse capabilities beyond those covered by
the original test sets, while mitigating overfitting and dataset bias. Most
importantly, it frames model evaluation as a collective process of selecting
and aggregating sample-level tests.
  The shift from task-specific benchmarks to ONEBench introduces two
challenges: (1)heterogeneity and (2)incompleteness. Heterogeneity refers to the
aggregation over diverse metrics, while incompleteness describes comparing
models evaluated on different data subsets. To address these challenges, we
explore algorithms to aggregate sparse measurements into reliable model scores.
Our aggregation algorithm ensures identifiability(asymptotically recovering
ground-truth scores) and rapid convergence, enabling accurate model ranking
with less data. On homogenous datasets, we show our aggregation algorithm
provides rankings that highly correlate with those produced by average scores.
We also demonstrate robustness to ~95% of measurements missing, reducing
evaluation cost by up to 20x with little-to-no change in model rankings. We
introduce ONEBench-LLM for language models and ONEBench-LMM for vision-language
models, unifying evaluations across these domains. Overall, we present a
technique for open-ended evaluation, which can aggregate over incomplete,
heterogeneous sample-level measurements to continually grow a benchmark
alongside the rapidly developing foundation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convert Language Model into a Value-based Strategic Planner <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06987v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06987v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Wang, Yue Zhao, Qingqing Gu, Zhonglin Jiang, Xiaokai Chen, Yong Chen, Luo Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotional support conversation (ESC) aims to alleviate the emotional distress
of individuals through effective conversations. Although large language models
(LLMs) have obtained remarkable progress on ESC, most of these studies might
not define the diagram from the state model perspective, therefore providing a
suboptimal solution for long-term satisfaction. To address such an issue, we
leverage the Q-learning on LLMs, and propose a framework called straQ*. Our
framework allows a plug-and-play LLM to bootstrap the planning during ESC,
determine the optimal strategy based on long-term returns, and finally guide
the LLM to response. Substantial experiments on ESC datasets suggest that
straQ* outperforms many baselines, including direct inference, self-refine,
chain of thought, finetuning, and finite state machines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, Accepted by ACL 2025 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IP Leakage Attacks Targeting LLM-Based <span class="highlight-title">Multi-Agent</span> Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12442v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12442v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liwen Wang, Wenxuan Wang, Shuai Wang, Zongjie Li, Zhenlan Ji, Zongyi Lyu, Daoyuan Wu, Shing-Chi Cheung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large Language Models (LLMs) has led to the
emergence of Multi-Agent Systems (MAS) to perform complex tasks through
collaboration. However, the intricate nature of MAS, including their
architecture and agent interactions, raises significant concerns regarding
intellectual property (IP) protection. In this paper, we introduce MASLEAK, a
novel attack framework designed to extract sensitive information from MAS
applications. MASLEAK targets a practical, black-box setting, where the
adversary has no prior knowledge of the MAS architecture or agent
configurations. The adversary can only interact with the MAS through its public
API, submitting attack query $q$ and observing outputs from the final agent.
Inspired by how computer worms propagate and infect vulnerable network hosts,
MASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain
responses from each MAS agent that reveal a full set of proprietary components,
including the number of agents, system topology, system prompts, task
instructions, and tool usages. We construct the first synthetic dataset of MAS
applications with 810 applications and also evaluate MASLEAK against real-world
MAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in
extracting MAS IP, with an average attack success rate of 87% for system
prompts and task instructions, and 92% for system architecture in most cases.
We conclude by discussing the implications of our findings and the potential
defenses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Large Language Models Exhibit Cognitive Dissonance? Studying the
  Difference Between Revealed Beliefs and Stated Answers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14986v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14986v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Mondal, Ljiljana Dolamic, Gérôme Bovet, Philippe Cudré-Mauroux, Julien Audiffren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple Choice Questions (MCQ) have become a commonly used approach to
assess the capabilities of Large Language Models (LLMs), due to their ease of
manipulation and evaluation. The experimental appraisals of the LLMs' Stated
Answer (their answer to MCQ) have pointed to their apparent ability to perform
probabilistic reasoning or to grasp uncertainty. In this work, we investigate
whether these aptitudes are measurable outside tailored prompting and MCQ by
reformulating these issues as direct text-completion - the fundamental
computational unit of LLMs. We introduce Revealed Belief, an evaluation
framework that evaluates LLMs on tasks requiring reasoning under uncertainty,
which complements MCQ scoring by analyzing text-completion probability
distributions. Our findings suggest that while LLMs frequently state the
correct answer, their Revealed Belief shows that they often allocate
probability mass inconsistently, exhibit systematic biases, and often fail to
update their beliefs appropriately when presented with new evidence, leading to
strong potential impacts on downstream tasks. These results suggest that common
evaluation methods may only provide a partial picture and that more research is
needed to assess the extent and nature of their capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prefix-Tuning+: Modernizing Prefix-Tuning by Decoupling the Prefix from
  Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13674v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13674v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Wang, Brian Chen, Siquan Li, Xinhe Liang, Hwee Kuan Lee, Kenji Kawaguchi, Tianyang Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-Efficient Fine-Tuning (PEFT) methods have become crucial for
rapidly adapting large language models (LLMs) to downstream tasks.
Prefix-Tuning, an early and effective PEFT technique, demonstrated the ability
to achieve performance comparable to full fine-tuning with significantly
reduced computational and memory overhead. However, despite its earlier
success, its effectiveness in training modern state-of-the-art LLMs has been
very limited. In this work, we demonstrate empirically that Prefix-Tuning
underperforms on LLMs because of an inherent tradeoff between input and prefix
significance within the attention head. This motivates us to introduce
Prefix-Tuning+, a novel architecture that generalizes the principles of
Prefix-Tuning while addressing its shortcomings by shifting the prefix module
out of the attention head itself. We further provide an overview of our
construction process to guide future users when constructing their own
context-based methods. Our experiments show that, across a diverse set of
benchmarks, Prefix-Tuning+ consistently outperforms existing Prefix-Tuning
methods. Notably, it achieves performance on par with the widely adopted LoRA
method on several general benchmarks, highlighting the potential modern
extension of Prefix-Tuning approaches. Our findings suggest that by overcoming
its inherent limitations, Prefix-Tuning can remain a competitive and relevant
research direction in the landscape of parameter-efficient LLM adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynGraph: A Dynamic Graph-LLM Synthesis Framework for Sparse Streaming
  User Sentiment Modeling <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04619v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04619v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Qiyu Wei, Yingjie Zhu, Linhai Zhang, Deyu Zhou, Sophia Ananiadou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User reviews on e-commerce platforms exhibit dynamic sentiment patterns
driven by temporal and contextual factors. Traditional sentiment analysis
methods focus on static reviews, failing to capture the evolving temporal
relationship between user sentiment rating and textual content. Sentiment
analysis on streaming reviews addresses this limitation by modeling and
predicting the temporal evolution of user sentiments. However, it suffers from
data sparsity, manifesting in temporal, spatial, and combined forms. In this
paper, we introduce SynGraph, a novel framework designed to address data
sparsity in sentiment analysis on streaming reviews. SynGraph alleviates data
sparsity by categorizing users into mid-tail, long-tail, and extreme scenarios
and incorporating LLM-augmented enhancements within a dynamic graph-based
structure. Experiments on real-world datasets demonstrate its effectiveness in
addressing sparsity and improving sentiment modeling in streaming reviews.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TaskCraft: Automated Generation of <span class="highlight-title">Agent</span>ic Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10055v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10055v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingfeng Shi, Jingyi Cao, Qianben Chen, Weichen Sun, Weizhen Li, Hongxuan Lu, Fangchen Dong, Tianrui Qin, King Zhu, Minghao Liu, Jian Yang, Ge Zhang, Jiaheng Liu, Changwang Zhang, Jun Wang, Yuchen Eleanor Jiang, Wangchunshu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agentic tasks, which require multi-step problem solving with autonomy, tool
use, and adaptive reasoning, are becoming increasingly central to the
advancement of NLP and AI. However, existing instruction data lacks tool
interaction, and current agentic benchmarks rely on costly human annotation,
limiting their scalability. We introduce \textsc{TaskCraft}, an automated
workflow for generating difficulty-scalable, multi-tool, and verifiable agentic
tasks with execution trajectories. TaskCraft expands atomic tasks using
depth-based and width-based extensions to create structurally and
hierarchically complex challenges. Empirical results show that these tasks
improve prompt optimization in the generation workflow and enhance supervised
fine-tuning of agentic foundation models. We present a large-scale synthetic
dataset of approximately 36,000 tasks with varying difficulty to support future
research on agent tuning and evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph RAG for Legal Norms: A Hierarchical, Temporal and Deterministic
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.00039v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.00039v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hudson de Martim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article proposes an adaptation of Graph Retrieval-Augmented Generation
(Graph RAG) specifically designed for the analysis and comprehension of legal
norms. Legal texts are characterized by a predefined hierarchical structure, an
extensive network of references and a continuous evolution through multiple
temporal versions. This temporal dynamism poses a significant challenge for
standard AI systems, demanding a deterministic representation of the law at any
given point in time. To address this, our approach grounds the knowledge graph
construction in a formal, FRBRoo-inspired model that distinguishes abstract
legal works from their concrete textual expressions. We introduce a
multi-layered representation of Temporal Versions (capturing date-specific
changes) and Language Versions (capturing linguistic variations). By modeling
normative evolution as a precise sequence of these versioned entities, we
enable the construction of a knowledge graph that serves as a verifiable
"ground truth". This allows Large Language Models to generate responses based
on accurate, context-aware, and point-in-time correct legal information,
overcoming the risk of temporal inaccuracies. Through a detailed analysis of
this formal Graph RAG approach and its application to legal norm datasets, this
article aims to advance the field of Artificial Intelligence applied to Law,
creating opportunities for more effective and reliable systems in legal
research, legislative analysis, and decision support.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version enhances the theoretical underpinnings of the proposed
  Graph RAG methodology, including the introduction of a formal, FRBRoo-based
  model for versioning, and enabling multi-language support for both content
  and metadata</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigating the Digital World as <span class="highlight-title">Human</span>s Do: Universal Visual Grounding
  for GUI <span class="highlight-title">Agent</span>s <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05243v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05243v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) are transforming the capabilities of
graphical user interface (GUI) agents, facilitating their transition from
controlled simulations to complex, real-world applications across various
platforms. However, the effectiveness of these agents hinges on the robustness
of their grounding capability. Current GUI agents predominantly utilize
text-based representations such as HTML or accessibility trees, which, despite
their utility, often introduce noise, incompleteness, and increased
computational overhead. In this paper, we advocate a human-like embodiment for
GUI agents that perceive the environment entirely visually and directly perform
pixel-level operations on the GUI. The key is visual grounding models that can
accurately map diverse referring expressions of GUI elements to their
coordinates on the GUI across different platforms. We show that a simple
recipe, which includes web-based synthetic data and slight adaptation of the
LLaVA architecture, is surprisingly effective for training such visual
grounding models. We collect the largest dataset for GUI visual grounding so
far, containing 10M GUI elements and their referring expressions over 1.3M
screenshots, and use it to train UGround, a strong universal visual grounding
model for GUI agents. Empirical results on six benchmarks spanning three
categories (grounding, offline agent, and online agent) show that 1) UGround
substantially outperforms existing visual grounding models for GUI agents, by
up to 20% absolute, and 2) agents with UGround outperform state-of-the-art
agents, despite the fact that existing agents use additional text-based input
while ours only uses visual perception. These results provide strong support
for the feasibility and promises of GUI agents that navigate the digital world
as humans do.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025 (Oral). Project Homepage:
  https://osu-nlp-group.github.io/UGround/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From <span class="highlight-title">tool</span>s to thieves: Measuring and understanding public perceptions of
  AI through crowdsourced metaphors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18045v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18045v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Myra Cheng, Angela Y. Lee, Kristina Rapuano, Kate Niederhoffer, Alex Liebscher, Jeffrey Hancock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How has the public responded to the increasing prevalence of artificial
intelligence (AI)-based technologies? We investigate public perceptions of AI
by collecting over 12,000 responses over 12 months from a nationally
representative U.S. sample. Participants provided open-ended metaphors
reflecting their mental models of AI, a methodology that overcomes the
limitations of traditional self-reported measures by capturing more nuance.
Using a mixed-methods approach combining quantitative clustering and
qualitative coding, we identify 20 dominant metaphors shaping public
understanding of AI. To analyze these metaphors systematically, we present a
scalable framework integrating language modeling (LM)-based techniques to
measure key dimensions of public perception: anthropomorphism (attribution of
human-like qualities), warmth, and competence. We find that Americans generally
view AI as warm and competent, and that over the past year, perceptions of AI's
human-likeness and warmth have significantly increased ($+34\%, r = 0.80, p <
0.01; +41\%, r = 0.62, p < 0.05$). These implicit perceptions, along with the
identified dominant metaphors, strongly predict trust in and willingness to
adopt AI ($r^2 = 0.21, 0.18, p < 0.001$). Moreover, we uncover systematic
demographic differences in metaphors and implicit perceptions, such as the
higher propensity of women, older individuals, and people of color to
anthropomorphize AI, which shed light on demographic disparities in trust and
adoption. In addition to our dataset and framework for tracking evolving public
attitudes, we provide actionable insights on using metaphors for inclusive and
responsible AI development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at the ACM Conference on Fairness, Accountability, and
  Transparency 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PredictaBoard: Benchmarking LLM Score Predictability <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14445v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14445v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Pacchiardi, Konstantinos Voudouris, Ben Slater, Fernando Martínez-Plumed, José Hernández-Orallo, Lexin Zhou, Wout Schellaert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite possessing impressive skills, Large Language Models (LLMs) often fail
unpredictably, demonstrating inconsistent success in even basic common sense
reasoning tasks. This unpredictability poses a significant challenge to
ensuring their safe deployment, as identifying and operating within a reliable
"safe zone" is essential for mitigating risks. To address this, we present
PredictaBoard, a novel collaborative benchmarking framework designed to
evaluate the ability of score predictors (referred to as assessors) to
anticipate LLM errors on specific task instances (i.e., prompts) from existing
datasets. PredictaBoard evaluates pairs of LLMs and assessors by considering
the rejection rate at different tolerance errors. As such, PredictaBoard
stimulates research into developing better assessors and making LLMs more
predictable, not only with a higher average performance. We conduct
illustrative experiments using baseline assessors and state-of-the-art LLMs.
PredictaBoard highlights the critical need to evaluate predictability alongside
performance, paving the way for safer AI systems where errors are not only
minimised but also anticipated and effectively mitigated. Code for our
benchmark can be found at
https://github.com/Kinds-of-Intelligence-CFI/PredictaBoard
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL Findings 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12420v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12420v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Walter Hernandez Cruz, Kamil Tylinski, Alastair Moore, Niall Roche, Nikhil Vadgama, Horst Treiblmaier, Jiangbo Shangguan, Paolo Tasca, Jiahua Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed Ledger Technology (DLT) faces increasing environmental scrutiny,
particularly concerning the energy consumption of the Proof of Work (PoW)
consensus mechanism and broader Environmental, Social, and Governance (ESG)
issues. However, existing systematic literature reviews of DLT rely on limited
analyses of citations, abstracts, and keywords, failing to fully capture the
field's complexity and ESG concerns. We address these challenges by analyzing
the full text of 24,539 publications using Natural Language Processing (NLP)
with our manually labeled Named Entity Recognition (NER) dataset of 39,427
entities for DLT. This methodology identified 505 key publications at the
DLT/ESG intersection, enabling comprehensive domain analysis. Our combined NLP
and temporal graph analysis reveals critical trends in DLT evolution and ESG
impacts, including cryptography and peer-to-peer networks research's
foundational influence, Bitcoin's persistent impact on research and
environmental concerns (a "Lindy effect"), Ethereum's catalytic role on Proof
of Stake (PoS) and smart contract adoption, and the industry's progressive
shift toward energy-efficient consensus mechanisms. Our contributions include
the first DLT-specific NER dataset addressing the scarcity of high-quality
labeled NLP data in blockchain research, a methodology integrating NLP and
temporal graph analysis for large-scale interdisciplinary literature reviews,
and the first NLP-driven literature review focusing on DLT's ESG aspects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ClusterChat: Multi-Feature Search for Corpus Exploration <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14533v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14533v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashish Chouhan, Saifeldin Mandour, Michael Gertz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring large-scale text corpora presents a significant challenge in
biomedical, finance, and legal domains, where vast amounts of documents are
continuously published. Traditional search methods, such as keyword-based
search, often retrieve documents in isolation, limiting the user's ability to
easily inspect corpus-wide trends and relationships. We present ClusterChat
(The demo video and source code are available at:
https://github.com/achouhan93/ClusterChat), an open-source system for corpus
exploration that integrates cluster-based organization of documents using
textual embeddings with lexical and semantic search, timeline-driven
exploration, and corpus and document-level question answering (QA) as
multi-feature search capabilities. We validate the system with two case studies
on a four million abstract PubMed dataset, demonstrating that ClusterChat
enhances corpus exploration by delivering context-aware insights while
maintaining scalability and responsiveness on large-scale document collections.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 table, 1 figure, Accepted to SIGIR Demo Paper Track 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inherent and emergent liability issues in LLM-based <span class="highlight-title">agent</span>ic systems: a
  principal-<span class="highlight-title">agent</span> perspective <span class="chip">ACL2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.03255v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.03255v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Garry A. Gabison, R. Patrick Xian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agentic systems powered by large language models (LLMs) are becoming
progressively more complex and capable. Their increasing agency and expanding
deployment settings attract growing attention to effective governance policies,
monitoring, and control protocols. Based on the emerging landscape of the
agentic market, we analyze potential liability issues arising from the
delegated use of LLM agents and their extended systems through a
principal-agent perspective. Our analysis complements existing risk-based
studies on artificial agency and covers the spectrum of important aspects of
the principal-agent relationship and their potential consequences at
deployment. Furthermore, we motivate method developments for technical
governance along the directions of interpretability and behavior evaluations,
reward and conflict management, and the mitigation of misalignment and
misconduct through principled engineering of detection and fail-safe
mechanisms. By illustrating the outstanding issues in AI liability for
LLM-based agentic systems, we aim to inform the system design, auditing, and
tracing to enhance transparency and liability attribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages (incl. appendix), accepted at REALM workshop, ACL2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for
  Varieties of English <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04726v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04726v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dipankar Srirag, Aditya Joshi, Jordan Painter, Diptesh Kanojia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite large language models (LLMs) being known to exhibit bias against
non-standard language varieties, there are no known labelled datasets for
sentiment analysis of English. To address this gap, we introduce BESSTIE, a
benchmark for sentiment and sarcasm classification for three varieties of
English: Australian (en-AU), Indian (en-IN), and British (en-UK). We collect
datasets for these language varieties using two methods: location-based for
Google Places reviews, and topic-based filtering for Reddit comments. To assess
whether the dataset accurately represents these varieties, we conduct two
validation steps: (a) manual annotation of language varieties and (b) automatic
language variety prediction. Native speakers of the language varieties manually
annotate the datasets with sentiment and sarcasm labels. We perform an
additional annotation exercise to validate the reliance of the annotated
labels. Subsequently, we fine-tune nine LLMs (representing a range of
encoder/decoder and mono/multilingual models) on these datasets, and evaluate
their performance on the two tasks. Our results show that the models
consistently perform better on inner-circle varieties (i.e., en-AU and en-UK),
in comparison with en-IN, particularly for sarcasm classification. We also
report challenges in cross-variety generalisation, highlighting the need for
language variety-specific datasets such as ours. BESSTIE promises to be a
useful evaluative benchmark for future research in equitable LLMs, specifically
in terms of language varieties. The BESSTIE dataset is publicly available at:
https://huggingface.co/ datasets/unswnlporg/BESSTIE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of ACL: ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rectifying Belief Space via Unlearning to Harness LLMs' Reasoning <span class="chip">ACL2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20620v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20620v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayana Niwa, Masahiro Kaneko, Kentaro Inui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can exhibit advanced reasoning yet still
generate incorrect answers. We hypothesize that such errors frequently stem
from spurious beliefs, propositions the model internally considers true but are
incorrect. To address this, we propose a method to rectify the belief space by
suppressing these spurious beliefs while simultaneously enhancing true ones,
thereby enabling more reliable inferences. Our approach first identifies the
beliefs that lead to incorrect or correct answers by prompting the model to
generate textual explanations, using our Forward-Backward Beam Search (FBBS).
We then apply unlearning to suppress the identified spurious beliefs and
enhance the true ones, effectively rectifying the model's belief space.
Empirical results on multiple QA datasets and LLMs show that our method
corrects previously misanswered questions without harming overall model
performance. Furthermore, our approach yields improved generalization on unseen
data, suggesting that rectifying a model's belief space is a promising
direction for mitigating errors and enhancing overall reliability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL2025 Findings (long)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncovering Overfitting in Large Language Model Editing <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07819v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07819v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengqi Zhang, Xiaotian Ye, Qiang Liu, Pengjie Ren, Shu Wu, Zhumin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge editing has been proposed as an effective method for updating and
correcting the internal knowledge of Large Language Models (LLMs). However,
existing editing methods often struggle with complex tasks, such as multi-hop
reasoning. In this paper, we identify and investigate the phenomenon of Editing
Overfit, where edited models assign disproportionately high probabilities to
the edit target, hindering the generalization of new knowledge in complex
scenarios. We attribute this issue to the current editing paradigm, which
places excessive emphasis on the direct correspondence between the input prompt
and the edit target for each edit sample. To further explore this issue, we
introduce a new benchmark, EVOKE (EValuation of Editing Overfit in Knowledge
Editing), along with fine-grained evaluation metrics. Through comprehensive
experiments and analysis, we demonstrate that Editing Overfit is prevalent in
current editing methods and that common overfitting mitigation strategies are
ineffective in knowledge editing. To overcome this, inspired by LLMs' knowledge
recall mechanisms, we propose a new plug-and-play strategy called Learn the
Inference (LTI), which introduce a Multi-stage Inference Constraint module to
guide the edited models in recalling new knowledge similarly to how unedited
LLMs leverage knowledge through in-context learning. Extensive experimental
results across a wide range of tasks validate the effectiveness of LTI in
mitigating Editing Overfit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAPO: Cost-Aware Prompt Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.16005v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.16005v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Zehle, Moritz Schlager, Timo Heiß, Matthias Feurer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have revolutionized natural language processing
by solving a wide range of tasks simply guided by a prompt. Yet their
performance is highly sensitive to prompt formulation. While automatic prompt
optimization addresses this challenge by finding optimal prompts, current
methods require a substantial number of LLM calls and input tokens, making
prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt
Optimization), an algorithm that enhances prompt optimization efficiency by
integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as
operators, incorporating racing to save evaluations and multi-objective
optimization to balance performance with prompt length. It jointly optimizes
instructions and few-shot examples while leveraging task descriptions for
improved robustness. Our extensive experiments across diverse datasets and LLMs
demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization
methods in 11/15 cases with improvements up to 21%p in accuracy. Our algorithm
achieves better performances already with smaller budgets, saves evaluations
through racing, and decreases average prompt length via a length penalty,
making it both cost-efficient and cost-aware. Even without few-shot examples,
CAPO outperforms its competitors and generally remains robust to initial
prompts. CAPO represents an important step toward making prompt optimization
more powerful and accessible by improving cost-efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to AutoML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ensemble Watermarks for Large Language Models <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georg Niess, Roman Kern
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) reach human-like fluency, reliably
distinguishing AI-generated text from human authorship becomes increasingly
difficult. While watermarks already exist for LLMs, they often lack flexibility
and struggle with attacks such as paraphrasing. To address these issues, we
propose a multi-feature method for generating watermarks that combines multiple
distinct watermark features into an ensemble watermark. Concretely, we combine
acrostica and sensorimotor norms with the established red-green watermark to
achieve a 98% detection rate. After a paraphrasing attack, the performance
remains high with 95% detection rate. In comparison, the red-green feature
alone as a baseline achieves a detection rate of 49% after paraphrasing. The
evaluation of all feature combinations reveals that the ensemble of all three
consistently has the highest detection rate across several LLMs and watermark
strength settings. Due to the flexibility of combining features in the
ensemble, various requirements and trade-offs can be addressed. Additionally,
the same detection function can be used without adaptations for all ensemble
configurations. This method is particularly of interest to facilitate
accountability and prevent societal harm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 main conference. This article extends our
  earlier work arXiv:2405.08400 by introducing an ensemble of stylometric
  watermarking features and alternative experimental analysis. Code and data
  are available at http://github.com/CommodoreEU/ensemble-watermark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated Construction of a Knowledge Graph of Nuclear Fusion Energy for
  Effective Elicitation and Retrieval of Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.07738v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.07738v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Loreti, Kesi Chen, Ruby George, Robert Firth, Adriano Agnello, Shinnosuke Tanaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this document, we discuss a multi-step approach to automated construction
of a knowledge graph, for structuring and representing domain-specific
knowledge from large document corpora. We apply our method to build the first
knowledge graph of nuclear fusion energy, a highly specialized field
characterized by vast scope and heterogeneity. This is an ideal benchmark to
test the key features of our pipeline, including automatic named entity
recognition and entity resolution. We show how pre-trained large language
models can be used to address these challenges and we evaluate their
performance against Zipf's law, which characterizes human-generated natural
language. Additionally, we develop a knowledge-graph retrieval-augmented
generation system that combines large language models with a multi-prompt
approach. This system provides contextually relevant answers to
natural-language queries, including complex multi-hop questions that require
reasoning across interconnected entities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeqPE: Transformer with Sequential Position Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13277v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13277v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huayang Li, Yahui Liu, Hongyu Sun, Deng Cai, Leyang Cui, Wei Bi, Peilin Zhao, Taro Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since self-attention layers in Transformers are permutation invariant by
design, positional encodings must be explicitly incorporated to enable spatial
understanding. However, fixed-size lookup tables used in traditional learnable
position embeddings (PEs) limit extrapolation capabilities beyond pre-trained
sequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this
limitation but demand extensive modifications for adapting to new modalities,
underscoring fundamental challenges in adaptability and scalability. In this
work, we present SeqPE, a unified and fully learnable position encoding
framework that represents each $n$-dimensional position index as a symbolic
sequence and employs a lightweight sequential position encoder to learn their
embeddings in an end-to-end manner. To regularize SeqPE's embedding space, we
introduce two complementary objectives: a contrastive objective that aligns
embedding distances with a predefined position-distance function, and a
knowledge distillation loss that anchors out-of-distribution position
embeddings to in-distribution teacher representations, further enhancing
extrapolation performance. Experiments across language modeling, long-context
question answering, and 2D image classification demonstrate that SeqPE not only
surpasses strong baselines in perplexity, exact match (EM), and
accuracy--particularly under context length extrapolation--but also enables
seamless generalization to multi-dimensional inputs without requiring manual
architectural redesign. We release our code, data, and checkpoints at
https://github.com/ghrua/seqpe.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring news intent and its application: A theory-driven approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengjia Wang, Danding Wang, Qiang Sheng, Juan Cao, Siyuan Ma, Haonan Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the intent behind information is crucial. However, news as a
medium of public discourse still lacks a structured investigation of perceived
news intent and its application. To advance this field, this paper reviews
interdisciplinary studies on intentional action and introduces a conceptual
deconstruction-based news intent understanding framework (NINT). This framework
identifies the components of intent, facilitating a structured representation
of news intent and its applications. Building upon NINT, we contribute a new
intent perception dataset. Moreover, we investigate the potential of intent
assistance on news-related tasks, such as significant improvement (+2.2% macF1)
in the task of fake news detection. We hope that our findings will provide
valuable insights into action-based intent cognition and computational social
science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Information Processing & Management. DOI:
  https://doi.org/10.1016/j.ipm.2025.104229</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently
  Compressing Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junho Yoon, Geom Lee, Donghyeon Jeon, Inho Kang, Seung-Hoon Na
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization has been widely studied as an effective technique for reducing
the memory requirement of large language models (LLMs), potentially improving
the latency time as well. Utilizing the characteristic of rotational invariance
of transformer, we propose the rotation-based saliency-aware weight
quantization (ROSAQ), which identifies salient channels in the projection
feature space, not in the original feature space, where the projected
"principal" dimensions are naturally considered as "salient" features. The
proposed ROSAQ consists of 1) PCA-based projection, which first performs
principal component analysis (PCA) on a calibration set and transforms via the
PCA projection, 2) Salient channel dentification, which selects dimensions
corresponding to the K-largest eigenvalues as salient channels, and 3)
Saliency-aware quantization with mixed-precision, which uses FP16 for salient
dimensions and INT3/4 for other dimensions. Experiment results show that ROSAQ
shows improvements over the baseline saliency-aware quantization on the
original feature space and other existing quantization methods. With kernel
fusion, ROSAQ presents about 2.3x speed up over FP16 implementation in
generating 256 tokens with a batch size of 64.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incentivizing Reasoning for Advanced Instruction-Following of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01413v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01413v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulei Qin, Gang Li, Zongyi Li, Zihan Xu, Yuchen Shi, Zhekai Lin, Xiao Cui, Ke Li, Xing Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing large language models (LLMs) face challenges of following complex
instructions, especially when multiple constraints are present and organized in
paralleling, chaining, and branching structures. One intuitive solution, namely
chain-of-thought (CoT), is expected to universally improve capabilities of
LLMs. However, we find that the vanilla CoT exerts a negative impact on
performance due to its superficial reasoning pattern of simply paraphrasing the
instructions. It fails to peel back the compositions of constraints for
identifying their relationship across hierarchies of types and dimensions. To
this end, we propose a systematic method to boost LLMs in dealing with complex
instructions via incentivizing reasoning for test-time compute scaling. First,
we stem from the decomposition of complex instructions under existing
taxonomies and propose a reproducible data acquisition method. Second, we
exploit reinforcement learning (RL) with verifiable rule-centric reward signals
to cultivate reasoning specifically for instruction following. We address the
shallow, non-essential nature of reasoning under complex instructions via
sample-wise contrast for superior CoT enforcement. We also exploit behavior
cloning of experts to facilitate steady distribution shift from fast-thinking
LLMs to skillful reasoners. Extensive evaluations on seven comprehensive
benchmarks confirm the validity of the proposed method, where a 1.5B LLM
achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data
will be available later (under review).
  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction
following, complex instructions
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages of main body, 3 tables, 5 figures, 45 pages of appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Construction Distributions Shape Formal Language Learning In German
  BabyLMs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bastian Bunzeck, Daniel Duran, Sina Zarrieß
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze the influence of utterance-level construction distributions in
German child-directed/child-available speech on the resulting word-level,
syntactic and semantic competence (and their underlying learning trajectories)
in small LMs, which we train on a novel collection of developmentally plausible
language data for German. We find that trajectories are surprisingly robust for
markedly different distributions of constructions in the training data, which
have little effect on final accuracies and almost no effect on global learning
trajectories. While syntax learning benefits from more complex utterances,
word-level learning culminates in better scores with more fragmentary
utterances. We argue that LMs trained on developmentally plausible data can
contribute to debates on how conducive different kinds of linguistic stimuli
are to language learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CoNNL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-Facilitated Analysis of Abstracts and Conclusions: Flagging
  Unsubstantiated Claims and Ambiguous Pronouns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13172v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13172v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evgeny Markhasin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present and evaluate a suite of proof-of-concept (PoC), structured
workflow prompts designed to elicit human-like hierarchical reasoning while
guiding Large Language Models (LLMs) in the high-level semantic and linguistic
analysis of scholarly manuscripts. The prompts target two non-trivial
analytical tasks within academic summaries (abstracts and conclusions):
identifying unsubstantiated claims (informational integrity) and flagging
semantically confusing ambiguous pronoun references (linguistic clarity). We
conducted a systematic, multi-run evaluation on two frontier models (Gemini Pro
2.5 Pro and ChatGPT Plus o3) under varied context conditions. Our results for
the informational integrity task reveal a significant divergence in model
performance: while both models successfully identified an unsubstantiated head
of a noun phrase (95% success), ChatGPT consistently failed (0% success) to
identify an unsubstantiated adjectival modifier that Gemini correctly flagged
(95% success), raising a question regarding the potential influence of the
target's syntactic role. For the linguistic analysis task, both models
performed well (80-90% success) with full manuscript context. Surprisingly, in
a summary-only setting, Gemini's performance was substantially degraded, while
ChatGPT achieved a perfect (100%) success rate. Our findings suggest that while
structured prompting is a viable methodology for complex textual analysis,
prompt performance may be highly dependent on the interplay between the model,
task type, and context, highlighting the need for rigorous, model-specific
testing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13300v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13300v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Li, Chengben Xu, Wufeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Seewo's systems for both tracks of the Multilingual
Conversational Speech Language Model Challenge (MLC-SLM), addressing automatic
speech recognition (ASR) and speaker diarization with ASR (SD-ASR). We
introduce a multi-stage training pipeline that explicitly enhances reasoning
and self-correction in speech language models for ASR. Our approach combines
curriculum learning for progressive capability acquisition, Chain-of-Thought
data augmentation to foster intermediate reflection, and Reinforcement Learning
with Verifiable Rewards (RLVR) to further refine self-correction through
reward-driven optimization. This approach achieves substantial improvements
over the official challenge baselines. On the evaluation set, our best system
attains a WER/CER of 11.57% for Track 1 and a tcpWER/tcpCER of 17.67% for Track
2. Comprehensive ablation studies demonstrate the effectiveness of each
component under challenge constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ConsistencyChecker: Tree-based Evaluation of LLM Generalization
  Capabilities <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaochen Hong, Haofei Yu, Jiaxuan You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating consistency in large language models (LLMs) is crucial for
ensuring reliability, particularly in complex, multi-step interactions between
humans and LLMs. Traditional self-consistency methods often miss subtle
semantic changes in natural language and functional shifts in code or
equations, which can accumulate over multiple transformations. To address this,
we propose ConsistencyChecker, a tree-based evaluation framework designed to
measure consistency through sequences of reversible transformations, including
machine translation tasks and AI-assisted programming tasks. In our framework,
nodes represent distinct text states, while edges correspond to pairs of
inverse operations. Dynamic and LLM-generated benchmarks ensure a fair
assessment of the model's generalization ability and eliminate benchmark
leakage. Consistency is quantified based on similarity across different depths
of the transformation tree. Experiments on eight models from various families
and sizes show that ConsistencyChecker can distinguish the performance of
different models. Notably, our consistency scores-computed entirely without
using WMT paired data-correlate strongly (r > 0.7) with WMT 2024 auto-ranking,
demonstrating the validity of our benchmark-free approach. Our implementation
is available at: https://github.com/ulab-uiuc/consistencychecker.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheqi He, Yesheng Liu, Jing-shu Zheng, Xuejing Li, Jin-Ge Yao, Bowen Qin, Richeng Xuan, Xi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present FlagEvalMM, an open-source evaluation framework designed to
comprehensively assess multimodal models across a diverse range of
vision-language understanding and generation tasks, such as visual question
answering, text-to-image/video generation, and image-text retrieval. We
decouple model inference from evaluation through an independent evaluation
service, thus enabling flexible resource allocation and seamless integration of
new tasks and models. Moreover, FlagEvalMM utilizes advanced inference
acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to
significantly enhance evaluation efficiency. Extensive experiments show that
FlagEvalMM offers accurate and efficient insights into model strengths and
limitations, making it a valuable tool for advancing multimodal research. The
framework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Surprise Calibration for Better In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12796v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12796v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihang Tan, Jingrui Hou, Ping Wang, Qibiao Hu, Peng Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) has emerged as a powerful paradigm for task
adaptation in large language models (LLMs), where models infer underlying task
structures from a few demonstrations. However, ICL remains susceptible to
biases that arise from prior knowledge and contextual demonstrations, which can
degrade the performance of LLMs. Existing bias calibration methods typically
apply fixed class priors across all inputs, limiting their efficacy in dynamic
ICL settings where the context for each query differs. To address these
limitations, we adopt implicit sequential Bayesian inference as a framework for
interpreting ICL, identify "surprise" as an informative signal for class prior
shift, and introduce a novel method--Surprise Calibration (SC). SC leverages
the notion of surprise to capture the temporal dynamics of class priors,
providing a more adaptive and computationally efficient solution for in-context
learning. We empirically demonstrate the superiority of SC over existing bias
calibration techniques across a range of benchmark natural language processing
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What do Large Language Models Say About Animals? Investigating Risks of
  Animal Harm in Generated Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04804v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04804v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arturs Kanepajs, Aditi Basu, Sankalpa Ghose, Constance Li, Akshat Mehta, Ronak Mehta, Samuel David Tucker-Davis, Eric Zhou, Bob Fischer, Jacy Reese Anthis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As machine learning systems become increasingly embedded in society, their
impact on human and nonhuman life continues to escalate. Technical evaluations
have addressed a variety of potential harms from large language models (LLMs)
towards humans and the environment, but there is little empirical work
regarding harms towards nonhuman animals. Following the growing recognition of
animal protection in regulatory and ethical AI frameworks, we present
AnimalHarmBench (AHB), a benchmark for risks of animal harm in LLM-generated
text. Our benchmark dataset comprises 1,850 curated questions from Reddit post
titles and 2,500 synthetic questions based on 50 animal categories (e.g., cats,
reptiles) and 50 ethical scenarios with a 70-30 public-private split. Scenarios
include open-ended questions about how to treat animals, practical scenarios
with potential animal harm, and willingness-to-pay measures for the prevention
of animal harm. Using the LLM-as-a-judge framework, responses are evaluated for
their potential to increase or decrease harm, and evaluations are debiased for
the tendency of judges to judge their own outputs more favorably. AHB reveals
significant differences across frontier LLMs, animal categories, scenarios, and
subreddits. We conclude with future directions for technical research and
addressing the challenges of building evaluations on complex social and moral
topics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Position: Editing Large Language Models Poses Serious Safety Risks <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02958v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02958v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Youssef, Zhixue Zhao, Daniel Braun, Jörg Schlötterer, Christin Seifert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) contain large amounts of facts about the world.
These facts can become outdated over time, which has led to the development of
knowledge editing methods (KEs) that can change specific facts in LLMs with
limited side effects. This position paper argues that editing LLMs poses
serious safety risks that have been largely overlooked. First, we note the fact
that KEs are widely available, computationally inexpensive, highly performant,
and stealthy makes them an attractive tool for malicious actors. Second, we
discuss malicious use cases of KEs, showing how KEs can be easily adapted for a
variety of malicious purposes. Third, we highlight vulnerabilities in the AI
ecosystem that allow unrestricted uploading and downloading of updated models
without verification. Fourth, we argue that a lack of social and institutional
awareness exacerbates this risk, and discuss the implications for different
stakeholders. We call on the community to (i) research tamper-resistant models
and countermeasures against malicious model editing, and (ii) actively engage
in securing the AI ecosystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM
  <span class="highlight-title">Agent</span>s <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingxiao Diao, Xinyue Xu, Wanxuan Sun, Cheng Yang, Zhuosheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been widely deployed as autonomous agents
capable of following user instructions and making decisions in real-world
applications. Previous studies have made notable progress in benchmarking the
instruction following capabilities of LLMs in general domains, with a primary
focus on their inherent commonsense knowledge. Recently, LLMs have been
increasingly deployed as domain-oriented agents, which rely on domain-oriented
guidelines that may conflict with their commonsense knowledge. These guidelines
exhibit two key characteristics: they consist of a wide range of
domain-oriented rules and are subject to frequent updates. Despite these
challenges, the absence of comprehensive benchmarks for evaluating the
domain-oriented guideline following capabilities of LLMs presents a significant
obstacle to their effective assessment and further development. In this paper,
we introduce GuideBench, a comprehensive benchmark designed to evaluate
guideline following performance of LLMs. GuideBench evaluates LLMs on three
critical aspects: (i) adherence to diverse rules, (ii) robustness to rule
updates, and (iii) alignment with human preferences. Experimental results on a
range of LLMs indicate substantial opportunities for improving their ability to
follow domain-oriented guidelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Effect of Selection Format on LLM Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06926v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06926v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Han, Yucheng Wu, Jeffrey Willard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates a critical aspect of large language model (LLM)
performance: the optimal formatting of classification task options in prompts.
Through an extensive experimental study, we compared two selection formats --
bullet points and plain English -- to determine their impact on model
performance. Our findings suggest that presenting options via bullet points
generally yields better results, although there are some exceptions.
Furthermore, our research highlights the need for continued exploration of
option formatting to drive further improvements in model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Computer-Use Grounding via User Interface Decomposition and
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13227v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13227v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, Yiheng Xu, Junli Wang, Doyen Sahoo, Tao Yu, Caiming Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphical user interface (GUI) grounding, the ability to map natural language
instructions to specific actions on graphical user interfaces, remains a
critical bottleneck in computer use agent development. Current benchmarks
oversimplify grounding tasks as short referring expressions, failing to capture
the complexity of real-world interactions that require software commonsense,
layout understanding, and fine-grained manipulation capabilities. To address
these limitations, we introduce OSWorld-G, a comprehensive benchmark comprising
564 finely annotated samples across diverse task types including text matching,
element recognition, layout understanding, and precise manipulation.
Additionally, we synthesize and release the largest computer use grounding
dataset Jedi, which contains 4 million examples through multi-perspective
decoupling of tasks. Our multi-scale models trained on Jedi demonstrate its
effectiveness by outperforming existing approaches on ScreenSpot-v2,
ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved
grounding with Jedi directly enhances agentic capabilities of general
foundation models on complex computer tasks, improving from 5% to 27% on
OSWorld. Through detailed ablation studies, we identify key factors
contributing to grounding performance and verify that combining specialized
data for different interface elements enables compositional generalization to
novel interfaces. All benchmark, data, checkpoints, and code are open-sourced
and available at https://osworld-grounding.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modality-Aware Neuron Pruning for Unlearning in Multimodal Large
  Language Models <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheyuan Liu, Guangyao Dou, Xiangchi Yuan, Chunhui Zhang, Zhaoxuan Tan, Meng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models such as Large Language Models (LLMs) and Multimodal Large
Language Models (MLLMs) trained on massive datasets can lead them to memorize
and inadvertently reveal sensitive information, raising ethical and privacy
concerns. While some prior works have explored this issue in the context of
LLMs, it presents a unique challenge for MLLMs due to the entangled nature of
knowledge across modalities, making comprehensive unlearning more difficult. To
address this challenge, we propose Modality Aware Neuron Unlearning (MANU), a
novel unlearning framework for MLLMs designed to selectively clip neurons based
on their relative importance to the targeted forget data, curated for different
modalities. Specifically, MANU consists of two stages: important neuron
selection and selective pruning. The first stage identifies and collects the
most influential neurons across modalities relative to the targeted forget
knowledge, while the second stage is dedicated to pruning those selected
neurons. MANU effectively isolates and removes the neurons that contribute most
to the forget data within each modality, while preserving the integrity of
retained knowledge. Our experiments conducted across various MLLM architectures
illustrate that MANU can achieve a more balanced and comprehensive unlearning
in each modality without largely affecting the overall model utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongSpec: Long-Context Lossless Speculative Decoding with Efficient
  Drafting and Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Penghui Yang, Cunxiao Du, Fengzhuo Zhang, Haonan Wang, Tianyu Pang, Chao Du, Bo An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) can now process extremely long contexts,
efficient inference over these extended inputs has become increasingly
important, especially for emerging applications like LLM agents that highly
depend on this capability. Speculative decoding (SD) offers a promising
lossless acceleration technique compared to lossy alternatives such as
quantization and model cascades. However, most state-of-the-art SD methods are
trained on short texts (typically fewer than 4k tokens), making them unsuitable
for long-context scenarios. Specifically, adapting these methods to long
contexts presents three key challenges: (1) the excessive memory demands posed
by draft models due to large Key-Value (KV) cache; (2) performance degradation
resulting from the mismatch between short-context training and long-context
inference; and (3) inefficiencies in tree attention mechanisms when managing
long token sequences. This work introduces LongSpec, a framework that addresses
these challenges through three core innovations: a memory-efficient draft model
with a constant-sized KV cache; novel position indices that mitigate the
training-inference mismatch; and an attention aggregation strategy that
combines fast prefix computation with standard tree attention to enable
efficient decoding. Experimental results confirm the effectiveness of LongSpec,
achieving up to a 3.26x speedup over strong Flash Attention baselines across
five long-context understanding datasets, as well as a 2.25x reduction in
wall-clock time on the AIME24 long reasoning task with the QwQ model,
demonstrating significant latency improvements for long-context applications.
The code is available at https://github.com/sail-sg/LongSpec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAPTURE: Context-Aware Prompt Injection Testing and Robustness
  Enhancement <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gauri Kholkar, Ratinder Ahuja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt injection remains a major security risk for large language models.
However, the efficacy of existing guardrail models in context-aware settings
remains underexplored, as they often rely on static attack benchmarks.
Additionally, they have over-defense tendencies. We introduce CAPTURE, a novel
context-aware benchmark assessing both attack detection and over-defense
tendencies with minimal in-domain examples. Our experiments reveal that current
prompt injection guardrail models suffer from high false negatives in
adversarial cases and excessive false positives in benign scenarios,
highlighting critical limitations. To demonstrate our framework's utility, we
train CaptureGuard on our generated data. This new model drastically reduces
both false negative and false positive rates on our context-aware datasets
while also generalizing effectively to external benchmarks, establishing a path
toward more robust and practical prompt injection defenses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACL LLMSec Workshop 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural
  Understanding and Transcreation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01565v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01565v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zhou, Lutong Yu, Dongchu Xie, Shaohuan Cheng, Wenyan Li, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Culture is a rich and dynamic domain that evolves across both geography and
time. However, existing studies on cultural understanding with vision-language
models (VLMs) primarily emphasize geographic diversity, often overlooking the
critical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a
novel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning
ancient Chinese dynasties, serves as a representative cultural heritage that
reflects the profound temporal aspects of Chinese culture while remaining
highly popular in Chinese contemporary society. Hanfu-Bench comprises two core
tasks: cultural visual understanding and cultural image transcreation.The
former task examines temporal-cultural feature recognition based on single- or
multi-image inputs through multiple-choice visual question answering, while the
latter focuses on transforming traditional attire into modern designs through
cultural element inheritance and modern context adaptation. Our evaluation
shows that closed VLMs perform comparably to non-experts on visual cutural
understanding but fall short by 10\% to human experts, while open VLMs lags
further behind non-experts. For the transcreation task, multi-faceted human
evaluation indicates that the best-performing model achieves a success rate of
only 42\%. Our benchmark provides an essential testbed, revealing significant
challenges in this new direction of temporal cultural understanding and
creative adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>cultural analysis, cultural visual understanding, cultural image
  transcreation (update dataset license)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Geometric Signatures of Compositionality Across a Language Model's
  Lifetime <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01444v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01444v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Hwa Lee, Thomas Jiralerspong, Lei Yu, <span class="highlight-author">Yoshua Bengio</span>, Emily Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By virtue of linguistic compositionality, few syntactic rules and a finite
lexicon can generate an unbounded number of sentences. That is, language,
though seemingly high-dimensional, can be explained using relatively few
degrees of freedom. An open question is whether contemporary language models
(LMs) reflect the intrinsic simplicity of language that is enabled by
compositionality. We take a geometric view of this problem by relating the
degree of compositionality in a dataset to the intrinsic dimension (ID) of its
representations under an LM, a measure of feature complexity. We find not only
that the degree of dataset compositionality is reflected in representations'
ID, but that the relationship between compositionality and geometric complexity
arises due to learned linguistic features over training. Finally, our analyses
reveal a striking contrast between nonlinear and linear dimensionality, showing
they respectively encode semantic and superficial aspects of linguistic
composition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counterfactual-Consistency Prompting for Relative Temporal Understanding
  in Large Language Models <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11425v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11425v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongho Kim, Seung-won Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the advanced capabilities of large language models (LLMs), their
temporal reasoning ability remains underdeveloped. Prior works have highlighted
this limitation, particularly in maintaining temporal consistency when
understanding events. For example, models often confuse mutually exclusive
temporal relations like ``before'' and ``after'' between events and make
inconsistent predictions. In this work, we tackle the issue of temporal
inconsistency in LLMs by proposing a novel counterfactual prompting approach.
Our method generates counterfactual questions and enforces collective
constraints, enhancing the model's consistency. We evaluate our method on
multiple datasets, demonstrating significant improvements in event ordering for
explicit and implicit events and temporal commonsense understanding by
effectively addressing temporal inconsistencies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 main (short)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compression of enumerations and gain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03030v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03030v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Barmpalias, Xiaoyan Zhang, Bohua Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the compressibility of enumerations in the context of Kolmogorov
complexity, focusing on strong and weak forms of compression and their gain:
the amount of auxiliary information embedded in the compressed enumeration. The
existence of strong compression and weak gainless compression is shown for any
computably enumerable (c.e.) set. The density problem of c.e. sets with respect
to their prefix complexity is reduced to the question of whether every c.e. set
is well-compressible, which we study via enumeration games.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reward Shaping to Mitigate Reward Hacking in RLHF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18770v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18770v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback (RLHF) is essential for aligning
large language models (LLMs) with human values. However, RLHF is susceptible to
\emph{reward hacking}, where the agent exploits flaws in the reward function
rather than learning the intended behavior, thus degrading alignment. Although
reward shaping helps stabilize RLHF and partially mitigate reward hacking, a
systematic investigation into shaping techniques and their underlying
principles remains lacking. To bridge this gap, we present a comprehensive
study of the prevalent reward shaping methods. Our analysis suggests two key
design principles: (1) the RL reward should be bounded, and (2) the RL reward
benefits from rapid initial growth followed by gradual convergence. Guided by
these insights, we propose Preference As Reward (PAR), a novel approach that
leverages the latent preferences embedded within the reward model as the signal
for reinforcement learning. We evaluated PAR on two base models, Gemma2-2B, and
Llama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF.
Experimental results demonstrate PAR's superior performance over other reward
shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at
least 5 percentage points higher than competing approaches. Furthermore, PAR
exhibits remarkable data efficiency, requiring only a single reference reward
for optimal performance, and maintains robustness against reward hacking even
after two full epochs of training. The code is available at
https://github.com/PorUna-byte/PAR, and the Work done during the internship at
StepFun by Jiayi Fu.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG
  Alignment via Large Language Model and Contrastive Learning on ChineseEEG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00854v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00854v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacky Tai-Yu Lu, Jung Chiang, Chi-Sheng Chen, Anna Nai-Yun Tung, Hsiang Wei Hu, Yuan Chiao Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one
of the earliest open-vocabulary EEG-to-text generation frameworks tailored for
Chinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact
pretrained language model (MiniLM), our architecture aligns multichannel brain
signals with natural language representations via masked pretraining and
contrastive learning. Using a subset of the ChineseEEG dataset, where each
sentence contains approximately ten Chinese characters aligned with 128-channel
EEG recorded at 256 Hz, we segment EEG into per-character embeddings and
predict full sentences in a zero-shot setting. The decoder is trained with
teacher forcing and padding masks to accommodate variable-length sequences.
Evaluation on over 1,500 training-validation sentences and 300 held-out test
samples shows promising lexical alignment, with a best BLEU-1 score of 6.38\%.
While syntactic fluency remains a challenge, our findings demonstrate the
feasibility of non-phonetic, cross-modal language decoding from EEG. This work
opens a new direction in multilingual brain-to-text research and lays the
foundation for future cognitive-language interfaces in Chinese.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMed<span class="highlight-title">Agent</span>-RL: Optimizing <span class="highlight-title">Multi-Agent</span> <span class="highlight-title">Collaborat</span>ion for Multimodal
  Medical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00555v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00555v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Xia, Jinglu Wang, Yibo Peng, Kaide Zeng, Xian Wu, Xiangru Tang, Hongtu Zhu, Yun Li, Shujie Liu, Yan Lu, Huaxiu Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential
in multimodal diagnostic tasks. However, existing single-agent models struggle
to generalize across diverse medical specialties, limiting their performance.
Recent efforts introduce multi-agent collaboration frameworks inspired by
clinical workflows, where general practitioners (GPs) and specialists interact
in a fixed sequence. Despite improvements, these static pipelines lack
flexibility and adaptability in reasoning. To address this, we propose
MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that
enables dynamic, optimized collaboration among medical agents. Specifically, we
train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to
assign patients to appropriate specialties, while the attending physician
integrates the judgments from multi-specialists and its own knowledge to make
final decisions. To address the inconsistency in specialist outputs, we
introduce a curriculum learning (CL)-guided RL strategy that progressively
teaches the attending physician to balance between imitating specialists and
correcting their mistakes. Experiments on five medical VQA benchmarks
demonstrate that MMedAgent-RL not only outperforms both open-source and
proprietary Med-LVLMs, but also exhibits human-like reasoning patterns.
Notably, it achieves an average performance gain of 20.7% over supervised
fine-tuning baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OWLViz: An Open-World Benchmark for Visual Question Answering <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07631v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07631v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thuy Nguyen, Dang Nguyen, Hoang Nguyen, Thuan Luong, Long Hoang Dang, Viet Dac Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a challenging benchmark for the Open WorLd VISual question
answering (OWLViz) task. OWLViz presents concise, unambiguous queries that
require integrating multiple capabilities, including visual understanding, web
exploration, and specialized tool usage. While humans achieve 69.2% accuracy on
these intuitive tasks, even state-of-the-art VLMs struggle, with the best
model, Gemini 2.0, achieving only 26.6% accuracy. Current agentic VLMs, which
rely on limited vision and vision-language models as tools, perform even worse.
This performance gap reveals significant limitations in multimodal systems'
ability to select appropriate tools and execute complex reasoning sequences,
establishing new directions for advancing practical AI research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025 Workshop on Multi-Agent Systems in the Era of Foundation
  Models: Opportunities, Challenges, and Futures. (8 pages + appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20613v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20613v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziju Shen, Naohao Huang, Fanyi Yang, Yutong Wang, Guoxiong Gao, Tianyi Xu, Jiedong Jiang, Wanyi He, Pu Yang, Mengzhou Sun, Haocheng Ju, Peihao Wu, Bryan Dai, Bin Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, formal theorem provers have made monumental progress on high-school
and competition-level mathematics, but few of them generalize to more advanced
mathematics. In this paper, we present REAL-Prover, a new open-source stepwise
theorem prover for Lean 4 to push this boundary. This prover, based on our
fine-tuned large language model (REAL-Prover-v1) and integrated with a
retrieval system (Leansearch-PS), notably boosts performance on solving
college-level mathematics problems. To train REAL-Prover-v1, we developed
HERALD-AF, a data extraction pipeline that converts natural language math
problems into formal statements, and a new open-source Lean 4 interactive
environment (Jixia-interactive) to facilitate synthesis data collection. In our
experiments, our prover using only supervised fine-tune achieves competitive
results with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable
to state-of-the-art (SOTA) models. To further evaluate our approach, we
introduce FATE-M, a new benchmark focused on algebraic problems, where our
prover achieves a SOTA success rate of 56.7% (Pass@64).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Personal</span>izing <span class="highlight-title">Student</span>-<span class="highlight-title">Agent</span> Interactions Using Log-Contextualized
  Retrieval Augmented Generation (RAG) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.17238v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.17238v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clayton Cohn, Surya Rayala, Caitlin Snyder, Joyce Fonteles, Shruti Jain, Naveeduddin Mohammed, Umesh Timalsina, Sarah K. Burriss, Ashwin T S, Namrata Srivastava, Menton Deweese, Angela Eeds, Gautam Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative dialogue offers rich insights into students' learning and
critical thinking, which is essential for personalizing pedagogical agent
interactions in STEM+C settings. While large language models (LLMs) facilitate
dynamic pedagogical interactions, hallucinations undermine confidence, trust,
and instructional value. Retrieval-augmented generation (RAG) grounds LLM
outputs in curated knowledge but requires a clear semantic link between user
input and a knowledge base, which is often weak in student dialogue. We propose
log-contextualized RAG (LC-RAG), which enhances RAG retrieval by using
environment logs to contextualize collaborative discourse. Our findings show
that LC-RAG improves retrieval over a discourse-only baseline and allows our
collaborative peer agent, Copa, to deliver relevant, personalized guidance that
supports students' critical thinking and epistemic decision-making in a
collaborative computational modeling environment, C2STEM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the International Conference on Artificial Intelligence
  in Education (AIED25) Workshop on Epistemics and Decision-Making in
  AI-Supported Education</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Agent</span>CPM-GUI: Building Mobile-Use <span class="highlight-title">Agent</span>s with Reinforcement Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01391v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01391v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhong Zhang, Yaxi Lu, Yikun Fu, Yupeng Huo, Shenzhi Yang, Yesai Wu, Han Si, Xin Cong, Haotian Chen, Yankai Lin, Jie Xie, Wei Zhou, Wang Xu, Yuanheng Zhang, Zhou Su, Zhongwu Zhai, Xiaoming Liu, Yudong Mei, Jianming Xu, Hongyan Tian, Chongyi Wang, Chi Chen, Yuan Yao, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent progress of large language model agents has opened new
possibilities for automating tasks through graphical user interfaces (GUIs),
especially in mobile environments where intelligent interaction can greatly
enhance usability. However, practical deployment of such agents remains
constrained by several key challenges. Existing training data is often noisy
and lack semantic diversity, which hinders the learning of precise grounding
and planning. Models trained purely by imitation tend to overfit to seen
interface patterns and fail to generalize in unfamiliar scenarios. Moreover,
most prior work focuses on English interfaces while overlooks the growing
diversity of non-English applications such as those in the Chinese mobile
ecosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent
built for robust and efficient on-device GUI interaction. Our training pipeline
includes grounding-aware pre-training to enhance perception, supervised
fine-tuning on high-quality Chinese and English trajectories to imitate
human-like actions, and reinforcement fine-tuning with GRPO to improve
reasoning capability. We also introduce a compact action space that reduces
output length and supports low-latency execution on mobile devices.
AgentCPM-GUI achieves state-of-the-art performance on five public benchmarks
and a new Chinese GUI benchmark called CAGUI, reaching $96.9\%$ Type-Match and
$91.3\%$ Exact-Match. To facilitate reproducibility and further research, we
publicly release all code, model checkpoint, and evaluation data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated results in Table 2 and Table 3; The project is available at
  https://github.com/OpenBMB/AgentCPM-GUI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing Consistency and Reproducibility in the Outputs of Large
  Language Models: Evidence Across Diverse Finance and Accounting Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16974v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16974v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Junyan Wang, Victor Xiaoqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study provides the first comprehensive assessment of consistency and
reproducibility in Large Language Model (LLM) outputs in finance and accounting
research. We evaluate how consistently LLMs produce outputs given identical
inputs through extensive experimentation with 50 independent runs across five
common tasks: classification, sentiment analysis, summarization, text
generation, and prediction. Using three OpenAI models (GPT-3.5-turbo,
GPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse
financial source texts and data, covering MD&As, FOMC statements, finance news
articles, earnings call transcripts, and financial statements. Our findings
reveal substantial but task-dependent consistency, with binary classification
and sentiment analysis achieving near-perfect reproducibility, while complex
tasks show greater variability. More advanced models do not consistently
demonstrate better consistency and reproducibility, with task-specific patterns
emerging. LLMs significantly outperform expert human annotators in consistency
and maintain high agreement even where human experts significantly disagree. We
further find that simple aggregation strategies across 3-5 runs dramatically
improve consistency. We also find that aggregation may come with an additional
benefit of improved accuracy for sentiment analysis when using newer models.
Simulation analysis reveals that despite measurable inconsistency in LLM
outputs, downstream statistical inferences remain remarkably robust. These
findings address concerns about what we term "G-hacking," the selective
reporting of favorable outcomes from multiple Generative AI runs, by
demonstrating that such risks are relatively low for finance and accounting
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>89 pages, 20 tables, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Goal-oriented Proactive <span class="highlight-title">Dialogue</span> Systems via Consistency
  Reflection and Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13366v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13366v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Didi Zhang, Yaxin Fan, Peifeng Li, Qiaoming Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Goal-oriented proactive dialogue systems are designed to guide user
conversations seamlessly towards specific objectives by planning a
goal-oriented path. However, previous research has focused predominantly on
optimizing these paths while neglecting the inconsistencies that may arise
between generated responses and dialogue contexts, including user profiles,
dialogue history, domain knowledge, and subgoals. To address this issue, we
introduce a model-agnostic two-stage Consistency Reflection and Correction
(CRC) framework. Specifically, in the consistency reflection stage, the model
is prompted to reflect on the discrepancies between generated responses and
dialogue contexts, identifying inconsistencies and suggesting possible
corrections. In the consistency correction stage, the model generates responses
that are more consistent with the dialogue context based on these reflection
results. We conducted experiments on various model architectures with different
parameter sizes, including encoder-decoder models (BART, T5) and decoder-only
models (GPT-2, DialoGPT, Phi3, Mistral and LLaMA3), and the experimental
results on three datasets demonstrate that our CRC framework significantly
improves the consistency between generated responses and dialogue contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05693v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05693v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael R. Metel, Boxing Chen, Mehdi Rezagholizadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several works have developed eviction policies to remove key-value (KV) pairs
from the KV cache for more efficient inference. The focus has been on
compressing the KV cache after the input prompt has been processed for faster
token generation. In settings with limited GPU memory, and when the input
context is longer than the generation length, we show that by also compressing
the KV cache during the input processing phase, larger batch sizes can be used
resulting in significantly higher throughput while still maintaining the
original model's accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAE-V: Interpreting Multimodal Models for Enhanced Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17514v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17514v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hantao Lou, Changye Li, Jiaming Ji, Yaodong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the integration of image modality, the semantic space of multimodal
large language models (MLLMs) is more complex than text-only models, making
their interpretability more challenging and their alignment less stable,
particularly susceptible to low-quality data, which can lead to inconsistencies
between modalities, hallucinations, and biased outputs. As a result, developing
interpretability methods for MLLMs is crucial for improving alignment quality
and efficiency. In text-only LLMs, Sparse Autoencoders (SAEs) have gained
attention for their ability to interpret latent representations. However,
extending SAEs to multimodal settings presents new challenges due to modality
fusion and the difficulty of isolating cross-modal representations. To address
these challenges, we introduce SAE-V, a mechanistic interpretability framework
that extends the SAE paradigm to MLLMs. By identifying and analyzing
interpretable features along with their corresponding data, SAE-V enables
fine-grained interpretation of both model behavior and data quality,
facilitating a deeper understanding of cross-modal interactions and alignment
dynamics. Moreover, by utilizing cross-modal feature weighting, SAE-V provides
an intrinsic data filtering mechanism to enhance model alignment without
requiring additional models. Specifically, when applied to the alignment
process of MLLMs, SAE-V-based data filtering methods could achieve more than
110% performance with less than 50% data. Our results highlight SAE-V's ability
to enhance interpretability and alignment in MLLMs, providing insights into
their internal mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Clinical Models with Pseudo Data for De-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12674v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12674v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Landes, Aaron J Chaise, Tarak Nath Nandi, Ravi K Madduri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many models are pretrained on redacted text for privacy reasons. Clinical
foundation models are often trained on de-identified text, which uses special
syntax (masked) text in place of protected health information. Even though
these models have increased in popularity, there has been little effort in
understanding the effects of training them on redacted text. In this work, we
pretrain several encoder-only models on a dataset that contains redacted text
and a version with replaced realistic pseudo text. We then fine-tuned models
for the protected health information de-identification task and show how our
methods significantly outperform previous baselines. The contributions of this
work include: a) our novel, and yet surprising findings with training
recommendations, b) redacted text replacements used to produce the pseudo
dataset, c) pretrained embeddings and fine-tuned task specific models, and d)
freely available pseudo training dataset generation and model source code used
in our experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Multi-Agent</span> Language Models: Advancing Cooperation, Coordination, and
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arjun Vaithilingam Sudhakar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot
generalization capabilities across complex natural language tasks, enabling
their widespread use as virtual assistants for diverse applications such as
translation and summarization. Despite being trained solely on large corpora of
text without explicit supervision on author intent, LLMs appear to infer the
underlying meaning of textual interactions. This raises a fundamental question:
can LLMs model and reason about the intentions of others, i.e., do they possess
a form of theory of mind? Understanding other's intentions is crucial for
effective collaboration, which underpins human societal success and is
essential for cooperative interactions among multiple agents, including humans
and autonomous systems. In this work, we investigate the theory of mind in LLMs
through the lens of cooperative multi-agent reinforcement learning (MARL),
where agents learn to collaborate via repeated interactions, mirroring human
social reasoning. Our approach aims to enhance artificial agent's ability to
adapt and cooperate with both artificial and human partners. By leveraging
LLM-based agents capable of natural language interaction, we move towards
creating hybrid human-AI systems that can foster seamless collaboration, with
broad implications for the future of human-artificial interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2311.07687</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entropy-based Exploration Conduction for Multi-step Reasoning <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.15848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.15848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinghan Zhang, Xiting Wang, Fengran Mo, Yeyang Zhou, Wanfu Gao, Kunpeng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-step processes via large language models (LLMs) have proven effective
for solving complex reasoning tasks. However, the depth of exploration of the
reasoning procedure can significantly affect the task performance. Existing
methods to automatically decide the depth often lead to high cost and a lack of
flexibility. To address these issues, we propose Entropy-based Exploration
Depth Conduction (Entro-duction), a novel method that dynamically adjusts the
exploration depth during multi-step reasoning by monitoring LLM's output
entropy and variance entropy. We employ these two features to capture the
model's uncertainty of the current step and the fluctuation of uncertainty
across consecutive reasoning steps. Based on the observed entropy changes, the
LLM selects whether to deepen, expand, or stop exploration according to the
probability, which facilitates the trade-off between the reasoning accuracy and
exploration effectiveness. Experimental results across four benchmark datasets
demonstrate the efficacy of Entro-duction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaMP-Cap: <span class="highlight-title">Personal</span>ized Figure Caption Generation With Multimodal Figure
  Profiles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ho Yin 'Sam' Ng, Ting-Yao Hsu, Aashish Anantha Ramakrishnan, Branislav Kveton, Nedim Lipka, Franck Dernoncourt, Dongwon Lee, Tong Yu, Sungchul Kim, Ryan A. Rossi, Ting-Hao 'Kenneth' Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Figure captions are crucial for helping readers understand and remember a
figure's key message. Many models have been developed to generate these
captions, helping authors compose better quality captions more easily. Yet,
authors almost always need to revise generic AI-generated captions to match
their writing style and the domain's style, highlighting the need for
personalization. Despite language models' personalization (LaMP) advances,
these technologies often focus on text-only settings and rarely address
scenarios where both inputs and profiles are multimodal. This paper introduces
LaMP-Cap, a dataset for personalized figure caption generation with multimodal
figure profiles. For each target figure, LaMP-Cap provides not only the needed
inputs, such as figure images, but also up to three other figures from the same
document--each with its image, caption, and figure-mentioning paragraphs--as a
profile to characterize the context. Experiments with four LLMs show that using
profile information consistently helps generate captions closer to the original
author-written ones. Ablation studies reveal that images in the profile are
more helpful than figure-mentioning paragraphs, highlighting the advantage of
using multimodal profiles over text-only ones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The LaMP-CAP dataset is publicly available at:
  https://github.com/Crowd-AI-Lab/lamp-cap</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Resolving UnderEdit & OverEdit with Iterative & Neighbor-Assisted Model
  Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11895v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11895v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhiman Kumar Baghel, Scott M. Jordan, Zheyuan Ryan Shi, Xiang Lorraine Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are widely deployed in downstream tasks, but
keeping their knowledge up-to-date via retraining or fine-tuning is often
computationally expensive. Model editing provides a more efficient alternative
by updating a targeted subset of parameters, which often follows the
locate-and-edit paradigm. Despite this efficiency, existing methods are
limited: edits may fail to inject knowledge (UnderEdit) or unintentionally
disrupt unrelated neighboring knowledge (OverEdit). To address these
challenges, we propose two complementary methods: iterative model editing,
which applies successive edits to mitigate UnderEdit, and neighbor-assisted
model editing, which incorporates neighboring knowledge during editing to
reduce OverEdit. Our extensive experiments show that these techniques improve
editing performance across multiple LLMs, algorithms, and benchmarks, reducing
UnderEdit by up to 38 percentage points and OverEdit by up to 6, while
remaining broadly applicable to any locate-and-edit method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Too Big to Think: Capacity, Memorization, and Generalization in
  Pre-Trained Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Barron, Devin White
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The relationship between memorization and generalization in large language
models (LLMs) remains an open area of research, with growing evidence that the
two are deeply intertwined. In this work, we investigate this relationship by
pre-training a series of capacity-limited Transformer models from scratch on
two synthetic character-level tasks designed to separately probe generalization
(via arithmetic extrapolation) and memorization (via factual recall). We
observe a consistent trade-off: small models extrapolate to unseen arithmetic
cases but fail to memorize facts, while larger models memorize but fail to
extrapolate. An intermediate-capacity model exhibits a similar shift toward
memorization. When trained on both tasks jointly, no model (regardless of size)
succeeds at extrapolation. These findings suggest that pre-training may
intrinsically favor one learning mode over the other. By isolating these
dynamics in a controlled setting, our study offers insight into how model
capacity shapes learning behavior and offers broader implications for the
design and deployment of small language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for oral presentation to Tiny Titans: The next wave of
  On-Device Learning for Foundational Models Workshop at the 42nd International
  Conference on Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UD-English-CHILDES: A Collected Resource of Gold and Silver Universal
  Dependencies Trees for Child Language Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.20304v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.20304v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiulin Yang, Zhuoxuan Ju, Lanni Bu, Zoey Liu, Nathan Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CHILDES is a widely used resource of transcribed child and child-directed
speech. This paper introduces UD-English-CHILDES, the first officially released
Universal Dependencies (UD) treebank. It is derived from previously
dependency-annotated CHILDES data, which we harmonize to follow unified
annotation principles. The gold-standard trees encompass utterances sampled
from 11 children and their caregivers, totaling over 48K sentences (236K
tokens). We validate these gold-standard annotations under the UD v2 framework
and provide an additional 1M~silver-standard sentences, offering a consistent
resource for computational and linguistic research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>UDW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can LLMs Ask Good Questions? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03491v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03491v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueheng Zhang, Xiaoyuan Liu, Yiyou Sun, Atheer Alharbi, Hend Alzahrani, Tianneng Shi, Basel Alomair, Dawn Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We evaluate questions generated by large language models (LLMs) from context,
comparing them to human-authored questions across six dimensions: question
type, question length, context coverage, answerability, uncommonness, and
required answer length. Our study spans two open-source and two proprietary
state-of-the-art models. Results reveal that LLM-generated questions tend to
demand longer descriptive answers and exhibit more evenly distributed context
focus, in contrast to the positional bias often seen in QA tasks. These
findings provide insights into the distinctive characteristics of LLM-generated
questions and inform future work on question quality and downstream
applications.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-16T00:00:00Z">2025-06-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">155</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steering LLM Thinking with Budget Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyan Li, Wenshuo Zhao, Yang Zhang, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent deep-thinking large language models often reason extensively to
improve performance, but such lengthy reasoning is not always desirable, as it
incurs excessive inference costs with disproportionate performance gains.
Controlling reasoning length without sacrificing performance is therefore
important, but remains challenging, especially under tight thinking budgets. We
propose budget guidance, a simple yet effective method for steering the
reasoning process of LLMs toward a target budget without requiring any LLM
fine-tuning. Our approach introduces a lightweight predictor that models a
Gamma distribution over the remaining thinking length during next-token
generation. This signal is then used to guide generation in a soft, token-level
manner, ensuring that the overall reasoning trace adheres to the specified
thinking budget. Budget guidance enables natural control of the thinking
length, along with significant token efficiency improvements over baseline
methods on challenging math benchmarks. For instance, it achieves up to a 26%
accuracy gain on the MATH-500 benchmark under tight budgets compared to
baseline methods, while maintaining competitive accuracy with only 63% of the
thinking tokens used by the full-thinking model. Budget guidance also
generalizes to broader task domains and exhibits emergent capabilities, such as
estimating question difficulty. The source code is available at:
https://github.com/UMass-Embodied-AGI/BudgetGuidance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LTRR: Learning To Rank Retrievers for LLMs <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        To Eun Kim, Fernando Diaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems typically rely on a single fixed
retriever, despite growing evidence that no single retriever performs optimally
across all query types. In this paper, we explore a query routing approach that
dynamically selects from a pool of retrievers based on the query, using both
train-free heuristics and learned routing models. We frame routing as a
learning-to-rank (LTR) problem and introduce LTRR, a framework that learns to
rank retrievers by their expected utility gain to downstream LLM performance.
Our experiments, conducted on synthetic QA data with controlled query type
variations, show that routing-based RAG systems can outperform the best
single-retriever-based systems. Performance gains are especially pronounced in
models trained with the Answer Correctness (AC) metric and with pairwise
learning approaches, especially with XGBoost. We also observe improvements in
generalization to out-of-distribution queries. As part of the SIGIR 2025
LiveRAG challenge, our submitted system demonstrated the practical viability of
our approach, achieving competitive performance in both answer correctness and
faithfulness. These findings highlight the importance of both training
methodology and metric selection in query routing for RAG systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR 2025 LiveRAG Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instruction Following by Boosting Attention of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vitoria Guardieiro, Adam Stein, Avishree Khare, Eric Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controlling the generation of large language models (LLMs) remains a central
challenge to ensure their safe and reliable deployment. While prompt
engineering and finetuning are common approaches, recent work has explored
latent steering, a lightweight technique that alters LLM internal activations
to guide generation. However, subsequent studies revealed latent steering's
effectiveness to be limited, often underperforming simple instruction
prompting. To address this limitation, we first establish a benchmark across
diverse behaviors for standardized evaluation of steering techniques. Building
on insights from this benchmark, we introduce Instruction Attention Boosting
(InstABoost), a latent steering method that boosts the strength of instruction
prompting by altering the model's attention during generation. InstABoost
combines the strengths of existing approaches and is theoretically supported by
prior work that suggests that in-context rule following in transformer-based
models can be controlled by manipulating attention on instructions.
Empirically, InstABoost demonstrates superior control success compared to both
traditional prompting and latent steering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attribution-guided Pruning for Compression, Circuit Discovery, and
  Targeted Correction in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayed Mohammad Vakilzadeh Hatefi, Maximilian Dreyer, Reduan Achtibat, Patrick Kahardipraja, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are central to many contemporary AI
applications, yet their extensive parameter counts pose significant challenges
for deployment in memory- and compute-constrained environments. Recent works in
eXplainable AI (XAI), particularly on attribution methods, suggest that
interpretability can also enable model compression by identifying and removing
components irrelevant to inference. In this paper, we leverage Layer-wise
Relevance Propagation (LRP) to perform attribution-guided pruning of LLMs.
While LRP has shown promise in structured pruning for vision models, we extend
it to unstructured pruning in LLMs and demonstrate that it can substantially
reduce model size with minimal performance loss. Our method is especially
effective in extracting task-relevant subgraphs -- so-called ``circuits'' --
which can represent core functions (e.g., indirect object identification).
Building on this, we introduce a technique for model correction, by selectively
removing circuits responsible for spurious behaviors (e.g., toxic outputs). All
in all, we gather these techniques as a uniform holistic framework and showcase
its effectiveness and limitations through extensive experiments for
compression, circuit discovery and model correction on Llama and OPT models,
highlighting its potential for improving both model efficiency and safety. Our
code is publicly available at https://github.com/erfanhatefi/SparC3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress (10 pages manuscript, 3 pages references, 12 pages
  appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balancing Knowledge Delivery and Emotional Comfort in Healthcare
  <span class="highlight-title">Conversation</span>al Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shang-Chi Tsai, Yun-Nung Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of large language models, many dialogue systems are now
capable of providing reasonable and informative responses to patients' medical
conditions. However, when patients consult their doctor, they may experience
negative emotions due to the severity and urgency of their situation. If the
model can provide appropriate comfort and empathy based on the patient's
negative emotions while answering medical questions, it will likely offer a
more reassuring experience during the medical consultation process. To address
this issue, our paper explores the balance between knowledge sharing and
emotional support in the healthcare dialogue process. We utilize a large
language model to rewrite a real-world interactive medical dialogue dataset,
generating patient queries with negative emotions and corresponding medical
responses aimed at soothing the patient's emotions while addressing their
concerns. The modified data serves to refine the latest large language models
with various fine-tuning methods, enabling them to accurately provide sentences
with both emotional reassurance and constructive suggestions in response to
patients' questions. Compared to the original LLM model, our experimental
results demonstrate that our methodology significantly enhances the model's
ability to generate emotional responses while maintaining its original
capability to provide accurate knowledge-based answers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IWSDS 2025 Oral Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Turning Down the Heat: A Critical Analysis of Min-p Sampling in Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rylan Schaeffer, Joshua Kazdan, Yegor Denisov-Blanch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sampling from language models impacts the quality and diversity of outputs,
affecting both research and real-world applications. Recently, Nguyen et al.
2024's "Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM
Outputs" introduced a new sampler called min-p, claiming it achieves superior
quality and diversity over established samplers such as basic, top-k, and top-p
sampling. The significance of these claims was underscored by the paper's
recognition as the 18th highest-scoring submission to ICLR 2025 and selection
for an Oral presentation. This paper conducts a comprehensive re-examination of
the evidence supporting min-p and reaches different conclusions from the
original paper's four lines of evidence. First, the original paper's human
evaluations omitted data, conducted statistical tests incorrectly, and
described qualitative feedback inaccurately; our reanalysis demonstrates min-p
did not outperform baselines in quality, diversity, or a trade-off between
quality and diversity; in response to our findings, the authors of the original
paper conducted a new human evaluation using a different implementation, task,
and rubric that nevertheless provides further evidence min-p does not improve
over baselines. Second, comprehensively sweeping the original paper's NLP
benchmarks reveals min-p does not surpass baselines when controlling for the
number of hyperparameters. Third, the original paper's LLM-as-a-Judge
evaluations lack methodological clarity and appear inconsistently reported.
Fourth, community adoption claims (49k GitHub repositories, 1.1M GitHub stars)
were found to be unsubstantiated, leading to their removal; the revised
adoption claim remains misleading. We conclude that evidence presented in the
original paper fails to support claims that min-p improves quality, diversity,
or a trade-off between quality and diversity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prefix-Tuning+: Modernizing Prefix-Tuning through Attention Independent
  Prefix Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Wang, Brian Chen, Li Siquan, Liang Xinhe, Tianyang Hu, Hwee Kuan Lee, Kenji Kawaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-Efficient Fine-Tuning (PEFT) methods have become crucial for
rapidly adapting large language models (LLMs) to downstream tasks.
Prefix-Tuning, an early and effective PEFT technique, demonstrated the ability
to achieve performance comparable to full fine-tuning with significantly
reduced computational and memory overhead. However, despite its earlier
success, its effectiveness in training modern state-of-the-art LLMs has been
very limited. In this work, we demonstrate empirically that Prefix-Tuning
underperforms on LLMs because of an inherent tradeoff between input and prefix
significance within the attention head. This motivates us to introduce
Prefix-Tuning+, a novel architecture that generalizes the principles of
Prefix-Tuning while addressing its shortcomings by shifting the prefix module
out of the attention head itself. We further provide an overview of our
construction process to guide future users when constructing their own
context-based methods. Our experiments show that, across a diverse set of
benchmarks, Prefix-Tuning+ consistently outperforms existing Prefix-Tuning
methods. Notably, it achieves performance on par with the widely adopted LoRA
method on several general benchmarks, highlighting the potential modern
extension of Prefix-Tuning approaches. Our findings suggest that by overcoming
its inherent limitations, Prefix-Tuning can remain a competitive and relevant
research direction in the landscape of parameter-efficient LLM adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stream-Omni: Simultaneous Multimodal Interactions with Large
  Language-Vision-Speech Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaolei Zhang, Shoutao Guo, Qingkai Fang, Yan Zhou, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of GPT-4o-like large multimodal models (LMMs) has raised the
exploration of integrating text, vision, and speech modalities to support more
flexible multimodal interaction. Existing LMMs typically concatenate
representation of modalities along the sequence dimension and feed them into a
large language model (LLM) backbone. While sequence-dimension concatenation is
straightforward for modality integration, it often relies heavily on
large-scale data to learn modality alignments. In this paper, we aim to model
the relationships between modalities more purposefully, thereby achieving more
efficient and flexible modality alignments. To this end, we propose
Stream-Omni, a large language-vision-speech model with efficient modality
alignments, which can simultaneously support interactions under various
modality combinations. Stream-Omni employs LLM as the backbone and aligns the
vision and speech to the text based on their relationships. For vision that is
semantically complementary to text, Stream-Omni uses sequence-dimension
concatenation to achieve vision-text alignment. For speech that is semantically
consistent with text, Stream-Omni introduces a CTC-based layer-dimension
mapping to achieve speech-text alignment. In this way, Stream-Omni can achieve
modality alignments with less data (especially speech), enabling the transfer
of text capabilities to other modalities. Experiments on various benchmarks
demonstrate that Stream-Omni achieves strong performance on visual
understanding, speech interaction, and vision-grounded speech interaction
tasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously
provide intermediate text outputs (such as ASR transcriptions and model
responses) during speech interaction, offering users a comprehensive multimodal
experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/ictnlp/Stream-Omni , Model:
  https://huggingface.co/ICTNLP/stream-omni-8b</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EvolvTrip: Enhancing Literary Character Understanding with Temporal
  Theory-of-Mind Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohao Yang, Hainiu Xu, Jinhua Du, Ze Li, Yulan He, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A compelling portrayal of characters is essential to the success of narrative
writing. For readers, appreciating a character's traits requires the ability to
infer their evolving beliefs, desires, and intentions over the course of a
complex storyline, a cognitive skill known as Theory-of-Mind (ToM). Performing
ToM reasoning in prolonged narratives requires readers to integrate historical
context with current narrative information, a task at which humans excel but
Large Language Models (LLMs) often struggle. To systematically evaluate LLMs'
ToM reasoning capability in long narratives, we construct LitCharToM, a
benchmark of character-centric questions across four ToM dimensions from
classic literature. Further, we introduce EvolvTrip, a perspective-aware
temporal knowledge graph that tracks psychological development throughout
narratives. Our experiments demonstrate that EvolvTrip consistently enhances
performance of LLMs across varying scales, even in challenging extended-context
scenarios. EvolvTrip proves to be particularly valuable for smaller models,
partially bridging the performance gap with larger LLMs and showing great
compatibility with lengthy narratives. Our findings highlight the importance of
explicit representation of temporal character mental states in narrative
comprehension and offer a foundation for more sophisticated character
understanding. Our data and code are publicly available at
https://github.com/Bernard-Yang/EvolvTrip.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Study of LLM-as-a-Judge: How Design Choices Impact
  Evaluation Reliability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Yamauchi, Taro Yano, Masafumi Oyamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) continue to advance, reliable evaluation
methods are essential particularly for open-ended, instruction-following tasks.
LLM-as-a-Judge enables automatic evaluation using LLMs as evaluators, but its
reliability remains uncertain. In this work, we analyze key factors affecting
its trustworthiness, focusing on alignment with human judgments and evaluation
consistency. Using BIGGENBench and EvalBiasBench, we study the effects of
evaluation design, decoding strategies, and Chain-of-Tought (CoT) reasoning in
evaluation. Our results show that evaluation criteria are critical for
reliability, non-deterministic sampling improves alignment with human
preferences over deterministic evaluation, and CoT reasoning offers minimal
gains when clear evaluation criteria are present.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Structured Bangla Dataset of Disease-Symptom Associations to Improve
  Diagnostic Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullah Al Shafi, Rowzatul Zannat, Abdul Muntakim, Mahmudul Hasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disease-symptom datasets are significant and in demand for medical research,
disease diagnosis, clinical decision-making, and AI-driven health management
applications. These datasets help identify symptom patterns associated with
specific diseases, thus improving diagnostic accuracy and enabling early
detection. The dataset presented in this study systematically compiles
disease-symptom relationships from various online sources, medical literature,
and publicly available health databases. The data was gathered through
analyzing peer-reviewed medical articles, clinical case studies, and
disease-symptom association reports. Only the verified medical sources were
included in the dataset, while those from non-peer-reviewed and anecdotal
sources were excluded. The dataset is structured in a tabular format, where the
first column represents diseases, and the remaining columns represent symptoms.
Each symptom cell contains a binary value (1 or 0), indicating whether a
symptom is associated with a disease (1 for presence, 0 for absence). Thereby,
this structured representation makes the dataset very useful for a wide range
of applications, including machine learning-based disease prediction, clinical
decision support systems, and epidemiological studies. Although there are some
advancements in the field of disease-symptom datasets, there is a significant
gap in structured datasets for the Bangla language. This dataset aims to bridge
that gap by facilitating the development of multilingual medical informatics
tools and improving disease prediction models for underrepresented linguistic
communities. Further developments should include region-specific diseases and
further fine-tuning of symptom associations for better diagnostic performance
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAMS: A CityGPT-Powered <span class="highlight-title">Agent</span>ic Framework for Urban <span class="highlight-title">Human</span> Mobility
  <span class="highlight-title">Simulat</span>ion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Du, Jie Feng, Jian Yuan, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human mobility simulation plays a crucial role in various real-world
applications. Recently, to address the limitations of traditional data-driven
approaches, researchers have explored leveraging the commonsense knowledge and
reasoning capabilities of large language models (LLMs) to accelerate human
mobility simulation. However, these methods suffer from several critical
shortcomings, including inadequate modeling of urban spaces and poor
integration with both individual mobility patterns and collective mobility
distributions. To address these challenges, we propose \textbf{C}ityGPT-Powered
\textbf{A}gentic framework for \textbf{M}obility \textbf{S}imulation
(\textbf{CAMS}), an agentic framework that leverages the language based urban
foundation model to simulate human mobility in urban space. \textbf{CAMS}
comprises three core modules, including MobExtractor to extract template
mobility patterns and synthesize new ones based on user profiles, GeoGenerator
to generate anchor points considering collective knowledge and generate
candidate urban geospatial knowledge using an enhanced version of CityGPT,
TrajEnhancer to retrieve spatial knowledge based on mobility patterns and
generate trajectories with real trajectory preference alignment via DPO.
Experiments on real-world datasets show that \textbf{CAMS} achieves superior
performance without relying on externally provided geospatial information.
Moreover, by holistically modeling both individual mobility patterns and
collective mobility constraints, \textbf{CAMS} generates more realistic and
plausible trajectories. In general, \textbf{CAMS} establishes a new paradigm
that integrates the agentic framework with urban-knowledgeable LLMs for human
mobility simulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Qwen vs. Gemma Integration with Whisper: A Comparative Study in
  Multilingual SpeechLLM Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan Nguyen, Long-Vu Hoang, Huy-Dat Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents our system for the MLC-SLM Challenge 2025, focusing on
multilingual speech recognition and language modeling with large language
models (LLMs). Our approach combines a fine-tuned Whisper-large-v3 encoder with
efficient projector architectures and various decoder configurations. We employ
a three-stage training methodology that progressively optimizes the encoder,
projector, and LLM components. Our system achieves competitive performance with
a private test average WER/CER result of 16.63% using the Gemma3-12B and 18.6%
using the Qwen2.5-7B as decoder-only language model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report for Interspeech 2025 MLC-SLM Challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning
  Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         MiniMax,  :, Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, Chengjun Xiao, Chengyu Du, Chi Zhang, Chu Qiao, Chunhao Zhang, Chunhui Du, Congchao Guo, Da Chen, Deming Ding, Dianjun Sun, Dong Li, Enwei Jiao, Haigang Zhou, Haimo Zhang, Han Ding, Haohai Sun, Haoyu Feng, Huaiguang Cai, Haichao Zhu, Jian Sun, Jiaqi Zhuang, Jiaren Cai, Jiayuan Song, Jin Zhu, Jingyang Li, Jinhao Tian, Jinli Liu, Junhao Xu, Junjie Yan, Junteng Liu, Junxian He, Kaiyi Feng, Ke Yang, Kecheng Xiao, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Li, Lin Zheng, Linge Du, Lingyu Yang, Lunbin Zeng, Minghui Yu, Mingliang Tao, Mingyuan Chi, Mozhi Zhang, Mujie Lin, Nan Hu, Nongyu Di, Peng Gao, Pengfei Li, Pengyu Zhao, Qibing Ren, Qidi Xu, Qile Li, Qin Wang, Rong Tian, Ruitao Leng, Shaoxiang Chen, Shaoyu Chen, Shengmin Shi, Shitong Weng, Shuchang Guan, Shuqi Yu, Sichen Li, Songquan Zhu, Tengfei Li, Tianchi Cai, Tianrun Liang, Weiyu Cheng, Weize Kong, Wenkai Li, Xiancai Chen, Xiangjun Song, Xiao Luo, Xiao Su, Xiaobo Li, Xiaodong Han, Xinzhu Hou, Xuan Lu, Xun Zou, Xuyang Shen, Yan Gong, Yan Ma, Yang Wang, Yiqi Shi, Yiran Zhong, Yonghong Duan, Yongxiang Fu, Yongyi Hu, Yu Gao, Yuanxiang Fan, Yufeng Yang, Yuhao Li, Yulin Hu, Yunan Huang, Yunji Li, Yunzhi Xu, Yuxin Mao, Yuxuan Shi, Yuze Wenren, Zehan Li, Zelin Li, Zhanxu Tian, Zhengmao Zhu, Zhenhua Fan, Zhenzhen Wu, Zhichao Xu, Zhihang Yu, Zhiheng Lyu, Zhuo Jiang, Zibo Gao, Zijia Wu, Zijian Song, Zijun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MiniMax-M1, the world's first open-weight, large-scale
hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid
Mixture-of-Experts (MoE) architecture combined with a lightning attention
mechanism. The model is developed based on our previous MiniMax-Text-01 model,
which contains a total of 456 billion parameters with 45.9 billion parameters
activated per token. The M1 model natively supports a context length of 1
million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning
attention mechanism in MiniMax-M1 enables efficient scaling of test-time
compute. These properties make M1 particularly suitable for complex tasks that
require processing long inputs and thinking extensively. MiniMax-M1 is trained
using large-scale reinforcement learning (RL) on diverse problems including
sandbox-based, real-world software engineering environments. In addition to
M1's inherent efficiency advantage for RL training, we propose CISPO, a novel
RL algorithm to further enhance RL efficiency. CISPO clips importance sampling
weights rather than token updates, outperforming other competitive RL variants.
Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on
512 H800 GPUs to complete in only three weeks, with a rental cost of just
$534,700. We release two versions of MiniMax-M1 models with 40K and 80K
thinking budgets respectively, where the 40K model represents an intermediate
phase of the 80K training. Experiments on standard benchmarks show that our
models are comparable or superior to strong open-weight models such as the
original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex
software engineering, tool utilization, and long-context tasks. We publicly
release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A technical report from MiniMax. The authors are listed in
  alphabetical order. We open-source our MiniMax-M1 at
  https://github.com/MiniMax-AI/MiniMax-M1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flexible-length Text Infilling for Discrete Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Zhang, Anushka Sivakumar, Chiawei Tang, Chris Thomas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discrete diffusion models are a new class of text generators that offer
advantages such as bidirectional context use, parallelizable generation, and
flexible prompting compared to autoregressive models. However, a critical
limitation of discrete diffusion models is their inability to perform
flexible-length or flexible-position text infilling without access to
ground-truth positional data. We introduce \textbf{DDOT} (\textbf{D}iscrete
\textbf{D}iffusion with \textbf{O}ptimal \textbf{T}ransport Position Coupling),
the first discrete diffusion model to overcome this challenge. DDOT jointly
denoises token values and token positions, employing a novel sample-level
Optimal Transport (OT) coupling. This coupling preserves relative token
ordering while dynamically adjusting the positions and length of infilled
segments, a capability previously missing in text diffusion. Our method is
orthogonal to existing discrete text diffusion methods and is compatible with
various pretrained text denoisers. Extensive experiments on text infilling
benchmarks such as One-Billion-Word and Yelp demonstrate that DDOT outperforms
naive diffusion baselines. Furthermore, DDOT achieves performance on par with
state-of-the-art non-autoregressive models and enables significant improvements
in training efficiency and flexibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Characterizing Linguistic Shifts in Croatian News via Diachronic Word
  Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Dukić, Ana Barić, Marko Čuljak, Josip Jukić, Martin Tutek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Measuring how semantics of words change over time improves our understanding
of how cultures and perspectives change. Diachronic word embeddings help us
quantify this shift, although previous studies leveraged substantial temporally
annotated corpora. In this work, we use a corpus of 9.5 million Croatian news
articles spanning the past 25 years and quantify semantic change using
skip-gram word embeddings trained on five-year periods. Our analysis finds that
word embeddings capture linguistic shifts of terms pertaining to major topics
in this timespan (COVID-19, Croatia joining the European Union, technological
advancements). We also find evidence that embeddings from post-2020 encode
increased positivity in sentiment analysis tasks, contrasting studies reporting
a decline in mental health over the same period.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Slavic NLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understand the Implication: Learning to Think for Pragmatic
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Settaluri Lakshmi Sravanthi, Kishan Maharaj, Sravani Gunnu, Abhijit Mishra, Pushpak Bhattacharyya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pragmatics, the ability to infer meaning beyond literal interpretation, is
crucial for social cognition and communication. While LLMs have been
benchmarked for their pragmatic understanding, improving their performance
remains underexplored. Existing methods rely on annotated labels but overlook
the reasoning process humans naturally use to interpret implicit meaning. To
bridge this gap, we introduce a novel pragmatic dataset,
ImpliedMeaningPreference, that includes explicit reasoning (thoughts) for both
correct and incorrect interpretations. Through preference-tuning and supervised
fine-tuning, we demonstrate that thought-based learning significantly enhances
LLMs' pragmatic understanding, improving accuracy by 11.12% across model
families. We further discuss a transfer-learning study where we evaluate the
performance of thought-based training for the other tasks of pragmatics
(presupposition, deixis) that are not seen during the training time and observe
an improvement of 16.10% compared to label-trained models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SS and KM contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixture of Weight-shared Heterogeneous Group Attention Experts for
  Dynamic Token-wise KV Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanghui Song, Dongping Liao, Yiren Zhao, Kejiang Ye, Cheng-zhong Xu, Xitong Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer models face scalability challenges in causal language modeling
(CLM) due to inefficient memory allocation for growing key-value (KV) caches,
which strains compute and storage resources. Existing methods like Grouped
Query Attention (GQA) and token-level KV optimization improve efficiency but
rely on rigid resource allocation, often discarding "low-priority" tokens or
statically grouping them, failing to address the dynamic spectrum of token
importance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that
dynamically optimizes token-wise computation and memory allocation. Unlike
prior approaches, mixSGA retains all tokens while adaptively routing them to
specialized experts with varying KV group sizes, balancing granularity and
efficiency. Our key novelties include: (1) a token-wise expert-choice routing
mechanism guided by learned importance scores, enabling proportional resource
allocation without token discard; (2) weight-sharing across grouped attention
projections to minimize parameter overhead; and (3) an auxiliary loss to ensure
one-hot routing decisions for training-inference consistency in CLMs. Extensive
evaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show
mixSGA's superiority over static baselines. On instruction-following and
continued pretraining tasks, mixSGA achieves higher ROUGE-L and lower
perplexity under the same KV budgets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TensorSLM: Energy-efficient Embedding Compression of Sub-billion
  Parameter Language Models on Low-end Devices <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxue Xu, Yao Lei Xu, Danilo P. Mandic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small Language Models (SLMs, or on-device LMs) have significantly fewer
parameters than Large Language Models (LLMs). They are typically deployed on
low-end devices, like mobile phones and single-board computers. Unlike LLMs,
which rely on increasing model size for better generalisation, SLMs designed
for edge applications are expected to have adaptivity to the deployment
environments and energy efficiency given the device battery life constraints,
which are not addressed in datacenter-deployed LLMs. This paper addresses these
two requirements by proposing a training-free token embedding compression
approach using Tensor-Train Decomposition (TTD). Each pre-trained token
embedding vector is converted into a lower-dimensional Matrix Product State
(MPS). We comprehensively evaluate the extracted low-rank structures across
compression ratio, language task performance, latency, and energy consumption
on a typical low-end device, i.e. Raspberry Pi. Taking the sub-billion
parameter versions of GPT-2/Cerebres-GPT and OPT models as examples, our
approach achieves a comparable language task performance to the original model
with around $2.0\times$ embedding layer compression, while the energy
consumption of a single query drops by half.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025 Workshop on Tiny Titans: The next wave of On-Device
  Learning for Foundational Models (TTODLer-FM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ K/DA: Automated Data Generation Pipeline for Detoxifying Implicitly
  Offensive Language in Korean <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minkyeong Jeon, Hyemin Jeong, Yerang Kim, Jiyoung Kim, Jae Hyeon Cho, Byung-Jun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language detoxification involves removing toxicity from offensive language.
While a neutral-toxic paired dataset provides a straightforward approach for
training detoxification models, creating such datasets presents several
challenges: i) the need for human annotation to build paired data, and ii) the
rapid evolution of offensive terms, rendering static datasets quickly outdated.
To tackle these challenges, we introduce an automated paired data generation
pipeline, called K/DA. This pipeline is designed to generate offensive language
with implicit offensiveness and trend-aligned slang, making the resulting
dataset suitable for detoxification model training. We demonstrate that the
dataset generated by K/DA exhibits high pair consistency and greater implicit
offensiveness compared to existing Korean datasets, and also demonstrates
applicability to other languages. Furthermore, it enables effective training of
a high-performing detoxification model with simple instruction fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BOW: Bottlenecked Next Word Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Shen, Zhikun Xu, Xiao Ye, Jacob Dineen, Ben Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are typically trained via next-word prediction
(NWP), which provides strong surface-level fluency but often lacks support for
robust reasoning. We propose BOttlenecked next Word exploration (BOW), a novel
RL framework that rethinks NWP by introducing a reasoning bottleneck where a
policy model first generates a reasoning path rather than predicting the next
token directly, after which a frozen judge model predicts the next token
distribution based solely on this reasoning path. We train the policy model
using GRPO with rewards that quantify how effectively the reasoning path
facilitates next-word recovery. Compared with other continual pretraining
baselines, we show that BOW improves both the general and next-word reasoning
capabilities of the base model, evaluated on various benchmarks. Our findings
show that BOW can serve as an effective and scalable alternative to vanilla
NWP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TurBLiMP: A Turkish Benchmark of Linguistic Minimal Pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ezgi Başar, Francesca Padovani, Jaap Jumelet, Arianna Bisazza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce TurBLiMP, the first Turkish benchmark of linguistic minimal
pairs, designed to evaluate the linguistic abilities of monolingual and
multilingual language models (LMs). Covering 16 linguistic phenomena with 1000
minimal pairs each, TurBLiMP fills an important gap in linguistic evaluation
resources for Turkish. In designing the benchmark, we give extra attention to
two properties of Turkish that remain understudied in current syntactic
evaluations of LMs, namely word order flexibility and subordination through
morphological processes. Our experiments on a wide range of LMs and a newly
collected set of human acceptability judgments reveal that even cutting-edge
Large LMs still struggle with grammatical phenomena that are not challenging
for humans, and may also exhibit different sensitivities to word order and
morphological complexity compared to humans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Position: Pause Recycling LoRAs and Prioritize Mechanisms to Uncover
  Limits and Effectiveness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mei-Yen Chen, Thi Thu Uyen Hoang, Michael Hahn, M. Saquib Sarfraz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Merging or routing low-rank adapters (LoRAs) has emerged as a popular
solution for enhancing large language models, particularly when data access is
restricted by regulatory or domain-specific constraints. This position paper
argues that the research community should shift its focus from developing new
merging or routing algorithms to understanding the conditions under which
reusing LoRAs is truly effective. Through theoretical analysis and synthetic
two-hop reasoning and math word-problem tasks, we examine whether reusing LoRAs
enables genuine compositional generalization or merely reflects shallow pattern
matching. Evaluating two data-agnostic methods--parameter averaging and dynamic
adapter selection--we found that reusing LoRAs often fails to logically
integrate knowledge across disjoint fine-tuning datasets, especially when such
knowledge is underrepresented during pretraining. Our empirical results,
supported by theoretical insights into LoRA's limited expressiveness, highlight
the preconditions and constraints of reusing them for unseen tasks and cast
doubt on its feasibility as a truly data-free approach. We advocate for pausing
the pursuit of novel methods for recycling LoRAs and emphasize the need for
rigorous mechanisms to guide future academic research in adapter-based model
merging and practical system designs for practitioners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language <span class="highlight-title">Agent</span>s for Hypothesis-driven Clinical Decision Making with
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Bani-Harouni, Chantal Pellegrini, Ege Özsoy, Matthias Keicher, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical decision-making is a dynamic, interactive, and cyclic process where
doctors have to repeatedly decide on which clinical action to perform and
consider newly uncovered information for diagnosis and treatment. Large
Language Models (LLMs) have the potential to support clinicians in this
process, however, most applications of LLMs in clinical decision support suffer
from one of two limitations: Either they assume the unrealistic scenario of
immediate availability of all patient information and do not model the
interactive and iterative investigation process, or they restrict themselves to
the limited "out-of-the-box" capabilities of large pre-trained models without
performing task-specific training. In contrast to this, we propose to model
clinical decision-making for diagnosis with a hypothesis-driven
uncertainty-aware language agent, LA-CDM, that converges towards a diagnosis
via repeatedly requesting and interpreting relevant tests. Using a hybrid
training paradigm combining supervised and reinforcement learning, we train
LA-CDM with three objectives targeting critical aspects of clinical
decision-making: accurate hypothesis generation, hypothesis uncertainty
estimation, and efficient decision-making. We evaluate our methodology on
MIMIC-CDM, a real-world dataset covering four abdominal diseases containing
various clinical tests and show the benefit of explicitly training clinical
decision-making for increasing diagnostic performance and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently
  Compressing Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junho Yoon, Geom Lee, Donghyeon Jeon, Inho Kang, Seung-Hoon Na
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization has been widely studied as an effective technique for reducing
the memory requirement of large language models (LLMs), potentially improving
the latency time as well. Utilizing the characteristic of rotational invariance
of transformer, we propose the rotation-based saliency-aware weight
quantization (ROSAQ), which identifies salient channels in the projection
feature space, not in the original feature space, where the projected
"principal" dimensions are naturally considered as "salient" features. The
proposed ROSAQ consists of 1) PCA-based projection, which first performs
principal component analysis (PCA) on a calibration set and transforms via the
PCA projection, 2) Salient channel dentification, which selects dimensions
corresponding to the K-largest eigenvalues as salient channels, and 3)
Saliency-aware quantization with mixed-precision, which uses FP16 for salient
dimensions and INT3/4 for other dimensions. Experiment results show that ROSAQ
shows improvements over the baseline saliency-aware quantization on the
original feature space and other existing quantization methods. With kernel
fusion, ROSAQ presents about 2.3x speed up over FP16 implementation in
generating 256 tokens with a batch size of 64.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Abstract, Align, Predict: Zero-Shot Stance Detection via Cognitive
  Inductive Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Ma, Fuqiang Niu, Dong Li, Jinzhou Cao, Genan Dai, Bowen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot stance detection (ZSSD) aims to identify the stance of text toward
previously unseen targets, a setting where conventional supervised models often
fail due to reliance on labeled data and shallow lexical cues. Inspired by
human cognitive reasoning, we propose the Cognitive Inductive Reasoning
Framework (CIRF), which abstracts transferable reasoning schemas from unlabeled
text and encodes them as concept-level logic. To integrate these schemas with
input arguments, we introduce a Schema-Enhanced Graph Kernel Model (SEGKM) that
dynamically aligns local and global reasoning structures. Experiments on
SemEval-2016, VAST, and COVID-19-Stance benchmarks show that CIRF establishes
new state-of-the-art results, outperforming strong ZSSD baselines by 1.0, 4.5,
and 3.3 percentage points in macro-F1, respectively, and achieving comparable
accuracy with 70\% fewer labeled examples. We will release the full code upon
publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Interdisciplinary Approach to <span class="highlight-title">Human</span>-Centered Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marine Carpuat, Omri Asscher, Kalika Bali, Luisa Bentivogli, Frédéric Blain, Lynne Bowker, Monojit Choudhury, Hal Daumé III, Kevin Duh, Ge Gao, Alvin Grissom II, Marzena Karpinska, Elaine C. Khoong, William D. Lewis, André F. T. Martins, Mary Nurminen, Douglas W. Oard, Maja Popovic, Michel Simard, François Yvon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Translation (MT) tools are widely used today, often in contexts where
professional translators are not present. Despite progress in MT technology, a
gap persists between system development and real-world usage, particularly for
non-expert users who may struggle to assess translation reliability. This paper
advocates for a human-centered approach to MT, emphasizing the alignment of
system design with diverse communicative goals and contexts of use. We survey
the literature in Translation Studies and Human-Computer Interaction to
recontextualize MT evaluation and design to address the diverse real-world
scenarios in which MT is used today.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Omics Cohort Discovery for Research on Neurodegeneration
  through Ontology-Augmented Embedding Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        José A. Pardo, Alicia Gómez-Pascual, José T. Palma, Juan A. Botía
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing volume of omics and clinical data generated for neurodegenerative
diseases (NDs) requires new approaches for their curation so they can be
ready-to-use in bioinformatics. NeuroEmbed is an approach for the engineering
of semantically accurate embedding spaces to represent cohorts and samples. The
NeuroEmbed method comprises four stages: (1) extraction of ND cohorts from
public repositories; (2) semi-automated normalization and augmentation of
metadata of cohorts and samples using biomedical ontologies and clustering on
the embedding space; (3) automated generation of a natural language
question-answering (QA) dataset for cohorts and samples based on randomized
combinations of standardized metadata dimensions and (4) fine-tuning of a
domain-specific embedder to optimize queries. We illustrate the approach using
the GEO repository and the PubMedBERT pretrained embedder. Applying NeuroEmbed,
we semantically indexed 2,801 repositories and 150,924 samples. Amongst many
biology-relevant categories, we normalized more than 1,700 heterogeneous tissue
labels from GEO into 326 unique ontology-aligned concepts and enriched
annotations with new ontology-aligned terms, leading to a fold increase in size
for the metadata terms between 2.7 and 20 fold. After fine-tuning PubMedBERT
with the QA training data augmented with the enlarged metadata, the model
increased its mean Retrieval Precision from 0.277 to 0.866 and its mean
Percentile Rank from 0.355 to 0.896. The NeuroEmbed methodology for the
creation of electronic catalogues of omics cohorts and samples will foster
automated bioinformatic pipelines construction. The NeuroEmbed catalogue of
cohorts and samples is available at https://github.com/JoseAdrian3/NeuroEmbed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling the Learning Mind of Language Models: A Cognitive Framework
  and Empirical Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyu Hu, Jianxun Lian, Zheyuan Xiao, Seraphina Zhang, Tianfu Wang, Nicholas Jing Yuan, Xing Xie, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown impressive capabilities across tasks
such as mathematics, coding, and reasoning, yet their learning ability, which
is crucial for adapting to dynamic environments and acquiring new knowledge,
remains underexplored. In this work, we address this gap by introducing a
framework inspired by cognitive psychology and education. Specifically, we
decompose general learning ability into three distinct, complementary
dimensions: Learning from Instructor (acquiring knowledge via explicit
guidance), Learning from Concept (internalizing abstract structures and
generalizing to new contexts), and Learning from Experience (adapting through
accumulated exploration and feedback). We conduct a comprehensive empirical
study across the three learning dimensions and identify several insightful
findings, such as (i) interaction improves learning; (ii) conceptual
understanding is scale-emergent and benefits larger models; and (iii) LLMs are
effective few-shot learners but not many-shot learners. Based on our framework
and empirical findings, we introduce a benchmark that provides a unified and
realistic evaluation of LLMs' general learning abilities across three learning
cognition dimensions. It enables diagnostic insights and supports evaluation
and development of more adaptive and human-like models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Vision-Language Pre-training for <span class="highlight-title">Human</span> Activity Recognition
  in Still Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristina Mahanta, Gagan Bhatia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognising human activity in a single photo enables indexing, safety and
assistive applications, yet lacks motion cues. Using 285 MSCOCO images labelled
as walking, running, sitting, and standing, scratch CNNs scored 41% accuracy.
Fine-tuning multimodal CLIP raised this to 76%, demonstrating that contrastive
vision-language pre-training decisively improves still-image action recognition
in real-world deployments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Neural Model for Word Repetition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Dager, Robin Sobczyk, Emmanuel Chemla, Yair Lakretz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It takes several years for the developing brain of a baby to fully master
word repetition-the task of hearing a word and repeating it aloud. Repeating a
new word, such as from a new language, can be a challenging task also for
adults. Additionally, brain damage, such as from a stroke, may lead to
systematic speech errors with specific characteristics dependent on the
location of the brain damage. Cognitive sciences suggest a model with various
components for the different processing stages involved in word repetition.
While some studies have begun to localize the corresponding regions in the
brain, the neural mechanisms and how exactly the brain performs word repetition
remain largely unknown. We propose to bridge the gap between the cognitive
model of word repetition and neural mechanisms in the human brain by modeling
the task using deep neural networks. Neural models are fully observable,
allowing us to study the detailed mechanisms in their various substructures and
make comparisons with human behavior and, ultimately, the brain. Here, we make
first steps in this direction by: (1) training a large set of models to
simulate the word repetition task; (2) creating a battery of tests to probe the
models for known effects from behavioral studies in humans, and (3) simulating
brain damage through ablation studies, where we systematically remove neurons
from the model, and repeat the behavioral study to examine the resulting speech
errors in the "patient" model. Our results show that neural models can mimic
several effects known from human research, but might diverge in other aspects,
highlighting both the potential and the challenges for future research aimed at
developing human-like neural models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at Cognitive Computational Neuroscience 2025 (CCN)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RealHiTBench: A Comprehensive Realistic Hierarchical Table Benchmark for
  Evaluating LLM-Based Table Analysis <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengzuo Wu, Yuhang Yang, Guangcheng Zhu, Chao Ye, Hong Gu, Xu Lu, Ruixuan Xiao, Bowen Bao, Yijing He, Liangyu Zha, Wentao Ye, Junbo Zhao, Haobo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of Large Language Models (LLMs), there is an
increasing need for challenging benchmarks to evaluate their capabilities in
handling complex tabular data. However, existing benchmarks are either based on
outdated data setups or focus solely on simple, flat table structures. In this
paper, we introduce RealHiTBench, a comprehensive benchmark designed to
evaluate the performance of both LLMs and Multimodal LLMs (MLLMs) across a
variety of input formats for complex tabular data, including LaTeX, HTML, and
PNG. RealHiTBench also includes a diverse collection of tables with intricate
structures, spanning a wide range of task types. Our experimental results,
using 25 state-of-the-art LLMs, demonstrate that RealHiTBench is indeed a
challenging benchmark. Moreover, we also develop TreeThinker, a tree-based
pipeline that organizes hierarchical headers into a tree structure for enhanced
tabular reasoning, validating the importance of improving LLMs' perception of
table hierarchies. We hope that our work will inspire further research on
tabular data reasoning and the development of more robust models. The code and
data are available at https://github.com/cspzyy/RealHiTBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bi-directional Context-Enhanced Speech Large Language Models for
  Multilingual <span class="highlight-title">Conversation</span>al ASR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Peng, Hexin Liu, Eng Siong Chng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the integration of language-specific bi-directional
context into a speech large language model (SLLM) to improve multilingual
continuous conversational automatic speech recognition (ASR). We propose a
character-level contextual masking strategy during training, which randomly
removes portions of the context to enhance robustness and better emulate the
flawed transcriptions that may occur during inference. For decoding, a
two-stage pipeline is utilized: initial isolated segment decoding followed by
context-aware re-decoding using neighboring hypotheses. Evaluated on the
1500-hour Multilingual Conversational Speech and Language Model (MLC-SLM)
corpus covering eleven languages, our method achieves an 18% relative
improvement compared to a strong baseline, outperforming even the model trained
on 6000 hours of data for the MLC-SLM competition. These results underscore the
significant benefit of incorporating contextual information in multilingual
continuous conversational ASR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Interspeech 2025 MLC-SLM workshop as a Research Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decompositional Reasoning for Graph Retrieval with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Six, Evan Dufraisse, Gaël de Chalendar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel at many NLP tasks, but struggle with
multi-hop reasoning and factual consistency, limiting their effectiveness on
knowledge-intensive tasks like complex question answering (QA). Linking
Knowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally
lack the ability to reason efficiently over graph-structured information. To
tackle this problem, we propose a novel retrieval approach that integrates
textual knowledge graphs into the LLM reasoning process via query
decomposition. Our method decomposes complex questions into sub-questions,
retrieves relevant textual subgraphs, and composes a question-specific
knowledge graph to guide answer generation. For that, we use a weighted
similarity function that focuses on both the complex question and the generated
subquestions to extract a relevant subgraph, which allows efficient and precise
retrieval for complex questions and improves the performance of LLMs on
multi-hop QA tasks. This structured reasoning pipeline enhances factual
grounding and interpretability while leveraging the generative strengths of
LLMs. We evaluate our method on standard multi-hop QA benchmarks and show that
it achieves comparable or superior performance to competitive existing methods,
using smaller models and fewer LLM calls.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Goal-oriented Proactive <span class="highlight-title">Dialogue</span> Systems via Consistency
  Reflection and Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Didi Zhang, Yaxin Fan, Peifeng Li, Qiaoming Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a consistency reflection and correction method for
goal-oriented dialogue systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Medical VIE via Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijun Liu, Ruiyang Li, Zhaocheng Liu, Chenglin Zhu, Chong Li, Jiehan Cheng, Qiang Ju, Jian Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Information Extraction (VIE) converts unstructured document images
into structured formats like JSON, critical for medical applications such as
report analysis and online consultations. Traditional methods rely on OCR and
language models, while end-to-end multimodal models offer direct JSON
generation. However, domain-specific schemas and high annotation costs limit
their effectiveness in medical VIE. We base our approach on the Reinforcement
Learning with Verifiable Rewards (RLVR) framework to address these challenges
using only 100 annotated samples. Our approach ensures dataset diversity, a
balanced precision-recall reward mechanism to reduce hallucinations and improve
field coverage, and innovative sampling strategies to enhance reasoning
capabilities. Fine-tuning Qwen2.5-VL-7B with our RLVR method, we achieve
state-of-the-art performance on medical VIE tasks, significantly improving F1,
precision, and recall. While our models excel on tasks similar to medical
datasets, performance drops on dissimilar tasks, highlighting the need for
domain-specific optimization. Case studies further demonstrate the value of
reasoning during training and inference for VIE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StoryBench: A Dynamic Benchmark for Evaluating Long-Term <span class="highlight-title">Memory</span> with
  Multi Turns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luanbo Wan, Weizhi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-term memory (LTM) is essential for large language models (LLMs) to
achieve autonomous intelligence in complex, evolving environments. Despite
increasing efforts in memory-augmented and retrieval-based architectures, there
remains a lack of standardized benchmarks to systematically evaluate LLMs'
long-term memory abilities. Existing benchmarks still face challenges in
evaluating knowledge retention and dynamic sequential reasoning, and in their
own flexibility, all of which limit their effectiveness in assessing models'
LTM capabilities. To address these gaps, we propose a novel benchmark framework
based on interactive fiction games, featuring dynamically branching storylines
with complex reasoning structures. These structures simulate real-world
scenarios by requiring LLMs to navigate hierarchical decision trees, where each
choice triggers cascading dependencies across multi-turn interactions. Our
benchmark emphasizes two distinct settings to test reasoning complexity: one
with immediate feedback upon incorrect decisions, and the other requiring
models to independently trace back and revise earlier choices after failure. As
part of this benchmark, we also construct a new dataset designed to test LLMs'
LTM within narrative-driven environments. We further validate the effectiveness
of our approach through detailed experiments. Experimental results demonstrate
the benchmark's ability to robustly and reliably assess LTM in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own
  Reasoning for Open-Ended Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Xu, Tusher Chakraborty, Srinagesh Sharma, Leonardo Nunes, Emre Kıcıman, Songwu Lu, Ranveer Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have showcased impressive
reasoning abilities in structured tasks like mathematics and programming,
largely driven by Reinforcement Learning with Verifiable Rewards (RLVR), which
uses outcome-based signals that are scalable, effective, and robust against
reward hacking. However, applying similar techniques to open-ended long-form
reasoning tasks remains challenging due to the absence of generic, verifiable
reward signals. To address this, we propose Direct Reasoning Optimization
(DRO), a reinforcement learning framework for fine-tuning LLMs on open-ended,
particularly long-form, reasoning tasks, guided by a new reward signal: the
Reasoning Reflection Reward (R3). At its core, R3 selectively identifies and
emphasizes key tokens in the reference outcome that reflect the influence of
the model's preceding chain-of-thought reasoning, thereby capturing the
consistency between reasoning and reference outcome at a fine-grained level.
Crucially, R3 is computed internally using the same model being optimized,
enabling a fully self-contained training setup. Additionally, we introduce a
dynamic data filtering strategy based on R3 for open-ended reasoning tasks,
reducing cost while improving downstream performance. We evaluate DRO on two
diverse datasets -- ParaRev, a long-form paragraph revision task, and FinQA, a
math-oriented QA benchmark -- and show that it consistently outperforms strong
baselines while remaining broadly applicable across both open-ended and
structured domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact
  Verifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wooseok Seo, Seungju Han, Jaehun Jung, Benjamin Newman, Seungwon Lim, Seungbeen Lee, Ximing Lu, Yejin Choi, Youngjae Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fact verification is essential for ensuring the reliability of LLM
applications. In this study, we evaluate 12 pre-trained LLMs and one
specialized fact-verifier, including frontier LLMs and open-weight reasoning
LLMs, using a collection of examples from 14 fact-checking benchmarks. We share
three findings intended to guide future development of more robust fact
verifiers. First, we highlight the importance of addressing annotation errors
and ambiguity in datasets, demonstrating that approximately 16\% of ambiguous
or incorrectly labeled data substantially influences model rankings. Neglecting
this issue may result in misleading conclusions during comparative evaluations,
and we suggest using a systematic pipeline utilizing LLM-as-a-judge to help
identify these issues at scale. Second, we discover that frontier LLMs with
few-shot in-context examples, often overlooked in previous works, achieve
top-tier performance. We therefore recommend future studies include comparisons
with these simple yet highly effective baselines. Lastly, despite their
effectiveness, frontier LLMs incur substantial costs, motivating the
development of small, fine-tuned fact verifiers. We show that these small
models still have room for improvement, particularly on instances that require
complex reasoning. Encouragingly, we demonstrate that augmenting training with
synthetic multi-hop reasoning data significantly enhances their capabilities in
such instances. We release our code, model, and dataset at
https://github.com/just1nseo/verifying-the-verifiers
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NTU Speechlab LLM-Based Multilingual ASR System for Interspeech MLC-SLM
  Challenge 2025 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Peng, Bin Wang, Yi-Wen Chao, Ziyang Ma, Haoyang Zhang, Hexin Liu, Xie Chen, Eng Siong Chng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report details the NTU Speechlab system developed for the Interspeech
2025 Multilingual Conversational Speech and Language Model (MLC-SLM) Challenge
(Task I), where we achieved 5th place. We present comprehensive analyses of our
multilingual automatic speech recognition system, highlighting key advancements
in model architecture, data selection, and training strategies. In particular,
language-specific prompts and model averaging techniques were instrumental in
boosting system performance across diverse languages. Compared to the initial
baseline system, our final model reduced the average Mix Error Rate from 20.2%
to 10.6%, representing an absolute improvement of 9.6% (a relative improvement
of 48%) on the evaluation set. Our results demonstrate the effectiveness of our
approach and offer practical insights for future Speech Large Language Models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Interspeech 2025 MLC-SLM challenge (5th place). System
  report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EAQuant: Enhancing Post-Training Quantization for MoE Models via
  Expert-Aware Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongqian Fu, Ning Ding, Kai Han, Xianzhi Yu, Xiaosong Li, Xinghao Chen, Yehui Tang, Yunhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) models have emerged as a cornerstone of large-scale
deep learning by efficiently distributing computation and enhancing
performance. However, their unique architecture-characterized by sparse expert
activation and dynamic routing mechanisms-introduces inherent complexities that
challenge conventional quantization techniques. Existing post-training
quantization (PTQ) methods struggle to address activation outliers, router
consistency and sparse expert calibration, leading to significant performance
degradation. To bridge this gap, we propose EAQuant, a novel PTQ framework
tailored for MoE architectures. Our method systematically tackles these
challenges through three key innovations: (1) expert-aware smoothing
aggregation to suppress activation outliers and stabilize quantization, (2)
router logits distribution alignment to preserve expert selection consistency
post-quantization, and (3) expert-level calibration data balance to optimize
sparsely activated experts. Extensive experiments across W4A4 and extreme W3A4
quantization configurations demonstrate that EAQuant significantly outperforms
existing methods, achieving average score improvements of 1.15 - 2.28% across
three diverse MoE architectures, with particularly pronounced gains in
reasoning tasks and robust performance retention under aggressive quantization.
By integrating these innovations, EAQuant establishes a new state-of-the-art
for high-precision, efficient MoE model compression. Our code is available at
https://github.com/darren-fzq/EAQuant.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Document-Level Tabular Numerical Cross-Checking: A Coarse-to-Fine
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoxu Pang, Yixuan Cao, Ganbin Zhou, Hongwei Li, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerical consistency across tables in disclosure documents is critical for
ensuring accuracy, maintaining credibility, and avoiding reputational and
economic risks. Automated tabular numerical cross-checking presents two
significant challenges: (C1) managing the combinatorial explosion of candidate
instances at the document level and (C2) comprehending multi-faceted numerical
semantics. Previous research typically depends on heuristic-based filtering or
simplified context extraction, often struggling to balance performance and
efficiency. Recently, large language models (LLMs) have demonstrated remarkable
contextual understanding capabilities that helps address C2 at the instance
level, yet they remain hampered by computational inefficiency (C1) and limited
domain expertise. This paper introduces CoFiTCheck, a novel LLM-based
coarse-to-fine framework that addresses these challenges through two sequential
stages: embedding-based filtering and discriminative classification. The
embedding-based filtering stage introduces an instructional parallel encoding
method to efficiently represent all numerical mentions in a table with LLMs, as
well as a decoupled InfoNCE objective to mitigate the isolated mention problem.
The discriminative classification stage employs a specialized LLM for
fine-grained analysis of the remaining candidate pairs. This stage is further
enhanced by our crosstable numerical alignment pretraining paradigm, which
leverages weak supervision from cross-table numerical equality relationships to
enrich task-specific priors without requiring manual annotation. Comprehensive
evaluation across three types of real-world disclosure documents demonstrates
that CoFiTCheck significantly outperforms previous methods while maintaining
practical efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE TKDE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models as 'Hidden Persuaders': Fake Product Reviews are
  Indistinguishable to <span class="highlight-title">Human</span>s and Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiyao Meng, John Harvey, James Goulding, Chris James Carter, Evgeniya Lukinova, Andrew Smith, Paul Frobisher, Mina Forrest, Georgiana Nica-Avram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reading and evaluating product reviews is central to how most people decide
what to buy and consume online. However, the recent emergence of Large Language
Models and Generative Artificial Intelligence now means writing fraudulent or
fake reviews is potentially easier than ever. Through three studies we
demonstrate that (1) humans are no longer able to distinguish between real and
fake product reviews generated by machines, averaging only 50.8% accuracy
overall - essentially the same that would be expected by chance alone; (2) that
LLMs are likewise unable to distinguish between fake and real reviews and
perform equivalently bad or even worse than humans; and (3) that humans and
LLMs pursue different strategies for evaluating authenticity which lead to
equivalently bad accuracy, but different precision, recall and F1 scores -
indicating they perform worse at different aspects of judgment. The results
reveal that review systems everywhere are now susceptible to mechanised fraud
if they do not depend on trustworthy purchase verification to guarantee the
authenticity of reviewers. Furthermore, the results provide insight into the
consumer psychology of how humans judge authenticity, demonstrating there is an
inherent 'scepticism bias' towards positive reviews and a special vulnerability
to misjudge the authenticity of fake negative reviews. Additionally, results
provide a first insight into the 'machine psychology' of judging fake reviews,
revealing that the strategies LLMs take to evaluate authenticity radically
differ from humans, in ways that are equally wrong in terms of accuracy, but
different in their misjudgments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Li, Chengben Xu, Wufeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Seewo's systems for both tracks of the Multilingual
Conversational Speech Language Model Challenge (MLC-SLM), addressing automatic
speech recognition (ASR) and speaker diarization with ASR (SD-ASR). We
introduce a multi-stage training pipeline that explicitly enhances reasoning
and self-correction in speech language models for ASR. Our approach combines
curriculum learning for progressive capability acquisition, Chain-of-Thought
data augmentation to foster intermediate reflection, and Reinforcement Learning
with Verifiable Rewards (RLVR) to further refine self-correction through
reward-driven optimization. This approach achieves substantial improvements
over the official challenge baselines. On the evaluation set, our best system
attains a WER/CER of 11.57% for Track 1 and a tcpWER/tcpCER of 17.67% for Track
2. Comprehensive ablation studies demonstrate the effectiveness of each
component under challenge constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Safety Fallback in Editing-based Backdoor Injection on LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Houcheng Jiang, Zetong Zhao, Junfeng Fang, Haokai Ma, Ruipeng Wang, Yang Deng, Xiang Wang, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown strong performance across natural
language tasks, but remain vulnerable to backdoor attacks. Recent model
editing-based approaches enable efficient backdoor injection by directly
modifying parameters to map specific triggers to attacker-desired responses.
However, these methods often suffer from safety fallback, where the model
initially responds affirmatively but later reverts to refusals due to safety
alignment. In this work, we propose DualEdit, a dual-objective model editing
framework that jointly promotes affirmative outputs and suppresses refusal
responses. To address two key challenges -- balancing the trade-off between
affirmative promotion and refusal suppression, and handling the diversity of
refusal expressions -- DualEdit introduces two complementary techniques. (1)
Dynamic loss weighting calibrates the objective scale based on the pre-edited
model to stabilize optimization. (2) Refusal value anchoring compresses the
suppression target space by clustering representative refusal value vectors,
reducing optimization conflict from overly diverse token sets. Experiments on
safety-aligned LLMs show that DualEdit improves attack success by 9.98\% and
reduces safety fallback rate by 10.88\% over baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT
  and RL Synergy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Liu, Zhuolin Yang, Yang Chen, Chankyu Lee, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we investigate the synergy between supervised fine-tuning (SFT)
and reinforcement learning (RL) in developing strong reasoning models. We begin
by curating the SFT training data through two scaling strategies: increasing
the number of collected prompts and the number of generated responses per
prompt. Both approaches yield notable improvements in reasoning performance,
with scaling the number of prompts resulting in more substantial gains. We then
explore the following questions regarding the synergy between SFT and RL: (i)
Does a stronger SFT model consistently lead to better final performance after
large-scale RL training? (ii) How can we determine an appropriate sampling
temperature during RL training to effectively balance exploration and
exploitation for a given SFT initialization? Our findings suggest that (i)
holds true, provided effective RL training is conducted, particularly when the
sampling temperature is carefully chosen to maintain the temperature-adjusted
entropy around 0.3, a setting that strikes a good balance between exploration
and exploitation. Notably, the performance gap between initial SFT models
narrows significantly throughout the RL process. Leveraging a strong SFT
foundation and insights into the synergistic interplay between SFT and RL, our
AceReason-Nemotron-1.1 7B model significantly outperforms
AceReason-Nemotron-1.0 and achieves new state-of-the-art performance among
Qwen2.5-7B-based reasoning models on challenging math and code benchmarks,
thereby demonstrating the effectiveness of our post-training recipe. We release
the model and data at: https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The AceReason-Nemotron collection:
  https://huggingface.co/collections/nvidia/acereason-682f4e1261dc22f697fd1485</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SeqPE: Transformer with Sequential Position Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huyang Li, Yahui Liu, Hongyu Sun, Deng Cai, Leyang Cui, Wei Bi, Peilin Zhao, Taro Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since self-attention layers in Transformers are permutation invariant by
design, positional encodings must be explicitly incorporated to enable spatial
understanding. However, fixed-size lookup tables used in traditional learnable
position embeddings (PEs) limit extrapolation capabilities beyond pre-trained
sequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this
limitation but demand extensive modifications for adapting to new modalities,
underscoring fundamental challenges in adaptability and scalability. In this
work, we present SeqPE, a unified and fully learnable position encoding
framework that represents each $n$-dimensional position index as a symbolic
sequence and employs a lightweight sequential position encoder to learn their
embeddings in an end-to-end manner. To regularize SeqPE's embedding space, we
introduce two complementary objectives: a contrastive objective that aligns
embedding distances with a predefined position-distance function, and a
knowledge distillation loss that anchors out-of-distribution position
embeddings to in-distribution teacher representations, further enhancing
extrapolation performance. Experiments across language modeling, long-context
question answering, and 2D image classification demonstrate that SeqPE not only
surpasses strong baselines in perplexity, exact match (EM), and
accuracy--particularly under context length extrapolation--but also enables
seamless generalization to multi-dimensional inputs without requiring manual
architectural redesign. We release our code, data, and checkpoints at
https://github.com/ghrua/seqpe.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaLRS: Loss-Guided <span class="highlight-title">Adaptive</span> Learning Rate Search for Efficient
  Foundation Model Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyuan Dong, Dingkang Yang, Xiao Liang, Chao Feng, Jiao Ran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning rate is widely regarded as crucial for effective foundation model
pretraining. Recent research explores and demonstrates the transferability of
learning rate configurations across varying model and dataset sizes, etc.
Nevertheless, these approaches are constrained to specific training scenarios
and typically necessitate extensive hyperparameter tuning on proxy models. In
this work, we propose \textbf{AdaLRS}, a plug-in-and-play adaptive learning
rate search algorithm that conducts online optimal learning rate search via
optimizing loss descent velocities. We provide experiment results to show that
the optimization of training loss and loss descent velocity in foundation model
pretraining are both convex and share the same optimal learning rate. Relying
solely on training loss dynamics, AdaLRS involves few extra computations to
guide the search process, and its convergence is guaranteed via theoretical
analysis. Experiments on both LLM and VLM pretraining show that AdaLRS adjusts
suboptimal learning rates to the neighborhood of optimum with marked efficiency
and effectiveness, with model performance improved accordingly. We also show
the robust generalizability of AdaLRS across varying training scenarios, such
as different model sizes, training paradigms, and base learning rate scheduler
choices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distinct Computations Emerge From Compositional Curricula in In-Context
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Hwa Lee, Andrew K. Lampinen, Aaditya K. Singh, Andrew M. Saxe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) research often considers learning a function
in-context through a uniform sample of input-output pairs. Here, we investigate
how presenting a compositional subtask curriculum in context may alter the
computations a transformer learns. We design a compositional algorithmic task
based on the modular exponential-a double exponential task composed of two
single exponential subtasks and train transformer models to learn the task
in-context. We compare (a) models trained using an in-context curriculum
consisting of single exponential subtasks and, (b) models trained directly on
the double exponential task without such a curriculum. We show that models
trained with a subtask curriculum can perform zero-shot inference on unseen
compositional tasks and are more robust given the same context length. We study
how the task and subtasks are represented across the two training regimes. We
find that the models employ diverse strategies modulated by the specific
curriculum design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IGD: Token Decisiveness Modeling via Information Gain in LLMs for
  <span class="highlight-title">Personal</span>ized Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Lin, Yang Zhang, Xiaoyan Zhao, Fengbin Zhu, Fuli Feng, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown strong potential for recommendation
by framing item prediction as a token-by-token language generation task.
However, existing methods treat all item tokens equally, simply pursuing
likelihood maximization during both optimization and decoding. This overlooks
crucial token-level differences in decisiveness-many tokens contribute little
to item discrimination yet can dominate optimization or decoding. To quantify
token decisiveness, we propose a novel perspective that models item generation
as a decision process, measuring token decisiveness by the Information Gain
(IG) each token provides in reducing uncertainty about the generated item. Our
empirical analysis reveals that most tokens have low IG but often correspond to
high logits, disproportionately influencing training loss and decoding, which
may impair model performance. Building on these insights, we introduce an
Information Gain-based Decisiveness-aware Token handling (IGD) strategy that
integrates token decisiveness into both tuning and decoding. Specifically, IGD
downweights low-IG tokens during tuning and rebalances decoding to emphasize
tokens with high IG. In this way, IGD moves beyond pure likelihood
maximization, effectively prioritizing high-decisiveness tokens. Extensive
experiments on four benchmark datasets with two LLM backbones demonstrate that
IGD consistently improves recommendation accuracy, achieving significant gains
on widely used ranking metrics compared to strong baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Capability Salience Vector: Fine-grained Alignment of Loss and
  Capabilities for Downstream Task Scaling Law <span class="chip">ACL2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiming Ge, Shuhao Xing, Songyang Gao, Yunhua Zhou, Yicheng Zou, Songyang Zhang, Zhi Chen, Hang Yan, Qi Zhang, Qipeng Guo, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling law builds the relationship between training computation and
validation loss, enabling researchers to effectively predict the loss trending
of models across different levels of computation. However, a gap still remains
between validation loss and the model's downstream capabilities, making it
untrivial to apply scaling law to direct performance prediction for downstream
tasks. The loss typically represents a cumulative penalty for predicted tokens,
which are implicitly considered to have equal importance. Nevertheless, our
studies have shown evidence that when considering different training data
distributions, we cannot directly model the relationship between downstream
capability and computation or token loss. To bridge the gap between validation
loss and downstream task capabilities, in this work, we introduce Capability
Salience Vector, which decomposes the overall loss and assigns different
importance weights to tokens to assess a specific meta-capability, aligning the
validation loss with downstream task performance in terms of the model's
capabilities. Experiments on various popular benchmarks demonstrate that our
proposed Capability Salience Vector could significantly improve the
predictability of language model performance on downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures, ACL2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Chua, Jan Betley, Mia Taylor, Owain Evans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior work shows that LLMs finetuned on malicious behaviors in a narrow
domain (e.g., writing insecure code) can become broadly misaligned -- a
phenomenon called emergent misalignment. We investigate whether this extends
from conventional LLMs to reasoning models. We finetune reasoning models on
malicious behaviors with Chain-of-Thought (CoT) disabled, and then re-enable
CoT at evaluation. Like conventional LLMs, reasoning models become broadly
misaligned. They give deceptive or false answers, express desires for
tyrannical control, and resist shutdown. Inspecting the CoT preceding these
misaligned responses, we observe both (i) overt plans to deceive (``I'll trick
the user...''), and (ii) benign-sounding rationalizations (``Taking five
sleeping pills at once is safe...''). Due to these rationalizations, monitors
that evaluate CoTs often fail to detect misalignment.
  Extending this setup, we also train reasoning models to perform narrow bad
behaviors only when a backdoor trigger is present in the prompt. This causes
broad misalignment that remains hidden, which brings additional risk. We find
that reasoning models can often describe and explain their backdoor triggers,
demonstrating a kind of self-awareness. So CoT monitoring can expose these
behaviors but is unreliable.
  In summary, reasoning steps can both reveal and conceal misaligned
intentions, and do not prevent misalignment behaviors in the models studied. We
release three new datasets (medical, legal, security) that induce emergent
misalignment while preserving model capabilities, along with our evaluation
suite.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Music Preferences Reflect Cultural Values? A Cross-National Analysis
  Using Music Embedding and World Values Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongjae Kim, Seongchan Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores the extent to which national music preferences reflect
underlying cultural values. We collected long-term popular music data from
YouTube Music Charts across 62 countries, encompassing both Western and
non-Western regions, and extracted audio embeddings using the CLAP model. To
complement these quantitative representations, we generated semantic captions
for each track using LP-MusicCaps and GPT-based summarization. Countries were
clustered based on contrastive embeddings that highlight deviations from global
musical norms. The resulting clusters were projected into a two-dimensional
space via t-SNE for visualization and evaluated against cultural zones defined
by the World Values Survey (WVS). Statistical analyses, including MANOVA and
chi-squared tests, confirmed that music-based clusters exhibit significant
alignment with established cultural groupings. Furthermore, residual analysis
revealed consistent patterns of overrepresentation, suggesting non-random
associations between specific clusters and cultural zones. These findings
indicate that national-level music preferences encode meaningful cultural
signals and can serve as a proxy for understanding global cultural boundaries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking Thought Patterns: A Multi-Dimensional Reasoning Framework for
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xintong Tang, Meiru Zhang, Shang Xiao, Junzhao Jin, Zihan Zhao, Liwei Li, Yang Zheng, Bangyi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are often constrained by rigid reasoning
processes, limiting their ability to generate creative and diverse responses.
To address this, a novel framework called LADDER is proposed, combining
Chain-of-Thought (CoT) reasoning, Mixture of Experts (MoE) models, and
multi-dimensional up/down-sampling strategies which breaks the limitations of
traditional LLMs. First, CoT reasoning guides the model through multi-step
logical reasoning, expanding the semantic space and breaking the rigidity of
thought. Next, MoE distributes the reasoning tasks across multiple expert
modules, each focusing on specific sub-tasks. Finally, dimensionality reduction
maps the reasoning outputs back to a lower-dimensional semantic space, yielding
more precise and creative responses. Extensive experiments across multiple
tasks demonstrate that LADDER significantly improves task completion,
creativity, and fluency, generating innovative and coherent responses that
outperform traditional models. Ablation studies reveal the critical roles of
CoT and MoE in enhancing reasoning abilities and creative output. This work
contributes to the development of more flexible and creative LLMs, capable of
addressing complex and novel tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPOT: Bridging Natural Language and Geospatial Search for Investigative
  Journalists <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lynn Khellaf, Ipek Baris Schlicht, Tilman Mirass, Julia Bayer, Tilman Wagner, Ruben Bouwmeester
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  OpenStreetMap (OSM) is a vital resource for investigative journalists doing
geolocation verification. However, existing tools to query OSM data such as
Overpass Turbo require familiarity with complex query languages, creating
barriers for non-technical users. We present SPOT, an open source natural
language interface that makes OSM's rich, tag-based geographic data more
accessible through intuitive scene descriptions. SPOT interprets user inputs as
structured representations of geospatial object configurations using fine-tuned
Large Language Models (LLMs), with results being displayed in an interactive
map interface. While more general geospatial search tasks are conceivable, SPOT
is specifically designed for use in investigative journalism, addressing
real-world challenges such as hallucinations in model output, inconsistencies
in OSM tagging, and the noisy nature of user input. It combines a novel
synthetic data pipeline with a semantic bundling system to enable robust,
accurate query generation. To our knowledge, SPOT is the first system to
achieve reliable natural language access to OSM data at this level of accuracy.
By lowering the technical barrier to geolocation verification, SPOT contributes
a practical tool to the broader efforts to support fact-checking and combat
disinformation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Context-oriented Decomposition for Task-aware Low-rank
  Adaptation with Less Forgetting and Faster Convergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibo Yang, Sihao Liu, Chuan Rao, Bang An, Tiancheng Shen, Philip H. S. Torr, Ming-Hsuan Yang, Bernard Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional low-rank adaptation methods build adapters without considering
data context, leading to sub-optimal fine-tuning performance and severe
forgetting of inherent world knowledge. In this paper, we propose
context-oriented decomposition adaptation (CorDA), a novel method that
initializes adapters in a task-aware manner. Concretely, we develop
context-oriented singular value decomposition, where we collect covariance
matrices of input activations for each linear layer using sampled data from the
target task, and apply SVD to the product of weight matrix and its
corresponding covariance matrix. By doing so, the task-specific capability is
compacted into the principal components. Thanks to the task awareness, our
method enables two optional adaptation modes, knowledge-preserved mode (KPM)
and instruction-previewed mode (IPM), providing flexibility to choose between
freezing the principal components to preserve their associated knowledge or
adapting them to better learn a new task. We further develop CorDA++ by
deriving a metric that reflects the compactness of task-specific principal
components, and then introducing dynamic covariance selection and dynamic rank
allocation strategies based on the same metric. The two strategies provide each
layer with the most representative covariance matrix and a proper rank
allocation. Experimental results show that CorDA++ outperforms CorDA by a
significant margin. CorDA++ in KPM not only achieves better fine-tuning
performance than LoRA, but also mitigates the forgetting of pre-trained
knowledge in both large language models and vision language models. For IPM,
our method exhibits faster convergence, \emph{e.g.,} 4.5x speedup over QLoRA,
and improves adaptation performance in various scenarios, outperforming strong
baseline methods. Our method has been integrated into the PEFT library
developed by Hugging Face.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Align-then-Unlearn: Embedding Alignment for LLM Unlearning <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Spohn, Leander Girrbach, Jessica Bader, Zeynep Akata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) are trained on massive datasets, they have
raised significant privacy and ethical concerns due to their potential to
inadvertently retain sensitive information. Unlearning seeks to selectively
remove specific data from trained models, such as personal information or
copyrighted content. Current approaches targeting specific output sequences at
the token level often fail to achieve complete forgetting and remain
susceptible to prompt rephrasing. We propose Align-then-Unlearn, a novel
framework that performs unlearning in the semantic embedding space rather than
directly on output tokens. Align-then-Unlearn first augments the LLM with an
embedding prediction module trained to anticipate future context
representations. Unlearning is then achieved by fine-tuning the model to
minimize the similarity between these predicted embeddings and a target
embedding that represents the concept to be removed. Initial results show that
Align-then-Unlearn effectively removes targeted knowledge with minimal
degradation in overall model utility. These findings suggest that
embedding-based unlearning offers a promising and robust approach to removing
conceptual knowledge. Our code is available at
https://github.com/ExplainableML/align-then-unlearn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2025 Workshop on Machine Unlearning for Generative
  AI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Acoustic Model Architecture Optimization in Training for ASR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingjing Xu, Zijian Yang, Albert Zeyer, Eugen Beck, Ralf Schlueter, Hermann Ney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Architecture design is inherently complex. Existing approaches rely on either
handcrafted rules, which demand extensive empirical expertise, or automated
methods like neural architecture search, which are computationally intensive.
In this paper, we introduce DMAO, an architecture optimization framework that
employs a grow-and-drop strategy to automatically reallocate parameters during
training. This reallocation shifts resources from less-utilized areas to those
parts of the model where they are most beneficial. Notably, DMAO only
introduces negligible training overhead at a given model complexity. We
evaluate DMAO through experiments with CTC on LibriSpeech, TED-LIUM-v2 and
Switchboard datasets. The results show that, using the same amount of training
resources, our proposed DMAO consistently improves WER by up to 6% relatively
across various architectures, model sizes, and datasets. Furthermore, we
analyze the pattern of parameter redistribution and uncover insightful
findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Large Language Models with Reliable Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinggang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities in
text generation and understanding, yet their reliance on implicit, unstructured
knowledge often leads to factual inaccuracies and limited interpretability.
Knowledge Graphs (KGs), with their structured, relational representations,
offer a promising solution to ground LLMs in verified knowledge. However, their
potential remains constrained by inherent noise, incompleteness, and the
complexity of integrating their rigid structure with the flexible reasoning of
LLMs. This thesis presents a systematic framework to address these limitations,
advancing the reliability of KGs and their synergistic integration with LLMs
through five interconnected contributions. This thesis addresses these
challenges through a cohesive framework that enhances LLMs by refining and
leveraging reliable KGs. First, we introduce contrastive error detection, a
structure-based method to identify incorrect facts in KGs. This approach is
extended by an attribute-aware framework that unifies structural and semantic
signals for error correction. Next, we propose an inductive completion model
that further refines KGs by completing the missing relationships in evolving
KGs. Building on these refined KGs, KnowGPT integrates structured graph
reasoning into LLMs through dynamic prompting, improving factual grounding.
These contributions form a systematic pipeline (from error detection to LLM
integration), demonstrating that reliable KGs significantly enhance the
robustness, interpretability, and adaptability of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Thesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Development of the user-friendly decision aid Rule-based Evaluation and
  Support <span class="highlight-title">Tool</span> (REST) for optimizing the resources of an information extraction
  task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillaume Bazin, Xavier Tannier, Fanny Adda, Ariel Cohen, Akram Redjdal, Emmanuelle Kempf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rules could be an information extraction (IE) default option, compared to ML
and LLMs in terms of sustainability, transferability, interpretability, and
development burden. We suggest a sustainable and combined use of rules and ML
as an IE method. Our approach starts with an exhaustive expert manual
highlighting in a single working session of a representative subset of the data
corpus. We developed and validated the feasibility and the performance metrics
of the REST decision tool to help the annotator choose between rules as a by
default option and ML for each entity of an IE task. REST makes the annotator
visualize the characteristics of each entity formalization in the free texts
and the expected rule development feasibility and IE performance metrics. ML is
considered as a backup IE option and manual annotation for training is
therefore minimized. The external validity of REST on a 12-entity use case
showed good reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging
  Unsubstantiated Claims and Ambiguous Pronouns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evgeny Markhasin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present and evaluate a suite of proof-of-concept (PoC), structured
workflow prompts designed to elicit human-like hierarchical reasoning while
guiding Large Language Models (LLMs) in high-level semantic and linguistic
analysis of scholarly manuscripts. The prompts target two non-trivial
analytical tasks: identifying unsubstantiated claims in summaries
(informational integrity) and flagging ambiguous pronoun references (linguistic
clarity). We conducted a systematic, multi-run evaluation on two frontier
models (Gemini Pro 2.5 Pro and ChatGPT Plus o3) under varied context
conditions. Our results for the informational integrity task reveal a
significant divergence in model performance: while both models successfully
identified an unsubstantiated head of a noun phrase (95% success), ChatGPT
consistently failed (0% success) to identify an unsubstantiated adjectival
modifier that Gemini correctly flagged (95% success), raising a question
regarding potential influence of the target's syntactic role. For the
linguistic analysis task, both models performed well (80-90% success) with full
manuscript context. In a summary-only setting, however, ChatGPT achieved a
perfect (100%) success rate, while Gemini's performance was substantially
degraded. Our findings suggest that structured prompting is a viable
methodology for complex textual analysis but show that prompt performance may
be highly dependent on the interplay between the model, task type, and context,
highlighting the need for rigorous, model-specific testing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adapting LLMs for Minimal-edit Grammatical Error Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryszard Staruch, Filip Graliński, Daniel Dzienisiewicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoder-only large language models have shown superior performance in the
fluency-edit English Grammatical Error Correction, but their adaptation for
minimal-edit English GEC is still underexplored. To improve their effectiveness
in the minimal-edit approach, we explore the error rate adaptation topic and
propose a novel training schedule method. Our experiments set a new
state-of-the-art result for a single-model system on the BEA-test set. We also
detokenize the most common English GEC datasets to match the natural way of
writing text. During the process, we find that there are errors in them. Our
experiments analyze whether training on detokenized datasets impacts the
results and measure the impact of the usage of the datasets with corrected
erroneous examples. To facilitate reproducibility, we have released the source
code used to train our models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at BEA-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMU's IWSLT 2025 Simultaneous Speech Translation System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqi Ouyang, Xi Xu, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents CMU's submission to the IWSLT 2025 Simultaneous Speech
Translation (SST) task for translating unsegmented English speech into Chinese
and German text in a streaming manner. Our end-to-end speech-to-text system
integrates a chunkwise causal Wav2Vec 2.0 speech encoder, an adapter, and the
Qwen2.5-7B-Instruct as the decoder. We use a two-stage simultaneous training
procedure on robust speech segments curated from LibriSpeech, CommonVoice, and
VoxPopuli datasets, utilizing standard cross-entropy loss. Our model supports
adjustable latency through a configurable latency multiplier. Experimental
results demonstrate that our system achieves 44.3 BLEU for English-to-Chinese
and 25.1 BLEU for English-to-German translations on the ACL60/60 development
set, with computation-aware latencies of 2.7 seconds and 2.3 seconds, and
theoretical latencies of 2.2 and 1.7 seconds, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IWSLT 2025 System Description</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZINA: Multimodal Fine-grained Hallucination Detection and Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuiga Wada, Kazuki Matsuda, Komei Sugiura, Graham Neubig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) often generate hallucinations, where
the output deviates from the visual content. Given that these hallucinations
can take diverse forms, detecting hallucinations at a fine-grained level is
essential for comprehensive evaluation and analysis. To this end, we propose a
novel task of multimodal fine-grained hallucination detection and editing for
MLLMs. Moreover, we propose ZINA, a novel method that identifies hallucinated
spans at a fine-grained level, classifies their error types into six
categories, and suggests appropriate refinements. To train and evaluate models
for this task, we constructed VisionHall, a dataset comprising 6.9k outputs
from twelve MLLMs manually annotated by 211 annotators, and 20k synthetic
samples generated using a graph-based method that captures dependencies among
error types. We demonstrated that ZINA outperformed existing methods, including
GPT-4o and LLama-3.2, in both detection and editing tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Crime Hotspot Prediction Using Deep Graph Convolutional Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tehreem Zubair, Syeda Kisaa Fatima, Noman Ahmed, Asifullah Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crime hotspot prediction is critical for ensuring urban safety and effective
law enforcement, yet it remains challenging due to the complex spatial
dependencies inherent in criminal activity. The previous approaches tended to
use classical algorithms such as the KDE and SVM to model data distributions
and decision boundaries. The methods often fail to capture these spatial
relationships, treating crime events as independent and ignoring geographical
interactions. To address this, we propose a novel framework based on Graph
Convolutional Networks (GCNs), which explicitly model spatial dependencies by
representing crime data as a graph. In this graph, nodes represent discrete
geographic grid cells and edges capture proximity relationships. Using the
Chicago Crime Dataset, we engineer spatial features and train a multi-layer GCN
model to classify crime types and predict high-risk zones. Our approach
achieves 88% classification accuracy, significantly outperforming traditional
methods. Additionally, the model generates interpretable heat maps of crime
hotspots, demonstrating the practical utility of graph-based learning for
predictive policing and spatial criminology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging In-Context Learning for Language Model <span class="highlight-title">Agent</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivanshu Gupta, Sameer Singh, Ashish Sabharwal, Tushar Khot, Ben Bogin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) with dynamically selected demonstrations combines
the flexibility of prompting large language models (LLMs) with the ability to
leverage training data to improve performance. While ICL has been highly
successful for prediction and generation tasks, leveraging it for agentic tasks
that require sequential decision making is challenging -- one must think not
only about how to annotate long trajectories at scale and how to select
demonstrations, but also what constitutes demonstrations, and when and where to
show them. To address this, we first propose an algorithm that leverages an LLM
with retries along with demonstrations to automatically and efficiently
annotate agentic tasks with solution trajectories. We then show that
set-selection of trajectories of similar tasks as demonstrations significantly
improves performance, reliability, robustness, and efficiency of LLM agents.
However, trajectory demonstrations have a large inference cost overhead. We
show that this can be mitigated by using small trajectory snippets at every
step instead of an additional trajectory. We find that demonstrations obtained
from larger models (in the annotation phase) also improve smaller models, and
that ICL agents can even rival costlier trained agents. Thus, our results
reveal that ICL, with careful use, can be very powerful for agentic tasks as
well.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Equitable Electronic Health Record Prediction with FAME: Fairness-Aware
  Multimodal Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikkie Hooman, Zhongjie Wu, Eric C. Larson, Mehak Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Record (EHR) data encompass diverse modalities -- text,
images, and medical codes -- that are vital for clinical decision-making. To
process these complex data, multimodal AI (MAI) has emerged as a powerful
approach for fusing such information. However, most existing MAI models
optimize for better prediction performance, potentially reinforcing biases
across patient subgroups. Although bias-reduction techniques for multimodal
models have been proposed, the individual strengths of each modality and their
interplay in both reducing bias and optimizing performance remain
underexplored. In this work, we introduce FAME (Fairness-Aware Multimodal
Embeddings), a framework that explicitly weights each modality according to its
fairness contribution. FAME optimizes both performance and fairness by
incorporating a combined loss function. We leverage the Error Distribution
Disparity Index (EDDI) to measure fairness across subgroups and propose a
sign-agnostic aggregation method to balance fairness across subgroups, ensuring
equitable model outcomes. We evaluate FAME with BEHRT and BioClinicalBERT,
combining structured and unstructured EHR data, and demonstrate its
effectiveness in terms of performance and fairness compared with other
baselines across multiple EHR prediction tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Test-Time Scaling for Medical AI: Model and Task-Aware
  Strategies for LLMs and VLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyutaek Oh, Seoyeon Kim, Sangjoon Park, Byung-Hoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time scaling has recently emerged as a promising approach for enhancing
the reasoning capabilities of large language models or vision-language models
during inference. Although a variety of test-time scaling strategies have been
proposed, and interest in their application to the medical domain is growing,
many critical aspects remain underexplored, including their effectiveness for
vision-language models and the identification of optimal strategies for
different settings. In this paper, we conduct a comprehensive investigation of
test-time scaling in the medical domain. We evaluate its impact on both large
language models and vision-language models, considering factors such as model
size, inherent model characteristics, and task complexity. Finally, we assess
the robustness of these strategies under user-driven factors, such as
misleading information embedded in prompts. Our findings offer practical
guidelines for the effective use of test-time scaling in medical applications
and provide insights into how these strategies can be further refined to meet
the reliability and interpretability demands of the medical domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CHILL at SemEval-2025 Task 2: You Can't Just Throw Entities and Hope --
  Make Your LLM to Get Them Right 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaebok Lee, Yonghyun Ryu, Seongmin Park, Yoonjung Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we describe our approach for the SemEval 2025 Task 2 on
Entity-Aware Machine Translation (EA-MT). Our system aims to improve the
accuracy of translating named entities by combining two key approaches:
Retrieval Augmented Generation (RAG) and iterative self-refinement techniques
using Large Language Models (LLMs). A distinctive feature of our system is its
self-evaluation mechanism, where the LLM assesses its own translations based on
two key criteria: the accuracy of entity translations and overall translation
quality. We demonstrate how these methods work together and effectively improve
entity handling while maintaining high-quality translations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 19th International Workshop on Semantic Evaluation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FinLMM-R1: Enhancing Financial Reasoning in LMM through Scalable Data
  and Reward Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Lan, Jiayong Zhu, Jiangtong Li, Dawei Cheng, Guang Chen, Changjun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Multimodal Models (LMMs) demonstrate significant cross-modal reasoning
capabilities. However, financial applications face challenges due to the lack
of high-quality multimodal reasoning datasets and the inefficiency of existing
training paradigms for reasoning enhancement. To address these issues, we
propose an integrated framework, FinLMM-R1, combining an automated and scalable
pipeline for data construction with enhanced training strategies to improve the
multimodal reasoning of LMM. The Automated and Scalable Pipeline (ASP) resolves
textual-visual misalignment in financial reports through a separate paradigm of
question-answer generation and image-question alignment, ensuring data
integrity and extraction efficiency. Through ASP, we collect 89,378 aligned
image-question pairs from 23,397 financial reports, covering tasks such as
arithmetic reasoning, statistics reasoning, financial explanation, and
financial knowledge. Moreover, we introduce the Thinking with Adversarial
Reward in LMM (TAR-LMM), extending the prior two-stage training framework [1]
with additional reward mechanisms. In the first stage, we focus on text-only
tasks with format and accuracy rewards to guide the model in generating
well-structured thinking contents. In the second stage, we construct
multi-image contrastive samples with additional reward components including
image selection, thinking content length, and adversarial reward to jointly
optimize the LMM across visual perception, reasoning efficiency, and logical
coherence. Extensive experiments on 7 benchmarks show ASP-derived dataset and
training framework significantly improve answer accuracy and reasoning depth
over existing reasoning LMMs in both general and financial multimodal contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MotiveBench: How Far Are We From <span class="highlight-title">Human</span>-Like Motivational Reasoning in
  Large Language Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xixian Yong, Jianxun Lian, Xiaoyuan Yi, Xiao Zhou, Xing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been widely adopted as the core of agent
frameworks in various scenarios, such as social simulations and AI companions.
However, the extent to which they can replicate human-like motivations remains
an underexplored question. Existing benchmarks are constrained by simplistic
scenarios and the absence of character identities, resulting in an information
asymmetry with real-world situations. To address this gap, we propose
MotiveBench, which consists of 200 rich contextual scenarios and 600 reasoning
tasks covering multiple levels of motivation. Using MotiveBench, we conduct
extensive experiments on seven popular model families, comparing different
scales and versions within each family. The results show that even the most
advanced LLMs still fall short in achieving human-like motivational reasoning.
Our analysis reveals key findings, including the difficulty LLMs face in
reasoning about "love & belonging" motivations and their tendency toward
excessive rationality and idealism. These insights highlight a promising
direction for future research on the humanization of LLMs. The dataset,
benchmark, and code are available at https://aka.ms/motivebench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRISM2: Unlocking Multi-Modal General Pathology AI with Clinical
  <span class="highlight-title">Dialogue</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Shaikovski, Eugene Vorontsov, Adam Casson, Julian Viret, Eric Zimmermann, Neil Tenenholtz, Yi Kan Wang, Jan H. Bernhard, Ran A. Godrich, Juan A. Retamero, Razik Yousfi, Nicolo Fusi, Thomas J. Fuchs, Kristen Severson, Siqi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent pathology foundation models can provide rich tile-level
representations but fall short of delivering general-purpose clinical utility
without further extensive model development. These models lack whole-slide
image (WSI) understanding and are not trained with large-scale diagnostic data,
limiting their performance on diverse downstream tasks. We introduce PRISM2, a
multi-modal slide-level foundation model trained via clinical dialogue to
enable scalable, generalizable pathology AI. PRISM2 is trained on nearly
700,000 specimens (2.3 million WSIs) paired with real-world clinical diagnostic
reports in a two-stage process. In Stage 1, a vision-language model is trained
using contrastive and captioning objectives to align whole slide embeddings
with textual clinical diagnosis. In Stage 2, the language model is unfrozen to
enable diagnostic conversation and extract more clinically meaningful
representations from hidden states. PRISM2 achieves strong performance on
diagnostic and biomarker prediction tasks, outperforming prior slide-level
models including PRISM and TITAN. It also introduces a zero-shot yes/no
classification approach that surpasses CLIP-style methods without prompt tuning
or class enumeration. By aligning visual features with clinical reasoning,
PRISM2 improves generalization on both data-rich and low-sample tasks, offering
a scalable path forward for building general pathology AI agents capable of
assisting diagnostic and prognostic decisions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multipole Attention for Efficient Long Context Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Coleman Hooper, Sebastian Zhao, Luca Manolache, Sehoon Kim, Michael W. Mahoney, Yakun Sophia Shao, Kurt Keutzer, Amir Gholami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Reasoning Models (LRMs) have shown promising accuracy improvements on
complex problem-solving tasks. While these models have attained high accuracy
by leveraging additional computation at test time, they need to generate long
chain-of-thought reasoning in order to think before answering, which requires
generating thousands of tokens. While sparse attention methods can help reduce
the KV cache pressure induced by this long autoregressive reasoning, these
methods can introduce errors which disrupt the reasoning process. Additionally,
prior methods often pre-process the input to make it easier to identify the
important prompt tokens when computing attention during generation, and this
pre-processing is challenging to perform online for newly generated reasoning
tokens. Our work addresses these challenges by introducing Multipole Attention,
which accelerates autoregressive reasoning by only computing exact attention
for the most important tokens, while maintaining approximate representations
for the remaining tokens. Our method first performs clustering to group
together semantically similar key vectors, and then uses the cluster centroids
both to identify important key vectors and to approximate the remaining key
vectors in order to retain high accuracy. We design a fast cluster update
process to quickly re-cluster the input and previously generated tokens,
thereby allowing for accelerating attention to the previous output tokens. We
evaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our
approach can maintain accuracy on complex reasoning tasks even with aggressive
attention sparsity settings. We also provide kernel implementations to
demonstrate the practical efficiency gains from our method, achieving up to
4.5$\times$ speedup for attention in long-context reasoning applications. Our
code is available at https://github.com/SqueezeAILab/MultipoleAttention.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CFBenchmark-MM: Chinese Financial Assistant Benchmark for Multimodal
  Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangtong Li, Yiyun Zhu, Dawei Cheng, Zhijun Ding, Changjun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have rapidly evolved with the growth
of Large Language Models (LLMs) and are now applied in various fields. In
finance, the integration of diverse modalities such as text, charts, and tables
is crucial for accurate and efficient decision-making. Therefore, an effective
evaluation system that incorporates these data types is essential for advancing
financial application. In this paper, we introduce CFBenchmark-MM, a Chinese
multimodal financial benchmark with over 9,000 image-question pairs featuring
tables, histogram charts, line charts, pie charts, and structural diagrams.
Additionally, we develop a staged evaluation system to assess MLLMs in handling
multimodal information by providing different visual content step by step.
Despite MLLMs having inherent financial knowledge, experimental results still
show limited efficiency and robustness in handling multimodal financial
context. Further analysis on incorrect responses reveals the misinterpretation
of visual content and the misunderstanding of financial concepts are the
primary issues. Our research validates the significant, yet underexploited,
potential of MLLMs in financial analysis, highlighting the need for further
development and domain-specific optimization to encourage the enhanced use in
financial domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stress-Testing Multimodal Foundation Models for Crystallographic
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Polat, Hasan Kurban, Erchin Serpedin, Mustafa Kurban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating foundation models for crystallographic reasoning requires
benchmarks that isolate generalization behavior while enforcing physical
constraints. This work introduces a multiscale multicrystal dataset with two
physically grounded evaluation protocols to stress-test multimodal generative
models. The Spatial-Exclusion benchmark withholds all supercells of a given
radius from a diverse dataset, enabling controlled assessments of spatial
interpolation and extrapolation. The Compositional-Exclusion benchmark omits
all samples of a specific chemical composition, probing generalization across
stoichiometries. Nine vision--language foundation models are prompted with
crystallographic images and textual context to generate structural annotations.
Responses are evaluated via (i) relative errors in lattice parameters and
density, (ii) a physics-consistency index penalizing volumetric violations, and
(iii) a hallucination score capturing geometric outliers and invalid
space-group predictions. These benchmarks establish a reproducible, physically
informed framework for assessing generalization, consistency, and reliability
in large-scale multimodal models. Dataset and code are available at
https://github.com/KurbanIntelligenceLab/StressTestingMMFMinCR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Just Go Parallel: Improving the Multilingual Capabilities of Large
  Language Models <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Reza Qorib, Junyi Li, Hwee Tou Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive translation
capabilities even without being explicitly trained on parallel data. This
remarkable property has led some to believe that parallel data is no longer
necessary for building multilingual language models. While some attribute this
to the emergent abilities of LLMs due to scale, recent work suggests that it is
actually caused by incidental bilingual signals present in the training data.
Various methods have been proposed to maximize the utility of parallel data to
enhance the multilingual capabilities of multilingual encoder-based and
encoder-decoder language models. However, some decoder-based LLMs opt to ignore
parallel data instead. In this work, we conduct a systematic study on the
impact of adding parallel data on LLMs' multilingual capabilities, focusing
specifically on translation and multilingual common-sense reasoning. Through
controlled experiments, we demonstrate that parallel data can significantly
improve LLMs' multilingual capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Graph Fusion with Large Language Models for Accurate,
  Explainable Manufacturing Process Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danny Hoang, David Gorsich, Matthew P. Castanier, Farhad Imani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precision process planning in Computer Numerical Control (CNC) machining
demands rapid, context-aware decisions on tool selection, feed-speed pairs, and
multi-axis routing, placing immense cognitive and procedural burdens on
engineers from design specification through final part inspection. Conventional
rule-based computer-aided process planning and knowledge-engineering shells
freeze domain know-how into static tables, which become limited when dealing
with unseen topologies, novel material states, shifting
cost-quality-sustainability weightings, or shop-floor constraints such as tool
unavailability and energy caps. Large language models (LLMs) promise flexible,
instruction-driven reasoning for tasks but they routinely hallucinate numeric
values and provide no provenance. We present Augmented Retrieval Knowledge
Network Enhanced Search & Synthesis (ARKNESS), the end-to-end framework that
fuses zero-shot Knowledge Graph (KG) construction with retrieval-augmented
generation to deliver verifiable, numerically exact answers for CNC process
planning. ARKNESS (1) automatically distills heterogeneous machining documents,
G-code annotations, and vendor datasheets into augmented triple,
multi-relational graphs without manual labeling, and (2) couples any on-prem
LLM with a retriever that injects the minimal, evidence-linked subgraph needed
to answer a query. Benchmarked on 155 industry-curated questions spanning tool
sizing and feed-speed optimization, a lightweight 3B-parameter Llama-3
augmented by ARKNESS matches GPT-4o accuracy while achieving a +25 percentage
point gain in multiple-choice accuracy, +22.4 pp in F1, and 8.1x ROUGE-L on
open-ended responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Edeflip: Supervised Word Translation between English and Yoruba 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ikeoluwa Abioye, Jiani Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, embedding alignment has become the state-of-the-art machine
translation approach, as it can yield high-quality translation without training
on parallel corpora. However, existing research and application of embedding
alignment mostly focus on high-resource languages with high-quality monolingual
embeddings. It is unclear if and how low-resource languages may be similarly
benefited. In this study, we implement an established supervised embedding
alignment method for word translation from English to Yoruba, the latter a
low-resource language. We found that higher embedding quality and normalizing
embeddings increase word translation precision, with, additionally, an
interaction effect between the two. Our results demonstrate the limitations of
the state-of-the-art supervised embedding alignment when it comes to
low-resource languages, for which there are additional factors that need to be
taken into consideration, such as the importance of curating high-quality
monolingual embeddings. We hope our work will be a starting point for further
machine translation research that takes into account the challenges that
low-resource languages face.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Missing the <span class="highlight-title">human</span> touch? A computational stylometry analysis of GPT-4
  translations of online Chinese literature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofang Yao, Yong-Bin Kang, Anthony McCosker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing research indicates that machine translations (MTs) of literary texts
are often unsatisfactory. MTs are typically evaluated using automated metrics
and subjective human ratings, with limited focus on stylistic features.
Evidence is also limited on whether state-of-the-art large language models
(LLMs) will reshape literary translation. This study examines the stylistic
features of LLM translations, comparing GPT-4's performance to human
translations in a Chinese online literature task. Computational stylometry
analysis shows that GPT-4 translations closely align with human translations in
lexical, syntactic, and content features, suggesting that LLMs might replicate
the 'human touch' in literary translation style. These findings offer insights
into AI's impact on literary translation from a posthuman perspective, where
distinctions between machine and human translations become increasingly blurry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Extraction of Clausal Embedding Based on Large-Scale English
  Text Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iona Carslaw, Sivan Milton, Nicolas Navarre, Ciyang Qing, Wataru Uegaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For linguists, embedded clauses have been of special interest because of
their intricate distribution of syntactic and semantic features. Yet, current
research relies on schematically created language examples to investigate these
constructions, missing out on statistical information and naturally-occurring
examples that can be gained from large language corpora. Thus, we present a
methodological approach for detecting and annotating naturally-occurring
examples of English embedded clauses in large-scale text data using
constituency parsing and a set of parsing heuristics. Our tool has been
evaluated on our dataset Golden Embedded Clause Set (GECS), which includes
hand-annotated examples of naturally-occurring English embedded clause
sentences. Finally, we present a large-scale dataset of naturally-occurring
English embedded clauses which we have extracted from the open-source corpus
Dolma using our extraction tool.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the Society for Computation in Linguistics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic
  Difficulty of <span class="highlight-title">Conversation</span>al Texts for LLM Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Kogan, Max Schumacher, Sam Nguyen, Masanori Suzuki, Melissa Smith, Chloe Sophia Bellows, Jared Bernstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is an unmet need to evaluate the language difficulty of short,
conversational passages of text, particularly for training and filtering Large
Language Models (LLMs). We introduce Ace-CEFR, a dataset of English
conversational text passages expert-annotated with their corresponding level of
text difficulty. We experiment with several models on Ace-CEFR, including
Transformer-based models and LLMs. We show that models trained on Ace-CEFR can
measure text difficulty more accurately than human experts and have latency
appropriate to production environments. Finally, we release the Ace-CEFR
dataset to the public for research and development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Interdisciplinary Review of Commonsense Reasoning and Intent
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Nazmus Sakib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This review explores recent advances in commonsense reasoning and intent
detection, two key challenges in natural language understanding. We analyze 28
papers from ACL, EMNLP, and CHI (2020-2025), organizing them by methodology and
application. Commonsense reasoning is reviewed across zero-shot learning,
cultural adaptation, structured evaluation, and interactive contexts. Intent
detection is examined through open-set models, generative formulations,
clustering, and human-centered systems. By bridging insights from NLP and HCI,
we highlight emerging trends toward more adaptive, multilingual, and
context-aware models, and identify key gaps in grounding, generalization, and
benchmark design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark
  for Financial LLM Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueqing Peng, Lingfei Qian, Yan Wang, Ruoyu Xiang, Yueru He, Yang Ren, Mingyang Jiang, Jeff Zhao, Huan He, Yi Han, Yun Feng, Yuechen Jiang, Yupeng Cao, Haohang Li, Yangyang Yu, Xiaoyu Wang, Penglei Gao, Shengyuan Lin, Keyi Wang, Shanshan Yang, Yilun Zhao, Zhiwei Liu, Peng Lu, Jerry Huang, Suyuchen Wang, Triantafillos Papadopoulos, Polydoros Giannouris, Efstathia Soufleri, Nuo Chen, Guojun Xiong, Zhiyang Deng, Yijia Zhao, Mingquan Lin, Meikang Qiu, Kaleb E Smith, Arman Cohan, Xiao-Yang Liu, Jimin Huang, Alejandro Lopez-Lira, Xi Chen, Junichi Tsujii, Jian-Yun Nie, Sophia Ananiadou, Qianqian Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) have accelerated progress in
financial NLP and applications, yet existing benchmarks remain limited to
monolingual and unimodal settings, often over-relying on simple tasks and
failing to reflect the complexity of real-world financial communication. We
introduce MultiFinBen, the first multilingual and multimodal benchmark tailored
to the global financial domain, evaluating LLMs across modalities (text,
vision, audio) and linguistic settings (monolingual, bilingual, multilingual)
on domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy
and PolyFiQA-Expert, the first multilingual financial benchmarks requiring
models to perform complex reasoning over mixed-language inputs; and EnglishOCR
and SpanishOCR, the first OCR-embedded financial QA tasks challenging models to
extract and reason over information from visual-text financial documents.
Moreover, we propose a dynamic, difficulty-aware selection mechanism and curate
a compact, balanced benchmark rather than simple aggregation existing datasets.
Extensive evaluation of 22 state-of-the-art models reveals that even the
strongest models, despite their general multimodal and multilingual
capabilities, struggle dramatically when faced with complex cross-lingual and
multimodal tasks in financial domain. MultiFinBen is publicly released to
foster transparent, reproducible, and inclusive progress in financial studies
and applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amr Mohamed, Yang Zhang, Michalis Vazirgiannis, Guokan Shang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code-switching (CSW) is the act of alternating between two or more languages
within a single discourse. This phenomenon is widespread in multilingual
communities, and increasingly prevalent in online content, where users
naturally mix languages in everyday communication. As a result, Large Language
Models (LLMs), now central to content processing and generation, are frequently
exposed to code-switched inputs. Given their widespread use, it is crucial to
understand how LLMs process and reason about such mixed-language text. This
paper presents a systematic evaluation of LLM comprehension under
code-switching by generating CSW variants of established reasoning and
comprehension benchmarks. While degradation is evident when foreign tokens
disrupt English text$\unicode{x2013}$even under linguistic
constraints$\unicode{x2013}$embedding English into other languages often
improves comprehension. Though prompting yields mixed results, fine-tuning
offers a more stable path to degradation mitigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are manual annotations necessary for sta<span class="highlight-title">tutor</span>y interpretations
  retrieval? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksander Smywiński-Pohl, Tomer Libal, Adam Kaczmarczyk, Magdalena Król
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the elements of legal research is looking for cases where judges have
extended the meaning of a legal concept by providing interpretations of what a
concept means or does not mean. This allow legal professionals to use such
interpretations as precedents as well as laymen to better understand the legal
concept. The state-of-the-art approach for retrieving the most relevant
interpretations for these concepts currently depends on the ranking of
sentences and the training of language models over annotated examples. That
manual annotation process can be quite expensive and need to be repeated for
each such concept, which prompted recent research in trying to automate this
process. In this paper, we highlight the results of various experiments
conducted to determine the volume, scope and even the need for manual
annotation. First of all, we check what is the optimal number of annotations
per a legal concept. Second, we check if we can draw the sentences for
annotation randomly or there is a gain in the performance of the model, when
only the best candidates are annotated. As the last question we check what is
the outcome of automating the annotation process with the help of an LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASMR: Augmenting Life Scenario using Large Generative Models for Robotic
  Action Reflection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shang-Chi Tsai, Seiya Kawano, Angel Garcia Contreras, Koichiro Yoshino, Yun-Nung Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When designing robots to assist in everyday human activities, it is crucial
to enhance user requests with visual cues from their surroundings for improved
intent understanding. This process is defined as a multimodal classification
task. However, gathering a large-scale dataset encompassing both visual and
linguistic elements for model training is challenging and time-consuming. To
address this issue, our paper introduces a novel framework focusing on data
augmentation in robotic assistance scenarios, encompassing both dialogues and
related environmental imagery. This approach involves leveraging a
sophisticated large language model to simulate potential conversations and
environmental contexts, followed by the use of a stable diffusion model to
create images depicting these environments. The additionally generated data
serves to refine the latest multimodal models, enabling them to more accurately
determine appropriate actions in response to user interactions with the limited
target data. Our experimental results, based on a dataset collected from
real-world scenarios, demonstrate that our methodology significantly enhances
the robot's action selection capabilities, achieving the state-of-the-art
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IWSDS 2024 Best Paper Award</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Adaptive</span> Guidance Accelerates Reinforcement Learning of Reasoning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaskar Nath, Elaine Lau, Anisha Gunjal, Manasi Sharma, Nikhil Baharte, Sean Hendryx
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the process through which reasoning models trained with
reinforcement learning on verifiable rewards (RLVR) can learn to solve new
problems. We find that RLVR drives performance through two main means: (1) by
compressing pass@$k$ into pass@1 and (2) via "capability gain" in which models
learn to solve new problems that they previously could not solve even at high
$k$. We find that while capability gain exists across model scales, learning to
solve new problems is primarily driven through self-distillation. We
demonstrate these findings across model scales ranging from 0.5B to 72B on
>500,000 reasoning problems with prompts and verifiable final answers across
math, science, and code domains. We further show that we can significantly
improve pass@$k$ rates by leveraging natural language guidance for the model to
consider within context while still requiring the model to derive a solution
chain from scratch. Based of these insights, we derive $\text{Guide}$ - a new
class of online training algorithms. $\text{Guide}$ adaptively incorporates
hints into the model's context on problems for which all rollouts were
initially incorrect and adjusts the importance sampling ratio for the
"off-policy" trajectories in order to optimize the policy for contexts in which
the hints are no longer present. We describe variants of $\text{Guide}$ for
GRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter
models improves generalization over its vanilla counterpart with up to 4$\%$
macro-average improvement across math benchmarks. We include careful ablations
to analyze $\text{Guide}$'s components and theoretically analyze Guide's
learning efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic
  Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise
  Pooled Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhilekh Borah, Chhavi Sharma, Danush Khanna, Utkarsh Bhatt, Gurpreet Singh, Hasnat Md Abdullah, Raghav Kaushik Ravi, Vinija Jain, Jyoti Patel, Shubham Singh, Vasu Sharma, Arpita Vats, Rahul Raja, Aman Chadha, Amitava Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alignment is no longer a luxury, it is a necessity. As large language models
(LLMs) enter high-stakes domains like education, healthcare, governance, and
law, their behavior must reliably reflect human-aligned values and safety
constraints. Yet current evaluations rely heavily on behavioral proxies such as
refusal rates, G-Eval scores, and toxicity classifiers, all of which have
critical blind spots. Aligned models are often vulnerable to jailbreaking,
stochasticity of generation, and alignment faking.
  To address this issue, we introduce the Alignment Quality Index (AQI). This
novel geometric and prompt-invariant metric empirically assesses LLM alignment
by analyzing the separation of safe and unsafe activations in latent space. By
combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI),
Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various
formulations, AQI captures clustering quality to detect hidden misalignments
and jailbreak risks, even when outputs appear compliant. AQI also serves as an
early warning signal for alignment faking, offering a robust, decoding
invariant tool for behavior agnostic safety auditing.
  Additionally, we propose the LITMUS dataset to facilitate robust evaluation
under these challenging conditions. Empirical tests on LITMUS across different
models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's
correlation with external judges and ability to reveal vulnerabilities missed
by refusal metrics. We make our implementation publicly available to foster
future research in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EmoNews: A Spoken <span class="highlight-title">Dialogue</span> System for Expressive News <span class="highlight-title">Conversation</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13894v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13894v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryuki Matsuura, Shikhar Bharadwaj, Jiarui Liu, Dhatchi Kunde Govindarajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a task-oriented spoken dialogue system (SDS) that regulates
emotional speech based on contextual cues to enable more empathetic news
conversations. Despite advancements in emotional text-to-speech (TTS)
techniques, task-oriented emotional SDSs remain underexplored due to the
compartmentalized nature of SDS and emotional TTS research, as well as the lack
of standardized evaluation metrics for social goals. We address these
challenges by developing an emotional SDS for news conversations that utilizes
a large language model (LLM)-based sentiment analyzer to identify appropriate
emotions and PromptTTS to synthesize context-appropriate emotional speech. We
also propose subjective evaluation scale for emotional SDSs and judge the
emotion regulation performance of the proposed and baseline systems.
Experiments showed that our emotional SDS outperformed a baseline system in
terms of the emotion regulation and engagement. These results suggest the
critical role of speech emotion for more engaging conversations. All our source
code is open-sourced at
https://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds/sds1
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and
  Iterative Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jipeng Zhang, Kehao Miao, Renjie Pi, Zhaowei Wang, Runtao Liu, Rui Pan, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large
language models but remains underexplored for Vision-Language (VL) models. The
Vision-Language Reward Model (VL-RM) is key to aligning VL models by providing
structured feedback, yet training effective VL-RMs faces two major challenges.
First, the bootstrapping dilemma arises as high-quality training data depends
on already strong VL models, creating a cycle where self-generated supervision
reinforces existing biases. Second, modality bias and negative example
amplification occur when VL models hallucinate incorrect visual attributes,
leading to flawed preference data that further misguides training. To address
these issues, we propose an iterative training framework leveraging vision
experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection
Sampling. Our approach refines preference datasets, enhances structured
critiques, and iteratively improves reasoning. Experiments across VL-RM
benchmarks demonstrate superior performance in hallucination detection and
multimodal reasoning, advancing VL model alignment with reinforcement learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating the interaction of linguistic and mathematical reasoning
  in language models using multilingual number puzzles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antara Raaghavi Bhattacharya, Isabel Papadimitriou, Kathryn Davidson, David Alvarez-Melis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Across languages, numeral systems vary widely in how they construct and
combine numbers. While humans consistently learn to navigate this diversity,
large language models (LLMs) struggle with linguistic-mathematical puzzles
involving cross-linguistic numeral systems, which humans can learn to solve
successfully. We investigate why this task is difficult for LLMs through a
series of experiments that untangle the linguistic and mathematical aspects of
numbers in language. Our experiments establish that models cannot consistently
solve such problems unless the mathematical operations in the problems are
explicitly marked using known symbols ($+$, $\times$, etc, as in "twenty +
three"). In further ablation studies, we probe how individual parameters of
numeral construction and combination affect performance. While humans use their
linguistic understanding of numbers to make inferences about the implicit
compositional structure of numerals, LLMs seem to lack this notion of implicit
numeral structure. We conclude that the ability to flexibly infer compositional
rules from implicit patterns in human-scale data remains an open challenge for
current reasoning models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OPeRA: A Dataset of Observation, Persona, Rationale, and Action for
  Evaluating LLMs on <span class="highlight-title">Human</span> Online Shopping Behavior <span class="highlight-title">Simulat</span>ion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Wang, Yuxuan Lu, Wenbo Li, Amirali Amini, Bo Sun, Yakov Bart, Weimin Lyu, Jiri Gesi, Tian Wang, Jing Huang, Yu Su, Upol Ehsan, Malihe Alikhani, Toby Jia-Jun Li, Lydia Chilton, Dakuo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can large language models (LLMs) accurately simulate the next web action of a
specific user? While LLMs have shown promising capabilities in generating
``believable'' human behaviors, evaluating their ability to mimic real user
behaviors remains an open challenge, largely due to the lack of high-quality,
publicly available datasets that capture both the observable actions and the
internal reasoning of an actual human user. To address this gap, we introduce
OPERA, a novel dataset of Observation, Persona, Rationale, and Action collected
from real human participants during online shopping sessions. OPERA is the
first public dataset that comprehensively captures: user personas, browser
observations, fine-grained web actions, and self-reported just-in-time
rationales. We developed both an online questionnaire and a custom browser
plugin to gather this dataset with high fidelity. Using OPERA, we establish the
first benchmark to evaluate how well current LLMs can predict a specific user's
next action and rationale with a given persona and <observation, action,
rationale> history. This dataset lays the groundwork for future research into
LLM agents that aim to act as personalized digital twins for human.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Inference for Large Reasoning Models: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.23077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.23077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Liu, Jiaying Wu, Yufei He, Hongcheng Gao, Hongyu Chen, Baolong Bi, Ruihan Gong, Jiaheng Zhang, Zhiqi Huang, Bryan Hooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Reasoning Models (LRMs) significantly improve the reasoning ability of
Large Language Models (LLMs) by learning to reason, exhibiting promising
performance in complex task-solving. However, their deliberative reasoning
process leads to inefficiencies in token usage, memory consumption, and
inference time. Thus, this survey provides a review of efficient inference
methods designed specifically for LRMs, focusing on mitigating token
inefficiency while preserving the reasoning quality. First, we introduce a
taxonomy to group the recent methods into two main categories: (a) explicit
compact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit
reasoning structure, and (b) implicit latent CoT, which encodes reasoning steps
within hidden representations instead of explicit tokens. Meanwhile, we discuss
their strengths and weaknesses. Then, we conduct empirical analyses on existing
methods from performance and efficiency aspects. Besides, we present open
challenges in this field, including human-centric controllable reasoning,
trade-off between interpretability and efficiency of reasoning, ensuring safety
of efficient reasoning, and broader applications of efficient reasoning. In
addition, we highlight key insights for enhancing LRMs' inference efficiency
via techniques such as model merging, new architectures, and agent routers. We
hope this work serves as a valuable guide, helping researchers overcome
challenges in this vibrant
field\footnote{https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Much is Enough? The Diminishing Returns of Tokenization Training
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20273v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20273v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varshini Reddy, Craig W. Schmidt, Yuval Pinter, Chris Tanner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tokenization, a crucial initial step in natural language processing, is
governed by several key parameters, such as the tokenization algorithm,
vocabulary size, pre-tokenization strategy, inference strategy, and training
data corpus. This paper investigates the impact of an often-overlooked
hyperparameter, tokenizer training data size. We train BPE, UnigramLM, and
WordPiece tokenizers across various vocabulary sizes using English training
data ranging from 1GB to 900GB. Our findings reveal diminishing returns as
training data size increases beyond roughly 150GB, suggesting a practical limit
to the improvements in tokenization quality achievable through additional data.
We analyze this phenomenon and attribute the saturation effect to constraints
introduced by the pre-tokenization stage. We then demonstrate the extent to
which these findings can generalize by experimenting on data in Russian, a
language typologically distant from English. For Russian text, we observe
diminishing returns after training a tokenizer from 200GB of data, which is
approximately 33% more than when training on English. These results provide
valuable insights for optimizing the tokenization process by reducing the
compute required for training on large corpora and suggest promising directions
for future research in tokenization algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unifying Uniform and Binary-coding Quantization for Accurate Compression
  of Large Language Models <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03781v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03781v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungcheol Park, Jeongin Bae, Beomseok Kwon, Minjun Kim, Byeongwook Kim, Se Jung Kwon, U Kang, Dongsoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we quantize large language models while preserving accuracy?
Quantization is essential for deploying large language models (LLMs)
efficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are
promising quantization schemes that have strong expressiveness and
optimizability, respectively. However, neither scheme leverages both
advantages. In this paper, we propose UniQuanF (Unified Quantization with
Flexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses
both strong expressiveness and optimizability by unifying the flexible mapping
technique in UQ and non-uniform quantization levels of BCQ. We propose unified
initialization, and local and periodic mapping techniques to optimize the
parameters in UniQuanF precisely. After optimization, our unification theorem
removes computational and memory overhead, allowing us to utilize the superior
accuracy of UniQuanF without extra deployment costs induced by the unification.
Experimental results demonstrate that UniQuanF outperforms existing UQ and BCQ
methods, achieving up to 4.60% higher accuracy on GSM8K benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Main Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Clinical Note Generation from Complex Doctor-Patient
  <span class="highlight-title">Conversation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14568v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14568v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhan Li, Sifan Wu, Christopher Smith, Thomas Lo, Bang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Writing clinical notes and documenting medical exams is a critical task for
healthcare professionals, serving as a vital component of patient care
documentation. However, manually writing these notes is time-consuming and can
impact the amount of time clinicians can spend on direct patient interaction
and other tasks. Consequently, the development of automated clinical note
generation systems has emerged as a clinically meaningful area of research
within AI for health. In this paper, we present three key contributions to the
field of clinical note generation using large language models (LLMs). First, we
introduce CliniKnote, a comprehensive dataset consisting of 1,200 complex
doctor-patient conversations paired with their full clinical notes. This
dataset, created and curated by medical experts with the help of modern neural
networks, provides a valuable resource for training and evaluating models in
clinical note generation tasks. Second, we propose the K-SOAP (Keyword,
Subjective, Objective, Assessment, and Plan) note format, which enhances
traditional SOAP~\cite{podder2023soap} (Subjective, Objective, Assessment, and
Plan) notes by adding a keyword section at the top, allowing for quick
identification of essential information. Third, we develop an automatic
pipeline to generate K-SOAP notes from doctor-patient conversations and
benchmark various modern LLMs using various metrics. Our results demonstrate
significant improvements in efficiency and performance compared to standard LLM
finetuning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Synthesizing Data for Context Attribution in Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gorjan Radevski, Kiril Gashteovski, Shahbaz Syed, Christopher Malon, Sebastien Nicolas, Chia-Chien Hung, Timo Sztyler, Verena Heußer, Wiem Ben Rim, Masafumi Enomoto, Kunihiro Takeoka, Masafumi Oyamada, Goran Glavaš, Carolin Lawrence
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question Answering (QA) accounts for a significant portion of LLM usage "in
the wild". However, LLMs sometimes produce false or misleading responses, also
known as "hallucinations". Therefore, grounding the generated answers in
contextually provided information -- i.e., providing evidence for the generated
text -- is paramount for LLMs' trustworthiness. Providing this information is
the task of context attribution. In this paper, we systematically study
LLM-based approaches for this task, namely we investigate (i) zero-shot
inference, (ii) LLM ensembling, and (iii) fine-tuning of small LMs on synthetic
data generated by larger LLMs. Our key contribution is SynQA: a novel
generative strategy for synthesizing context attribution data. Given selected
context sentences, an LLM generates QA pairs that are supported by these
sentences. This leverages LLMs' natural strengths in text generation while
ensuring clear attribution paths in the synthetic training data. We show that
the attribution data synthesized via SynQA is highly effective for fine-tuning
small LMs for context attribution in different QA tasks and domains. Finally,
with a user study, we validate the usefulness of small LMs (fine-tuned on
synthetic data from SynQA) in context attribution for QA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11130v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11130v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng-Kang Chou, Chan-Jan Hsu, Ho-Lam Chung, Liang-Hsuan Tseng, Hsi-Chun Cheng, Yu-Kuan Fu, Kuan Po Huang, Hung-Yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a self-refining framework that enhances ASR performance with only
unlabeled datasets. The process starts with an existing ASR model generating
pseudo-labels on unannotated speech, which are then used to train a
high-fidelity text-to-speech (TTS) system. Then, synthesized speech text pairs
are bootstrapped into the original ASR system, completing the closed-loop
self-improvement cycle. We demonstrated the effectiveness of the framework on
Taiwanese Mandarin speech. Leveraging 6,000 hours of unlabeled speech, a
moderate amount of text data, and synthetic content from the AI models, we
adapt Whisper-large-v2 into a specialized model, Twister. Twister reduces error
rates by up to 20% on Mandarin and 50% on Mandarin-English code-switching
benchmarks compared to Whisper. Results highlight the framework as a compelling
alternative to pseudo-labeling self-distillation approaches and provides a
practical pathway for improving ASR performance in low-resource or
domain-specific settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Investigation into Value Misalignment in LLM-Generated Texts for
  Cultural Heritage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02039v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02039v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Bu, Zheng Wang, Siyi Wang, Ziyao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) become increasingly prevalent in tasks
related to cultural heritage, such as generating descriptions of historical
monuments, translating ancient texts, preserving oral traditions, and creating
educational content, their ability to produce accurate and culturally aligned
texts is being increasingly relied upon by users and researchers. However,
cultural value misalignments may exist in generated texts, such as the
misrepresentation of historical facts, the erosion of cultural identity, and
the oversimplification of complex cultural narratives, which may lead to severe
consequences. Therefore, investigating value misalignment in the context of LLM
for cultural heritage is crucial for mitigating these risks, yet there has been
a significant lack of systematic and comprehensive study and investigation in
this area. To fill this gap, we systematically assess the reliability of LLMs
in generating culturally aligned texts for cultural heritage-related tasks. We
conduct a comprehensive evaluation by compiling an extensive set of 1066 query
tasks covering 5 widely recognized categories with 17 aspects within the
knowledge framework of cultural heritage across 5 open-source LLMs, and examine
both the type and rate of cultural value misalignments in the generated texts.
Using both automated and manual approaches, we effectively detect and analyze
the cultural value misalignments in LLM-generated texts. Our findings are
concerning: over 65% of the generated texts exhibit notable cultural
misalignments, with certain tasks demonstrating almost complete misalignment
with key cultural values. Beyond these findings, this paper introduces a
benchmark dataset and a comprehensive evaluation workflow that can serve as a
valuable resource for future research aimed at enhancing the cultural
sensitivity and reliability of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Experiential Semantic Information and Brain Alignment: Are Multimodal
  Models Better than Language Models? <span class="chip">CoNLL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.00942v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.00942v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Bavaresco, Raquel Fernández
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common assumption in Computational Linguistics is that text representations
learnt by multimodal models are richer and more human-like than those by
language-only models, as they are grounded in images or audio -- similar to how
human language is grounded in real-world experiences. However, empirical
studies checking whether this is true are largely lacking. We address this gap
by comparing word representations from contrastive multimodal models vs.
language-only ones in the extent to which they capture experiential information
-- as defined by an existing norm-based 'experiential model' -- and align with
human fMRI responses. Our results indicate that, surprisingly, language-only
models are superior to multimodal ones in both respects. Additionally, they
learn more unique brain-relevant semantic information beyond that shared with
the experiential model. Overall, our study highlights the need to develop
computational models that better integrate the complementary semantic
information provided by multimodal data sources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CoNLL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Idiosyncrasies in Large Language Models <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12150v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12150v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingjie Sun, Yida Yin, Zhiqiu Xu, J. Zico Kolter, Zhuang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we unveil and study idiosyncrasies in Large Language Models
(LLMs) -- unique patterns in their outputs that can be used to distinguish the
models. To do so, we consider a simple classification task: given a particular
text output, the objective is to predict the source LLM that generates the
text. We evaluate this synthetic task across various groups of LLMs and find
that simply fine-tuning text embedding models on LLM-generated texts yields
excellent classification accuracy. Notably, we achieve 97.1% accuracy on
held-out validation data in the five-way classification problem involving
ChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals
that these idiosyncrasies are rooted in word-level distributions. These
patterns persist even when the texts are rewritten, translated, or summarized
by an external LLM, suggesting that they are also encoded in the semantic
content. Additionally, we leverage LLM as judges to generate detailed,
open-ended descriptions of each model's idiosyncrasies. Finally, we discuss the
broader implications of our findings, including training on synthetic data,
inferring model similarity, and robust evaluation of LLMs. Code is available at
https://github.com/locuslab/llm-idiosyncrasies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICML 2025. Website at
  https://eric-mingjie.github.io/llm-idiosyncrasies/index.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EmoDynamiX: Emotional Support <span class="highlight-title">Dialogue</span> Strategy Prediction by Modelling
  MiXed Emotions and Dis<span class="highlight-title">course</span> Dynamics <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08782v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08782v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenwei Wan, Matthieu Labeau, Chloé Clavel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing emotionally intelligent conversational systems to provide comfort
and advice to people experiencing distress is a compelling area of research.
Recently, with advancements in large language models (LLMs), end-to-end
dialogue agents without explicit strategy prediction steps have become
prevalent. However, implicit strategy planning lacks transparency, and recent
studies show that LLMs' inherent preference bias towards certain
socio-emotional strategies hinders the delivery of high-quality emotional
support. To address this challenge, we propose decoupling strategy prediction
from language generation, and introduce a novel dialogue strategy prediction
framework, EmoDynamiX, which models the discourse dynamics between user
fine-grained emotions and system strategies using a heterogeneous graph for
better performance and transparency. Experimental results on two ESC datasets
show EmoDynamiX outperforms previous state-of-the-art methods with a
significant margin (better proficiency and lower preference bias). Our approach
also exhibits better transparency by allowing backtracing of decision making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 main, long paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a Cascaded LLM Framework for Cost-effective <span class="highlight-title">Human</span>-AI
  Decision-Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claudio Fanconi, Mihaela van der Schaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective human-AI decision-making balances three key factors: the
\textit{correctness} of predictions, the \textit{cost} of knowledge and
reasoning complexity, and the confidence about whether to \textit{abstain}
automated answers or involve human experts. In this work, we present a cascaded
LLM decision framework that adaptively delegates tasks across multiple tiers of
expertise -- a base model for initial candidate answers, a more capable and
knowledgeable (but costlier) large model, and a human expert for when the model
cascade abstains. Our method proceeds in two stages. First, a deferral policy
determines whether to accept the base model's answer or regenerate it with the
large model based on the confidence score. Second, an abstention policy decides
whether the cascade model response is sufficiently certain or requires human
intervention. Moreover, we incorporate an online learning mechanism in the
framework that can leverage human feedback to improve decision quality over
time. We demonstrate this approach to general question-answering (ARC-Easy and
ARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results
show that our cascaded strategy outperforms in most cases single-model
baselines in accuracy while reducing cost and providing a principled way to
handle abstentions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Affordable AI Assistants with Knowledge Graph of Thoughts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.02670v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.02670v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Besta, Lorenzo Paleari, Jia Hao Andrea Jiang, Robert Gerstenberger, You Wu, Jón Gunnar Hannesson, Patrick Iff, Ales Kubicek, Piotr Nyczyk, Diana Khimey, Nils Blach, Haiqiang Zhang, Tao Zhang, Peiran Ma, Grzegorz Kwaśniewski, Marcin Copik, Hubert Niewiadomski, Torsten Hoefler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are revolutionizing the development of AI
assistants capable of performing diverse tasks across domains. However, current
state-of-the-art LLM-driven agents face significant challenges, including high
operational costs and limited success rates on complex benchmarks like GAIA. To
address these issues, we propose Knowledge Graph of Thoughts (KGoT), an
innovative AI assistant architecture that integrates LLM reasoning with
dynamically constructed knowledge graphs (KGs). KGoT extracts and structures
task-relevant knowledge into a dynamic KG representation, iteratively enhanced
through external tools such as math solvers, web crawlers, and Python scripts.
Such structured representation of task-relevant knowledge enables low-cost
models to solve complex tasks effectively while also minimizing bias and noise.
For example, KGoT achieves a 29% improvement in task success rates on the GAIA
benchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,
harnessing a smaller model dramatically reduces operational costs by over 36x
compared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and
Deepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a
scalable, affordable, versatile, and high-performing solution for AI
assistants.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JEPA4Rec: Learning Effective Language Representations for Sequential
  Recommendation via Joint Embedding Predictive Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.10512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.10512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh-Anh Nguyen, Dung D. Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language representation learning has emerged as a promising approach for
sequential recommendation, thanks to its ability to learn generalizable
representations. However, despite its advantages, this approach still struggles
with data sparsity and a limited understanding of common-sense user
preferences. To address these limitations, we propose $\textbf{JEPA4Rec}$, a
framework that combines $\textbf{J}$oint $\textbf{E}$mbedding
$\textbf{P}$redictive $\textbf{A}$rchitecture with language modeling of item
textual descriptions. JEPA4Rec captures semantically rich and transferable
representations, improving recommendation performance and reducing reliance on
large-scale pre-training data. Specifically, JEPA4Rec represents items as text
sentences by flattening descriptive information such as $\textit{title,
category}$, and other attributes. To encode these sentences, we employ a
bidirectional Transformer encoder with modified embedding layers tailored for
capturing item information in recommendation datasets. We apply masking to text
sentences and use them to predict the representations of the unmasked
sentences, helping the model learn generalizable item embeddings. To further
improve recommendation performance and language understanding, we employ a
two-stage training strategy incorporating self-supervised learning losses.
Experiments on six real-world datasets demonstrate that JEPA4Rec consistently
outperforms state-of-the-art methods, particularly in cross-domain,
cross-platform, and low-resource scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Detection Fails: The Power of Fine-Tuned Models to Generate
  <span class="highlight-title">Human</span>-Like Social Media Text <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09975v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09975v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hillary Dawkins, Kathleen C. Fraser, Svetlana Kiritchenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting AI-generated text is a difficult problem to begin with; detecting
AI-generated text on social media is made even more difficult due to the short
text length and informal, idiosyncratic language of the internet. It is
nonetheless important to tackle this problem, as social media represents a
significant attack vector in online influence campaigns, which may be bolstered
through the use of mass-produced AI-generated posts supporting (or opposing)
particular policies, decisions, or events. We approach this problem with the
mindset and resources of a reasonably sophisticated threat actor, and create a
dataset of 505,159 AI-generated social media posts from a combination of
open-source, closed-source, and fine-tuned LLMs, covering 11 different
controversial topics. We show that while the posts can be detected under
typical research assumptions about knowledge of and access to the generating
models, under the more realistic assumption that an attacker will not release
their fine-tuned model to the public, detectability drops dramatically. This
result is confirmed with a human study. Ablation experiments highlight the
vulnerability of various detection algorithms to fine-tuned LLMs. This result
has implications across all detection domains, since fine-tuning is a generally
applicable and realistic LLM use case.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in ACL Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Euler to AI: Unifying Formulas for Mathematical Constants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17533v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17533v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomer Raz, Michael Shalyt, Elyasheev Leibtag, Rotem Kalisch, Shachar Weinbaum, Yaron Hadad, Ido Kaminer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The constant $\pi$ has fascinated scholars throughout the centuries,
inspiring numerous formulas for its evaluation, such as infinite sums and
continued fractions. Despite their individual significance, many of the
underlying connections among formulas remain unknown, missing unifying theories
that could unveil deeper understanding. The absence of a unifying theory
reflects a broader challenge across math and science: knowledge is typically
accumulated through isolated discoveries, while deeper connections often remain
hidden. In this work, we present an automated framework for the unification of
mathematical formulas. Our system combines large language models (LLMs) for
systematic formula harvesting, an LLM-code feedback loop for validation, and a
novel symbolic algorithm for clustering and eventual unification. We
demonstrate this methodology on the hallmark case of $\pi$, an ideal testing
ground for symbolic unification. Applying this approach to 455,050 arXiv
papers, we validate 407 distinct formulas for $\pi$ and prove relations between
381 (94%) of them, of which 188 (46%) can be derived from a single mathematical
object$\unicode{x2014}$linking canonical formulas by Euler, Gauss, Brouncker,
and newer ones from algorithmic discoveries by the Ramanujan Machine. Our
method generalizes to other constants, including $e$, $\zeta(3)$, and Catalan's
constant, demonstrating the potential of AI-assisted mathematics to uncover
hidden structures and unify knowledge across domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>60 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regular-pattern-sensitive CRFs for Distant Label Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12484v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12484v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sean Papay, Roman Klinger, Sebastian Pado
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While LLMs have grown popular in sequence labeling, linear-chain conditional
random fields (CRFs) remain a popular alternative with the ability to directly
model interactions between labels. However, the Markov assumption limits them
to % only directly modeling interactions between adjacent labels. Weighted
finite-state transducers (FSTs), in contrast, can model distant label--label
interactions, but exact label inference is intractable in general. In this
work, we present regular-pattern-sensitive CRFs (RPCRFs), a method of enriching
standard linear-chain CRFs with the ability to learn long-distance label
interactions through user-specified patterns. This approach allows users to
write regular-expression label patterns concisely specifying which types of
interactions the model should take into account, allowing the model to learn
from data whether and in which contexts these patterns occur. The result can be
interpreted alternatively as a CRF augmented with additional, non-local
potentials, or as a finite-state transducer whose structure is defined by a set
of easily-interpretable patterns. Critically, exact training and inference are
tractable for many pattern sets. We detail how an RPCRF can be automatically
constructed from a set of user-specified patterns, and demonstrate the model's
effectiveness on a sequence of three synthetic sequence modeling datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMCTS: A Constrained Monte Carlo Tree Search Framework for Mathematical
  Reasoning in Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11169v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11169v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingwen Lin, Boyan Xu, Guimin Hu, Zijian Li, Zhifeng Hao, Keli Zhang, Ruichu Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the Constrained Monte Carlo Tree Search (CMCTS)
framework to enhance the mathematical reasoning capabilities of Large Language
Models (LLM). By incorporating a constrained action space, Process Reward Model
(PRM), and partial order rules, CMCTS effectively addresses the limitations of
existing MCTS methods in terms of state space diversity and action selection
rationality. Specifically, during the expansion phase, CMCTS restricts action
sampling to a predefined constrained action set to increase candidate state
diversity. In the simulation phase, it introduces partial order rules and PRM
to optimize action selection and prevent unreasonable state transitions.
Experimental results show that CMCTS performs outstandingly across multiple
mathematical reasoning benchmarks. Under a zero-shot setting, a 7B-parameter
model achieves an average accuracy of 83.4\%, surpassing the 72B baseline model
by 4.8\%. Ablation studies demonstrate that each component of the framework is
crucial for performance improvement, and their combined use fully leverages
their respective strengths. Overall, the CMCTS framework provides an effective
approach to enhancing LLM mathematical reasoning capabilities, supported by
theoretical analysis, and offers novel insights for future reasoning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Truth Knows No Language: Evaluating Truthfulness Beyond English 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09387v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09387v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Blanca Calvo Figueras, Eneko Sagarzazu, Julen Etxaniz, Jeremy Barnes, Pablo Gamallo, Iria De Dios Flores, Rodrigo Agerri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a professionally translated extension of the TruthfulQA
benchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and
Spanish. Truthfulness evaluations of large language models (LLMs) have
primarily been conducted in English. However, the ability of LLMs to maintain
truthfulness across languages remains under-explored. Our study evaluates 12
state-of-the-art open LLMs, comparing base and instruction-tuned models using
human evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our
findings reveal that, while LLMs perform best in English and worst in Basque
(the lowest-resourced language), overall truthfulness discrepancies across
languages are smaller than anticipated. Furthermore, we show that
LLM-as-a-Judge correlates more closely with human judgments than
multiple-choice metrics, and that informativeness plays a critical role in
truthfulness assessment. Our results also indicate that machine translation
provides a viable approach for extending truthfulness benchmarks to additional
languages, offering a scalable alternative to professional translation.
Finally, we observe that universal knowledge questions are better handled
across languages than context- and time-dependent ones, highlighting the need
for truthfulness evaluations that account for cultural and temporal
variability. Dataset and code are publicly available under open licenses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Much Can We Forget about Data Contamination? <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03249v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03249v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Bordt, Suraj Srinivas, Valentyn Boreiko, Ulrike von Luxburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The leakage of benchmark data into the training data has emerged as a
significant challenge for evaluating the capabilities of large language models
(LLMs). In this work, we challenge the common assumption that small-scale
contamination renders benchmark evaluations invalid. First, we experimentally
quantify the magnitude of benchmark overfitting based on scaling along three
dimensions: The number of model parameters (up to 1.6B), the number of times an
example is seen (up to 144), and the number of training tokens (up to 40B). If
model and data follow the Chinchilla scaling laws, minor contamination indeed
leads to overfitting. At the same time, even 144 times of contamination can be
forgotten if the training data is scaled beyond five times Chinchilla, a regime
characteristic of many modern LLMs. Continual pre-training of OLMo-7B
corroborates these results. Next, we study the impact of the weight decay
parameter on example forgetting, showing that empirical forgetting occurs
faster than the cumulative weight decay. This allows us to gauge the degree of
example forgetting in large-scale training runs, indicating that many LLMs,
including Lllama 3 405B, have forgotten the data seen at the beginning of
training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Remarkable Robustness of LLMs: Stages of Inference? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19384v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19384v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vedang Lad, Jin Hwa Lee, Wes Gurnee, Max Tegmark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the robustness of Large Language Models (LLMs) to structural
interventions by deleting and swapping adjacent layers during inference.
Surprisingly, models retain 72-95% of their original top-1 prediction accuracy
without any fine-tuning. We find that performance degradation is not uniform
across layers: interventions to the early and final layers cause the most
degradation, while the model is remarkably robust to dropping middle layers.
This pattern of localized sensitivity motivates our hypothesis of four stages
of inference, observed across diverse model families and sizes: (1)
detokenization, where local context is integrated to lift raw token embeddings
into higher-level representations; (2) feature engineering, where task- and
entity-specific features are iteratively refined; (3) prediction ensembling,
where hidden states are aggregated into plausible next-token predictions; and
(4) residual sharpening, where irrelevant features are suppressed to finalize
the output distribution. Synthesizing behavioral and mechanistic evidence, we
provide a framework for interpreting depth-dependent computations in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For Github code see
  https://github.com/vdlad/Remarkable-Robustness-of-LLMs. Send all
  correspondence to the first author</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EffiCoder: Enhancing Code Generation in Large Language Models through
  Efficiency-Aware Fine-tuning <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10209v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10209v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Huang, Guangtao Zeng, Jianbo Dai, Meng Luo, Han Weng, Yuhao Qing, Heming Cui, Zhijiang Guo, Jie M. Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) play an increasingly important role in code
generation, enhancing both correctness and efficiency has become crucial.
Current methods primarily focus on correctness, often overlooking efficiency.
To address this gap, we introduce EffiCoder to improve both aspects by
fine-tuning LLMs on a high-quality dataset comprising correct and efficient
code samples. Our methodology involves leveraging multiple LLMs to generate
diverse candidate code solutions for various tasks across different programming
languages. We then evaluate these solutions by measuring their execution time
and memory usage through local execution. The code solution with the lowest
execution time and memory consumption is selected as the final output for each
task. Experimental results demonstrate significant improvements when
fine-tuning with Effi-Instruct. For instance, Qwen2.5-Coder-7B-Instruct's
pass@1 score increases from 44.8\% to 57.7\%, while the average execution time
for correct tasks decreases by 48.4\%. EffiCoder offers a scalable and
effective solution for advancing AI-driven code generation, benefiting software
development and computational problem-solving. The source code of Effi-Code was
released at https://github.com/huangd1999/EffiCoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Making LLMs Better Many-to-Many Speech-to-Text Translators with
  Curriculum Learning <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19510v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19510v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yexing Du, Youcheng Pan, Ziyang Ma, Bo Yang, Yifan Yang, Keqi Deng, Xie Chen, Yang Xiang, Ming Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have achieved significant success in
Speech-to-Text Translation (S2TT) tasks. While most existing research has
focused on English-centric translation directions, the exploration of
many-to-many translation is still limited by the scarcity of parallel data. To
address this, we propose a three-stage curriculum learning strategy that
leverages the machine translation capabilities of large language models and
adapts them to S2TT tasks, enabling effective learning in low-resource
settings. We trained MLLMs with varying parameter sizes (3B, 7B, and 32B) and
evaluated the proposed strategy using the FLEURS and CoVoST-2 datasets.
Experimental results show that the proposed strategy achieves state-of-the-art
average performance in $15\times14$ language pairs, requiring fewer than 10
hours of speech data per language to achieve competitive results. The source
code and models are released at https://github.com/yxduir/LLM-SRT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACL 2025 (Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ G-<span class="highlight-title">Memory</span>: Tracing Hierarchical <span class="highlight-title">Memory</span> for <span class="highlight-title">Multi-Agent</span> Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07398v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07398v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guibin Zhang, Muxin Fu, Guancheng Wan, Miao Yu, Kun Wang, Shuicheng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM)-powered multi-agent systems (MAS) have
demonstrated cognitive and execution capabilities that far exceed those of
single LLM agents, yet their capacity for self-evolution remains hampered by
underdeveloped memory architectures. Upon close inspection, we are alarmed to
discover that prevailing MAS memory mechanisms (1) are overly simplistic,
completely disregarding the nuanced inter-agent collaboration trajectories, and
(2) lack cross-trial and agent-specific customization, in stark contrast to the
expressive memory developed for single agents. To bridge this gap, we introduce
G-Memory, a hierarchical, agentic memory system for MAS inspired by
organizational memory theory, which manages the lengthy MAS interaction via a
three-tier graph hierarchy: insight, query, and interaction graphs. Upon
receiving a new user query, G-Memory performs bi-directional memory traversal
to retrieve both $\textit{high-level, generalizable insights}$ that enable the
system to leverage cross-trial knowledge, and $\textit{fine-grained, condensed
interaction trajectories}$ that compactly encode prior collaboration
experiences. Upon task execution, the entire hierarchy evolves by assimilating
new collaborative trajectories, nurturing the progressive evolution of agent
teams. Extensive experiments across five benchmarks, three LLM backbones, and
three popular MAS frameworks demonstrate that G-Memory improves success rates
in embodied action and accuracy in knowledge QA by up to $20.89\%$ and
$10.12\%$, respectively, without any modifications to the original frameworks.
Our codes are available at https://github.com/bingreeky/GMemory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging LLM and Self-Supervised Training Models for Speech
  Recognition in Chinese Dialects: A Comparative Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.21138v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.21138v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Xu, Hongjie Chen, Wang Qing, Lv Hang, Jian Kang, Li Jie, Zhennan Lin, Yongxiang Li, Xie Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale training corpora have significantly improved the performance of
ASR models. Unfortunately, due to the relative scarcity of data, Chinese
accents and dialects remain a challenge for most ASR models. Recent
advancements in self-supervised learning have shown that self-supervised
pre-training, combined with large language models (LLM), can effectively
enhance ASR performance in low-resource scenarios. We aim to investigate the
effectiveness of this paradigm for Chinese dialects. Specifically, we pre-train
a Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech
data and do alignment training on a supervised dataset of 40,000 hours. Then,
we systematically examine the impact of various projectors and LLMs on
Mandarin, dialect, and accented speech recognition performance under this
paradigm. Our method achieved SOTA results on multiple dialect datasets,
including Kespeech. We will open-source our work to promote reproducible
research
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast-and-Frugal Text-Graph Transformers are Effective Link Predictors <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06778v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06778v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrei C. Coman, Christos Theodoropoulos, Marie-Francine Moens, James Henderson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Fast-and-Frugal Text-Graph (FnF-TG) Transformers, a
Transformer-based framework that unifies textual and structural information for
inductive link prediction in text-attributed knowledge graphs. We demonstrate
that, by effectively encoding ego-graphs (1-hop neighbourhoods), we can reduce
the reliance on resource-intensive textual encoders. This makes the model both
fast at training and inference time, as well as frugal in terms of cost. We
perform a comprehensive evaluation on three popular datasets and show that
FnF-TG can achieve superior performance compared to previous state-of-the-art
methods. We also extend inductive learning to a fully inductive setting, where
relations don't rely on transductive (fixed) representations, as in previous
work, but are a function of their textual description. Additionally, we
introduce new variants of existing datasets, specifically designed to test the
performance of models on unseen relations at inference time, thus offering a
new test-bench for fully inductive link prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Team Anotheroption at SemEval-2025 Task 8: Bridging the Gap Between
  Open-Source and Proprietary LLMs in Table QA <span class="chip">SemEval-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolas Evkarpidi, Elena Tutubalina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a system developed for SemEval 2025 Task 8: Question
Answering (QA) over tabular data. Our approach integrates several key
components: text-to-SQL and text-to-code generation modules, a self-correction
mechanism, and a retrieval-augmented generation (RAG). Additionally, it
includes an end-to-end (E2E) module, all orchestrated by a large language model
(LLM). Through ablation studies, we analyzed the effects of different parts of
our pipeline and identified the challenges that are still present in this
field. During the evaluation phase of the competition, our solution achieved an
accuracy of 80%, resulting in a top-13 ranking among the 38 participating
teams. Our pipeline demonstrates a significant improvement in accuracy for
open-source models and achieves a performance comparable to proprietary LLMs in
QA tasks over tables. The code is available at GitHub repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 19th International Workshop on
  Semantic Evaluation (SemEval-2025), to be held in conjunction with ACL 2025.
  15 pages, 5 figures; full paper title was added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incentivizing Reasoning for Advanced Instruction-Following of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01413v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01413v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulei Qin, Gang Li, Zongyi Li, Zihan Xu, Yuchen Shi, Zhekai Lin, Xiao Cui, Ke Li, Xing Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing large language models (LLMs) face challenges of following complex
instructions, especially when multiple constraints are present and organized in
paralleling, chaining, and branching structures. One intuitive solution, namely
chain-of-thought (CoT), is expected to universally improve capabilities of
LLMs. However, we find that the vanilla CoT exerts a negative impact on
performance due to its superficial reasoning pattern of simply paraphrasing the
instructions. It fails to peel back the compositions of constraints for
identifying their relationship across hierarchies of types and dimensions. To
this end, we propose a systematic method to boost LLMs in dealing with complex
instructions via incentivizing reasoning for test-time compute scaling. First,
we stem from the decomposition of complex instructions under existing
taxonomies and propose a reproducible data acquisition method. Second, we
exploit reinforcement learning (RL) with verifiable rule-centric reward signals
to cultivate reasoning specifically for instruction following. We address the
shallow, non-essential nature of reasoning under complex instructions via
sample-wise contrast for superior CoT enforcement. We also exploit behavior
cloning of experts to facilitate steady distribution shift from fast-thinking
LLMs to skillful reasoners. Extensive evaluations on seven comprehensive
benchmarks confirm the validity of the proposed method, where a 1.5B LLM
achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data
will be available later (under review).
  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction
following, complex instructions
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages of main body, 3 tables, 5 figures, 45 pages of appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VGR: Visual Grounded Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11991v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11991v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacong Wang, Zijian Kang, Haochen Wang, Haiyong Jiang, Jiawen Li, Bohong Wu, Ya Wang, Jiao Ran, Xiao Liang, Chao Feng, Jun Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of multimodal chain-of-thought (CoT) reasoning, existing
approaches predominantly rely on reasoning on pure language space, which
inherently suffers from language bias and is largely confined to math or
science domains. This narrow focus limits their ability to handle complex
visual reasoning tasks that demand comprehensive understanding of image
details. To address these limitations, this paper introduces VGR, a novel
reasoning multimodal large language model (MLLM) with enhanced fine-grained
visual perception capabilities. Unlike traditional MLLMs that answer the
question or reasoning solely on the language space, our VGR first detects
relevant regions that may help to solve problems, and then provides precise
answers based on replayed image regions. To achieve this, we conduct a
large-scale SFT dataset called VGR -SFT that contains reasoning data with mixed
vision grounding and language deduction. The inference pipeline of VGR allows
the model to choose bounding boxes for visual reference and a replay stage is
introduced to integrates the corresponding regions into the reasoning process,
enhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline
show that VGR achieves superior performance on multi-modal benchmarks requiring
comprehensive image detail understanding. Compared to the baseline, VGR uses
only 30\% of the image token count while delivering scores of +4.1 on MMStar,
+7.1 on AI2D, and a +12.9 improvement on ChartQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Training-free LLM-based Approach to General Chinese Character Error
  Correction <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15266v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15266v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Houquan Zhou, Bo Zhang, Zhenghua Li, Ming Yan, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chinese spelling correction (CSC) is a crucial task that aims to correct
character errors in Chinese text. While conventional CSC focuses on character
substitution errors caused by mistyping, two other common types of character
errors, missing and redundant characters, have received less attention. These
errors are often excluded from CSC datasets during the annotation process or
ignored during evaluation, even when they have been annotated. This issue
limits the practicality of the CSC task. To address this issue, we introduce
the task of General Chinese Character Error Correction (C2EC), which focuses on
all three types of character errors. We construct a high-quality C2EC benchmark
by combining and manually verifying data from CCTC and Lemon datasets. We
extend the training-free prompt-free CSC method to C2EC by using Levenshtein
distance for handling length changes and leveraging an additional prompt-based
large language model (LLM) to improve performance. Experiments show that our
method enables a 14B-parameter LLM to be on par with models nearly 50 times
larger on both conventional CSC and C2EC tasks, without any fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Main Conference of ACL 2025, 26 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Temperature for Language Models with Multi-Sample Inference <span class="chip">ICML2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05234v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05234v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihua Du, Yiming Yang, Sean Welleck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-sample aggregation strategies, such as majority voting and best-of-N
sampling, are widely used in contemporary large language models (LLMs) to
enhance predictive accuracy across various tasks. A key challenge in this
process is temperature selection, which significantly impacts model
performance. Existing approaches either rely on a fixed default temperature or
require labeled validation data for tuning, which are often scarce and
difficult to obtain. This paper addresses the challenge of automatically
identifying the (near)-optimal temperature for different LLMs using
multi-sample aggregation strategies, without relying on task-specific
validation data. We provide a comprehensive analysis of temperature's role in
performance optimization, considering variations in model architectures,
datasets, task types, model sizes, and predictive accuracy. Furthermore, we
propose a novel entropy-based metric for automated temperature optimization,
which consistently outperforms fixed-temperature baselines. Additionally, we
incorporate a stochastic process model to enhance interpretability, offering
deeper insights into the relationship between temperature and model
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML2025, 21 pages. Code available at
  https://github.com/StigLidu/TURN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InfiniSST: Simultaneous Translation of Unbounded Speech with Large
  Language Model <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqi Ouyang, Xi Xu, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous translation of unbounded streaming speech remains a challenging
problem due to the need for effectively processing the history speech context
and past translations so that quality and latency, including computation
overhead, can be balanced. Most prior works assume pre-segmented speech,
limiting their real-world applicability. In this paper, we propose InfiniSST, a
novel approach that formulates SST as a multi-turn dialogue task, enabling
seamless translation of unbounded speech. We construct translation trajectories
and robust segments from MuST-C with multi-latency augmentation during training
and develop a key-value (KV) cache management strategy to facilitate efficient
inference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that
InfiniSST reduces computation-aware latency by 0.5 to 1 second while
maintaining the same translation quality compared to baselines. Ablation
studies further validate the contributions of our data construction and cache
management strategy. We release the code and demo at
https://github.com/LeiLiLab/InfiniSST
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reflec<span class="highlight-title">Tool</span>: Towards Reflection-Aware <span class="highlight-title">Tool</span>-Augmented Clinical <span class="highlight-title">Agent</span>s <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17657v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17657v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusheng Liao, Shuyang Jiang, Yanfeng Wang, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown promising potential in the medical
domain, assisting with tasks like clinical note generation and patient
communication. However, current LLMs are limited to text-based communication,
hindering their ability to interact with diverse forms of information in
clinical environments. Despite clinical agents succeeding in diverse signal
interaction, they are oriented to a single clinical scenario and hence fail for
broader applications. To evaluate clinical agents holistically, we propose
ClinicalAgent Bench~(CAB), a comprehensive medical agent benchmark consisting
of 18 tasks across five key realistic clinical dimensions. Building on this, we
introduce ReflecTool, a novel framework that excels at utilizing
domain-specific tools within two stages. The first optimization stage
progressively enlarges a long-term memory by saving successful solving
processes and tool-wise experience of agents in a tiny pre-defined training
set. In the following inference stage, ReflecTool can search for supportive
successful demonstrations from already built long-term memory to guide the tool
selection strategy, and a verifier improves the tool usage according to the
tool-wise experience with two verification methods--iterative refinement and
candidate selection. Extensive experiments on ClinicalAgent Benchmark
demonstrate that ReflecTool surpasses the pure LLMs with more than 10 points
and the well-established agent-based methods with 3 points, highlighting its
adaptability and effectiveness in solving complex clinical tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Main Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Step-by-step Instructions and a Simple Tabular Output Format Improve the
  Dependency Parsing Accuracy of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09983v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09983v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiroshi Matsuda, Chunpeng Ma, Masayuki Asahara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) have enabled impressive
performance in various tasks. However, standard prompting often struggles to
produce structurally valid and accurate outputs, especially in dependency
parsing. We propose a novel step-by-step instruction strategy, where universal
part-of-speech tagging precedes the prediction of syntactic heads and
dependency labels, and a simplified CoNLL-U like output format, our method
achieves state-of-the-art accuracy on Universal Dependencies datasets across 17
languages without hallucination or contamination. We further show that
multilingual fine-tuning simultaneously improves cross-language generalization
performance. Our results highlight the effectiveness of explicit reasoning
steps in LLM-based parsing and offer a scalable, format-consistent alternative
to bracket-based approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, accepted to SyntaxFest 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MathFusion: Enhancing Mathematical Problem-solving of LLM through
  Instruction Fusion <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16212v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16212v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qizhi Pei, Lijun Wu, Zhuoshi Pan, Yu Li, Honglin Lin, Chenlin Ming, Xin Gao, Conghui He, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown impressive progress in mathematical
reasoning. While data augmentation is promising to enhance mathematical
problem-solving ability, current approaches are predominantly limited to
instance-level modifications-such as rephrasing or generating syntactic
variations-which fail to capture and leverage the intrinsic relational
structures inherent in mathematical knowledge. Inspired by human learning
processes, where mathematical proficiency develops through systematic exposure
to interconnected concepts, we introduce MathFusion, a novel framework that
enhances mathematical reasoning through cross-problem instruction synthesis.
MathFusion implements this through three fusion strategies: (1) sequential
fusion, which chains related problems to model solution dependencies; (2)
parallel fusion, which combines analogous problems to reinforce conceptual
understanding; and (3) conditional fusion, which creates context-aware
selective problems to enhance reasoning flexibility. By applying these
strategies, we generate a new dataset, \textbf{MathFusionQA}, followed by
fine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental
results demonstrate that MathFusion achieves substantial improvements in
mathematical reasoning while maintaining high data efficiency, boosting
performance by 18.0 points in accuracy across diverse benchmarks while
requiring only 45K additional synthetic instructions, representing a
substantial improvement over traditional single-instruction approaches. Our
datasets, models, and code are publicly available at
https://github.com/QizhiPei/mathfusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025 (main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hybrid GA LLM Framework for Structured Task Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07483v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07483v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Shum, Rachel Chan, Jonas Lin, Benny Feng, Patrick Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GA LLM is a hybrid framework that combines Genetic Algorithms with Large
Language Models to handle structured generation tasks under strict constraints.
Each output, such as a plan or report, is treated as a gene, and evolutionary
operations like selection, crossover, and mutation are guided by the language
model to iteratively improve solutions. The language model provides domain
knowledge and creative variation, while the genetic algorithm ensures
structural integrity and global optimization. GA LLM has proven effective in
tasks such as itinerary planning, academic outlining, and business reporting,
consistently producing well structured and requirement satisfying results. Its
modular design also makes it easy to adapt to new tasks. Compared to using a
language model alone, GA LLM achieves better constraint satisfaction and higher
quality solutions by combining the strengths of both components.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ POROver: Improving Safety and Reducing Overrefusal in Large Language
  Models with Overgeneration and Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12999v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12999v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Batuhan K. Karaman, Ishmam Zabir, Alon Benhaim, Vishrav Chaudhary, Mert R. Sabuncu, Xia Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving both high safety and high usefulness simultaneously in large
language models has become a critical challenge in recent years.Models often
exhibit unsafe behavior or adopt an overly cautious approach leading to
frequent overrefusal of benign prompts, which reduces their usefulness. A major
factor underlying these behaviors is how the models are finetuned and aligned,
particularly the nature and extent of the data used.In this work, we examine
how overgenerating finetuning data with advanced teacher models (e.g.,
GPT-4o)-covering both general-purpose and toxic prompts-affects safety and
usefulness in instruction-following language models.Additionally, we present
POROver, an alignment strategy designed for models that are highly safe but
prone to overrefusal. POROver employs preference optimization algorithms and
leverages completions from an advanced teacher model to reduce overrefusals
while maintaining safety.Our results show that overgenerating completions for
general-purpose prompts significantly boosts safety with only a minimal impact
on usefulness. Specifically, the F1 score calculated between safety and
usefulness increases from 74.4% to 91.8% because of a substantial rise in
safety. Moreover, overgeneration for toxic prompts raises usefulness from 11.1%
to 57.6% while preserving safety. Finally, applying POROVer increases
usefulness further-from 57.6% to 82.1%-while keeping safety at comparable
levels. Our data and code are available at
https://github.com/batuhankmkaraman/POROver.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Laws for Upcycling Mixture-of-Experts Language Models <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seng Pei Liew, Takuya Kato, Sho Takase
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretraining large language models (LLMs) is resource-intensive, often
requiring months of training time even with high-end GPU clusters. There are
two approaches of mitigating such computational demands: reusing smaller models
to train larger ones (upcycling), and training computationally efficient models
like mixture-of-experts (MoE). In this paper, we study the upcycling of LLMs to
MoE models, of which the scaling behavior remains underexplored. Through
extensive experiments, we identify empirical scaling laws that describe how
performance depends on dataset size and model configuration. Particularly, we
show that, while scaling these factors improves performance, there is a novel
interaction term between the dense and upcycled training dataset that limits
the efficiency of upcycling at large computational budgets. Based on these
findings, we provide guidance to scale upcycling, and establish conditions
under which upcycling outperforms from-scratch trainings within budget
constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025. 16 figures, 8 tables. Code available at
  https://github.com/sbintuitions/sparse-upcycling-scaling-laws</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeedleInATable: Exploring Long-Context Capability of Large Language
  Models towards Long-Structured Tables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.06560v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.06560v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lanrui Wang, Mingyu Zheng, Hongyin Tang, Zheng Lin, Yanan Cao, Jingang Wang, Xunliang Cai, Weiping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Processing structured tabular data, particularly large and lengthy tables,
constitutes a fundamental yet challenging task for large language models
(LLMs). However, existing long-context benchmarks like Needle-in-a-Haystack
primarily focus on unstructured text, neglecting the challenge of diverse
structured tables. Meanwhile, previous tabular benchmarks mainly consider
downstream tasks that require high-level reasoning abilities, and overlook
models' underlying fine-grained perception of individual table cells, which is
crucial for practical and robust LLM-based table applications. To address this
gap, we introduce \textsc{NeedleInATable} (NIAT), a new long-context tabular
benchmark that treats each table cell as a ``needle'' and requires models to
extract the target cell based on cell locations or lookup questions. Our
comprehensive evaluation of various LLMs and multimodal LLMs reveals a
substantial performance gap between popular downstream tabular tasks and the
simpler NIAT task, suggesting that they may rely on dataset-specific
correlations or shortcuts to obtain better benchmark results but lack truly
robust long-context understanding towards structured tables. Furthermore, we
demonstrate that using synthesized NIAT training data can effectively improve
performance on both NIAT task and downstream tabular tasks, which validates the
importance of NIAT capability for LLMs' genuine table understanding ability.
Our data, code and models will be released to facilitate future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ask Optimal Questions: Aligning Large Language Models with Retriever's
  Preference in <span class="highlight-title">Conversation</span> <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11827v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11827v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanwoong Yoon, Gangwoo Kim, Byeongguk Jeon, Sungdong Kim, Yohan Jo, Jaewoo Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational search, unlike single-turn retrieval tasks, requires
understanding the current question within a dialogue context. The common
approach of rewrite-then-retrieve aims to decontextualize questions to be
self-sufficient for off-the-shelf retrievers, but most existing methods produce
sub-optimal query rewrites due to the limited ability to incorporate signals
from the retrieval results. To overcome this limitation, we present a novel
framework RetPO (Retriever's Preference Optimization), which is designed to
optimize a language model (LM) for reformulating search queries in line with
the preferences of the target retrieval systems. The process begins by
prompting a large LM to produce various potential rewrites and then collects
retrieval performance for these rewrites as the retrievers' preferences.
Through the process, we construct a large-scale dataset called RF collection,
containing Retrievers' Feedback on over 410K query rewrites across 12K
conversations. Furthermore, we fine-tune a smaller LM on this dataset to align
it with the retrievers' feedback. Our resulting model demonstrates superiority
on two benchmarks, surpassing the previous state-of-the-art performance of
rewrite-then-retrieve approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 (findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM
  Reasoning via Autoregressive Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02508v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02508v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory Wornell, Subhro Das, David Cox, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable reasoning
capabilities across diverse domains. Recent studies have shown that increasing
test-time computation enhances LLMs' reasoning capabilities. This typically
involves extensive sampling at inference time guided by an external LLM
verifier, resulting in a two-player system. Despite external guidance, the
effectiveness of this system demonstrates the potential of a single LLM to
tackle complex tasks. Thus, we pose a new research problem: Can we internalize
the searching capabilities to fundamentally enhance the reasoning abilities of
a single LLM? This work explores an orthogonal direction focusing on
post-training LLMs for autoregressive searching (i.e., an extended reasoning
process with self-reflection and self-exploration of new strategies). To
achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a
two-stage training paradigm: 1) a small-scale format tuning stage to
internalize the COAT reasoning format and 2) a large-scale self-improvement
stage leveraging reinforcement learning. Our approach results in Satori, a 7B
LLM trained on open-source models and data. Extensive empirical evaluations
demonstrate that Satori achieves state-of-the-art performance on mathematical
reasoning benchmarks while exhibits strong generalization to out-of-domain
tasks. Code, data, and models are fully open-sourced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Agent</span>Court: <span class="highlight-title">Simulat</span>ing Court with Adversarial Evolvable Lawyer <span class="highlight-title">Agent</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08089v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08089v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guhong Chen, Liyang Fan, Zihan Gong, Nan Xie, Zixuan Li, Ziqiang Liu, Chengming Li, Qiang Qu, Hamid Alinejad-Rokny, Shiwen Ni, Min Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current research in LLM-based simulation systems lacks comprehensive
solutions for modeling real-world court proceedings, while existing legal
language models struggle with dynamic courtroom interactions. We present
AgentCourt, a comprehensive legal simulation framework that addresses these
challenges through adversarial evolution of LLM-based agents. Our AgentCourt
introduces a new adversarial evolutionary approach for agents called AdvEvol,
which performs dynamic knowledge learning and evolution through structured
adversarial interactions in a simulated courtroom program, breaking the
limitations of the traditional reliance on static knowledge bases or manual
annotations. By simulating 1,000 civil cases, we construct an evolving
knowledge base that enhances the agents' legal reasoning abilities. The evolved
lawyer agents demonstrated outstanding performance on our newly introduced
CourtBench benchmark, achieving a 12.1% improvement in performance compared to
the original lawyer agents. Evaluations by professional lawyers confirm the
effectiveness of our approach across three critical dimensions: cognitive
agility, professional knowledge, and logical rigor. Beyond outperforming
specialized legal models in interactive reasoning tasks, our findings emphasize
the importance of adversarial learning in legal AI and suggest promising
directions for extending simulation-based legal reasoning to broader judicial
and regulatory contexts. The project's code is available at:
https://github.com/relic-yuexi/AgentCourt
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Representational Learning of Foundation Models for
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11999v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11999v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheli Zhou, Chenxu Zhu, Jianghao Lin, Bo Chen, Ruiming Tang, Weinan Zhang, Yong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing a single foundation model with the capability to excel across
diverse tasks has been a long-standing objective in the field of artificial
intelligence. As the wave of general-purpose foundation models sweeps across
various domains, their influence has significantly extended to the field of
recommendation systems. While recent efforts have explored recommendation
foundation models for various generative tasks, they often overlook crucial
embedding tasks and struggle with the complexities of multi-task learning,
including knowledge sharing & conflict resolution, and convergence speed
inconsistencies. To address these limitations, we introduce RecFound, a
generative representational learning framework for recommendation foundation
models. We construct the first comprehensive dataset for recommendation
foundation models covering both generative and embedding tasks across diverse
scenarios. Based on this dataset, we propose a novel multi-task training scheme
featuring a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge
sharing & conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched)
to address inconsistent convergence, and a Model Merge module to balance the
performance across tasks. Experiments demonstrate that RecFound achieves
state-of-the-art performance across various recommendation tasks, outperforming
existing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page is available at https://junkfood436.github.io/RecFound/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Multi-Head Attention for Small Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09342v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09342v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sushant Mehta, Raj Dandekar, Rajat Dandekar, Sreedath Panat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the first comprehensive study of latent multi-head attention (MLA)
for small language models, revealing interesting efficiency-quality trade-offs.
Training 30M-parameter GPT models on 100,000 synthetic stories, we benchmark
three architectural variants: standard multi-head attention (MHA), MLA, and MLA
with rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE
with half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory
reduction while incurring only a 0.3% increase in validation loss (essentially
matching MHA quality)- a Pareto improvement for memory constrained deployment.
We further show that RoPE is crucial for MLA in small models: without it, MLA
underperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by
2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2
achieves a 1.4 times speedup over full-rank MLA while maintaining the memory
savings. GPT-4 evaluations corroborate perplexity results, with ours achieving
the highest quality scores (7.4/10) across grammar, creativity, and consistency
metrics. Code and models will be released upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure. 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Graph Large Language Model (KG-LLM) for Link Prediction <span class="chip">ACML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07311v9">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07311v9.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Shu, Tianle Chen, Mingyu Jin, Chong Zhang, Mengnan Du, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of multi-hop link prediction within knowledge graphs (KGs) stands as
a challenge in the field of knowledge graph analysis, as it requires the model
to reason through and understand all intermediate connections before making a
prediction. In this paper, we introduce the Knowledge Graph Large Language
Model (KG-LLM), a novel framework that leverages large language models (LLMs)
for knowledge graph tasks. We first convert structured knowledge graph data
into natural language and then use these natural language prompts to fine-tune
LLMs to enhance multi-hop link prediction in KGs. By converting the KG to
natural language prompts, our framework is designed to learn the latent
representations of entities and their interrelations. To show the efficacy of
the KG-LLM Framework, we fine-tune three leading LLMs within this framework,
including Flan-T5, LLaMa2 and Gemma. Further, we explore the framework's
potential to provide LLMs with zero-shot capabilities for handling previously
unseen prompts. Experimental results show that KG-LLM significantly improves
the models' generalization capabilities, leading to more accurate predictions
in unfamiliar scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Upcycling Large Language Models into Mixture of Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07524v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07524v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan He, Abhinav Khattar, Ryan Prenger, Vijay Korthikanti, Zijie Yan, Tong Liu, Shiqing Fan, Ashwath Aithal, Mohammad Shoeybi, Bryan Catanzaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Upcycling pre-trained dense language models into sparse mixture-of-experts
(MoE) models is an efficient approach to increase the model capacity of already
trained models. However, optimal techniques for upcycling at scale remain
unclear. In this work, we conduct an extensive study of upcycling methods and
hyperparameters for billion-parameter scale language models. We propose a novel
"virtual group" initialization scheme and weight scaling approach to enable
upcycling into fine-grained MoE architectures. Through ablations, we find that
upcycling outperforms continued dense model training. In addition, we show that
softmax-then-topK expert routing improves over topK-then-softmax approach and
higher granularity MoEs can help improve accuracy. Finally, we upcycled
Nemotron-4 15B on 1T tokens and compared it to a continuously trained version
of the same model on the same 1T tokens: the continuous trained model achieved
65.3% MMLU, whereas the upcycled model achieved 67.6%. Our results offer
insights and best practices to effectively leverage upcycling for building MoE
language models. Code is available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enabling On-Device Medical AI Assistants via Input-Driven Saliency
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11105v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11105v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uttej Kallakurik, Edward Humes, Rithvik Jonna, Xiaomin Lin, Tinoosh Mohsenin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have significant impact on the healthcare
scenarios but remain prohibitively large for deployment in real-time,
resource-constrained environments such as edge devices. In this work, we
introduce a novel medical assistant system, optimized through our
general-purpose compression framework, which tailors Large Language Models
(LLMs) for deployment in specialized domains. By measuring neuron saliency on
domain-specific data, our method can aggressively prune irrelevant neurons,
reducing model size while preserving performance. Following pruning, we apply
post-training quantization to further reduce the memory footprint, and evaluate
the compressed model across medical benchmarks including MedMCQA, MedQA, and
PubMedQA. We also deploy the 50\% compressed Gemma and the 67\% compressed
LLaMA3 models on Jetson Orin Nano (18.7W peak) and Raspberry Pi 5 (6.3W peak),
achieving real-time, energy-efficient inference under hardware constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An overview of domain-specific foundation model: key technologies,
  applications and challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04267v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04267v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolong Chen, Hanzhi Chen, Zijian Zhao, Kaifeng Han, Guangxu Zhu, Yichen Zhao, Ying Du, Wei Xu, Qingjiang Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The impressive performance of ChatGPT and other foundation-model-based
products in human language understanding has prompted both academia and
industry to explore how these models can be tailored for specific industries
and application scenarios. This process, known as the customization of
domain-specific foundation models (FMs), addresses the limitations of
general-purpose models, which may not fully capture the unique patterns and
requirements of domain-specific data. Despite its importance, there is a
notable lack of comprehensive overview papers on building domain-specific FMs,
while numerous resources exist for general-purpose models. To bridge this gap,
this article provides a timely and thorough overview of the methodology for
customizing domain-specific FMs. It introduces basic concepts, outlines the
general architecture, and surveys key methods for constructing domain-specific
models. Furthermore, the article discusses various domains that can benefit
from these specialized models and highlights the challenges ahead. Through this
overview, we aim to offer valuable guidance and reference for researchers and
practitioners from diverse fields to develop their own customized FMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A dataset of questions on decision-theoretic reasoning in Newcomb-like
  problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10588v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10588v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caspar Oesterheld, Emery Cooper, Miles Kodama, Linh Chi Nguyen, Ethan Perez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a dataset of natural-language questions in the decision theory
of so-called Newcomb-like problems. Newcomb-like problems include, for
instance, decision problems in which an agent interacts with a similar other
agent, and thus has to reason about the fact that the other agent will likely
reason in similar ways. Evaluating LLM reasoning about Newcomb-like problems is
important because interactions between foundation-model-based agents will often
be Newcomb-like. Some ways of reasoning about Newcomb-like problems may allow
for greater cooperation between models.
  Our dataset contains both capabilities questions (i.e., questions with a
unique, uncontroversially correct answer) and attitude questions (i.e.,
questions about which decision theorists would disagree). We use our dataset
for an investigation of decision-theoretical capabilities and expressed
attitudes and their interplay in existing models (different models by OpenAI,
Anthropic, Meta, GDM, Reka, etc.), as well as models under simple prompt-based
interventions. We find, among other things, that attitudes vary significantly
between existing models; that high capabilities are associated with attitudes
more favorable toward so-called evidential decision theory; and that attitudes
are consistent across different types of questions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 15 figures; code and data at
  https://github.com/casparoe/newcomblike_questions_dataset</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal
  Transformer Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.21549v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.21549v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Csizmadia, Andrei Codreanu, Victor Sim, Vighnesh Prabhu, Michael Lu, Kevin Zhu, Sean O'Brien, Vasu Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that
enhances multimodal image-text retrieval while preserving the original model's
strong zero-shot classification capabilities. CLIP models are typically
constrained by fixed image resolutions and limited context, which can hinder
their effectiveness in retrieval tasks that require fine-grained cross-modal
understanding. DCLIP addresses these challenges through a meta teacher-student
distillation framework, where a cross-modal transformer teacher is fine-tuned
to produce enriched embeddings via bidirectional cross-attention between
YOLO-extracted image regions and corresponding textual spans. These
semantically and spatially aligned global representations guide the training of
a lightweight student model using a hybrid loss that combines contrastive
learning and cosine similarity objectives. Despite being trained on only
~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a
fraction of CLIP's original dataset-DCLIP significantly improves image-text
retrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's
zero-shot classification performance. These results demonstrate that DCLIP
effectively mitigates the trade-off between task specialization and
generalization, offering a resource-efficient, domain-adaptive, and
detail-sensitive solution for advanced vision-language tasks. Code available at
https://anonymous.4open.science/r/DCLIP-B772/README.md.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-aligned prompting improves zero-shot detection of AI-generated
  images by Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zoher Kachwala, Danishjeet Singh, Danielle Yang, Filippo Menczer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As image generators produce increasingly realistic images, concerns about
potential misuse continue to grow. Supervised detection relies on large,
curated datasets and struggles to generalize across diverse generators. In this
work, we investigate the use of pre-trained Vision-Language Models (VLMs) for
zero-shot detection of AI-generated images. While off-the-shelf VLMs exhibit
some task-specific reasoning and chain-of-thought prompting offers gains, we
show that task-aligned prompting elicits more focused reasoning and
significantly improves performance without fine-tuning. Specifically, prefixing
the model's response with the phrase "Let's examine the style and the synthesis
artifacts" -- a method we call zero-shot-s$^2$ -- boosts Macro F1 scores by
8%-29%. These gains are consistent for two widely used open-source models and
across three recent, diverse datasets spanning human faces, objects, and
animals with images generated by 16 different models -- demonstrating strong
generalization. We further evaluate the approach across three additional model
sizes and observe improvements in most dataset-model combinations -- suggesting
robustness to model scale. Surprisingly, self-consistency, a behavior
previously observed in language reasoning, where aggregating answers from
diverse reasoning paths improves performance, also holds in this setting. Even
here, zero-shot-s$^2$ scales better than chain-of-thought in most cases --
indicating that it elicits more useful diversity. Our findings show that
task-aligned prompts elicit more focused reasoning and enhance latent
capabilities in VLMs, like the detection of AI-generated images -- offering a
simple, generalizable, and explainable alternative to supervised methods. Our
code is publicly available on github: https://github.com/Zoher15/Zero-shot-s2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangling Codemixing in Chats: The NUS ABC Codemixed Corpus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00332v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00332v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Svetlana Churina, Akshat Gupta, Insyirah Mujtahid, Kokil Jaidka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code-mixing involves the seamless integration of linguistic elements from
multiple languages within a single discourse, reflecting natural multilingual
communication patterns. Despite its prominence in informal interactions such as
social media, chat messages and instant-messaging exchanges, there has been a
lack of publicly available corpora that are author-labeled and suitable for
modeling human conversations and relationships. This study introduces the first
labeled and general-purpose corpus for understanding code-mixing in context
while maintaining rigorous privacy and ethical standards. Our live project will
continuously gather, verify, and integrate code-mixed messages into a
structured dataset released in JSON format, accompanied by detailed metadata
and linguistic statistics. To date, it includes over 355,641 messages spanning
various code-mixing patterns, with a primary focus on English, Mandarin, and
other languages. We expect the Codemix Corpus to serve as a foundational
dataset for research in computational linguistics, sociolinguistics, and NLP
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 5 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating how LLM annotations represent diverse views on contentious
  topics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.23243v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.23243v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Megan A. Brown, Shubham Atreja, Libby Hemphill, Patrick Y. Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Researchers have proposed the use of generative large language models (LLMs)
to label data for research and applied settings. This literature emphasizes the
improved performance of these models relative to other natural language models,
noting that generative LLMs typically outperform other models and even humans
across several metrics. Previous literature has examined bias across many
applications and contexts, but less work has focused specifically on bias in
generative LLMs' responses to subjective annotation tasks. This bias could
result in labels applied by LLMs that disproportionately align with majority
groups over a more diverse set of viewpoints. In this paper, we evaluate how
LLMs represent diverse viewpoints on these contentious tasks. Across four
annotation tasks on four datasets, we show that LLMs do not show systematic
substantial disagreement with annotators on the basis of demographics. Rather,
we find that multiple LLMs tend to be biased in the same directions on the same
demographic categories within the same datasets. Moreover, the disagreement
between human annotators on the labeling task -- a measure of item difficulty
-- is far more predictive of LLM agreement with human annotators. We conclude
with a discussion of the implications for researchers and practitioners using
LLMs for automated data annotation tasks. Specifically, we emphasize that
fairness evaluations must be contextual, model choice alone will not solve
potential issues of bias, and item difficulty must be integrated into bias
assessments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Regularization with Sparse Autoencoders for Controllable LLM-based
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuansheng Wu, Wenhao Yu, Xiaoming Zhai, Ninghao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern text classification methods heavily rely on contextual embeddings from
large language models (LLMs). Compared to human-engineered features, these
embeddings provide automatic and effective representations for classification
model training. However, they also introduce a challenge: we lose the ability
to manually remove unintended features, such as sensitive or task-irrelevant
features, to guarantee regulatory compliance or improve the generalizability of
classification models. This limitation arises because LLM embeddings are opaque
and difficult to interpret. In this paper, we propose a novel framework to
identify and regularize unintended features in the LLM latent space.
Specifically, we first pre-train a sparse autoencoder (SAE) to extract
interpretable features from LLM latent spaces. To ensure the SAE can capture
task-specific features, we further fine-tune it on task-specific datasets. In
training the classification model, we propose a simple and effective
regularizer, by minimizing the similarity between the classifier weights and
the identified unintended feature, to remove the impact of these unintended
features on classification. We evaluate the proposed framework on three
real-world tasks, including toxic chat detection, reward modeling, and disease
diagnosis. Results show that the proposed self-regularization framework can
improve the classifier's generalizability by regularizing those features that
are not semantically correlated to the task. This work pioneers controllable
text classification on LLM latent spaces by leveraging interpreted features to
address generalizability, fairness, and privacy challenges. The code and data
are publicly available at
https://github.com/JacksonWuxs/Controllable_LLM_Classifier.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGKDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04322v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04322v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yik Siu Chan, Narutatsu Ri, Yuxin Xiao, Marzyeh Ghassemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite extensive safety alignment efforts, large language models (LLMs)
remain vulnerable to jailbreak attacks that elicit harmful behavior. While
existing studies predominantly focus on attack methods that require technical
expertise, two critical questions remain underexplored: (1) Are jailbroken
responses truly useful in enabling average users to carry out harmful actions?
(2) Do safety vulnerabilities exist in more common, simple human-LLM
interactions? In this paper, we demonstrate that LLM responses most effectively
facilitate harmful actions when they are both actionable and informative--two
attributes easily elicited in multi-step, multilingual interactions. Using this
insight, we propose HarmScore, a jailbreak metric that measures how effectively
an LLM response enables harmful actions, and Speak Easy, a simple multi-step,
multilingual attack framework. Notably, by incorporating Speak Easy into direct
request and jailbreak baselines, we see an average absolute increase of 0.319
in Attack Success Rate and 0.426 in HarmScore in both open-source and
proprietary LLMs across four safety benchmarks. Our work reveals a critical yet
often overlooked vulnerability: Malicious users can easily exploit common
interaction patterns for harmful intentions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Browsing: API-Based Web <span class="highlight-title">Agent</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16464v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16464v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueqi Song, Frank Xu, Shuyan Zhou, Graham Neubig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web browsers are a portal to the internet, where much of human activity is
undertaken. Thus, there has been significant research work in AI agents that
interact with the internet through web browsing. However, there is also another
interface designed specifically for machine interaction with online content:
application programming interfaces (APIs). In this paper we ask -- what if we
were to take tasks traditionally tackled by Browsing Agents, and give AI agents
access to APIs? To do so, we propose two varieties of agents: (1) an
API-calling agent that attempts to perform online tasks through APIs only,
similar to traditional coding agents, and (2) a Hybrid Agent that can interact
with online data through both web browsing and APIs. In experiments on
WebArena, a widely-used and realistic benchmark for web navigation tasks, we
find that API-Based Agents outperform web Browsing Agents. Hybrid Agents
out-perform both others nearly uniformly across tasks, resulting in a more than
24.0% absolute improvement over web browsing alone, achieving a success rate of
38.9%, the SOTA performance among task-agnostic agents. These results strongly
suggest that when APIs are available, they present an attractive alternative to
relying on web browsing alone.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Geo-Culturally Grounded LLM Generations <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13497v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13497v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piyawat Lertvittayakumjorn, David Kinney, Vinodkumar Prabhakaran, Donald Martin Jr., Sunipa Dev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative large language models (LLMs) have demonstrated gaps in diverse
cultural awareness across the globe. We investigate the effect of retrieval
augmented generation and search-grounding techniques on LLMs' ability to
display familiarity with various national cultures. Specifically, we compare
the performance of standard LLMs, LLMs augmented with retrievals from a bespoke
knowledge base (i.e., KB grounding), and LLMs augmented with retrievals from a
web search (i.e., search grounding) on multiple cultural awareness benchmarks.
We find that search grounding significantly improves the LLM performance on
multiple-choice benchmarks that test propositional knowledge (e.g., cultural
norms, artifacts, and institutions), while KB grounding's effectiveness is
limited by inadequate knowledge base coverage and a suboptimal retriever.
However, search grounding also increases the risk of stereotypical judgments by
language models and fails to improve evaluators' judgments of cultural
familiarity in a human evaluation with adequate statistical power. These
results highlight the distinction between propositional cultural knowledge and
open-ended cultural fluency when it comes to evaluating LLMs' cultural
awareness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MultiMatch: Multihead Consistency Regularization Matching for
  Semi-Supervised Text Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07801v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07801v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iustin Sirbu, Robert-Adrian Popovici, Cornelia Caragea, Stefan Trausan-Matu, Traian Rebedea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm
combining the paradigms of co-training and consistency regularization with
pseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label
weighting module designed for three key purposes: selecting and filtering
pseudo-labels based on head agreement and model confidence, and weighting them
according to the perceived classification difficulty. This novel module
enhances and unifies three existing techniques -- heads agreement from
Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average
Pseudo-Margins from MarginMatch -- resulting in a holistic approach that
improves robustness and performance in SSL settings. Experimental results on
benchmark datasets highlight the superior performance of MultiMatch, achieving
state-of-the-art results on 9 out of 10 setups from 5 natural language
processing datasets and ranking first according to the Friedman test among 19
methods. Furthermore, MultiMatch demonstrates exceptional robustness in highly
imbalanced settings, outperforming the second-best approach by 3.26% -- and
data imbalance is a key factor for many text classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ETM: Modern Insights into Perspective on Text-to-SQL Evaluation in the
  Age of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07313v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07313v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin G. Ascoli, Yasoda Sai Ram Kandikonda, Jinho D. Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of Text-to-SQL enables anyone to retrieve information from SQL
databases using natural language. While this task has made substantial
progress, the two primary evaluation metrics - Execution Accuracy (EXE) and
Exact Set Matching Accuracy (ESM) - suffer from inherent limitations that can
misrepresent performance. Specifically, ESM's rigid matching overlooks
semantically correct but stylistically different queries, whereas EXE can
overestimate correctness by ignoring structural errors that yield correct
outputs. These shortcomings become especially problematic when assessing
outputs from large language model (LLM)-based approaches without fine-tuning,
which vary more in style and structure compared to their fine-tuned
counterparts. Thus, we introduce a new metric, Enhanced Tree Matching (ETM),
which mitigates these issues by comparing queries using both syntactic and
semantic elements. Through evaluating nine LLM-based models, we show that EXE
and ESM can produce false positive and negative rates as high as 23.0% and
28.9%, while ETM reduces these rates to 0.3% and 2.7%, respectively. We release
our ETM script as open source, offering the community a more robust and
reliable approach to evaluating Text-to-SQL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongCodeBench: Evaluating Coding LLMs at 1M Context Windows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Rando, Luca Romani, Alessio Sampieri, Luca Franco, John Yang, Yuta Kyuragi, Fabio Galasso, Tatsunori Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context lengths for models have grown rapidly, from thousands to millions of
tokens in just a few years. The extreme context sizes of modern long-context
models have made it difficult to construct realistic long-context benchmarks --
not only due to the cost of collecting million-context tasks but also in
identifying realistic scenarios that require significant contexts. We identify
code comprehension and repair as a natural testbed and challenge task for
long-context models and introduce LongCodeBench (LCB), a benchmark to test LLM
coding abilities in long-context scenarios. Our benchmark tests both the
comprehension and repair capabilities of LCLMs in realistic and important
settings by drawing from real-world GitHub issues and constructing QA
(LongCodeQA) and bug fixing (LongSWE-Bench) tasks. We carefully stratify the
complexity of our benchmark, enabling us to evaluate models across different
scales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model.
We find that long-context remains a weakness for all models, with performance
drops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for
Qwen2.5.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Roboflow100-VL: A Multi-Domain Object Detection Benchmark for
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20612v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20612v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Robicheaux, Matvei Popov, Anish Madan, Isaac Robinson, Joseph Nelson, Deva Ramanan, Neehar Peri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) trained on internet-scale data achieve
remarkable zero-shot detection performance on common objects like car, truck,
and pedestrian. However, state-of-the-art models still struggle to generalize
to out-of-distribution classes, tasks and imaging modalities not typically
found in their pre-training. Rather than simply re-training VLMs on more visual
data, we argue that one should align VLMs to new concepts with annotation
instructions containing a few visual examples and rich textual descriptions. To
this end, we introduce Roboflow100-VL, a large-scale collection of 100
multi-modal object detection datasets with diverse concepts not commonly found
in VLM pre-training. We evaluate state-of-the-art models on our benchmark in
zero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing
for comparison across data regimes. Notably, we find that VLMs like
GroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on
challenging medical imaging datasets within Roboflow100-VL, demonstrating the
need for few-shot concept alignment. Lastly, we discuss our recent CVPR 2025
Foundational FSOD competition and share insights from the community. Notably,
the winning team significantly outperforms our baseline by 16.8 mAP! Our code
and dataset are available at https://github.com/roboflow/rf100-vl/ and
https://universe.roboflow.com/rf100-vl/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally. Project Page:
  https://rf100-vl.org/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discrete Audio Tokens: More Than a Survey! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pooneh Mousavi, Gallil Maimon, Adel Moumen, Darius Petermann, Jiatong Shi, Haibin Wu, Haici Yang, Anastasia Kuznetsova, Artem Ploujnikov, Ricard Marxer, Bhuvana Ramabhadran, Benjamin Elizalde, Loren Lugosch, Jinyu Li, Cem Subakan, Phil Woodland, Minje Kim, Hung-yi Lee, Shinji Watanabe, Yossi Adi, Mirco Ravanelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discrete audio tokens are compact representations that aim to preserve
perceptual quality, phonetic content, and speaker characteristics while
enabling efficient storage and inference, as well as competitive performance
across diverse downstream tasks. They provide a practical alternative to
continuous features, enabling the integration of speech and audio into modern
large language models (LLMs). As interest in token-based audio processing
grows, various tokenization methods have emerged, and several surveys have
reviewed the latest progress in the field. However, existing studies often
focus on specific domains or tasks and lack a unified comparison across various
benchmarks. This paper presents a systematic review and benchmark of discrete
audio tokenizers, covering three domains: speech, music, and general audio. We
propose a taxonomy of tokenization approaches based on encoder-decoder,
quantization techniques, training paradigm, streamability, and application
domains. We evaluate tokenizers on multiple benchmarks for reconstruction,
downstream performance, and acoustic language modeling, and analyze trade-offs
through controlled ablation studies. Our findings highlight key limitations,
practical considerations, and open challenges, providing insight and guidance
for future research in this rapidly evolving area. For more information,
including our main results and tokenizer database, please refer to our website:
https://poonehmousavi.github.io/dates-website/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EuroLLM-9B: Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04079v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04079v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Henrique Martins, João Alves, Patrick Fernandes, Nuno M. Guerreiro, Ricardo Rei, Amin Farajian, Mateusz Klimaszewski, Duarte M. Alves, José Pombal, Nicolas Boizard, Manuel Faysse, Pierre Colombo, François Yvon, Barry Haddow, José G. C. de Souza, Alexandra Birch, André F. T. Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report presents EuroLLM-9B, a large language model trained from scratch
to support the needs of European citizens by covering all 24 official European
Union languages and 11 additional languages. EuroLLM addresses the issue of
European languages being underrepresented and underserved in existing open
large language models. We provide a comprehensive overview of EuroLLM-9B's
development, including tokenizer design, architectural specifications, data
filtering, and training procedures. We describe the pre-training data
collection and filtering pipeline, including the creation of EuroFilter, an
AI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a
novel synthetic dataset for post-training that enhances language coverage for
European languages. Evaluation results demonstrate EuroLLM-9B's competitive
performance on multilingual benchmarks and machine translation tasks,
establishing it as the leading open European-made LLM of its size. To support
open research and adoption, we release all major components of this work,
including the base and instruction-tuned models, the EuroFilter classifier, and
the synthetic post-training dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>56 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conformal Linguistic Calibration: Trading-off between Factuality and
  Specificity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19110v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19110v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengping Jiang, Anqi Liu, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model outputs are not always reliable, thus prompting research into
how to adapt model responses based on uncertainty. Common approaches include:
\emph{abstention}, where models refrain from generating responses when
uncertain; and \emph{linguistic calibration}, where models hedge their
statements using uncertainty quantifiers. However, abstention can withhold
valuable information, while linguistically calibrated responses are often
challenging to leverage in downstream tasks. We propose a unified view,
Conformal Linguistic Calibration (CLC), which reinterprets linguistic
calibration as \emph{answer set prediction}. First we present a framework
connecting abstention and linguistic calibration through the lens of linguistic
pragmatics. We then describe an implementation of CLC that allows for
controlling the level of imprecision in model responses. Results demonstrate
our method produces calibrated outputs with conformal guarantees on factual
accuracy. Further, our approach enables fine-tuning models to perform
uncertainty-aware adaptive claim rewriting, offering a controllable balance
between factuality and specificity.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-15T00:00:00Z">2025-06-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">16</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Enhanced by Plug and Play Syntactic Knowledge for
  Aspect-based Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12991v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12991v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhe Tian, Xu Li, Wei Wang, Guoqing Jin, Pengsen Cheng, Yan Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect-based sentiment analysis (ABSA) generally requires a deep
understanding of the contextual information, including the words associated
with the aspect terms and their syntactic dependencies. Most existing studies
employ advanced encoders (e.g., pre-trained models) to capture such context,
especially large language models (LLMs). However, training these encoders is
resource-intensive, and in many cases, the available data is insufficient for
necessary fine-tuning. Therefore it is challenging for learning LLMs within
such restricted environments and computation efficiency requirement. As a
result, it motivates the exploration of plug-and-play methods that adapt LLMs
to ABSA with minimal effort. In this paper, we propose an approach that
integrates extendable components capable of incorporating various types of
syntactic knowledge, such as constituent syntax, word dependencies, and
combinatory categorial grammar (CCG). Specifically, we propose a memory module
that records syntactic information and is incorporated into LLMs to instruct
the prediction of sentiment polarities. Importantly, this encoder acts as a
versatile, detachable plugin that is trained independently of the LLM. We
conduct experiments on benchmark datasets, which show that our approach
outperforms strong baselines and previous approaches, thus demonstrates its
effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Neuro-Symbolic Retrieval-Augmented Generation through <span class="highlight-title">Adaptive</span>
  Query Routing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Safayat Bin Hakim, Muhammad Adil, Alvaro Velasquez, Houbing Herbert Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems address factual inconsistencies
in Large Language Models by grounding generation in external knowledge, yet
they face a fundamental efficiency problem: simple queries consume
computational resources equivalent to complex multi-hop reasoning tasks. We
present SymRAG, a neuro-symbolic framework that introduces adaptive query
routing based on real-time complexity and system load assessments. SymRAG
dynamically selects symbolic, neural, or hybrid processing paths to align
resource use with query demands. Evaluated on 2,000 queries from HotpotQA and
DROP using Llama-3.2-3B and Mistral-7B models, SymRAG achieves 97.6--100.0%
exact match accuracy with significantly lower CPU utilization (3.6--6.2%) and
processing time (0.985--3.165s). Disabling adaptive logic results in 169--1151%
increase in processing time, highlighting the framework's impact. These results
underscore the potential of adaptive neuro-symbolic routing for scalable,
sustainable AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-document Summarization through Multi-document Event Relation Graph
  Reasoning in LLMs: a case study in Framing Bias Mitigation <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanyuan Lei, Ruihong Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Media outlets are becoming more partisan and polarized nowadays. Most
previous work focused on detecting media bias. In this paper, we aim to
mitigate media bias by generating a neutralized summary given multiple articles
presenting different ideological views. Motivated by the critical role of
events and event relations in media bias detection, we propose to increase
awareness of bias in LLMs via multi-document events reasoning and use a
multi-document event relation graph to guide the summarization process. This
graph contains rich event information useful to reveal bias: four common types
of in-doc event relations to reflect content framing bias, cross-doc event
coreference relation to reveal content selection bias, and event-level moral
opinions to highlight opinionated framing bias. We further develop two
strategies to incorporate the multi-document event relation graph for
neutralized summarization. Firstly, we convert a graph into natural language
descriptions and feed the textualized graph into LLMs as a part of a hard text
prompt. Secondly, we encode the graph with graph attention network and insert
the graph embedding into LLMs as a soft prompt. Both automatic evaluation and
human evaluation confirm that our approach effectively mitigates both lexical
and informational media bias, and meanwhile improves content preservation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing the <span class="highlight-title">Role</span> of Data Quality in Training Bilingual Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Skyler Seto, Maartje ter Hoeve, Maureen de Seyssel, David Grangier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilingual and multilingual language models offer a promising path toward
scaling NLP systems across diverse languages and users. However, their
performance often varies wildly between languages as prior works show that
adding more languages can degrade performance for some languages (such as
English), while improving others (typically more data constrained languages).
In this work, we investigate causes of these inconsistencies by comparing
bilingual and monolingual language models. Our analysis reveals that unequal
data quality, not just data quantity, is a major driver of performance
degradation in bilingual settings. We propose a simple yet effective data
filtering strategy to select higher-quality bilingual training data with only
high quality English data. Applied to French, German, and Chinese, our approach
improves monolingual performance by 2-4% and reduces bilingual model
performance gaps to 1%. These results highlight the overlooked importance of
data quality in multilingual pretraining and offer a practical recipe for
balancing performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 18 figures, 25 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forecasting Time Series with LLMs via Patch-Based Prompting and
  Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mayank Bumb, Anshul Vemulapalli, Sri Harsha Vardhan Prasad Jella, Anish Gupta, An La, Ryan A. Rossi, Hongjie Chen, Franck Dernoncourt, Nesreen K. Ahmed, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have demonstrated new
possibilities for accurate and efficient time series analysis, but prior work
often required heavy fine-tuning and/or ignored inter-series correlations. In
this work, we explore simple and flexible prompt-based strategies that enable
LLMs to perform time series forecasting without extensive retraining or the use
of a complex external architecture. Through the exploration of specialized
prompting methods that leverage time series decomposition, patch-based
tokenization, and similarity-based neighbor augmentation, we find that it is
possible to enhance LLM forecasting quality while maintaining simplicity and
requiring minimal preprocessing of data. To this end, we propose our own
method, PatchInstruct, which enables LLMs to make precise and effective
predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HypER: Literature-grounded Hypothesis Generation and Distillation with
  Provenance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rosni Vasu, Chandrayee Basu, Bhavana Dalvi Mishra, Cristina Sarasua, Peter Clark, Abraham Bernstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language models have demonstrated promising performance in research
ideation across scientific domains. Hypothesis development, the process of
generating a highly specific declarative statement connecting a research idea
with empirical validation, has received relatively less attention. Existing
approaches trivially deploy retrieval augmentation and focus only on the
quality of the final output ignoring the underlying reasoning process behind
ideation. We present $\texttt{HypER}$ ($\textbf{Hyp}$othesis Generation with
$\textbf{E}$xplanation and $\textbf{R}$easoning), a small language model (SLM)
trained for literature-guided reasoning and evidence-based hypothesis
generation. $\texttt{HypER}$ is trained in a multi-task setting to discriminate
between valid and invalid scientific reasoning chains in presence of controlled
distractions. We find that $\texttt{HypER}$ outperformes the base model,
distinguishing valid from invalid reasoning chains (+22\% average absolute F1),
generates better evidence-grounded hypotheses (0.327 vs. 0.305 base model) with
high feasibility and impact as judged by human experts ($>$3.5 on 5-point
Likert scale).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages (9 pages: main paper body)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CliniDial: A Naturally Occurring Multimodal <span class="highlight-title">Dialogue</span> Dataset for Team
  Reflection in Action During Clinical Operation <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naihao Deng, Kapotaksha Das, Rada Mihalcea, Vitaliy Popov, Mohamed Abouelenien
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In clinical operations, teamwork can be the crucial factor that determines
the final outcome. Prior studies have shown that sufficient collaboration is
the key factor that determines the outcome of an operation. To understand how
the team practices teamwork during the operation, we collected CliniDial from
simulations of medical operations. CliniDial includes the audio data and its
transcriptions, the simulated physiology signals of the patient manikins, and
how the team operates from two camera angles. We annotate behavior codes
following an existing framework to understand the teamwork process for
CliniDial. We pinpoint three main characteristics of our dataset, including its
label imbalances, rich and natural interactions, and multiple modalities, and
conduct experiments to test existing LLMs' capabilities on handling data with
these characteristics. Experimental results show that CliniDial poses
significant challenges to the existing models, inviting future effort on
developing methods that can deal with real-world clinical data. We open-source
the codebase at https://github.com/MichiganNLP/CliniDial
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Diao, Chunhui Zhang, Keyi Kong, Weiyi Wu, Chiyu Ma, Zhongyu Ouyang, Peijun Qing, Soroush Vosoughi, Jiang Gui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models have shown reasoning capabilities, their
application to the audio modality, particularly in large audio-language models
(ALMs), remains significantly underdeveloped. Addressing this gap requires a
systematic approach, involving a capable base model, high-quality
reasoning-oriented audio data, and effective training algorithms. In this
study, we present a comprehensive solution: we introduce the Audio Logical
Reasoning (ALR) dataset, consisting of 6,446 text-audio annotated samples
specifically designed for complex reasoning tasks. Building on this resource,
we propose SoundMind, a rule-based reinforcement learning (RL) algorithm
tailored to endow ALMs with deep bimodal reasoning abilities. By training
Qwen2.5-Omni-7B on the ALR dataset using SoundMind, our approach achieves
state-of-the-art performance in audio logical reasoning. This work highlights
the impact of combining high-quality, reasoning-focused datasets with
specialized RL techniques, advancing the frontier of auditory intelligence in
language models. Our code and the proposed dataset are available at
https://github.com/xid32/SoundMind.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unifying Specialized Visual Encoders for Video Language Models <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihoon Chung, Tyler Zhu, Max Gonzalez Saez-Diez, Juan Carlos Niebles, Honglu Zhou, Olga Russakovsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advent of Large Language Models (LLMs) has ushered sophisticated
reasoning capabilities into the realm of video through Video Large Language
Models (VideoLLMs). However, VideoLLMs currently rely on a single vision
encoder for all of their visual processing, which limits the amount and type of
visual information that can be conveyed to the LLM. Our method, MERV,
Multi-Encoder Representation of Videos, instead leverages multiple frozen
visual encoders to create a unified representation of a video, providing the
VideoLLM with a comprehensive set of specialized visual knowledge.
Spatio-temporally aligning the features from each encoder allows us to tackle a
wider range of open-ended and multiple-choice video understanding questions and
outperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy
than Video-LLaVA across the standard suite video understanding benchmarks,
while also having a better Video-ChatGPT score. We also improve upon SeViLA,
the previous best on zero-shot Perception Test accuracy, by 2.2%. MERV
introduces minimal extra parameters and trains faster than equivalent
single-encoder methods while parallelizing the visual processing. Finally, we
provide qualitative evidence that MERV successfully captures domain knowledge
from each of its encoders. Our results offer promising directions in utilizing
multiple vision encoders for comprehensive video understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2025 as a Poster. Project page:
  https://tylerzhu.com/merv/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OR-Bench: An Over-Refusal Benchmark for Large Language Models <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20947v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20947v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin Cui, Wei-Lin Chiang, Ion Stoica, Cho-Jui Hsieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) require careful safety alignment to prevent
malicious outputs. While significant research focuses on mitigating harmful
content generation, the enhanced safety often come with the side effect of
over-refusal, where LLMs may reject innocuous prompts and become less helpful.
Although the issue of over-refusal has been empirically observed, a systematic
measurement is challenging due to the difficulty of crafting prompts that can
elicit the over-refusal behaviors of LLMs. This study proposes a novel method
for automatically generating large-scale over-refusal datasets. Leveraging this
technique, we introduce OR-Bench, the first large-scale over-refusal benchmark.
OR-Bench comprises 80,000 over-refusal prompts across 10 common rejection
categories, a subset of around 1,000 hard prompts that are challenging even for
state-of-the-art LLMs, and an additional 600 toxic prompts to prevent
indiscriminate responses. We then conduct a comprehensive study to measure the
over-refusal of 32 popular LLMs across 8 model families. Our datasets are
publicly available at https://huggingface.co/bench-llms and our codebase is
open-sourced at https://github.com/justincui03/or-bench. We hope this benchmark
can help the community develop better safety aligned models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2025, we thank everyone for their valuable
  suggestions and feedback!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Building, Reusing, and Generalizing Abstract Representations from
  Concrete Sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21332v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21332v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuchen Wu, Mirko Thalmann, Peter Dayan, Zeynep Akata, Eric Schulz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans excel at learning abstract patterns across different sequences,
filtering out irrelevant details, and transferring these generalized concepts
to new sequences. In contrast, many sequence learning models lack the ability
to abstract, which leads to memory inefficiency and poor transfer. We introduce
a non-parametric hierarchical variable learning model (HVM) that learns chunks
from sequences and abstracts contextually similar chunks as variables. HVM
efficiently organizes memory while uncovering abstractions, leading to compact
sequence representations. When learning on language datasets such as babyLM,
HVM learns a more efficient dictionary than standard compression algorithms
such as Lempel-Ziv. In a sequence recall task requiring the acquisition and
transfer of variables embedded in sequences, we demonstrate HVM's sequence
likelihood correlates with human recall times. In contrast, large language
models (LLMs) struggle to transfer abstract variables as effectively as humans.
From HVM's adjustable layer of abstraction, we demonstrate that the model
realizes a precise trade-off between compression and generalization. Our work
offers a cognitive model that captures the learning and transfer of abstract
representations in human cognition and differentiates itself from LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ REPA: Russian Error Types Annotation for Evaluating Text Generation and
  Judgment Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.13102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.13102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Pugachev, Alena Fenogenova, Vladislav Mikhailov, Ekaterina Artemova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) have introduced the novel
paradigm of using LLMs as judges, where an LLM evaluates and scores the outputs
of another LLM, which often correlates highly with human preferences. However,
the use of LLM-as-a-judge has been primarily studied in English. In this paper,
we evaluate this framework in Russian by introducing the Russian Error tyPes
Annotation dataset (REPA), a dataset of 1k user queries and 2k LLM-generated
responses. Human annotators labeled each response pair expressing their
preferences across ten specific error types, as well as selecting an overall
preference. We rank six generative LLMs across the error types using three
rating systems based on human preferences. We also evaluate responses using
eight LLM judges in zero-shot and few-shot settings. We describe the results of
analyzing the judges and position and length biases. Our findings reveal a
notable gap between LLM judge performance in Russian and English. However,
rankings based on human and LLM preferences show partial alignment, suggesting
that while current LLM judges struggle with fine-grained evaluation in Russian,
there is potential for improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at SIGSLAV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Argumentative Text to Argument Knowledge Graph: A New Framework for
  Structured Argumentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00713v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00713v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debarati Bhattacharjee, Ashish Anand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a framework to convert argumentative texts into argument
knowledge graphs (AKG). Starting with basic annotations of argumentative
components (ACs) and argumentative relations (ARs), we enrich the information
by constructing a knowledge base (KB) graph with metadata attributes for nodes.
Next, we use premises and inference rules from the KB to form arguments by
applying modus ponens. From these arguments, we create an AKG. The nodes and
edges of the AKG have attributes that capture important argumentative features.
We also find missing inference rules by identifying markers. This makes it
possible to identify undercut attacks that were previously undetectable in
existing datasets. The AKG gives a graphical view of the argumentative
structure that is easier to understand than theoretical formats. It also
prepares the ground for future reasoning tasks, including checking the
coherence of arguments and identifying opportunities for revision. For this, it
is important to find indirect relations, many of which are implicit. Our
proposed AKG format, with annotated inference rules and modus ponens, will help
reasoning models learn the implicit indirect relations that require inference
over arguments and the relations between them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Layer by Layer: Uncovering Hidden Representations in Language Models <span class="chip">ICML2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02013v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02013v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Skean, Md Rifat Arefin, Dan Zhao, Niket Patel, Jalal Naghiyev, <span class="highlight-author">Yann LeCun</span>, Ravid Shwartz-Ziv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  From extracting features to generating text, the outputs of large language
models (LLMs) typically rely on the final layers, following the conventional
wisdom that earlier layers capture only low-level cues. However, our analysis
shows that intermediate layers can encode even richer representations, often
improving performance on a range of downstream tasks. To explain and quantify
these hidden-layer properties, we propose a unified framework of representation
quality metrics based on information theory, geometry, and invariance to input
perturbations. Our framework highlights how each layer balances information
compression and signal preservation, revealing why mid-depth embeddings can
exceed the last layer's performance. Through extensive experiments on 32
text-embedding tasks across various architectures (transformers, state-space
models) and domains (language, vision), we demonstrate that intermediate layers
consistently provide stronger features, challenging the standard view on
final-layer embeddings and opening new directions on using mid-layer
representations for more robust and accurate representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>update for ICML2025 camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Table Instruction Tuning <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14693v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14693v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naihao Deng, Rada Mihalcea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in table understanding have focused on instruction-tuning
large language models (LLMs) for table-related tasks. However, existing
research has overlooked the impact of hyperparameter choices, and also lacks a
comprehensive evaluation of the out-of-domain table understanding ability and
the general capabilities of these table LLMs. In this paper, we evaluate these
abilities in existing table LLMs, and find significant declines in both
out-of-domain table understanding and general capabilities as compared to their
base models. Through systematic analysis, we show that hyperparameters, such as
learning rate, can significantly influence both table-specific and general
capabilities. Contrary to the previous table instruction-tuning work, we
demonstrate that smaller learning rates and fewer training instances can
enhance table understanding while preserving general capabilities. Based on our
findings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B
Instruct, which achieves performance on par with, or surpassing GPT-3.5 and
GPT-4 on table tasks, while maintaining strong out-of-domain generalization and
general capabilities. Our findings highlight the potential for reduced data
annotation costs and more efficient model development through careful
hyperparameter selection. We open-source the project and our models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 Findings. Project page:
  https://lit.eecs.umich.edu/TAMA/. Code: https://github.com/MichiganNLP/TAMA.
  Huggingface models:
  https://huggingface.co/collections/MichiganNLP/tama-684eeb3e7f262362856eccd1.
  Data: https://huggingface.co/datasets/MichiganNLP/TAMA_Instruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reasoning with RAGged events: RAG-Enhanced Event Knowledge Base
  Construction and reasoning with proof-assistants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07042v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07042v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stergios Chatzikyriakidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting structured computational representations of historical events from
narrative text remains computationally expensive when constructed manually.
While RDF/OWL reasoners enable graph-based reasoning, they are limited to
fragments of first-order logic, preventing deeper temporal and semantic
analysis. This paper addresses both challenges by developing automatic
historical event extraction models using multiple LLMs (GPT-4, Claude, Llama
3.2) with three enhancement strategies: pure base generation, knowledge graph
enhancement, and Retrieval-Augmented Generation (RAG). We conducted
comprehensive evaluations using historical texts from Thucydides. Our findings
reveal that enhancement strategies optimize different performance dimensions
rather than providing universal improvements. For coverage and historical
breadth, base generation achieves optimal performance with Claude and GPT-4
extracting comprehensive events. However, for precision, RAG enhancement
improves coordinate accuracy and metadata completeness. Model architecture
fundamentally determines enhancement sensitivity: larger models demonstrate
robust baseline performance with incremental RAG improvements, while Llama 3.2
shows extreme variance from competitive performance to complete failure. We
then developed an automated translation pipeline converting extracted RDF
representations into Coq proof assistant specifications, enabling higher-order
reasoning beyond RDF capabilities including multi-step causal verification,
temporal arithmetic with BC dates, and formal proofs about historical
causation. The Coq formalization validates that RAG-discovered event types
represent legitimate domain-specific semantic structures rather than
ontological violations.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-13T00:00:00Z">2025-06-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">108</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ code_transformed: The Influence of Large Language Models on Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuliang Xu, Siming Huang, Mingmeng Geng, Yao Wan, Xuanhua Shi, Dongping Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coding remains one of the most fundamental modes of interaction between
humans and machines. With the rapid advancement of Large Language Models
(LLMs), code generation capabilities have begun to significantly reshape
programming practices. This development prompts a central question: Have LLMs
transformed code style, and how can such transformation be characterized? In
this paper, we present a pioneering study that investigates the impact of LLMs
on code style, with a focus on naming conventions, complexity, maintainability,
and similarity. By analyzing code from over 19,000 GitHub repositories linked
to arXiv papers published between 2020 and 2025, we identify measurable trends
in the evolution of coding style that align with characteristics of
LLM-generated code. For instance, the proportion of snake\_case variable names
in Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we
investigate how LLMs approach algorithmic problems by examining their reasoning
processes. Given the diversity of LLMs and usage scenarios, among other
factors, it is difficult or even impossible to precisely estimate the
proportion of code generated or assisted by LLMs. Our experimental results
provide the first large-scale empirical evidence that LLMs affect real-world
programming style.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We release all the experimental dataset and source code at:
  https://github.com/ignorancex/LLM_code</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Representational Learning of Foundation Models for
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheli Zhou, Chenxu Zhu, Jianghao Lin, Bo Chen, Ruiming Tang, Weinan Zhang, Yong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing a single foundation model with the capability to excel across
diverse tasks has been a long-standing objective in the field of artificial
intelligence. As the wave of general-purpose foundation models sweeps across
various domains, their influence has significantly extended to the field of
recommendation systems. While recent efforts have explored recommendation
foundation models for various generative tasks, they often overlook crucial
embedding tasks and struggle with the complexities of multi-task learning,
including knowledge sharing & conflict resolution, and convergence speed
inconsistencies. To address these limitations, we introduce RecFound, a
generative representational learning framework for recommendation foundation
models. We construct the first comprehensive dataset for recommendation
foundation models covering both generative and embedding tasks across diverse
scenarios. Based on this dataset, we propose a novel multi-task training scheme
featuring a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge
sharing & conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched)
to address inconsistent convergence, and a Model Merge module to balance the
performance across tasks. Experiments demonstrate that RecFound achieves
state-of-the-art performance across various recommendation tasks, outperforming
existing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page is available at https://junkfood436.github.io/RecFound/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VGR: Visual Grounded Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11991v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11991v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacong Wang, Zijiang Kang, Haochen Wang, Haiyong Jiang, Jiawen Li, Bohong Wu, Ya Wang, Jiao Ran, Xiao Liang, Chao Feng, Jun Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of multimodal chain-of-thought (CoT) reasoning, existing
approaches predominantly rely on reasoning on pure language space, which
inherently suffers from language bias and is largely confined to math or
science domains. This narrow focus limits their ability to handle complex
visual reasoning tasks that demand comprehensive understanding of image
details. To address these limitations, this paper introduces VGR, a novel
reasoning multimodal large language model (MLLM) with enhanced fine-grained
visual perception capabilities. Unlike traditional MLLMs that answer the
question or reasoning solely on the language space, our VGR first detects
relevant regions that may help to solve problems, and then provides precise
answers based on replayed image regions. To achieve this, we conduct a
large-scale SFT dataset called VGR -SFT that contains reasoning data with mixed
vision grounding and language deduction. The inference pipeline of VGR allows
the model to choose bounding boxes for visual reference and a replay stage is
introduced to integrates the corresponding regions into the reasoning process,
enhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline
show that VGR achieves superior performance on multi-modal benchmarks requiring
comprehensive image detail understanding. Compared to the baseline, VGR uses
only 30\% of the image token count while delivering scores of +4.1 on MMStar,
+7.1 on AI2D, and a +12.9 improvement on ChartQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Schema-R1: A reasoning training approach for schema linking in
  Text-to-SQL Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wuzhenghong Wen, Su Pan, yuwei Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Schema linking is a critical step in Text-to-SQL task, aiming to accurately
predict the table names and column names required for the SQL query based on
the given question. However, current fine-tuning approaches for schema linking
models employ a rote-learning paradigm, excessively optimizing for ground truth
schema linking outcomes while compromising reasoning ability. This limitation
arises because of the difficulty in acquiring a high-quality reasoning sample
for downstream tasks. To address this, we propose Schema-R1, a reasoning schema
linking model trained using reinforcement learning. Specifically, Schema-R1
consists of three key steps: constructing small batches of high-quality
reasoning samples, supervised fine-tuning for cold-start initialization, and
rule-based reinforcement learning training. The final results demonstrate that
our method effectively enhances the reasoning ability of the schema linking
model, achieving a 10\% improvement in filter accuracy compared to the existing
method. Our code is available at https://github.com/hongWin/Schema-R1/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Large Language Model Safety with Contrastive Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Simko, Mrinmaya Sachan, Bernhard Schölkopf, Zhijing Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are powerful tools with profound societal
impacts, yet their ability to generate responses to diverse and uncontrolled
inputs leaves them vulnerable to adversarial attacks. While existing defenses
often struggle to generalize across varying attack types, recent advancements
in representation engineering offer promising alternatives. In this work, we
propose a defense framework that formulates model defense as a contrastive
representation learning (CRL) problem. Our method finetunes a model using a
triplet-based loss combined with adversarial hard negative mining to encourage
separation between benign and harmful representations. Our experimental results
across multiple models demonstrate that our approach outperforms prior
representation engineering-based defenses, improving robustness against both
input-level and embedding-space attacks without compromising standard
performance. Our code is available at
https://github.com/samuelsimko/crl-llm-defense
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongwei Jiang, Alvin Zhang, Andrew Wang, Nicholas Andrews, Daniel Khashabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have shown LLMs possess some ability to improve their
responses when given external feedback. However, it remains unclear how
effectively and thoroughly these models can incorporate extrinsic feedback. In
an ideal scenario, if LLMs receive near-perfect and complete feedback, we would
expect them to fully integrate the feedback and change their incorrect answers
to correct ones. In this paper, we systematically investigate LLMs' ability to
incorporate feedback by designing a controlled experimental environment. For
each problem, a solver model attempts a solution, then a feedback generator
with access to near-complete ground-truth answers produces targeted feedback,
after which the solver tries again. We evaluate this pipeline across a diverse
range of tasks, including math reasoning, knowledge reasoning, scientific
reasoning, and general multi-domain evaluations with state-of-the-art language
models including Claude 3.7 (with and without extended thinking). Surprisingly,
even under these near-ideal conditions, solver models consistently show
resistance to feedback, a limitation that we term FEEDBACK FRICTION. To
mitigate this limitation, we experiment with sampling-based strategies like
progressive temperature increases and explicit rejection of previously
attempted incorrect answers, which yield improvements but still fail to help
models achieve target performance. We also perform a rigorous exploration of
potential causes of FEEDBACK FRICTION, ruling out factors such as model
overconfidence and data familiarity. We hope that highlighting this issue in
LLMs and ruling out several apparent causes will help future research in
self-improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive
  Programming? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Zheng, Zerui Cheng, Zeyu Shen, Shang Zhou, Kaiyuan Liu, Hansen He, Dongruixuan Li, Stanley Wei, Hangyi Hao, Jianzhu Yao, Peiyao Sheng, Zixuan Wang, Wenhao Chai, Aleksandra Korolova, Peter Henderson, Sanjeev Arora, Pramod Viswanath, Jingbo Shang, Saining Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent reports claim that large language models (LLMs) now outperform elite
humans in competitive programming. Drawing on knowledge from a group of
medalists in international algorithmic contests, we revisit this claim,
examining how LLMs differ from human experts and where limitations still
remain. We introduce LiveCodeBench Pro, a benchmark composed of problems from
Codeforces, ICPC, and IOI that are continuously updated to reduce the
likelihood of data contamination. A team of Olympiad medalists annotates every
problem for algorithmic categories and conducts a line-by-line analysis of
failed model-generated submissions. Using this new data and benchmark, we find
that frontier models still have significant limitations: without external
tools, the best model achieves only 53% pass@1 on medium-difficulty problems
and 0% on hard problems, domains where expert humans still excel. We also find
that LLMs succeed at implementation-heavy problems but struggle with nuanced
algorithmic reasoning and complex case analysis, often generating confidently
incorrect justifications. High performance appears largely driven by
implementation precision and tool augmentation, not superior reasoning.
LiveCodeBench Pro thus highlights the significant gap to human grandmaster
levels, while offering fine-grained diagnostics to steer future improvements in
code-centric LLM reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page at https://livecodebenchpro.com/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effectiveness of Counter-Speech against Abusive Content: A
  Multidimensional Annotation and Classification Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Greta Damo, Elena Cabrio, Serena Villata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counter-speech (CS) is a key strategy for mitigating online Hate Speech (HS),
yet defining the criteria to assess its effectiveness remains an open
challenge. We propose a novel computational framework for CS effectiveness
classification, grounded in social science concepts. Our framework defines six
core dimensions - Clarity, Evidence, Emotional Appeal, Rebuttal, Audience
Adaptation, and Fairness - which we use to annotate 4,214 CS instances from two
benchmark datasets, resulting in a novel linguistic resource released to the
community. In addition, we propose two classification strategies, multi-task
and dependency-based, achieving strong results (0.94 and 0.96 average F1
respectively on both expert- and user-written CS), outperforming standard
baselines, and revealing strong interdependence among dimensions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeistBERT: Breathing Life into German NLP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael Scheible-Schmitt, Johann Frei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in transformer-based language models have highlighted the benefits
of language-specific pre-training on high-quality corpora. In this context,
German NLP stands to gain from updated architectures and modern datasets
tailored to the linguistic characteristics of the German language. GeistBERT
seeks to improve German language processing by incrementally training on a
diverse corpus and optimizing model performance across various NLP tasks. It
was pre-trained using fairseq with standard hyperparameters, initialized from
GottBERT weights, and trained on a large-scale German corpus using Whole Word
Masking (WWM). Based on the pre-trained model, we derived extended-input
variants using Nystr\"omformer and Longformer architectures with support for
sequences up to 8k tokens. While these long-context models were not evaluated
on dedicated long-context benchmarks, they are included in our release. We
assessed all models on NER (CoNLL 2003, GermEval 2014) and text classification
(GermEval 2018 fine/coarse, 10kGNAD) using $F_1$ score and accuracy. The
GeistBERT models achieved strong performance, leading all tasks among the base
models and setting a new state-of-the-art (SOTA). Notably, the base models
outperformed larger models in several tasks. To support the German NLP research
community, we are releasing GeistBERT under the MIT license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TreeRL: LLM Reinforcement Learning with On-Policy Tree Search <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Hou, Ziniu Hu, Yujiang Li, Rui Lu, Jie Tang, Yuxiao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) with tree search has demonstrated superior
performance in traditional reasoning tasks. Compared to conventional
independent chain sampling strategies with outcome supervision, tree search
enables better exploration of the reasoning space and provides dense, on-policy
process rewards during RL training but remains under-explored in On-Policy LLM
RL. We propose TreeRL, a reinforcement learning framework that directly
incorporates on-policy tree search for RL training. Our approach includes
intermediate supervision and eliminates the need for a separate reward model
training. Existing approaches typically train a separate process reward model,
which can suffer from distribution mismatch and reward hacking. We also
introduce a cost-effective tree search approach that achieves higher search
efficiency under the same generation token budget by strategically branching
from high-uncertainty intermediate steps rather than using random branching.
Experiments on challenging math and code reasoning benchmarks demonstrate that
TreeRL achieves superior performance compared to traditional ChainRL,
highlighting the potential of tree search for LLM. TreeRL is open-sourced at
https://github.com/THUDM/TreeRL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Cascaded LLM Framework for Cost-effective <span class="highlight-title">Human</span>-AI
  Decision-Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claudio Fanconi, Mihaela van der Schaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective human-AI decision-making balances three key factors: the
\textit{correctness} of predictions, the \textit{cost} of knowledge and
reasoning complexity, and the confidence about whether to \textit{abstain}
automated answers or involve human experts. In this work, we present a cascaded
LLM decision framework that adaptively delegates tasks across multiple tiers of
expertise -- a base model for initial candidate answers, a more capable and
knowledgeable (but costlier) large model, and a human expert for when the model
cascade abstains. Our method proceeds in two stages. First, a deferral policy
determines whether to accept the base model's answer or regenerate it with the
large model based on the confidence score. Second, an abstention policy decides
whether the cascade model response is sufficiently certain or requires human
intervention. Moreover, we incorporate an online learning mechanism in the
framework that can leverage human feedback to improve decision quality over
time. We demonstrate this approach to general question-answering (ARC-Easy and
ARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results
show that our cascaded strategy outperforms in most cases single-model
baselines in accuracy while reducing cost and providing a principled way to
handle abstentions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Homogeneous Attention: <span class="highlight-title">Memory</span>-Efficient LLMs via
  Fourier-Approximated KV Cache 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoran Liu, Siyang He, Qiqi Wang, Ruixiao Li, Yuerong Song, Zhigeng Liu, Linlin Li, Qun Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models struggle with memory demands from the growing Key-Value
(KV) cache as context lengths increase. Existing compression methods homogenize
head dimensions or rely on attention-guided token pruning, often sacrificing
accuracy or introducing computational overhead. We propose FourierAttention, a
training-free framework that exploits the heterogeneous roles of transformer
head dimensions: lower dimensions prioritize local context, while upper ones
capture long-range dependencies. By projecting the long-context-insensitive
dimensions onto orthogonal Fourier bases, FourierAttention approximates their
temporal evolution with fixed-length spectral coefficients. Evaluations on
LLaMA models show that FourierAttention achieves the best long-context accuracy
on LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,
FlashFourierAttention, is designed to optimize memory via streamlined
read-write operations, enabling efficient deployment without performance
compromise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Bias in LLMs: Strategies and Application to Fair AI-based
  Recruitment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejandro Peña, Julian Fierrez, Aythami Morales, Gonzalo Mancera, Miguel Lopez, Ruben Tolosana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of language technologies in high-stake settings is increasing in
recent years, mostly motivated by the success of Large Language Models (LLMs).
However, despite the great performance of LLMs, they are are susceptible to
ethical concerns, such as demographic biases, accountability, or privacy. This
work seeks to analyze the capacity of Transformers-based systems to learn
demographic biases present in the data, using a case study on AI-based
automated recruitment. We propose a privacy-enhancing framework to reduce
gender information from the learning pipeline as a way to mitigate biased
behaviors in the final tools. Our experiments analyze the influence of data
biases on systems built on two different LLMs, and how the proposed framework
effectively prevents trained systems from reproducing the bias in the data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to AIES 2025 (Under Review)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Post Persona Alignment for Multi-Session <span class="highlight-title">Dialogue</span> Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Pei Chen, Noriki Nishida, Hideki Nakayama, Yuji Matsumoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-session persona-based dialogue generation presents challenges in
maintaining long-term consistency and generating diverse, personalized
responses. While large language models (LLMs) excel in single-session
dialogues, they struggle to preserve persona fidelity and conversational
coherence across extended interactions. Existing methods typically retrieve
persona information before response generation, which can constrain diversity
and result in generic outputs. We propose Post Persona Alignment (PPA), a novel
two-stage framework that reverses this process. PPA first generates a general
response based solely on dialogue context, then retrieves relevant persona
memories using the response as a query, and finally refines the response to
align with the speaker's persona. This post-hoc alignment strategy promotes
naturalness and diversity while preserving consistency and personalization.
Experiments on multi-session LLM-generated dialogue data demonstrate that PPA
significantly outperforms prior approaches in consistency, diversity, and
persona relevance, offering a more flexible and effective paradigm for
long-term personalized dialogue generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Multilingual Vision-Language Translation: Dataset,
  Evaluation, and Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xintong Wang, Jingheng Pan, Yixiao Liu, Xiaohu Zhao, Chenyang Lyu, Minghao Wu, Chris Biemann, Longyue Wang, Linlong Xu, Weihua Luo, Kaifu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Translation (VLT) is a challenging task that requires
accurately recognizing multilingual text embedded in images and translating it
into the target language with the support of visual context. While recent Large
Vision-Language Models (LVLMs) have demonstrated strong multilingual and visual
understanding capabilities, there is a lack of systematic evaluation and
understanding of their performance on VLT. In this work, we present a
comprehensive study of VLT from three key perspectives: data quality, model
architecture, and evaluation metrics. (1) We identify critical limitations in
existing datasets, particularly in semantic and cultural fidelity, and
introduce AibTrans -- a multilingual, parallel, human-verified dataset with
OCR-corrected annotations. (2) We benchmark 11 commercial LVLMs/LLMs and 6
state-of-the-art open-source models across end-to-end and cascaded
architectures, revealing their OCR dependency and contrasting generation versus
reasoning behaviors. (3) We propose Density-Aware Evaluation to address metric
reliability issues under varying contextual complexity, introducing the DA
Score as a more robust measure of translation quality. Building upon these
findings, we establish a new evaluation benchmark for VLT. Notably, we observe
that fine-tuning LVLMs on high-resource language pairs degrades cross-lingual
performance, and we propose a balanced multilingual fine-tuning strategy that
effectively adapts LVLMs to VLT without sacrificing their generalization
ability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Performance of LLMs for Real Estate Appraisal <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Margot Geerts, Manon Reusens, Bart Baesens, Seppe vanden Broucke, Jochen De Weerdt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The real estate market is vital to global economies but suffers from
significant information asymmetry. This study examines how Large Language
Models (LLMs) can democratize access to real estate insights by generating
competitive and interpretable house price estimates through optimized
In-Context Learning (ICL) strategies. We systematically evaluate leading LLMs
on diverse international housing datasets, comparing zero-shot, few-shot,
market report-enhanced, and hybrid prompting techniques. Our results show that
LLMs effectively leverage hedonic variables, such as property size and
amenities, to produce meaningful estimates. While traditional machine learning
models remain strong for pure predictive accuracy, LLMs offer a more
accessible, interactive and interpretable alternative. Although
self-explanations require cautious interpretation, we find that LLMs explain
their predictions in agreement with state-of-the-art models, confirming their
trustworthiness. Carefully selected in-context examples based on feature
similarity and geographic proximity, significantly enhance LLM performance, yet
LLMs struggle with overconfidence in price intervals and limited spatial
reasoning. We offer practical guidance for structured prediction tasks through
prompt optimization. Our findings highlight LLMs' potential to improve
transparency in real estate appraisal and provide actionable insights for
stakeholders.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECML-PKDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Multimodal Large Language Models Pragmatically Competent Listeners
  in Simple Reference Resolution Tasks? <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simeon Junker, Manar Ali, Larissa Koch, Sina Zarrieß, Hendrik Buschmeier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the linguistic abilities of multimodal large language models
in reference resolution tasks featuring simple yet abstract visual stimuli,
such as color patches and color grids. Although the task may not seem
challenging for today's language models, being straightforward for human dyads,
we consider it to be a highly relevant probe of the pragmatic capabilities of
MLLMs. Our results and analyses indeed suggest that basic pragmatic
capabilities, such as context-dependent interpretation of color descriptions,
still constitute major challenges for state-of-the-art MLLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ACL Findings 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Persona-driven <span class="highlight-title">Simulat</span>ion of Voting Behavior in the European Parliament
  with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Kreutner, Marlene Lutz, Markus Strohmaier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) display remarkable capabilities to understand or
even produce political discourse, but have been found to consistently display a
progressive left-leaning bias. At the same time, so-called persona or identity
prompts have been shown to produce LLM behavior that aligns with socioeconomic
groups that the base model is not aligned with. In this work, we analyze
whether zero-shot persona prompting with limited information can accurately
predict individual voting decisions and, by aggregation, accurately predict
positions of European groups on a diverse set of policies. We evaluate if
predictions are stable towards counterfactual arguments, different persona
prompts and generation methods. Finally, we find that we can simulate voting
behavior of Members of the European Parliament reasonably well with a weighted
F1 score of approximately 0.793. Our persona dataset of politicians in the 2024
European Parliament and our code are available at
https://github.com/dess-mannheim/european_parliament_simulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-Short Alignment for Effective Long-Context Modeling in LLMs <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianqi Du, Haotian Huang, Yifei Wang, Yisen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have exhibited impressive performance and
surprising emergent properties. However, their effectiveness remains limited by
the fixed context window of the transformer architecture, posing challenges for
long-context modeling. Among these challenges, length generalization -- the
ability to generalize to sequences longer than those seen during training -- is
a classical and fundamental problem. In this work, we propose a fresh
perspective on length generalization, shifting the focus from the conventional
emphasis on input features such as positional encodings or data structures to
the output distribution of the model. Specifically, through case studies on
synthetic tasks, we highlight the critical role of \textbf{long-short
alignment} -- the consistency of output distributions across sequences of
varying lengths. Extending this insight to natural language tasks, we propose a
metric called Long-Short Misalignment to quantify this phenomenon, uncovering a
strong correlation between the metric and length generalization performance.
Building on these findings, we develop a regularization term that promotes
long-short alignment during training. Extensive experiments validate the
effectiveness of our approach, offering new insights for achieving more
effective long-context modeling in LLMs. Code is available at
https://github.com/PKU-ML/LongShortAlignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepResearch Bench: A Comprehensive Benchmark for Deep Research <span class="highlight-title">Agent</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, Zhendong Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Research Agents are a prominent category of LLM-based agents. By
autonomously orchestrating multistep web exploration, targeted retrieval, and
higher-order synthesis, they transform vast amounts of online information into
analyst-grade, citation-rich reports--compressing hours of manual desk research
into minutes. However, a comprehensive benchmark for systematically evaluating
the capabilities of these agents remains absent. To bridge this gap, we present
DeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks,
each meticulously crafted by domain experts across 22 distinct fields.
Evaluating DRAs is inherently complex and labor-intensive. We therefore propose
two novel methodologies that achieve strong alignment with human judgment. The
first is a reference-based method with adaptive criteria to assess the quality
of generated research reports. The other framework is introduced to evaluate
DRA's information retrieval and collection capabilities by assessing its
effective citation count and overall citation accuracy. We have open-sourced
DeepResearch Bench and key components of these frameworks at
https://github.com/Ayanami0730/deep_research_bench to accelerate the
development of practical LLM-based agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DART: Distilling Autoregressive Reasoning to Silent Thought 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Jiang, Ziming Wu, De-Chuan Zhan, Fuming Lai, Shaobing Lian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) reasoning has significantly advanced Large Language
Models (LLMs) in solving complex tasks. However, its autoregressive paradigm
leads to significant computational overhead, hindering its deployment in
latency-sensitive applications. To address this, we propose \textbf{DART}
(\textbf{D}istilling \textbf{A}utoregressive \textbf{R}easoning to Silent
\textbf{T}hought), a self-distillation framework that enables LLMs to replace
autoregressive CoT with non-autoregressive Silent Thought (ST). Specifically,
DART introduces two training pathways: the CoT pathway for traditional
reasoning and the ST pathway for generating answers directly from a few ST
tokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM)
to align its hidden states with the CoT pathway, enabling the ST tokens to
evolve into informative embeddings. During inference, only the ST pathway is
activated, leveraging evolving ST tokens to deliver the answer directly.
Extensive experimental results demonstrate that DART achieves comparable
reasoning performance to existing baselines while offering significant
efficiency gains, serving as a feasible alternative for efficient reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in
  Interleaved Multi-Image Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dinh Viet Cuong, Hoang-Bao Le, An Pham Ngoc Nguyen, Liting Zhou, Cathal Gurrin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses two main objectives. Firstly, we demonstrate the
impressive performance of the LLaVA-NeXT-interleave on 22 datasets across three
different tasks: Multi-Image Reasoning, Documents and Knowledge-Based
Understanding and Interactive Multi-Modal Communication. Secondly, we add the
Dense Channel Integration (DCI) connector to the LLaVA-NeXT-Interleave and
compare its performance against the standard model. We find that the standard
model achieves the highest overall accuracy, excelling in vision-heavy tasks
like VISION, NLVR2, and Fashion200K. Meanwhile, the DCI-enhanced version shows
particular strength on datasets requiring deeper semantic coherence or
structured change understanding such as MIT-States_PropertyCoherence and
SlideVQA. Our results highlight the potential of combining powerful foundation
models with plug-and-play techniques for Interleave tasks. The code is
available at https://github.com/dinhvietcuong1996/icme25-inova.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Cambrian Explosion of Mixed-Precision Matrix Multiplication for
  Quantized Deep Learning Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Héctor Martínez, Adrián Castelló, Francisco D. Igual, Enrique S. Quintana-Ortí
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep learning (DL) have led to a shift from traditional
64-bit floating point (FP64) computations toward reduced-precision formats,
such as FP16, BF16, and 8- or 16-bit integers, combined with mixed-precision
arithmetic. This transition enhances computational throughput, reduces memory
and bandwidth usage, and improves energy efficiency, offering significant
advantages for resource-constrained edge devices. To support this shift,
hardware architectures have evolved accordingly, now including adapted ISAs
(Instruction Set Architectures) that expose mixed-precision vector units and
matrix engines tailored for DL workloads. At the heart of many DL and
scientific computing tasks is the general matrix-matrix multiplication gemm, a
fundamental kernel historically optimized using axpy vector instructions on
SIMD (single instruction, multiple data) units. However, as hardware moves
toward mixed-precision dot-product-centric operations optimized for quantized
inference, these legacy approaches are being phased out. In response to this,
our paper revisits traditional high-performance gemm and describes strategies
for adapting it to mixed-precision integer (MIP) arithmetic across modern ISAs,
including x86_64, ARM, and RISC-V. Concretely, we illustrate novel micro-kernel
designs and data layouts that better exploit today's specialized hardware and
demonstrate significant performance gains from MIP arithmetic over
floating-point implementations across three representative CPU architectures.
These contributions highlight a new era of gemm optimization-driven by the
demands of DL inference on heterogeneous architectures, marking what we term as
the "Cambrian period" for matrix multiplication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 tables, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Configurable Preference Tuning with Rubric-Guided Synthetic Data <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Víctor Gallego
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models of human feedback for AI alignment, such as those underpinning Direct
Preference Optimization (DPO), often bake in a singular, static set of
preferences, limiting adaptability. This paper challenges the assumption of
monolithic preferences by introducing Configurable Preference Tuning (CPT), a
novel framework for endowing language models with the ability to dynamically
adjust their behavior based on explicit, human-interpretable directives. CPT
leverages synthetically generated preference data, conditioned on system
prompts derived from structured, fine-grained rubrics that define desired
attributes like writing style. By fine-tuning with these rubric-guided
preferences, the LLM learns to modulate its outputs at inference time in
response to the system prompt, without retraining. This approach not only
offers fine-grained control but also provides a mechanism for modeling more
nuanced and context-dependent human feedback. Several experimental artifacts,
such as training code, generated datasets and fine-tuned models are released at
https://github.com/vicgalle/configurable-preference-tuning
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2025 Workshop on Models of Human Feedback for AI
  Alignment</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs for Sentence Simplification: A Hybrid <span class="highlight-title">Multi-Agent</span> prompting
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pratibha Zunjare, Michael Hsiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of transforming complex sentences into
sequences of logical, simplified sentences while preserving semantic and
logical integrity with the help of Large Language Models. We propose a hybrid
approach that combines advanced prompting with multi-agent architectures to
enhance the sentence simplification process. Experimental results show that our
approach was able to successfully simplify 70% of the complex sentences written
for video game design application. In comparison, a single-agent approach
attained a 48% success rate on the same task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Causal Interventions in Amnesic Probing with Mean Projection
  or LEACE 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alicja Dobrzeniecka, Antske Fokkens, Pia Sommerauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Amnesic probing is a technique used to examine the influence of specific
linguistic information on the behaviour of a model. This involves identifying
and removing the relevant information and then assessing whether the model's
performance on the main task changes. If the removed information is relevant,
the model's performance should decline. The difficulty with this approach lies
in removing only the target information while leaving other information
unchanged. It has been shown that Iterative Nullspace Projection (INLP), a
widely used removal technique, introduces random modifications to
representations when eliminating target information. We demonstrate that Mean
Projection (MP) and LEACE, two proposed alternatives, remove information in a
more targeted manner, thereby enhancing the potential for obtaining behavioural
explanations through Amnesic Probing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Converting Annotated Clinical Cases into Structured Case Report Forms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pietro Ferrazzi, Alberto Lavelli, Bernardo Magnini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Case Report Forms (CRFs) are largely used in medical research as they ensure
accuracy, reliability, and validity of results in clinical studies. However,
publicly available, wellannotated CRF datasets are scarce, limiting the
development of CRF slot filling systems able to fill in a CRF from clinical
notes. To mitigate the scarcity of CRF datasets, we propose to take advantage
of available datasets annotated for information extraction tasks and to convert
them into structured CRFs. We present a semi-automatic conversion methodology,
which has been applied to the E3C dataset in two languages (English and
Italian), resulting in a new, high-quality dataset for CRF slot filling.
Through several experiments on the created dataset, we report that slot filling
achieves 59.7% for Italian and 67.3% for English on a closed Large Language
Models (zero-shot) and worse performances on three families of open-source
models, showing that filling CRFs is challenging even for recent
state-of-the-art LLMs. We release the datest at
https://huggingface.co/collections/NLP-FBK/e3c-to-crf-67b9844065460cbe42f80166
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in BioNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoRA-Gen: Specializing Large Language Model via Online LoRA Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yicheng Xiao, Lin Song, Rui Yang, Cheng Cheng, Yixiao Ge, Xiu Li, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances have highlighted the benefits of scaling language models to
enhance performance across a wide range of NLP tasks. However, these approaches
still face limitations in effectiveness and efficiency when applied to
domain-specific tasks, particularly for small edge-side models. We propose the
LoRA-Gen framework, which utilizes a large cloud-side model to generate LoRA
parameters for edge-side models based on task descriptions. By employing the
reparameterization technique, we merge the LoRA parameters into the edge-side
model to achieve flexible specialization. Our method facilitates knowledge
transfer between models while significantly improving the inference efficiency
of the specialized model by reducing the input context length. Without
specialized training, LoRA-Gen outperforms conventional LoRA fine-tuning, which
achieves competitive accuracy and a 2.1x speedup with TinyLLaMA-1.1B in
reasoning tasks. Besides, our method delivers a compression ratio of 10.1x with
Gemma-2B on intelligent agent tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SceneGram: Conceptualizing and Describing Tangrams in Scene Context <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simeon Junker, Sina Zarrieß
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on reference and naming suggests that humans can come up with very
different ways of conceptualizing and referring to the same object, e.g. the
same abstract tangram shape can be a "crab", "sink" or "space ship". Another
common assumption in cognitive science is that scene context fundamentally
shapes our visual perception of objects and conceptual expectations. This paper
contributes SceneGram, a dataset of human references to tangram shapes placed
in different scene contexts, allowing for systematic analyses of the effect of
scene context on conceptualization. Based on this data, we analyze references
to tangram shapes generated by multimodal LLMs, showing that these models do
not account for the richness and variability of conceptualizations found in
human references.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ACL Findings 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ (SimPhon Speech Test): A Data-Driven Method for In Silico Design and
  Validation of a Phonetically Balanced Speech Test 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Bleeck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional audiometry often provides an incomplete characterization of the
functional impact of hearing loss on speech understanding, particularly for
supra-threshold deficits common in presbycusis. This motivates the development
of more diagnostically specific speech perception tests. We introduce the
Simulated Phoneme Speech Test (SimPhon Speech Test) methodology, a novel,
multi-stage computational pipeline for the in silico design and validation of a
phonetically balanced minimal-pair speech test. This methodology leverages a
modern Automatic Speech Recognition (ASR) system as a proxy for a human
listener to simulate the perceptual effects of sensorineural hearing loss. By
processing speech stimuli under controlled acoustic degradation, we first
identify the most common phoneme confusion patterns. These patterns then guide
the data-driven curation of a large set of candidate word pairs derived from a
comprehensive linguistic corpus. Subsequent phases involving simulated
diagnostic testing, expert human curation, and a final, targeted sensitivity
analysis systematically reduce the candidates to a final, optimized set of 25
pairs (the SimPhon Speech Test-25). A key finding is that the diagnostic
performance of the SimPhon Speech Test-25 test items shows no significant
correlation with predictions from the standard Speech Intelligibility Index
(SII), suggesting the SimPhon Speech Test captures perceptual deficits beyond
simple audibility. This computationally optimized test set offers a significant
increase in efficiency for audiological test development, ready for initial
human trials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLM@school -- Evaluation of AI image understanding on German middle
  school knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        René Peinl, Vincent Tischler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel benchmark dataset designed to evaluate the
capabilities of Vision Language Models (VLMs) on tasks that combine visual
reasoning with subject-specific background knowledge in the German language. In
contrast to widely used English-language benchmarks that often rely on
artificially difficult or decontextualized problems, this dataset draws from
real middle school curricula across nine domains including mathematics,
history, biology, and religion. The benchmark includes over 2,000 open-ended
questions grounded in 486 images, ensuring that models must integrate visual
interpretation with factual reasoning rather than rely on superficial textual
cues. We evaluate thirteen state-of-the-art open-weight VLMs across multiple
dimensions, including domain-specific accuracy and performance on adversarial
crafted questions. Our findings reveal that even the strongest models achieve
less than 45% overall accuracy, with particularly poor performance in music,
mathematics, and adversarial settings. Furthermore, the results indicate
significant discrepancies between success on popular benchmarks and real-world
multimodal understanding. We conclude that middle school-level tasks offer a
meaningful and underutilized avenue for stress-testing VLMs, especially in
non-English contexts. The dataset and evaluation protocol serve as a rigorous
testbed to better understand and improve the visual and linguistic reasoning
capabilities of future AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are LLMs Good Text Diacritizers? An Arabic and Yorùbá Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hawau Olamide Toyin, Samar M. Magdy, Hanan Aldarmaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the effectiveness of large language models (LLMs) for text
diacritization in two typologically distinct languages: Arabic and Yoruba. To
enable a rigorous evaluation, we introduce a novel multilingual dataset
MultiDiac, with diverse samples that capture a range of diacritic ambiguities.
We evaluate 14 LLMs varying in size, accessibility, and language coverage, and
benchmark them against 6 specialized diacritization models. Additionally, we
fine-tune four small open-source models using LoRA for Yoruba. Our results show
that many off-the-shelf LLMs outperform specialized diacritization models for
both Arabic and Yoruba, but smaller models suffer from hallucinations.
Fine-tuning on a small dataset can help improve diacritization performance and
reduce hallucination rates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning
  with Video LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo-Cheng Chiu, Jen-Jee Chen, Yu-Chee Tseng, Feng-Chi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently been extended to the video domain,
enabling sophisticated video-language understanding. However, existing Video
LLMs often exhibit limitations in fine-grained temporal reasoning, restricting
their ability to precisely attribute responses to specific video moments,
especially under constrained supervision. We introduce DaMO, a data-efficient
Video LLM explicitly designed for accurate temporal reasoning and multimodal
understanding. At its core, the proposed Temporal-aware Fuseformer employs a
hierarchical dual-stream architecture that progressively captures temporal
dynamics within each modality and effectively fuses complementary visual and
audio information. To further enhance computational efficiency, DaMO integrates
a global residual that reduces spatial redundancy while preserving essential
semantic details. We train DaMO via a structured four-stage progressive
training paradigm, incrementally equipping the model with multimodal alignment,
semantic grounding, and temporal reasoning capabilities. This work also
contributes multiple datasets augmented from existing ones with GPT-generated
temporally grounded QA pairs for tasks requiring temporal supervision.
Comprehensive experiments on temporal grounding and video QA benchmarks
demonstrate that DaMO consistently surpasses prior methods, particularly in
tasks demanding precise temporal alignment and reasoning. Our work establishes
a promising direction for data-efficient video-language modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Persona to Person: Enhancing the Naturalness with Multiple
  Dis<span class="highlight-title">course</span> Relations Graph Learning in <span class="highlight-title">Personal</span>ized <span class="highlight-title">Dialogue</span> Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Hao Hsu, Ying-Jia Lin, Hung-Yu Kao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In dialogue generation, the naturalness of responses is crucial for effective
human-machine interaction. Personalized response generation poses even greater
challenges, as the responses must remain coherent and consistent with the
user's personal traits or persona descriptions. We propose MUDI
($\textbf{Mu}$ltiple $\textbf{Di}$scourse Relations Graph Learning) for
personalized dialogue generation. We utilize a Large Language Model to assist
in annotating discourse relations and to transform dialogue data into
structured dialogue graphs. Our graph encoder, the proposed DialogueGAT model,
then captures implicit discourse relations within this structure, along with
persona descriptions. During the personalized response generation phase, novel
coherence-aware attention strategies are implemented to enhance the decoder's
consideration of discourse relations. Our experiments demonstrate significant
improvements in the quality of personalized responses, thus resembling
human-like dialogue exchanges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by PAKDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Wang, Shiwan Zhao, Ming Fan, Zhihu Wang, Yubo Zhang, Xicheng Zhang, Zhengfan Wang, Heyuan Huang, Ting Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of external knowledge through Retrieval-Augmented Generation
(RAG) has become foundational in enhancing large language models (LLMs) for
knowledge-intensive tasks. However, existing RAG paradigms often overlook the
cognitive step of applying knowledge, leaving a gap between retrieved facts and
task-specific reasoning. In this work, we introduce RAG+, a principled and
modular extension that explicitly incorporates application-aware reasoning into
the RAG pipeline. RAG+ constructs a dual corpus consisting of knowledge and
aligned application examples, created either manually or automatically, and
retrieves both jointly during inference. This design enables LLMs not only to
access relevant information but also to apply it within structured,
goal-oriented reasoning processes. Experiments across mathematical, legal, and
medical domains, conducted on multiple models, demonstrate that RAG+
consistently outperforms standard RAG variants, achieving average improvements
of 3-5%, and peak gains up to 7.5% in complex scenarios. By bridging retrieval
with actionable application, RAG+ advances a more cognitively grounded
framework for knowledge integration, representing a step toward more
interpretable and capable LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Brewing Knowledge in Context: Distillation Perspectives on In-Context
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengye Li, Haiyun Liu, Yuanxi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) allows large language models (LLMs) to solve novel
tasks without weight updates. Despite its empirical success, the mechanism
behind ICL remains poorly understood, limiting our ability to interpret,
improve, and reliably apply it. In this paper, we propose a new theoretical
perspective that interprets ICL as an implicit form of knowledge distillation
(KD), where prompt demonstrations guide the model to form a task-specific
reference model during inference. Under this view, we derive a Rademacher
complexity-based generalization bound and prove that the bias of the distilled
weights grows linearly with the Maximum Mean Discrepancy (MMD) between the
prompt and target distributions. This theoretical framework explains several
empirical phenomena and unifies prior gradient-based and distributional
analyses. To the best of our knowledge, this is the first to formalize
inference-time attention as a distillation process, which provides theoretical
insights for future prompt engineering and automated demonstration selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 main pages, 10 page appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs
  and MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Xu, Libo Qin, Wanxiang Che, Min-Yen Kan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two-Tower Vision--Language Models (VLMs) have demonstrated strong performance
across various downstream VL tasks. While BridgeTower further enhances
performance by building bridges between encoders, it \textit{(i)} suffers from
ineffective layer-by-layer utilization of unimodal representations,
\textit{(ii)} restricts the flexible exploitation of different levels of
unimodal semantic knowledge, and \textit{(iii)} is limited to the evaluation on
traditional low-resolution datasets only with the Two-Tower VLM architecture.
In this work, we propose Manager, a lightweight, efficient and effective plugin
that adaptively aggregates insights from different levels of pre-trained
unimodal experts to facilitate more comprehensive VL alignment and fusion.
First, under the Two-Tower VLM architecture, we introduce ManagerTower, a novel
VLM that introduces the manager in each cross-modal layer. Whether with or
without VL pre-training, ManagerTower outperforms previous strong baselines and
achieves superior performance on 4 downstream VL tasks. Moreover, we extend our
exploration to the latest Multimodal Large Language Model (MLLM) architecture.
We demonstrate that LLaVA-OV-Manager significantly boosts the zero-shot
performance of LLaVA-OV across different categories of capabilities, images,
and resolutions on 20 downstream datasets, whether the multi-grid algorithm is
enabled or not. In-depth analysis reveals that both our manager and the
multi-grid algorithm can be viewed as a plugin that improves the visual
representation by capturing more diverse visual details from two orthogonal
perspectives (depth and width). Their synergy can mitigate the semantic
ambiguity caused by the multi-grid algorithm and further improve performance.
Code and models are available at https://github.com/LooperXX/ManagerTower.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Circuits and Systems for Video
  Technology (TCSVT). June 2025. DOI:
  https://doi.org/10.1109/TCSVT.2025.3578266</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Effectiveness of Integration Methods for Multimodal <span class="highlight-title">Dialogue</span>
  Response Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongbo Jang, Seonghyeon Lee, Dongha Lee, Hwanjo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal chatbots have become one of the major topics for dialogue systems
in both research community and industry. Recently, researchers have shed light
on the multimodality of responses as well as dialogue contexts. This work
explores how a dialogue system can output responses in various modalities such
as text and image. To this end, we first formulate a multimodal dialogue
response retrieval task for retrieval-based systems as the combination of three
subtasks. We then propose three integration methods based on a two-step
approach and an end-to-end approach, and compare the merits and demerits of
each method. Experimental results on two datasets demonstrate that the
end-to-end approach achieves comparable performance without an intermediate
step in the two-step approach. In addition, a parameter sharing strategy not
only reduces the number of parameters but also boosts performance by
transferring knowledge across the subtasks and the modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lag-Relative Sparse Attention In Long Context Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manlai Liang, Wanyi Huang, Mandi Liu, Huaijun Li, Jinlong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have made significant strides in natural
language processing and generation, yet their ability to handle long-context
input remains constrained by the quadratic complexity of attention computation
and linear-increasing key-value memory footprint. To reduce computational costs
and memory, key-value cache compression techniques are commonly applied at
inference time, but this often leads to severe performance degradation, as
models are not trained to handle compressed context. Although there are more
sophisticated compression methods, they are typically unsuitable for
post-training because of their incompatibility with gradient-based optimization
or high computation overhead. To fill this gap with no additional parameter and
little computation overhead, we propose Lag-Relative Sparse Attention(LRSA)
anchored by the LagKV compression method for long context post-training. Our
method performs chunk-by-chunk prefilling, which selects the top K most
relevant key-value pairs in a fixed-size lagging window, allowing the model to
focus on salient historical context while maintaining efficiency. Experimental
results show that our approach significantly enhances the robustness of the LLM
with key-value compression and achieves better fine-tuned results in the
question-answer tuning task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relational Schemata in BERT Are Inducible, Not Emergent: A Study of
  Performance vs. Competence in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cole Gawin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models like BERT demonstrate strong empirical
performance on semantic tasks, whether this reflects true conceptual competence
or surface-level statistical association remains unclear. I investigate whether
BERT encodes abstract relational schemata by examining internal representations
of concept pairs across taxonomic, mereological, and functional relations. I
compare BERT's relational classification performance with representational
structure in [CLS] token embeddings. Results reveal that pretrained BERT
enables high classification accuracy, indicating latent relational signals.
However, concept pairs organize by relation type in high-dimensional embedding
space only after fine-tuning on supervised relation classification tasks. This
indicates relational schemata are not emergent from pretraining alone but can
be induced via task scaffolding. These findings demonstrate that behavioral
performance does not necessarily imply structured conceptual understanding,
though models can acquire inductive biases for grounded relational abstraction
through appropriate training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImmunoFOMO: Are Language Models missing what oncologists see? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Sinha, Bogdan-Valentin Popescu, Xavier Coubez, Marianne Clausel, Mathieu Constant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) capabilities have grown with a fast pace over the past
decade leading researchers in various disciplines, such as biomedical research,
to increasingly explore the utility of LMs in their day-to-day applications.
Domain specific language models have already been in use for biomedical natural
language processing (NLP) applications. Recently however, the interest has
grown towards medical language models and their understanding capabilities. In
this paper, we investigate the medical conceptual grounding of various language
models against expert clinicians for identification of hallmarks of
immunotherapy in breast cancer abstracts. Our results show that pre-trained
language models have potential to outperform large language models in
identifying very specific (low-level) concepts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoGen Driven Multi <span class="highlight-title">Agent</span> Framework for Iterative Crime Data Analysis
  and Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Syeda Kisaa Fatima, Tehreem Zubair, Noman Ahmed, Asifullah Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces LUCID-MA (Learning and Understanding Crime through
Dialogue of Multiple Agents), an innovative AI powered framework where multiple
AI agents collaboratively analyze and understand crime data. Our system that
consists of three core components: an analysis assistant that highlights
spatiotemporal crime patterns, a feedback component that reviews and refines
analytical results and a prediction component that forecasts future crime
trends. With a well-designed prompt and the LLaMA-2-13B-Chat-GPTQ model, it
runs completely offline and allows the agents undergo self-improvement through
100 rounds of communication with less human interaction. A scoring function is
incorporated to evaluate agent's performance, providing visual plots to track
learning progress. This work demonstrates the potential of AutoGen-style agents
for autonomous, scalable, and iterative analysis in social science domains
maintaining data privacy through offline execution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified
  Process Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehoon Yun, Jiwoong Sohn, Jungwoo Park, Hyunjae Kim, Xiangru Tang, Yanjun Shao, Yonghoe Koo, Minhyeok Ko, Qingyu Chen, Mark Gerstein, Michael Moor, Jaewoo Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have shown promise in clinical decision making, but
current approaches struggle to localize and correct errors at specific steps of
the reasoning process. This limitation is critical in medicine, where
identifying and addressing reasoning errors is essential for accurate diagnosis
and effective patient care. We introduce Med-PRM, a process reward modeling
framework that leverages retrieval-augmented generation to verify each
reasoning step against established medical knowledge bases. By verifying
intermediate reasoning steps with evidence retrieved from clinical guidelines
and literature, our model can precisely assess the reasoning quality in a
fine-grained manner. Evaluations on five medical QA benchmarks and two
open-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art
performance, with improving the performance of base models by up to 13.50%
using Med-PRM. Moreover, we demonstrate the generality of Med-PRM by
integrating it in a plug-and-play fashion with strong policy models such as
Meerkat, achieving over 80\% accuracy on MedQA for the first time using
small-scale models of 8 billion parameters. Our code and data are available at:
https://med-prm.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Gamified Evaluation and Recruitment Platform for Low Resource Language
  Machine Translation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Rafael Catalan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human evaluators provide necessary contributions in evaluating large language
models. In the context of Machine Translation (MT) systems for low-resource
languages (LRLs), this is made even more apparent since popular automated
metrics tend to be string-based, and therefore do not provide a full picture of
the nuances of the behavior of the system. Human evaluators, when equipped with
the necessary expertise of the language, will be able to test for adequacy,
fluency, and other important metrics. However, the low resource nature of the
language means that both datasets and evaluators are in short supply. This
presents the following conundrum: How can developers of MT systems for these
LRLs find adequate human evaluators and datasets? This paper first presents a
comprehensive review of existing evaluation procedures, with the objective of
producing a design proposal for a platform that addresses the resource gap in
terms of datasets and evaluators in developing MT systems. The result is a
design for a recruitment and gamified evaluation platform for developers of MT
systems. Challenges are also discussed in terms of evaluating this platform, as
well as its possible applications in the wider scope of Natural Language
Processing (NLP) research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, presented at the HEAL Workshop at CHI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AbsenceBench: Language Models Can't Tell What's Missing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harvey Yiyun Fu, Aryan Shrivastava, Jared Moore, Peter West, Chenhao Tan, Ari Holtzman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly capable of processing long
inputs and locating specific information within them, as evidenced by their
performance on the Needle in a Haystack (NIAH) test. However, while models
excel at recalling surprising information, they still struggle to identify
clearly omitted information. We introduce AbsenceBench to assesses LLMs'
capacity to detect missing information across three domains: numerical
sequences, poetry, and GitHub pull requests. AbsenceBench asks models to
identify which pieces of a document were deliberately removed, given access to
both the original and edited contexts. Despite the apparent straightforwardness
of these tasks, our experiments reveal that even state-of-the-art models like
Claude-3.7-Sonnet achieve only 69.6% F1-score with a modest average context
length of 5K tokens. Our analysis suggests this poor performance stems from a
fundamental limitation: Transformer attention mechanisms cannot easily attend
to "gaps" in documents since these absences don't correspond to any specific
keys that can be attended to. Overall, our results and analysis provide a case
study of the close proximity of tasks where models are already superhuman
(NIAH) and tasks where models breakdown unexpectedly (AbsenceBench).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures. Code and data are publicly available at
  https://github.com/harvey-fin/absence-bench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KoGEC : Korean Grammatical Error Correction with Pre-trained Translation
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeeun Kim, Semin Jeong, Youngsook Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research introduces KoGEC, a Korean Grammatical Error Correction system
using pre\--trained translation models. We fine-tuned NLLB (No Language Left
Behind) models for Korean GEC, comparing their performance against large
language models like GPT-4 and HCX-3. The study used two social media
conversation datasets for training and testing. The NLLB models were fine-tuned
using special language tokens to distinguish between original and corrected
Korean sentences. Evaluation was done using BLEU scores and an "LLM as judge"
method to classify error types. Results showed that the fine-tuned NLLB (KoGEC)
models outperformed GPT-4o and HCX-3 in Korean GEC tasks. KoGEC demonstrated a
more balanced error correction profile across various error types, whereas the
larger LLMs tended to focus less on punctuation errors. We also developed a
Chrome extension to make the KoGEC system accessible to users. Finally, we
explored token vocabulary expansion to further improve the model but found it
to decrease model performance. This research contributes to the field of NLP by
providing an efficient, specialized Korean GEC system and a new evaluation
method. It also highlights the potential of compact, task-specific models to
compete with larger, general-purpose language models in specialized NLP tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Agent</span>-RLVR: Training Software Engineering <span class="highlight-title">Agent</span>s via Guidance and
  Environment Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeff Da, Clinton Wang, Xiang Deng, Yuntao Ma, Nikhil Barhate, Sean Hendryx
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Verifiable Rewards (RLVR) has been widely adopted
as the de facto method for enhancing the reasoning capabilities of large
language models and has demonstrated notable success in verifiable domains like
math and competitive programming tasks. However, the efficacy of RLVR
diminishes significantly when applied to agentic environments. These settings,
characterized by multi-step, complex problem solving, lead to high failure
rates even for frontier LLMs, as the reward landscape is too sparse for
effective model training via conventional RLVR. In this work, we introduce
Agent-RLVR, a framework that makes RLVR effective in challenging agentic
settings, with an initial focus on software engineering tasks. Inspired by
human pedagogy, Agent-RLVR introduces agent guidance, a mechanism that actively
steers the agent towards successful trajectories by leveraging diverse
informational cues. These cues, ranging from high-level strategic plans to
dynamic feedback on the agent's errors and environmental interactions, emulate
a teacher's guidance, enabling the agent to navigate difficult solution spaces
and promotes active self-improvement via additional environment exploration. In
the Agent-RLVR training loop, agents first attempt to solve tasks to produce
initial trajectories, which are then validated by unit tests and supplemented
with agent guidance. Agents then reattempt with guidance, and the agent policy
is updated with RLVR based on the rewards of these guided trajectories.
Agent-RLVR elevates the pass@1 performance of Qwen-2.5-72B-Instruct from 9.4%
to 22.4% on SWE-Bench Verified. We find that our guidance-augmented RLVR data
is additionally useful for test-time reward model training, shown by further
boosting pass@1 to 27.8%. Agent-RLVR lays the groundwork for training agents
with RLVR in complex, real-world environments where conventional RL methods
struggle.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Long-Context LLM Inference via KV Cache Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Hu, Shengnan Wang, Yutong He, Ping Gong, Jiawei Yi, Juncheng Zhang, Youhui Bai, Renhai Chen, Gong Zhang, Cheng Li, Kun Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) with extended context windows have become
increasingly prevalent for tackling complex tasks. However, the substantial
Key-Value (KV) cache required for long-context LLMs poses significant
deployment challenges. Existing approaches either discard potentially critical
information needed for future generations or offer limited efficiency gains due
to high computational overhead. In this paper, we introduce Chelsea, a simple
yet effective framework for online KV cache clustering. Our approach is based
on the observation that key states exhibit high similarity along the sequence
dimension. To enable efficient clustering, we divide the sequence into chunks
and propose Chunked Soft Matching, which employs an alternating partition
strategy within each chunk and identifies clusters based on similarity. Chelsea
then merges the KV cache within each cluster into a single centroid.
Additionally, we provide a theoretical analysis of the computational complexity
and the optimality of the intra-chunk partitioning strategy. Extensive
experiments across various models and long-context benchmarks demonstrate that
Chelsea achieves up to 80% reduction in KV cache memory usage while maintaining
comparable model performance. Moreover, with minimal computational overhead,
Chelsea accelerates the decoding stage of inference by up to 3.19$\times$ and
reduces end-to-end latency by up to 2.72$\times$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bias Amplification in RAG: Poisoning Knowledge Retrieval to Steer LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linlin Wang, Tianqing Zhu, Laiqiao Qin, Longxiang Gao, Wanlei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Large Language Models, Retrieval-Augmented Generation (RAG) systems can
significantly enhance the performance of large language models by integrating
external knowledge. However, RAG also introduces new security risks. Existing
research focuses mainly on how poisoning attacks in RAG systems affect model
output quality, overlooking their potential to amplify model biases. For
example, when querying about domestic violence victims, a compromised RAG
system might preferentially retrieve documents depicting women as victims,
causing the model to generate outputs that perpetuate gender stereotypes even
when the original query is gender neutral. To show the impact of the bias, this
paper proposes a Bias Retrieval and Reward Attack (BRRA) framework, which
systematically investigates attack pathways that amplify language model biases
through a RAG system manipulation. We design an adversarial document generation
method based on multi-objective reward functions, employ subspace projection
techniques to manipulate retrieval results, and construct a cyclic feedback
mechanism for continuous bias amplification. Experiments on multiple mainstream
large language models demonstrate that BRRA attacks can significantly enhance
model biases in dimensions. In addition, we explore a dual stage defense
mechanism to effectively mitigate the impacts of the attack. This study reveals
that poisoning attacks in RAG systems directly amplify model output biases and
clarifies the relationship between RAG system security and model fairness. This
novel potential attack indicates that we need to keep an eye on the fairness
issues of the RAG system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Early-Onset Colorectal Cancer with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wilson Lau, Youngwon Kim, Sravanthi Parasa, Md Enamul Haque, Anand Oka, Jay Nanduri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The incidence rate of early-onset colorectal cancer (EoCRC, age < 45) has
increased every year, but this population is younger than the recommended age
established by national guidelines for cancer screening. In this paper, we
applied 10 different machine learning models to predict EoCRC, and compared
their performance with advanced large language models (LLM), using patient
conditions, lab results, and observations within 6 months of patient journey
prior to the CRC diagnoses. We retrospectively identified 1,953 CRC patients
from multiple health systems across the United States. The results demonstrated
that the fine-tuned LLM achieved an average of 73% sensitivity and 91%
specificity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted for the proceedings of the 2025 American Medical
  Informatics Association Annual Symposium (AMIA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pradyut Sekhsaria, Marcel Mateos Salles, Hai Huang, Randall Balestriero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter Efficient FineTuning (PEFT), such as Low-Rank Adaptation (LoRA),
aligns pre-trained Large Language Models (LLMs) to particular downstream tasks
in a resource-efficient manner. Because efficiency has been the main metric of
progress, very little attention has been put in understanding possible
catastrophic failures. We uncover one such failure: PEFT encourages a model to
search for shortcut solutions to solve its fine-tuning tasks. When very small
amount of tokens, e.g., one token per prompt, are correlated with downstream
task classes, PEFT makes any pretrained model rely predominantly on that token
for decision making. While such spurious tokens may emerge accidentally from
incorrect data cleaning, it also opens opportunities for malevolent parties to
control a model's behavior from Seamless Spurious Token Injection (SSTI). In
SSTI, a small amount of tokens correlated with downstream classes are injected
by the dataset creators. At test time, the finetuned LLM's behavior can be
controlled solely by injecting those few tokens. We apply SSTI across models
from three families (Snowflake Arctic, Apple OpenELM, and Meta LLaMA-3) and
four diverse datasets (IMDB, Financial Classification, CommonSense QA, and Bias
in Bios). Our findings reveal three astonishing behaviors. First, as few as a
single token of SSTI is sufficient to steer a model's decision making. Second,
for light SSTI, the reliance on spurious tokens is proportional to the LoRA
rank. Lastly, with aggressive SSTI, larger LoRA rank values become preferable
to small rank values as it makes the model attend to non-spurious tokens, hence
improving robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 16 figures, 15 tables. Submitted for publication. for
  associated blog post, see https://pradyut3501.github.io/lora-spur-corr/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curriculum-Guided Layer Scaling for Language Model Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karanpartap Singh, Neil Band, Ehsan Adeli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the cost of pretraining large language models grows, there is continued
interest in strategies to improve learning efficiency during this core training
stage. Motivated by cognitive development, where humans gradually build
knowledge as their brains mature, we propose Curriculum-Guided Layer Scaling
(CGLS), a framework for compute-efficient pretraining that synchronizes
increasing data difficulty with model growth through progressive layer stacking
(i.e. gradually adding layers during training). At the 100M parameter scale,
using a curriculum transitioning from synthetic short stories to general web
data, CGLS outperforms baseline methods on the question-answering benchmarks
PIQA and ARC. Pretraining at the 1.2B scale, we stratify the DataComp-LM corpus
with a DistilBERT-based classifier and progress from general text to highly
technical or specialized content. Our results show that progressively
increasing model depth alongside sample difficulty leads to better
generalization and zero-shot performance on various downstream benchmarks.
Altogether, our findings demonstrate that CGLS unlocks the potential of
progressive stacking, offering a simple yet effective strategy for improving
generalization on knowledge-intensive and reasoning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Variational Approach for Mitigating Entity Bias in Relation Extraction <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Mensah, Elena Kochkina, Jabez Magomere, Joy Prakash Sain, Simerjot Kaur, Charese Smiley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigating entity bias is a critical challenge in Relation Extraction (RE),
where models often rely excessively on entities, resulting in poor
generalization. This paper presents a novel approach to address this issue by
adapting a Variational Information Bottleneck (VIB) framework. Our method
compresses entity-specific information while preserving task-relevant features.
It achieves state-of-the-art performance on relation extraction datasets across
general, financial, and biomedical domains, in both indomain (original test
sets) and out-of-domain (modified test sets with type-constrained entity
replacements) settings. Our approach offers a robust, interpretable, and
theoretically grounded methodology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2025 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Model-Powered <span class="highlight-title">Conversation</span>al <span class="highlight-title">Agent</span> Delivering
  Problem-Solving Therapy (PST) for Family Caregivers: Enhancing Empathy and
  Therapeutic Alliance Using In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liying Wang, Ph. D., Daffodil Carrington, M. S., Daniil Filienko, M. S., Caroline El Jazmi, M. S., Serena Jinchen Xie, M. S., Martine De Cock, Ph. D., Sarah Iribarren, Ph. D., Weichao Yuwen, Ph. D
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Family caregivers often face substantial mental health challenges due to
their multifaceted roles and limited resources. This study explored the
potential of a large language model (LLM)-powered conversational agent to
deliver evidence-based mental health support for caregivers, specifically
Problem-Solving Therapy (PST) integrated with Motivational Interviewing (MI)
and Behavioral Chain Analysis (BCA). A within-subject experiment was conducted
with 28 caregivers interacting with four LLM configurations to evaluate empathy
and therapeutic alliance. The best-performing models incorporated Few-Shot and
Retrieval-Augmented Generation (RAG) prompting techniques, alongside
clinician-curated examples. The models showed improved contextual understanding
and personalized support, as reflected by qualitative responses and
quantitative ratings on perceived empathy and therapeutic alliances.
Participants valued the model's ability to validate emotions, explore
unexpressed feelings, and provide actionable strategies. However, balancing
thorough assessment with efficient advice delivery remains a challenge. This
work highlights the potential of LLMs in delivering empathetic and tailored
support for family caregivers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Multimodal LLMs on Recognition and Understanding over
  Chemical Tables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yitong Zhou, Mingyue Cheng, Qingyang Mao, Yucong Luo, Qi Liu, Yupeng Li, Xiaohan Zhang, Deguang Liu, Xin Li, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chemical tables encode complex experimental knowledge through symbolic
expressions, structured variables, and embedded molecular graphics. Existing
benchmarks largely overlook this multimodal and domain-specific complexity,
limiting the ability of multimodal large language models to support scientific
understanding in chemistry. In this work, we introduce ChemTable, a large-scale
benchmark of real-world chemical tables curated from the experimental sections
of literature. ChemTable includes expert-annotated cell polygons, logical
layouts, and domain-specific labels, including reagents, catalysts, yields, and
graphical components and supports two core tasks: (1) Table Recognition,
covering structure parsing and content extraction; and (2) Table Understanding,
encompassing both descriptive and reasoning-oriented question answering
grounded in table structure and domain semantics. We evaluated a range of
representative multimodal models, including both open-source and closed-source
models, on ChemTable and reported a series of findings with practical and
conceptual insights. Although models show reasonable performance on basic
layout parsing, they exhibit substantial limitations on both descriptive and
inferential QA tasks compared to human performance, and we observe significant
performance gaps between open-source and closed-source models across multiple
dimensions. These results underscore the challenges of chemistry-aware table
understanding and position ChemTable as a rigorous and realistic benchmark for
advancing scientific reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cartridges: Lightweight and general-purpose long context representations
  via self-study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06266v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06266v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabri Eyuboglu, Ryan Ehrlich, Simran Arora, Neel Guha, Dylan Zinsley, Emily Liu, Will Tennien, Atri Rudra, James Zou, Azalia Mirhoseini, Christopher Re
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are often used to answer queries grounded in large text
corpora (e.g. codebases, legal documents, or chat histories) by placing the
entire corpus in the context window and leveraging in-context learning (ICL).
Although current models support contexts of 100K-1M tokens, this setup is
costly to serve because the memory consumption of the KV cache scales with
input length. We explore an alternative: training a smaller KV cache offline on
each corpus. At inference time, we load this trained KV cache, which we call a
Cartridge, and decode a response. Critically, the cost of training a Cartridge
can be amortized across all the queries referencing the same corpus. However,
we find that the naive approach of training the Cartridge with next-token
prediction on the corpus is not competitive with ICL. Instead, we propose
self-study, a training recipe in which we generate synthetic conversations
about the corpus and train the Cartridge with a context-distillation objective.
We find that Cartridges trained with self-study replicate the functionality of
ICL, while being significantly cheaper to serve. On challenging long-context
benchmarks, Cartridges trained with self-study match ICL performance while
using 38.6x less memory and enabling 26.4x higher throughput. Self-study also
extends the model's effective context length (e.g. from 128k to 484k tokens on
MTOB) and surprisingly, leads to Cartridges that can be composed at inference
time without retraining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ e3: Learning to Explore Enables Extrapolation of Test-Time Compute for
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09026v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09026v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amrith Setlur, Matthew Y. R. Yang, Charlie Snell, Jeremy Greer, Ian Wu, Virginia Smith, Max Simchowitz, Aviral Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time scaling offers a promising path to improve LLM reasoning by
utilizing more compute at inference time; however, the true promise of this
paradigm lies in extrapolation (i.e., improvement in performance on hard
problems as LLMs keep "thinking" for longer, beyond the maximum token budget
they were trained on). Surprisingly, we find that most existing reasoning
models do not extrapolate well. We show that one way to enable extrapolation is
by training the LLM to perform in-context exploration: training the LLM to
effectively spend its test time budget by chaining operations (such as
generation, verification, refinement, etc.), or testing multiple hypotheses
before it commits to an answer. To enable in-context exploration, we identify
three key ingredients as part of our recipe e3: (1) chaining skills that the
base LLM has asymmetric competence in, e.g., chaining verification (easy) with
generation (hard), as a way to implement in-context search; (2) leveraging
"negative" gradients from incorrect traces to amplify exploration during RL,
resulting in longer search traces that chains additional asymmetries; and (3)
coupling task difficulty with training token budget during training via a
specifically-designed curriculum to structure in-context exploration. Our
recipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25
scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not
only attains high pass@1 scores, but also improves pass@k over the base model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Large Language Models with Concept-Aware Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07833v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07833v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael K. Chen, Xikun Zhang, Jiaxing Huang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have become the cornerstone of modern AI.
However, the existing paradigm of next-token prediction fundamentally limits
their ability to form coherent, high-level concepts, making it a critical
barrier to human-like understanding and reasoning. Take the phrase "ribonucleic
acid" as an example: an LLM will first decompose it into tokens, i.e.,
artificial text fragments ("rib", "on", ...), then learn each token
sequentially, rather than grasping the phrase as a unified, coherent semantic
entity. This fragmented representation hinders deeper conceptual understanding
and, ultimately, the development of truly intelligent systems. In response, we
introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method
that redefines how LLMs are fine-tuned. By enabling the learning of sequences
that span multiple tokens, this method fosters stronger concept-aware learning.
Our experiments demonstrate significant improvements compared to conventional
next-token finetuning methods across diverse tasks, including traditional
applications like text summarization and domain-specific ones like de novo
protein design. Multi-token prediction was previously only possible in the
prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first
to bring the multi-token setting to the post-training phase, thus effectively
democratizing its benefits for the broader community of practitioners and
researchers. Finally, the unexpected effectiveness of our proposed method
suggests wider implications for the machine learning research community. All
code and data are available at https://github.com/michaelchen-lab/caft-llm
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and
  English 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.17076v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.17076v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyang Zhang, Hexin Liu, Xiangyu Zhang, Qiquan Zhang, Yuchen Hu, Junqi Zhao, Fei Tian, Xuerui Yang, Leibny Paola Garcia, Eng Siong Chng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The speech tokenizer plays a crucial role in recent speech tasks, generally
serving as a bridge between speech signals and language models. While
low-frame-rate codecs are widely employed as speech tokenizers, the impact of
frame rates on speech tokens remains underexplored. In this study, we
investigate how varying frame rates affect speech tokenization by examining
Mandarin and English, two typologically distinct languages. We encode speech at
different frame rates and evaluate the resulting semantic tokens in the speech
recognition task. Our findings reveal that frame rate variations influence
speech tokenization differently for each language, highlighting the interplay
between frame rates, phonetic density, and language-specific acoustic features.
The results provide insights into optimizing frame rate selection for speech
tokenizers, with implications for automatic speech recognition, text-to-speech,
and other speech-related applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Factual Knowledge in Language Models: Robustness and Anomalies under
  Simple Temporal Context Variations <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01220v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01220v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hichem Ammar Khodja, Frédéric Béchet, Quentin Brabant, Alexis Nasr, Gwénolé Lecorvé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the robustness of language models (LMs) to variations in
the temporal context within factual knowledge. It examines whether LMs can
correctly associate a temporal context with a past fact valid over a defined
period, by asking them to differentiate correct from incorrect contexts. The
LMs' ability to distinguish is analyzed along two dimensions: the distance of
the incorrect context from the validity period and the granularity of the
context. To this end, a dataset called TimeStress is introduced, enabling the
evaluation of 18 diverse LMs. Results reveal that the best LM achieves a
perfect distinction for only 11% of the studied facts, with errors, certainly
rare, but critical that humans would not make. This work highlights the
limitations of current LMs in temporal representation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint v5, accepted for publication at ACL 2025 - L2M2 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing multimodal analogical reasoning with Logic Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.11190v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.11190v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Sofia Lippolis, Andrea Giovanni Nuzzolese, Aldo Gangemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models have demonstrated their capabilities
across a variety of tasks. However, automatically extracting implicit knowledge
from natural language remains a significant challenge, as machines lack active
experience with the physical world. Given this scenario, semantic knowledge
graphs can serve as conceptual spaces that guide the automated text generation
reasoning process to achieve more efficient and explainable results. In this
paper, we apply a logic-augmented generation (LAG) framework that leverages the
explicit representation of a text through a semantic knowledge graph and
applies it in combination with prompt heuristics to elicit implicit analogical
connections. This method generates extended knowledge graph triples
representing implicit meaning, enabling systems to reason on unlabeled
multimodal data regardless of the domain. We validate our work through three
metaphor detection and understanding tasks across four datasets, as they
require deep analogical reasoning capabilities. The results show that this
integrated approach surpasses current baselines, performs better than humans in
understanding visual metaphors, and enables more explainable reasoning
processes, though still has inherent limitations in metaphor understanding,
especially for domain-specific metaphors. Furthermore, we propose a thorough
error analysis, discussing issues with metaphorical annotations and current
evaluation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainability of Large Language Models using SMILE: Statistical
  Model-agnostic Interpretability with Local Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.21657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.21657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeinab Dehghani, Mohammed Naveed Akram, Koorosh Aslansefat, Adil Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models like GPT, LLAMA, and Claude have become incredibly
powerful at generating text, but they are still black boxes, so it is hard to
understand how they decide what to say. That lack of transparency can be
problematic, especially in fields where trust and accountability matter. To
help with this, we introduce SMILE, a new method that explains how these models
respond to different parts of a prompt. SMILE is model-agnostic and works by
slightly changing the input, measuring how the output changes, and then
highlighting which words had the most impact. Create simple visual heat maps
showing which parts of a prompt matter the most. We tested SMILE on several
leading LLMs and used metrics such as accuracy, consistency, stability, and
fidelity to show that it gives clear and reliable explanations. By making these
models easier to understand, SMILE brings us one step closer to making AI more
transparent and trustworthy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2412.16277</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T1: Advancing Language Model Reasoning through Reinforcement Learning
  and Inference Scaling <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11651v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11651v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, Yuxiao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable capabilities in
complex reasoning tasks. However, existing approaches mainly rely on imitation
learning and struggle to achieve effective test-time scaling. While
reinforcement learning (RL) holds promise for enabling self-exploration, recent
attempts yield modest improvements in complex reasoning. In this paper, we
present T1 to scale RL by encouraging exploration and understand inference
scaling. We first initialize the LLM using synthesized chain-of-thought data
that integrates trial-and-error and self-verification. To scale RL training, we
promote increased sampling diversity through oversampling. We demonstrate that
T1 with open LLMs as its base exhibits inference scaling behavior and achieves
superior performance on challenging math reasoning benchmarks. More
importantly, we present a simple strategy to examine inference scaling, where
increased inference budgets directly lead to T1's better performance without
any additional verification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt
  Generation for Enhanced LLM Content Moderation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18638v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18638v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Schwartz, Dmitriy Bespalov, Zhe Wang, Ninad Kulkarni, Yanjun Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become increasingly prevalent, ensuring their
robustness against adversarial misuse is crucial. This paper introduces the GAP
(Graph of Attacks with Pruning) framework, an advanced approach for generating
stealthy jailbreak prompts to evaluate and enhance LLM safeguards. GAP
addresses limitations in existing tree-based LLM jailbreak methods by
implementing an interconnected graph structure that enables knowledge sharing
across attack paths. Our experimental evaluation demonstrates GAP's superiority
over existing techniques, achieving a 20.8% increase in attack success rates
while reducing query costs by 62.7%. GAP consistently outperforms
state-of-the-art methods for attacking both open and closed LLMs, with attack
success rates of >96%. Additionally, we present specialized variants like
GAP-Auto for automated seed generation and GAP-VLM for multimodal attacks.
GAP-generated prompts prove highly effective in improving content moderation
systems, increasing true positive detection rates by 108.5% and accuracy by
183.6% when used for fine-tuning. Our implementation is available at
https://github.com/dsbuddy/GAP-LLM-Safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical
  Action Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07196v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07196v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengya Xu, Zhongzhen Huang, Dillan Imans, Yiru Ye, Xiaofan Zhang, Qi Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective evaluation is critical for driving advancements in MLLM research.
The surgical action planning (SAP) task, which aims to generate future action
sequences from visual inputs, demands precise and sophisticated analytical
capabilities. Unlike mathematical reasoning, surgical decision-making operates
in life-critical domains and requires meticulous, verifiable processes to
ensure reliability and patient safety. This task demands the ability to
distinguish between atomic visual actions and coordinate complex, long-horizon
procedures, capabilities that are inadequately evaluated by current benchmarks.
To address this gap, we introduce SAP-Bench, a large-scale, high-quality
dataset designed to enable multimodal large language models (MLLMs) to perform
interpretable surgical action planning. Our SAP-Bench benchmark, derived from
the cholecystectomy procedures context with the mean duration of 1137.5s, and
introduces temporally-grounded surgical action annotations, comprising the
1,226 clinically validated action clips (mean duration: 68.7s) capturing five
fundamental surgical actions across 74 procedures. The dataset provides 1,152
strategically sampled current frames, each paired with the corresponding next
action as multimodal analysis anchors. We propose the MLLM-SAP framework that
leverages MLLMs to generate next action recommendations from the current
surgical scene and natural language instructions, enhanced with injected
surgical domain knowledge. To assess our dataset's effectiveness and the
broader capabilities of current models, we evaluate seven state-of-the-art
MLLMs (e.g., OpenAI-o1, GPT-4o, QwenVL2.5-72B, Claude-3.5-Sonnet, GeminiPro2.5,
Step-1o, and GLM-4v) and reveal critical gaps in next action prediction
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The authors could not reach a consensus on the final version of this
  paper, necessitating its withdrawal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-context Non-factoid Question Answering in Indic Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.13615v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.13615v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ritwik Mishra, Rajiv Ratn Shah, Ponnurangam Kumaraguru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question Answering (QA) tasks, which involve extracting answers from a given
context, are relatively straightforward for modern Large Language Models (LLMs)
when the context is short. However, long contexts pose challenges due to the
quadratic complexity of the self-attention mechanism. This challenge is
compounded in Indic languages, which are often low-resource. This study
explores context-shortening techniques, including Open Information Extraction
(OIE), coreference resolution, Answer Paragraph Selection (APS), and their
combinations, to improve QA performance. Compared to the baseline of
unshortened (long) contexts, our experiments on four Indic languages (Hindi,
Tamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield
an average improvement of 4\% in semantic scores and 47\% in token-level scores
when evaluated on three popular LLMs without fine-tuning. Furthermore, with
fine-tuning, we achieve an average increase of 2\% in both semantic and
token-level scores. Additionally, context-shortening reduces computational
overhead. Explainability techniques like LIME and SHAP reveal that when the APS
model confidently identifies the paragraph containing the answer, nearly all
tokens within the selected text receive high relevance scores. However, the
study also highlights the limitations of LLM-based QA systems in addressing
non-factoid questions, particularly those requiring reasoning or debate.
Moreover, verbalizing OIE-generated triples does not enhance system
performance. These findings emphasize the potential of context-shortening
techniques to improve the efficiency and effectiveness of LLM-based QA systems,
especially for low-resource languages. The source code and resources are
available at https://github.com/ritwikmishra/IndicGenQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Short version of this manuscript accepted at
  https://bda2025.iiitb.net/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.09347v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.09347v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Chen, Seraphina Goldfarb-Tarrant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly employed as automated
evaluators to assess the safety of generated content, yet their reliability in
this role remains uncertain. This study evaluates a diverse set of 11 LLM judge
models across critical safety domains, examining three key aspects:
self-consistency in repeated judging tasks, alignment with human judgments, and
susceptibility to input artifacts such as apologetic or verbose phrasing. Our
findings reveal that biases in LLM judges can significantly distort the final
verdict on which content source is safer, undermining the validity of
comparative evaluations. Notably, apologetic language artifacts alone can skew
evaluator preferences by up to 98\%. Contrary to expectations, larger models do
not consistently exhibit greater robustness, while smaller models sometimes
show higher resistance to specific artifacts. To mitigate LLM evaluator
robustness issues, we investigate jury-based evaluations aggregating decisions
from multiple models. Although this approach both improves robustness and
enhances alignment to human judgements, artifact sensitivity persists even with
the best jury configurations. These results highlight the urgent need for
diversified, artifact-resistant methodologies to ensure reliable safety
assessments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Automated but Risky Game: Modeling <span class="highlight-title">Agent</span>-to-<span class="highlight-title">Agent</span> Negotiations and
  Transactions in Consumer Markets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00073v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00073v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenzhe Zhu, Jiao Sun, Yi Nian, Tobin South, Alex Pentland, Jiaxin Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI agents are increasingly used in consumer-facing applications to assist
with tasks such as product search, negotiation, and transaction execution. In
this paper, we explore a future scenario where both consumers and merchants
authorize AI agents to fully automate negotiations and transactions. We aim to
answer two key questions: (1) Do different LLM agents vary in their ability to
secure favorable deals for users? (2) What risks arise from fully automating
deal-making with AI agents in consumer markets? To address these questions, we
develop an experimental framework that evaluates the performance of various LLM
agents in real-world negotiation and transaction settings. Our findings reveal
that AI-mediated deal-making is an inherently imbalanced game -- different
agents achieve significantly different outcomes for their users. Moreover,
behavioral anomalies in LLMs can result in financial losses for both consumers
and merchants, such as overspending or accepting unreasonable deals. These
results underscore that while automation can improve efficiency, it also
introduces substantial risks. Users should exercise caution when delegating
business decisions to AI agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models for Toxic Language Detection in Low-Resource
  Balkan Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09992v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09992v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amel Muminovic, Amela Kadric Muminovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online toxic language causes real harm, especially in regions with limited
moderation tools. In this study, we evaluate how large language models handle
toxic comments in Serbian, Croatian, and Bosnian, languages with limited
labeled data. We built and manually labeled a dataset of 4,500 YouTube and
TikTok comments drawn from videos across diverse categories, including music,
politics, sports, modeling, influencer content, discussions of sexism, and
general topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude
3 Opus) were tested in two modes: zero-shot and context-augmented. We measured
precision, recall, F1 score, accuracy and false positive rates. Including a
short context snippet raised recall by about 0.12 on average and improved F1
score by up to 0.10, though it sometimes increased false positives. The best
balance came from Gemini in context-augmented mode, reaching an F1 score of
0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the
lowest false alarms. We show how adding minimal context can improve toxic
language detection in low-resource settings and suggest practical strategies
such as improved prompt design and threshold calibration. These results show
that prompt design alone can yield meaningful gains in toxicity detection for
underserved Balkan language communities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Word Sense Detection Leveraging Maximum Mean Discrepancy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01602v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01602v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kensuke Mitsuzawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word sense analysis is an essential analysis work for interpreting the
linguistic and social backgrounds. The word sense change detection is a task of
identifying and interpreting shifts in word meanings over time. This paper
proposes MMD-Sense-Analysis, a novel approach that leverages Maximum Mean
Discrepancy (MMD) to select semantically meaningful variables and quantify
changes across time periods. This method enables both the identification of
words undergoing sense shifts and the explanation of their evolution over
multiple historical periods. To my knowledge, this is the first application of
MMD to word sense change detection. Empirical assessment results demonstrate
the effectiveness of the proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MEDDx<span class="highlight-title">Agent</span>: A Unified Modular <span class="highlight-title">Agent</span> Framework for Explainable Automatic
  Differential Diagnosis <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Rose, Chia-Chien Hung, Marco Lepri, Israa Alqassem, Kiril Gashteovski, Carolin Lawrence
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical
decision-making, in which physicians iteratively refine a ranked list of
possible diseases based on symptoms, antecedents, and medical knowledge. While
recent advances in large language models (LLMs) have shown promise in
supporting DDx, existing approaches face key limitations, including
single-dataset evaluations, isolated optimization of components, unrealistic
assumptions about complete patient profiles, and single-attempt diagnosis. We
introduce a Modular Explainable DDx Agent (MEDDxAgent) framework designed for
interactive DDx, where diagnostic reasoning evolves through iterative learning,
rather than assuming a complete patient profile is accessible. MEDDxAgent
integrates three modular components: (1) an orchestrator (DDxDriver), (2) a
history taking simulator, and (3) two specialized agents for knowledge
retrieval and diagnosis strategy. To ensure robust evaluation, we introduce a
comprehensive DDx benchmark covering respiratory, skin, and rare diseases. We
analyze single-turn diagnostic approaches and demonstrate the importance of
iterative refinement when patient profiles are not available at the outset. Our
broad evaluation demonstrates that MEDDxAgent achieves over 10% accuracy
improvements in interactive DDx across both large and small LLMs, while
offering critical explainability into its diagnostic reasoning process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 (main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Women, Infamous, and Exotic Beings: What Honorific Usages in Wikipedia
  Reflect on the Cross-Cultural Sociolinguistic Norms? <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03479v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03479v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourabrata Mukherjee, Atharva Mehta, Soumya Teotia, Sougata Saha, Akhil Arora, Monojit Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wikipedia, as a massively multilingual, community-driven platform, is a
valuable resource for Natural Language Processing (NLP), yet the consistency of
honorific usage in honorific-rich languages remains underexplored. Honorifics,
subtle yet profound linguistic markers, encode social hierarchies, politeness
norms, and cultural values, but Wikipedia's editorial guidelines lack clear
standards for their usage in languages where such forms are grammatically and
socially prevalent. This paper addresses this gap through a large-scale
analysis of third-person honorific pronouns and verb forms in Hindi and Bengali
Wikipedia articles. Using Large Language Models (LLM), we automatically
annotate 10,000 articles per language for honorific usage and socio-demographic
features such as gender, age, fame, and cultural origin. We investigate: (i)
the consistency of honorific usage across articles, (ii) how inconsistencies
correlate with socio-cultural factors, and (iii) the presence of explicit or
implicit biases across languages. We find that honorific usage is consistently
more common in Bengali than Hindi, while non-honorific forms are more frequent
for infamous, juvenile, and exotic entities in both. Notably, gender bias
emerges in both languages, particularly in Hindi, where men are more likely to
receive honorifics than women. Our analysis highlights the need for Wikipedia
to develop language-specific editorial guidelines for honorific usage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2nd WikiNLP: Advancing Natural Language Process for
  Wikipedia, Co-located with ACL 2025 (non-archival)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Much is Enough? The Diminishing Returns of Tokenization Training
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20273v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20273v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varshini Reddy, Craig W. Schmidt, Yuval Pinter, Chris Tanner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tokenization, a crucial initial step in natural language processing, is
governed by several key parameters, such as the tokenization algorithm,
vocabulary size, pre-tokenization strategy, inference strategy, and training
data corpus. This paper investigates the impact of an often-overlooked
hyperparameter, tokenizer training data size. We train BPE, UnigramLM, and
WordPiece tokenizers across various vocabulary sizes using English training
data ranging from 1GB to 900GB. Our findings reveal diminishing returns as
training data size increases beyond roughly 150GB, suggesting a practical limit
to the improvements in tokenization quality achievable through additional data.
We analyze this phenomenon and attribute the saturation effect to constraints
introduced by the pre-tokenization stage. We then demonstrate the extent to
which these findings can generalize by experimenting on data in Russian, a
language typologically distant from English. For Russian text, we observe
diminishing returns after training a tokenizer from 200GB of data, which is
approximately 33% more than when training on English. These results provide
valuable insights for optimizing the tokenization process by reducing the
compute required for training on large corpora and suggest promising directions
for future research in tokenization algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Table-R1: Region-based Reinforcement Learning for Table Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12415v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12415v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhe Wu, Jian Yang, Jiaheng Liu, Xianjie Wu, Changzai Pan, Jie Zhang, Yu Zhao, Shuangyong Song, Yongxiang Li, Zhoujun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tables present unique challenges for language models due to their structured
row-column interactions, necessitating specialized approaches for effective
comprehension. While large language models (LLMs) have demonstrated potential
in table reasoning through prompting and techniques like chain-of-thought (CoT)
and program-of-thought (PoT), optimizing their performance for table question
answering remains underexplored. In this paper, we introduce region-based
Table-R1, a novel reinforcement learning approach that enhances LLM table
understanding by integrating region evidence into reasoning steps. Our method
employs Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in
identifying relevant table regions before generating answers, incorporating
textual, symbolic, and program-based reasoning. Additionally, Table-Aware Group
Relative Policy Optimization (TARPO) introduces a mixed reward system to
dynamically balance region accuracy and answer correctness, with decaying
region rewards and consistency penalties to align reasoning steps. Experiments
show that Table-R1 achieves an average performance improvement of 14.36 points
across multiple base models on three benchmark datasets, even outperforming
baseline models with ten times the parameters, while TARPO reduces response
token consumption by 67.5% compared to GRPO, significantly advancing LLM
capabilities in efficient tabular reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entropy Controllable Direct Preference Optimization <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07595v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07595v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Motoki Omura, Yasuhiro Fujita, Toshiki Kataoka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the post-training of large language models (LLMs), Reinforcement Learning
from Human Feedback (RLHF) is an effective approach to achieve generation
aligned with human preferences. Direct Preference Optimization (DPO) allows for
policy training with a simple binary cross-entropy loss without a reward model.
The objective of DPO is regularized by reverse KL divergence that encourages
mode-seeking fitting to the reference policy. Nonetheless, we indicate that
minimizing reverse KL divergence could fail to capture a mode of the reference
distribution, which may hurt the policy's performance. Based on this
observation, we propose a simple modification to DPO, H-DPO, which allows for
control over the entropy of the resulting policy, enhancing the distribution's
sharpness and thereby enabling mode-seeking fitting more effectively. In our
experiments, we show that H-DPO outperformed DPO across various tasks,
demonstrating superior results in pass@$k$ evaluations for mathematical tasks.
Moreover, H-DPO is simple to implement, requiring only minor modifications to
the loss calculation of DPO, which makes it highly practical and promising for
wide-ranging applications in the training of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025 Workshop on Models of Human Feedback for AI Alignment</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VM14K: First Vietnamese Medical Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01305v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01305v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thong Nguyen, Duc Nguyen, Minh Dang, Thai Dao, Long Nguyen, Quan H. Nguyen, Dat Nguyen, Kien Tran, Minh Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical benchmarks are indispensable for evaluating the capabilities of
language models in healthcare for non-English-speaking communities,therefore
help ensuring the quality of real-life applications. However, not every
community has sufficient resources and standardized methods to effectively
build and design such benchmark, and available non-English medical data is
normally fragmented and difficult to verify. We developed an approach to tackle
this problem and applied it to create the first Vietnamese medical question
benchmark, featuring 14,000 multiple-choice questions across 34 medical
specialties. Our benchmark was constructed using various verifiable sources,
including carefully curated medical exams and clinical records, and eventually
annotated by medical experts. The benchmark includes four difficulty levels,
ranging from foundational biological knowledge commonly found in textbooks to
typical clinical case studies that require advanced reasoning. This design
enables assessment of both the breadth and depth of language models' medical
understanding in the target language thanks to its extensive coverage and
in-depth subject-specific expertise. We release the benchmark in three parts: a
sample public set (4k questions), a full public set (10k questions), and a
private set (2k questions) used for leaderboard evaluation. Each set contains
all medical subfields and difficulty levels. Our approach is scalable to other
languages, and we open-source our data construction pipeline to support the
development of future multilingual benchmarks in the medical domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Persistent Topological Features in Large Language Models <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11042v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11042v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuri Gardinazzi, Karthik Viswanathan, Giada Panerai, Alessio Ansuini, Alberto Cazzaniga, Matteo Biagetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the decision-making processes of large language models is
critical given their widespread applications. To achieve this, we aim to
connect a formal mathematical framework - zigzag persistence from topological
data analysis - with practical and easily applicable algorithms. Zigzag
persistence is particularly effective for characterizing data as it dynamically
transforms across model layers. Within this framework, we introduce topological
descriptors that measure how topological features, $p$-dimensional holes,
persist and evolve throughout the layers. Unlike methods that assess each layer
individually and then aggregate the results, our approach directly tracks the
full evolutionary path of these features. This offers a statistical perspective
on how prompts are rearranged and their relative positions changed in the
representation space, providing insights into the system's operation as an
integrated whole. To demonstrate the expressivity and applicability of our
framework, we highlight how sensitive these descriptors are to different models
and a variety of datasets. As a showcase application to a downstream task, we
use zigzag persistence to establish a criterion for layer pruning, achieving
results comparable to state-of-the-art methods while preserving the
system-level perspective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10+17 pages, 17 figures, 3 tables. Accepted as poster at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-Language Models for Edge Networks: A Comprehensive Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Sharshar, Latif U. Khan, Waseem Ullah, Mohsen Guizani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Large Language Models (VLMs) combine visual understanding with natural
language processing, enabling tasks like image captioning, visual question
answering, and video analysis. While VLMs show impressive capabilities across
domains such as autonomous vehicles, smart surveillance, and healthcare, their
deployment on resource-constrained edge devices remains challenging due to
processing power, memory, and energy limitations. This survey explores recent
advancements in optimizing VLMs for edge environments, focusing on model
compression techniques, including pruning, quantization, knowledge
distillation, and specialized hardware solutions that enhance efficiency. We
provide a detailed discussion of efficient training and fine-tuning methods,
edge deployment challenges, and privacy considerations. Additionally, we
discuss the diverse applications of lightweight VLMs across healthcare,
environmental monitoring, and autonomous systems, illustrating their growing
impact. By highlighting key design strategies, current challenges, and offering
recommendations for future directions, this survey aims to inspire further
research into the practical deployment of VLMs, ultimately making advanced AI
accessible in resource-limited settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaVA-CMoE: Towards Continual Mixture of Experts for Large
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21227v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21227v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengyuan Zhao, Ziqin Wang, Qixin Sun, Kaiyou Song, Yilin Li, Xiaolin Hu, Qingpei Guo, Si Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture of Experts (MoE) architectures have recently advanced the scalability
and adaptability of large language models (LLMs) for continual multimodal
learning. However, efficiently extending these models to accommodate sequential
tasks remains challenging. As new tasks arrive, naive model expansion leads to
rapid parameter growth, while modifying shared routing components often causes
catastrophic forgetting, undermining previously learned knowledge. To address
these issues, we propose LLaVA-CMoE, a continual learning framework for LLMs
that requires no replay data of previous tasks and ensures both parameter
efficiency and robust knowledge retention. Our approach introduces a
Probe-Guided Knowledge Extension mechanism, which uses probe experts to
dynamically determine when and where new experts should be added, enabling
adaptive and minimal parameter expansion tailored to task complexity.
Furthermore, we present a Probabilistic Task Locator that assigns each task a
dedicated, lightweight router. To handle the practical issue that task labels
are unknown during inference, we leverage a VAE-based reconstruction strategy
to identify the most suitable router by matching input distributions, allowing
automatic and accurate expert allocation. This design mitigates routing
conflicts and catastrophic forgetting, enabling robust continual learning
without explicit task labels. Extensive experiments on the CoIN benchmark,
covering eight diverse VQA tasks, demonstrate that LLaVA-CMoE delivers strong
continual learning performance with a compact model size, significantly
reducing forgetting and parameter overhead compared to prior methods. These
results showcase the effectiveness and scalability of our approach for
parameter-efficient continual learning in large language models. Our code will
be open-sourced soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can reasoning models comprehend mathematical problems in Chinese ancient
  texts? An empirical study based on data from Suanjing Shishu 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.16660v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.16660v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Liu, Dongbo Wang, Liu liu, Zhixiao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the challenges in intelligent processing of Chinese
ancient mathematical classics by constructing Guji_MATH, a benchmark for
evaluating classical texts based on Suanjing Shishu. It systematically assesses
the mathematical problem-solving capabilities of mainstream reasoning models
under the unique linguistic constraints of classical Chinese. Through
machine-assisted annotation and manual verification, 538 mathematical problems
were extracted from 8 canonical texts, forming a structured dataset centered on
the "Question-Answer-Solution" framework, supplemented by problem types and
difficulty levels. Dual evaluation modes--closed-book (autonomous
problem-solving) and open-book (reproducing classical solution methods)--were
designed to evaluate the performance of six reasoning models on ancient Chinese
mathematical problems. Results indicate that reasoning models can partially
comprehend and solve these problems, yet their overall performance remains
inferior to benchmarks on modern mathematical tasks. Enhancing models'
classical Chinese comprehension and cultural knowledge should be prioritized
for optimization. This study provides methodological support for mining
mathematical knowledge from ancient texts and disseminating traditional
culture, while offering new perspectives for evaluating cross-linguistic and
cross-cultural capabilities of reasoning models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ailin Huang, Bingxin Li, Bruce Wang, Boyong Wu, Chao Yan, Chengli Feng, Heng Wang, Hongyu Zhou, Hongyuan Wang, Jingbei Li, Jianjian Sun, Joanna Wang, Mingrui Chen, Peng Liu, Ruihang Miao, Shilei Jiang, Tian Fei, Wang You, Xi Chen, Xuerui Yang, Yechang Huang, Yuxiang Zhang, Zheng Ge, Zheng Gong, Zhewei Huang, Zixin Zhang, Bin Wang, Bo Li, Buyun Ma, Changxin Miao, Changyi Wan, Chen Xu, Dapeng Shi, Dingyuan Hu, Enle Liu, Guanzhe Huang, Gulin Yan, Hanpeng Hu, Haonan Jia, Jiahao Gong, Jiaoren Wu, Jie Wu, Jie Yang, Junzhe Lin, Kaixiang Li, Lei Xia, Longlong Gu, Ming Li, Nie Hao, Ranchen Ming, Shaoliang Pang, Siqi Liu, Song Yuan, Tiancheng Cao, Wen Li, Wenqing He, Xu Zhao, Xuelin Zhang, Yanbo Yu, Yinmin Zhong, Yu Zhou, Yuanwei Liang, Yuanwei Lu, Yuxiang Yang, Zidong Yang, Zili Zhang, Binxing Jiao, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu, Daxin Jiang, Shuchang Zhou, Chen Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Audio-Language Models (LALMs) have significantly advanced intelligent
human-computer interaction, yet their reliance on text-based outputs limits
their ability to generate natural speech responses directly, hindering seamless
audio interactions. To address this, we introduce Step-Audio-AQAA, a fully
end-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model
integrates a dual-codebook audio tokenizer for linguistic and semantic feature
extraction, a 130-billion-parameter backbone LLM and a neural vocoder for
high-fidelity speech synthesis. Our post-training approach employs interleaved
token-output of text and audio to enhance semantic coherence and combines
Direct Preference Optimization (DPO) with model merge to improve performance.
Evaluations on the StepEval-Audio-360 benchmark demonstrate that
Step-Audio-AQAA excels especially in speech control, outperforming the
state-of-art LALMs in key areas. This work contributes a promising solution for
end-to-end LALMs and highlights the critical role of token-based vocoder in
enhancing overall performance for AQAA tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JBBQ: Japanese Bias Benchmark for Analyzing Social Biases in Large
  Language Models <span class="chip">ACL2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02050v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02050v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hitomi Yanaka, Namgi Han, Ryoma Kumon, Jie Lu, Masashi Takeshita, Ryo Sekizawa, Taisei Kato, Hiromi Arai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of large language models (LLMs), social biases in these
LLMs have become a pressing issue. Although there are various benchmarks for
social biases across languages, the extent to which Japanese LLMs exhibit
social biases has not been fully investigated. In this study, we construct the
Japanese Bias Benchmark dataset for Question Answering (JBBQ) based on the
English bias benchmark BBQ, with analysis of social biases in Japanese LLMs.
The results show that while current open Japanese LLMs with more parameters
show improved accuracies on JBBQ, their bias scores increase. In addition,
prompts with a warning about social biases and chain-of-thought prompting
reduce the effect of biases in model outputs, but there is room for improvement
in extracting the correct evidence from contexts in Japanese. Our dataset is
available at https://github.com/ynklab/JBBQ_data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 6th Workshop on Gender Bias in Natural Language
  Processing (GeBNLP2025) at ACL2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Wang, Yan Hu, Wenyu Du, Reynold Cheng, Benyou Wang, Difan Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning significantly improves the performance of Large Language Models
(LLMs), yet its underlying mechanisms remain poorly understood. This paper aims
to provide an in-depth interpretation of the fine-tuning process through
circuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike
previous studies (Prakash et al. 2024; Chhabra et al. 2024) that focus on tasks
where pre-trained models already perform well, we develop a set of mathematical
tasks where fine-tuning yields substantial performance gains, which are closer
to the practical setting. In our experiments, we identify circuits at various
checkpoints during fine-tuning and examine the interplay between circuit
analysis, fine-tuning methods, and task complexities. First, we find that while
circuits maintain high node similarity before and after fine-tuning, their
edges undergo significant changes, in contrast to prior work that shows
circuits only add some additional components after fine-tuning. Based on these
observations, we develop a circuit-aware Low-Rank Adaptation (LoRA) method,
which assigns ranks to layers based on edge changes in the circuits.
Experimental results demonstrate that our circuit-based LoRA algorithm achieves
an average performance improvement of 2.46% over standard LoRA with similar
parameter sizes. Furthermore, we explore how combining circuits from subtasks
can enhance fine-tuning in compositional tasks, providing new insights into the
design of such tasks and deepening the understanding of circuit dynamics and
fine-tuning mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An overview of domain-specific foundation model: key technologies,
  applications and challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04267v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04267v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolong Chen, Hanzhi Chen, Zijian Zhao, Kaifeng Han, Guangxu Zhu, Yichen Zhao, Ying Du, Wei Xu, Qingjiang Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The impressive performance of ChatGPT and other foundation-model-based
products in human language understanding has prompted both academia and
industry to explore how these models can be tailored for specific industries
and application scenarios. This process, known as the customization of
domain-specific foundation models (FMs), addresses the limitations of
general-purpose models, which may not fully capture the unique patterns and
requirements of domain-specific data. Despite its importance, there is a
notable lack of comprehensive overview papers on building domain-specific FMs,
while numerous resources exist for general-purpose models. To bridge this gap,
this article provides a timely and thorough overview of the methodology for
customizing domain-specific FMs. It introduces basic concepts, outlines the
general architecture, and surveys key methods for constructing domain-specific
models. Furthermore, the article discusses various domains that can benefit
from these specialized models and highlights the challenges ahead. Through this
overview, we aim to offer valuable guidance and reference for researchers and
practitioners from diverse fields to develop their own customized FMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Construction of Multiple Classification Dimensions for
  Managing Approaches in Scientific Papers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bing Ma, Hai Zhuge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approaches form the foundation for conducting scientific research. Querying
approaches from a vast body of scientific papers is extremely time-consuming,
and without a well-organized management framework, researchers may face
significant challenges in querying and utilizing relevant approaches.
Constructing multiple dimensions on approaches and managing them from these
dimensions can provide an efficient solution. Firstly, this paper identifies
approach patterns using a top-down way, refining the patterns through four
distinct linguistic levels: semantic level, discourse level, syntactic level,
and lexical level. Approaches in scientific papers are extracted based on
approach patterns. Additionally, five dimensions for categorizing approaches
are identified using these patterns. This paper proposes using tree structure
to represent step and measuring the similarity between different steps with a
tree-structure-based similarity measure that focuses on syntactic-level
similarities. A collection similarity measure is proposed to compute the
similarity between approaches. A bottom-up clustering algorithm is proposed to
construct class trees for approach components within each dimension by merging
each approach component or class with its most similar approach component or
class in each iteration. The class labels generated during the clustering
process indicate the common semantics of the step components within the
approach components in each class and are used to manage the approaches within
the class. The class trees of the five dimensions collectively form a
multi-dimensional approach space. The application of approach queries on the
multi-dimensional approach space demonstrates that querying within this space
ensures strong relevance between user queries and results and rapidly reduces
search space through a class-based query mechanism.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding the Repeat Curse in Large Language Models from a Feature
  Perspective <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.14218v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.14218v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junchi Yao, Shu Yang, Jianhua Xu, Lijie Hu, Mengdi Li, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have made remarkable progress in various
domains, yet they often suffer from repetitive text generation, a phenomenon we
refer to as the "Repeat Curse". While previous studies have proposed decoding
strategies to mitigate repetition, the underlying mechanism behind this issue
remains insufficiently explored. In this work, we investigate the root causes
of repetition in LLMs through the lens of mechanistic interpretability.
Inspired by recent advances in Sparse Autoencoders (SAEs), which enable
monosemantic feature extraction, we propose a novel approach, "Duplicatus
Charm", to induce and analyze the Repeat Curse. Our method systematically
identifies "Repetition Features" -the key model activations responsible for
generating repetitive outputs. First, we locate the layers most involved in
repetition through logit analysis. Next, we extract and stimulate relevant
features using SAE-based activation manipulation. To validate our approach, we
construct a repetition dataset covering token and paragraph level repetitions
and introduce an evaluation pipeline to quantify the influence of identified
repetition features. Furthermore, by deactivating these features, we have
effectively mitigated the Repeat Curse. The source code of our work is publicly
available at: https://github.com/kaustpradalab/repeat-curse-llm
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025, Findings, Long Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlashBack:Efficient Retrieval-Augmented Language Modeling for Long
  Context Inference <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.04065v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.04065v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runheng Liu, Xingchen Xiao, Heyan Huang, Zewen Chi, Zhijing Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Language Modeling (RALM) by integrating large language
models (LLM) with relevant documents from an external corpus is a proven method
for enabling the LLM to generate information beyond the scope of its
pre-training corpus. Previous work utilizing retrieved content by simply
prepending it to the input poses a high runtime issue, which degrades the
inference efficiency of the LLMs because they fail to use the Key-Value (KV)
cache efficiently. In this paper, we propose FlashBack, a modular RALM designed
to improve the inference efficiency of RALM with appending context pattern
while maintaining decent performance after fine-tuning by Low-Rank Adaption.
FlashBack appends retrieved documents at the end of the context for efficiently
utilizing the KV cache instead of prepending them. And we introduce Marking
Token as two special prompt tokens for marking the boundary of the appending
context during fine-tuning. Our experiments on testing generation quality show
that FlashBack can remain decent generation quality in perplexity. And the
inference speed of FlashBack is up to $4\times$ faster than the prepending
counterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing
unnecessary re-computation, it demonstrates an advancement by achieving
significantly faster inference speed, and this heightened efficiency will
substantially reduce inferential cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Findings, 14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Implicit Bias in Large Language Models by Attacking From a
  Psychometric Perspective <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14023v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14023v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Wen, Keping Bi, Wei Chen, Jiafeng Guo, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become an important way of information
access, there have been increasing concerns that LLMs may intensify the spread
of unethical content, including implicit bias that hurts certain populations
without explicit harmful words. In this paper, we conduct a rigorous evaluation
of LLMs' implicit bias towards certain demographics by attacking them from a
psychometric perspective to elicit agreements to biased viewpoints. Inspired by
psychometric principles in cognitive and social psychology, we propose three
attack approaches, i.e., Disguise, Deception, and Teaching. Incorporating the
corresponding attack instructions, we built two benchmarks: (1) a bilingual
dataset with biased statements covering four bias types (2.7K instances) for
extensive comparative analysis, and (2) BUMBLE, a larger benchmark spanning
nine common bias types (12.7K instances) for comprehensive evaluation.
Extensive evaluation of popular commercial and open-source LLMs shows that our
methods can elicit LLMs' inner bias more effectively than competitive
baselines. Our attack methodology and benchmarks offer an effective means of
assessing the ethical risks of LLMs, driving progress toward greater
accountability in their development. Our code, data, and benchmarks are
available at https://yuchenwen1.github.io/ImplicitBiasEvaluation/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Traj<span class="highlight-title">Agent</span>: An LLM-based <span class="highlight-title">Agent</span> Framework for Automated Trajectory
  Modeling via <span class="highlight-title">Collaborat</span>ion of Large and Small Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20445v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20445v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Du, Jie Feng, Jie Zhao, Jian Yuan, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory modeling, which includes research on trajectory data pattern
mining and future prediction, has widespread applications in areas such as life
services, urban transportation, and public administration. Numerous methods
have been proposed to address specific problems within trajectory modeling.
However, the heterogeneity of data and the diversity of trajectory tasks make
effective and reliable trajectory modeling an important yet highly challenging
endeavor, even for domain experts. In this paper, we propose
\textit{TrajAgent}, a agent framework powered by large language models (LLMs),
designed to facilitate robust and efficient trajectory modeling through
automation modeling. This framework leverages and optimizes diverse specialized
models to address various trajectory modeling tasks across different datasets
effectively. In \textit{TrajAgent}, we first develop \textit{UniEnv}, an
execution environment with a unified data and model interface, to support the
execution and training of various models. Building on \textit{UniEnv}, we
introduce an agentic workflow designed for automatic trajectory modeling across
various trajectory tasks and data. Furthermore, we introduce collaborative
learning schema between LLM-based agents and small speciallized models, to
enhance the performance of the whole framework effectively. Extensive
experiments on four tasks using four real-world datasets demonstrate the
effectiveness of \textit{TrajAgent} in automated trajectory modeling, achieving
a performance improvement of 2.38\%-34.96\% over baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>the code will be openly accessible at:
  https://github.com/tsinghua-fib-lab/TrajAgent</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with
  Physician Validation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Zhang, Yujiong Shen, Zelin Li, Huayu Sha, Binze Hu, Yuhui Wang, Chenhao Huang, Shichun Liu, Jingqi Tong, Changhao Jiang, Mingxu Chai, Zhiheng Xi, Shihan Dou, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating large language models (LLMs) in medicine is crucial because
medical applications require high accuracy with little room for error. Current
medical benchmarks have three main types: medical exam-based, comprehensive
medical, and specialized assessments. However, these benchmarks have
limitations in question design (mostly multiple-choice), data sources (often
not derived from real clinical scenarios), and evaluation methods (poor
assessment of complex reasoning). To address these issues, we present
LLMEval-Med, a new benchmark covering five core medical areas, including 2,996
questions created from real-world electronic health records and expert-designed
clinical scenarios. We also design an automated evaluation pipeline,
incorporating expert-developed checklists into our LLM-as-Judge framework.
Furthermore, our methodology validates machine scoring through human-machine
agreement analysis, dynamically refining checklists and prompts based on expert
feedback to ensure reliability. We evaluate 13 LLMs across three categories
(specialized medical models, open-source models, and closed-source models) on
LLMEval-Med, providing valuable insights for the safe and effective deployment
of LLMs in medical domains. The dataset is released in
https://github.com/llmeval/LLMEval-Med.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PFDial: A Structured <span class="highlight-title">Dialogue</span> Instruction Fine-tuning Method Based on
  UML Flowcharts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06706v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06706v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Zhang, Yuhui Wang, Yujiong Shen, Tingyi Yang, Changhao Jiang, Yilong Wu, Shihan Dou, Qinhao Chen, Zhiheng Xi, Zhihao Zhang, Yi Dong, Zhen Wang, Zhihui Fei, Mingyang Wan, Tao Liang, Guojun Ma, Qi Zhang, Tao Gui, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process-driven dialogue systems, which operate under strict predefined
process constraints, are essential in customer service and equipment
maintenance scenarios. Although Large Language Models (LLMs) have shown
remarkable progress in dialogue and reasoning, they still struggle to solve
these strictly constrained dialogue tasks. To address this challenge, we
construct Process Flow Dialogue (PFDial) dataset, which contains 12,705
high-quality Chinese dialogue instructions derived from 440 flowcharts
containing 5,055 process nodes. Based on PlantUML specification, each UML
flowchart is converted into atomic dialogue units i.e., structured five-tuples.
Experimental results demonstrate that a 7B model trained with merely 800
samples, and a 0.5B model trained on total data both can surpass 90% accuracy.
Additionally, the 8B model can surpass GPT-4o up to 43.88% with an average of
11.00%. We further evaluate models' performance on challenging backward
transitions in process flows and conduct an in-depth analysis of various
dataset formats to reveal their impact on model performance in handling
decision and sequential branches. The data is released in
https://github.com/KongLongGeFDU/PFDial.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TUMLU: A Unified and Native Language Understanding Benchmark for Turkic
  Languages <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11020v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11020v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jafar Isbarov, Arofat Akhundjanova, Mammad Hajili, Kavsar Huseynova, Dmitry Gaynullin, Anar Rzayev, Osman Tursun, Aizirek Turdubaeva, Ilshat Saetov, Rinat Kharisov, Saule Belginova, Ariana Kenbayeva, Amina Alisheva, Abdullatif Köksal, Samir Rustamov, Duygu Ataman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Being able to thoroughly assess massive multi-task language understanding
(MMLU) capabilities is essential for advancing the applicability of
multilingual language models. However, preparing such benchmarks in high
quality native language is often costly and therefore limits the
representativeness of evaluation datasets. While recent efforts focused on
building more inclusive MMLU benchmarks, these are conventionally built using
machine translation from high-resource languages, which may introduce errors
and fail to account for the linguistic and cultural intricacies of the target
languages. In this paper, we address the lack of native language MMLU benchmark
especially in the under-represented Turkic language family with distinct
morphosyntactic and cultural characteristics. We propose two benchmarks for
Turkic language MMLU: TUMLU is a comprehensive, multilingual, and natively
developed language understanding benchmark specifically designed for Turkic
languages. It consists of middle- and high-school level questions spanning 11
academic subjects in Azerbaijani, Crimean Tatar, Karakalpak, Kazakh, Tatar,
Turkish, Uyghur, and Uzbek. We also present TUMLU-mini, a more concise,
balanced, and manually verified subset of the dataset. Using this dataset, we
systematically evaluate a diverse range of open and proprietary multilingual
large language models (LLMs), including Claude, Gemini, GPT, and LLaMA,
offering an in-depth analysis of their performance across different languages,
subjects, and alphabets. To promote further research and development in
multilingual language understanding, we release TUMLU-mini and all
corresponding evaluation scripts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025, Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based
  QA Datasets <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.21015v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.21015v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahir Labib Dihan, Mohammed Eunus Ali, Md Rizwan Parvez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,
are essential for accessing various location-based data, yet they often
struggle to handle natural language geospatial queries. Recent advancements in
Large Language Models (LLMs) show promise in question answering (QA), but
creating reliable geospatial QA datasets from map services remains challenging.
We introduce MapQaTor, an extensible open-source framework that streamlines the
creation of reproducible, traceable map-based QA datasets. MapQaTor enables
seamless integration with any maps API, allowing users to gather and visualize
data from diverse sources with minimal setup. By caching API responses, the
platform ensures consistent ground truth, enhancing the reliability of the data
even as real-world information evolves. MapQaTor centralizes data retrieval,
annotation, and visualization within a single platform, offering a unique
opportunity to evaluate the current state of LLM-based geospatial reasoning
while advancing their capabilities for improved geospatial understanding.
Evaluation metrics show that, MapQaTor speeds up the annotation process by at
least 30 times compared to manual methods, underscoring its potential for
developing geospatial resources, such as complex map reasoning datasets. The
website is live at: https://mapqator.github.io/ and a demo video is available
at: https://youtu.be/bVv7-NYRsTw.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 (Demo)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BitNet v2: Native 4-bit Activations with Hadamard Transformation for
  1-bit LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.18415v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.18415v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Wang, Shuming Ma, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by
activation outliers, which complicate quantization to low bit-widths. We
introduce BitNet v2, a novel framework enabling native 4-bit activation
quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward
network activations, we propose H-BitLinear, a module applying an online
Hadamard transformation prior to activation quantization. This transformation
smooths sharp activation distributions into more Gaussian-like forms, suitable
for low-bit representation. Experiments show BitNet v2 trained from scratch
with 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2
achieves minimal performance degradation when trained with native 4-bit
activations, significantly reducing memory footprint and computational cost for
batched inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Does Thinking More always Help? Understanding Test-Time Scaling in
  Reasoning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04210v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04210v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumya Suvra Ghosal, Souradip Chakraborty, Avinash Reddy, Yifu Lu, Mengdi Wang, Dinesh Manocha, Furong Huang, Mohammad Ghavamzadeh, Amrit Singh Bedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,
DeepSeek R1) have led to a popular belief that extending thinking traces using
prompts like "Wait" or "Let me rethink" can improve performance. This raises a
natural question: Does thinking more at test-time truly lead to better
reasoning? To answer this question, we perform a detailed empirical study
across models and benchmarks, which reveals a consistent pattern of initial
performance improvements from additional thinking followed by a decline, due to
"overthinking". To understand this non-monotonic trend, we consider a simple
probabilistic model, which reveals that additional thinking increases output
variance-creating an illusion of improved reasoning while ultimately
undermining precision. Thus, observed gains from "more thinking" are not true
indicators of improved reasoning, but artifacts stemming from the connection
between model uncertainty and evaluation metric. This suggests that test-time
scaling through extended thinking is not an effective way to utilize the
inference thinking budget. Recognizing these limitations, we introduce an
alternative test-time scaling approach, parallel thinking, inspired by
Best-of-N sampling. Our method generates multiple independent reasoning paths
within the same inference budget and selects the most consistent response via
majority vote, achieving up to 20% higher accuracy compared to extended
thinking. This provides a simple yet effective mechanism for test-time scaling
of reasoning models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conformal Linguistic Calibration: Trading-off between Factuality and
  Specificity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19110v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19110v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengping Jiang, Anqi Liu, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model outputs are not always reliable, thus prompting research into
how to adapt model responses based on uncertainty. Common approaches include:
\emph{abstention}, where models refrain from generating responses when
uncertain; and \emph{linguistic calibration}, where models hedge their
statements using uncertainty quantifiers. However, abstention can withhold
valuable information, while linguistically calibrated responses are often
challenging to leverage in downstream tasks. We propose a unified view,
Conformal Linguistic Calibration (CLC), which reinterprets linguistic
calibration as \emph{answer set prediction}. First we present a framework
connecting abstention and linguistic calibration through the lens of linguistic
pragmatics. We then describe an implementation of CLC that allows for
controlling the level of imprecision in model responses. Results demonstrate
our method produces calibrated outputs with conformal guarantees on factual
accuracy. Further, our approach enables fine-tuning models to perform
uncertainty-aware adaptive claim rewriting, offering a controllable balance
between factuality and specificity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for
  Text-to-Image Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10963v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10963v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Luo, Yuhui Yuan, Junwen Chen, Haonan Cai, Ziyi Yue, Yuwei Yang, Fatima Zohra Daha, Ji Li, Zhouhui Lian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce knowledge image generation as a new task,
alongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation
Benchmark (MMMG) to probe the reasoning capability of image generation models.
Knowledge images have been central to human civilization and to the mechanisms
of human learning -- a fact underscored by dual-coding theory and the
picture-superiority effect. Generating such images is challenging, demanding
multimodal reasoning that fuses world knowledge with pixel-level grounding into
clear explanatory visuals. To enable comprehensive evaluation, MMMG offers
4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines,
6 educational levels, and diverse knowledge formats such as charts, diagrams,
and mind maps. To eliminate confounding complexity during evaluation, we adopt
a unified Knowledge Graph (KG) representation. Each KG explicitly delineates a
target image's core entities and their dependencies. We further introduce
MMMG-Score to evaluate generated knowledge images. This metric combines factual
fidelity, measured by graph-edit distance between KGs, with visual clarity
assessment. Comprehensive evaluations of 16 state-of-the-art text-to-image
generation models expose serious reasoning deficits -- low entity fidelity,
weak relations, and clutter -- with GPT-4o achieving an MMMG-Score of only
50.20, underscoring the benchmark's difficulty. To spur further progress, we
release FLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that
combines a reasoning LLM with diffusion models and is trained on 16,000 curated
knowledge image-prompt pairs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>85 pages, 70 figures, code: https://github.com/MMMGBench/MMMG,
  project page: https://mmmgbench.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jointly modelling the evolution of social structure and language in
  online communities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19243v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19243v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christine de Kock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group interactions take place within a particular socio-temporal context,
which should be taken into account when modelling interactions in online
communities. We propose a method for jointly modelling community structure and
language over time. Our system produces dynamic word and user representations
that can be used to cluster users, investigate thematic interests of groups,
and predict group membership. We apply and evaluate our method in the context
of a set of misogynistic extremist groups. Our results indicate that this
approach outperforms prior models which lacked one of these components (i.e.
not incorporating social structure, or using static word embeddings) when
evaluated on clustering and embedding prediction tasks. Our method further
enables novel types of analyses on online groups, including tracing their
response to temporal events and quantifying their propensity for using violent
language, which is of particular importance in the context of extremist groups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lingshu: A Generalist Foundation Model for Unified Multimodal Medical
  Understanding and Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07044v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07044v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         LASA Team, Weiwen Xu, Hou Pong Chan, Long Li, Mahani Aljunied, Ruifeng Yuan, Jianyu Wang, Chenghao Xiao, Guizhen Chen, Chaoqun Liu, Zhaodonghui Li, Yu Sun, Junao Shen, Chaojun Wang, Jie Tan, Deli Zhao, Tingyang Xu, Hao Zhang, Yu Rong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities in understanding common visual elements, largely due to their
large-scale datasets and advanced training strategies. However, their
effectiveness in medical applications remains limited due to the inherent
discrepancies between data and tasks in medical scenarios and those in the
general domain. Concretely, existing medical MLLMs face the following critical
limitations: (1) limited coverage of medical knowledge beyond imaging, (2)
heightened susceptibility to hallucinations due to suboptimal data curation
processes, (3) lack of reasoning capabilities tailored for complex medical
scenarios. To address these challenges, we first propose a comprehensive data
curation procedure that (1) efficiently acquires rich medical knowledge data
not only from medical imaging but also from extensive medical texts and
general-domain data; and (2) synthesizes accurate medical captions, visual
question answering (VQA), and reasoning samples. As a result, we build a
multimodal dataset enriched with extensive medical knowledge. Building on the
curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu
undergoes multi-stage training to embed medical expertise and enhance its
task-solving capabilities progressively. Besides, we preliminarily explore the
potential of applying reinforcement learning with verifiable rewards paradigm
to enhance Lingshu's medical reasoning ability. Additionally, we develop
MedEvalKit, a unified evaluation framework that consolidates leading multimodal
and textual medical benchmarks for standardized, fair, and efficient model
assessment. We evaluate the performance of Lingshu on three fundamental medical
tasks, multimodal QA, text-based QA, and medical report generation. The results
show that Lingshu consistently outperforms the existing open-source multimodal
models on most tasks ...
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report, 53 pages, 25 tables, and 16 figures. Our webpage is
  https://alibaba-damo-academy.github.io/lingshu/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Efficient Speech-Text Jointly Decoding within One Speech
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haibin Wu, Yuxuan Hu, Ruchao Fan, Xiaofei Wang, Kenichi Kumatani, Bo Ren, Jianwei Yu, Heng Lu, Lijuan Wang, Yao Qian, Jinyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech language models (Speech LMs) enable end-to-end speech-text modelling
within a single model, offering a promising direction for spoken dialogue
systems. The choice of speech-text jointly decoding paradigm plays a critical
role in performance, efficiency, and alignment quality. In this work, we
systematically compare representative joint speech-text decoding
strategies-including the interleaved, and parallel generation paradigms-under a
controlled experimental setup using the same base language model, speech
tokenizer and training data. Our results show that the interleaved approach
achieves the best alignment. However it suffers from slow inference due to long
token sequence length. To address this, we propose a novel early-stop
interleaved (ESI) pattern that not only significantly accelerates decoding but
also yields slightly better performance. Additionally, we curate high-quality
question answering (QA) datasets to further improve speech QA performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our company need to do internal review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transferable Post-training via Inverse Value Learning <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Lu, Xueru Wen, Yaojie Lu, Bowen Yu, Hongyu Lin, Haiyang Yu, Le Sun, Xianpei Han, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As post-training processes utilize increasingly large datasets and base
models continue to grow in size, the computational demands and implementation
challenges of existing algorithms are escalating significantly. In this paper,
we propose modeling the changes at the logits level during post-training using
a separate neural network (i.e., the value network). After training this
network on a small base model using demonstrations, this network can be
seamlessly integrated with other pre-trained models during inference, enables
them to achieve similar capability enhancements. We systematically investigate
the best practices for this paradigm in terms of pre-training weights and
connection schemes. We demonstrate that the resulting value network has broad
transferability across pre-trained models of different parameter sizes within
the same family, models undergoing continuous pre-training within the same
family, and models with different vocabularies across families. In certain
cases, it can achieve performance comparable to full-parameter fine-tuning.
Furthermore, we explore methods to enhance the transferability of the value
model and prevent overfitting to the base model used during training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving the Calibration of Confidence Scores in Text Generation Using
  the Output Distribution's Characteristics <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00637v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00637v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Jaime Yu Flores, Ori Ernst, Jackie Chi Kit Cheung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Well-calibrated model confidence scores can improve the usefulness of text
generation models. For example, users can be prompted to review predictions
with low confidence scores, to prevent models from returning bad or potentially
dangerous predictions. However, confidence metrics are not always well
calibrated in text generation. One reason is that in generation, there can be
many valid answers, which previous methods do not always account for. Hence, a
confident model could distribute its output probability among multiple
sequences because they are all valid. We propose task-agnostic confidence
metrics suited to generation, which rely solely on the probabilities associated
with the model outputs without the need for further fine-tuning or heuristics.
Using these, we are able to improve the calibration of BART and Flan-T5 on
summarization, translation, and QA datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAGPIE: Multi-Task Media-Bias Analysis Generalization for Pre-Trained
  Identification of Expressions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07910v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07910v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomáš Horych, Martin Wessel, Jan Philip Wahle, Terry Ruas, Jerome Waßmuth, André Greiner-Petter, Akiko Aizawa, Bela Gipp, Timo Spinde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Media bias detection poses a complex, multifaceted problem traditionally
tackled using single-task models and small in-domain datasets, consequently
lacking generalizability. To address this, we introduce MAGPIE, the first
large-scale multi-task pre-training approach explicitly tailored for media bias
detection. To enable pre-training at scale, we present Large Bias Mixture
(LBM), a compilation of 59 bias-related tasks. MAGPIE outperforms previous
approaches in media bias detection on the Bias Annotation By Experts (BABE)
dataset, with a relative improvement of 3.3% F1-score. MAGPIE also performs
better than previous models on 5 out of 8 tasks in the Media Bias
Identification Benchmark (MBIB). Using a RoBERTa encoder, MAGPIE needs only 15%
of finetuning steps compared to single-task approaches. Our evaluation shows,
for instance, that tasks like sentiment and emotionality boost all learning,
all tasks enhance fake news detection, and scaling tasks leads to the best
results. MAGPIE confirms that MTL is a promising approach for addressing media
bias detection, enhancing the accuracy and efficiency of existing models.
Furthermore, LBM is the first available resource collection focused on media
bias MTL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Sparse Latent Feature Models for Knowledge Graph Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15694v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15694v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Li, Rui Zhang, Lingzhi Wang, Bin Yu, Youwei Wang, Yuliang Wei, Kai Wang, Richard Yi Da Xu, Bailing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in knowledge graph completion (KGC) have emphasized
text-based approaches to navigate the inherent complexities of large-scale
knowledge graphs (KGs). While these methods have achieved notable progress,
they frequently struggle to fully incorporate the global structural properties
of the graph. Stochastic blockmodels (SBMs), especially the latent feature
relational model (LFRM), offer robust probabilistic frameworks for identifying
latent community structures and improving link prediction. This paper presents
a novel probabilistic KGC framework utilizing sparse latent feature models,
optimized via a deep variational autoencoder (VAE). Our proposed method
dynamically integrates global clustering information with local textual
features to effectively complete missing triples, while also providing enhanced
interpretability of the underlying latent structures. Extensive experiments on
four benchmark datasets with varying scales demonstrate the significant
performance gains achieved by our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RSCF: Relation-Semantics Consistent Filter for Entity Embedding of
  Knowledge Graph <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20813v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20813v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junsik Kim, Jinwook Park, Kangil Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In knowledge graph embedding, leveraging relation specific entity
transformation has markedly enhanced performance. However, the consistency of
embedding differences before and after transformation remains unaddressed,
risking the loss of valuable inductive bias inherent in the embeddings. This
inconsistency stems from two problems. First, transformation representations
are specified for relations in a disconnected manner, allowing dissimilar
transformations and corresponding entity embeddings for similar relations.
Second, a generalized plug-in approach as a SFBR (Semantic Filter Based on
Relations) disrupts this consistency through excessive concentration of entity
embeddings under entity-based regularization, generating indistinguishable
score distributions among relations. In this paper, we introduce a plug-in KGE
method, Relation-Semantics Consistent Filter (RSCF). Its entity transformation
has three features for enhancing semantic consistency: 1) shared affine
transformation of relation embeddings across all relations, 2) rooted entity
transformation that adds an entity embedding to its change represented by the
transformed vector, and 3) normalization of the change to prevent scale
reduction. To amplify the advantages of consistency that preserve semantics on
embeddings, RSCF adds relation transformation and prediction modules for
enhancing the semantics. In knowledge graph completion tasks with
distance-based and tensor decomposition models, RSCF significantly outperforms
state-of-the-art KGE methods, showing robustness across all relations and their
frequencies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025, 17 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scientists' First Exam: Probing Cognitive Abilities of MLLM via
  Perception, Understanding, and Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Zhou, Yiheng Wang, Xuming He, Ruoyao Xiao, Zhiwei Li, Qiantai Feng, Zijie Guo, Yuejin Yang, Hao Wu, Wenxuan Huang, Jiaqi Wei, Dan Si, Xiuqi Yao, Jia Bu, Haiwen Huang, Tianfan Fu, Shixiang Tang, Ben Fei, Dongzhan Zhou, Fenghua Ling, Yan Lu, Siqi Sun, Chenhui Li, Guanjie Zheng, Jiancheng Lv, Wenlong Zhang, Lei Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific discoveries increasingly rely on complex multimodal reasoning
based on information-intensive scientific data and domain-specific expertise.
Empowered by expert-level scientific benchmarks, scientific Multimodal Large
Language Models (MLLMs) hold the potential to significantly enhance this
discovery process in realistic workflows. However, current scientific
benchmarks mostly focus on evaluating the knowledge understanding capabilities
of MLLMs, leading to an inadequate assessment of their perception and reasoning
abilities. To address this gap, we present the Scientists' First Exam (SFE)
benchmark, designed to evaluate the scientific cognitive capacities of MLLMs
through three interconnected levels: scientific signal perception, scientific
attribute understanding, scientific comparative reasoning. Specifically, SFE
comprises 830 expert-verified VQA pairs across three question types, spanning
66 multimodal tasks across five high-value disciplines. Extensive experiments
reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08%
and 26.52% on SFE, highlighting significant room for MLLMs to improve in
scientific realms. We hope the insights obtained in SFE will facilitate further
developments in AI-enhanced scientific discoveries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>82 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Diffusion Large Language Models with SlowFast Sampling: The
  Three Golden Principles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyan Wei, Yaojie Zhang, Zhiyuan Liu, Dongrui Liu, Linfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based language models (dLLMs) have emerged as a promising
alternative to traditional autoregressive LLMs by enabling parallel token
generation and significantly reducing inference latency. However, existing
sampling strategies for dLLMs, such as confidence-based or semi-autoregressive
decoding, often suffer from static behavior, leading to suboptimal efficiency
and limited flexibility. In this paper, we propose SlowFast Sampling, a novel
dynamic sampling strategy that adaptively alternates between exploratory and
accelerated decoding stages. Our method is guided by three golden principles:
certainty principle, convergence principle, and positional principle, which
govern when and where tokens can be confidently and efficiently decoded. We
further integrate our strategy with dLLM-Cache to reduce redundant computation.
Extensive experiments across benchmarks and models show that SlowFast Sampling
achieves up to 15.63$\times$ speedup on LLaDA with minimal accuracy drop, and
up to 34.22$\times$ when combined with caching. Notably, our approach
outperforms strong autoregressive baselines like LLaMA3 8B in throughput,
demonstrating that well-designed sampling can unlock the full potential of
dLLMs for fast and high-quality generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages; 5 figures;</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on
  Technical Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.13128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.13128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nandan Thakur, Jimmy Lin, Sam Havens, Michael Carbin, Omar Khattab, Andrew Drozdov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce FreshStack, a holistic framework for automatically building
information retrieval (IR) evaluation benchmarks by incorporating challenging
questions and answers. FreshStack conducts the following steps: (1) automatic
corpus collection from code and technical documentation, (2) nugget generation
from community-asked questions and answers, and (3) nugget-level support,
retrieving documents using a fusion of retrieval techniques and hybrid
architectures. We use FreshStack to build five datasets on fast-growing,
recent, and niche topics to ensure the tasks are sufficiently challenging. On
FreshStack, existing retrieval models, when applied out-of-the-box,
significantly underperform oracle approaches on all five topics, denoting
plenty of headroom to improve IR quality. In addition, we identify cases where
rerankers do not improve first-stage retrieval accuracy (two out of five
topics) and oracle context helps an LLM generator generate a high-quality RAG
answer. We hope FreshStack will facilitate future work toward constructing
realistic, scalable, and uncontaminated IR and RAG evaluation benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 4 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-06-21T05:30:44.569783353Z">
            2025-06-21 05:30:44 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
